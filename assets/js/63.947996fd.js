(window.webpackJsonp=window.webpackJsonp||[]).push([[63],{397:function(t,s,a){"use strict";a.r(s);var n=a(4),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"elasticsearch-分析器"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#elasticsearch-分析器"}},[t._v("#")]),t._v(" Elasticsearch 分析器")]),t._v(" "),s("p",[t._v("文本分析是把全文本转换为一系列单词（term/token）的过程，也叫分词。在 Elasticsearch 中，分词是通过 analyzer（分析器）来完成的，不管是索引还是搜索，都需要使用 analyzer（分析器）。分析器，分为"),s("strong",[t._v("内置分析器")]),t._v("和"),s("strong",[t._v("自定义的分析器")]),t._v("。")]),t._v(" "),s("p",[t._v("分析器可以进一步细分为"),s("strong",[t._v("字符过滤器")]),t._v("（"),s("strong",[t._v("Character Filters")]),t._v("）、"),s("strong",[t._v("分词器")]),t._v("（"),s("strong",[t._v("Tokenizer")]),t._v("）和"),s("strong",[t._v("词元过滤器")]),t._v("（"),s("strong",[t._v("Token Filters")]),t._v("）三部分。它的执行顺序如下：")]),t._v(" "),s("p",[s("strong",[s("em",[t._v("character filters")])]),t._v(" -> "),s("strong",[s("em",[t._v("tokenizer")])]),t._v(" -> "),s("strong",[s("em",[t._v("token filters")])])]),t._v(" "),s("h2",{attrs:{id:"字符过滤器-character-filters"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#字符过滤器-character-filters"}},[t._v("#")]),t._v(" 字符过滤器（Character Filters）")]),t._v(" "),s("p",[t._v("character filter 的输入是原始的文本 text，如果配置了多个，它会按照配置的顺序执行，目前 ES 自带的 character filter 主要有如下 3 类：")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("html strip character filter")]),t._v("：从文本中剥离 HTML 元素，并用其解码值替换 HTML 实体（如，将 "),s("strong",[s("em",[s("code",[t._v("＆amp;")])])]),t._v(" 替换为 "),s("strong",[s("em",[s("code",[t._v("＆")])])]),t._v("）。")]),t._v(" "),s("li",[s("strong",[t._v("mapping character filter")]),t._v("：自定义一个 map 映射，可以进行一些自定义的替换，如常用的大写变小写也可以在该环节设置。")]),t._v(" "),s("li",[s("strong",[t._v("pattern replace character filter")]),t._v("：使用 java 正则表达式来匹配应替换为指定替换字符串的字符，此外，替换字符串可以引用正则表达式中的捕获组。")])]),t._v(" "),s("h3",{attrs:{id:"html-strip-character-filter"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#html-strip-character-filter"}},[t._v("#")]),t._v(" HTML strip character filter")]),t._v(" "),s("p",[t._v("HTML strip 如下示例：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[t._v("GET /_analyze\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"keyword"')]),t._v(",\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char_filter"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"html_strip"')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(",\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"<p>I&apos;m so <b>happy</b>!</p>"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("经过 "),s("strong",[s("em",[s("code",[t._v("html_strip")])])]),t._v(" 字符过滤器处理后，输出如下：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("[ \\nI'm so happy!\\n ]\n")])])]),s("h3",{attrs:{id:"mapping-character-filter"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#mapping-character-filter"}},[t._v("#")]),t._v(" Mapping character filter")]),t._v(" "),s("p",[t._v("Mapping character filter 接收键和值映射（key => value）作为配置参数，每当在预处理过程中遇到与键值映射中的键相同的字符串时，就会使用该键对应的值去替换它。")]),t._v(" "),s("p",[t._v("原始文本中的字符串和键值映射中的键的匹配是贪心的，在对给定的文本进行预处理过程中如果配置的键值映射存在包含关系，会优先"),s("strong",[t._v("匹配最长键")]),t._v("。同样也可以用空字符串进行替换。")]),t._v(" "),s("p",[t._v("mapping char_filter 不像 html_strip 那样拆箱即可用，必须先进行配置才能使用，它有两个属性可以配置：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("参数名称")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("参数说明")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[s("em",[s("code",[t._v("mappings")])])])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("一组映射，每个元素的格式为 "),s("em",[t._v("key => value")]),t._v("。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[s("em",[s("code",[t._v("mappings_path")])])])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("一个相对或者绝对的文件路径，指向一个每行包含一个 "),s("em",[t._v("key =>value")]),t._v(" 映射的 UTF-8 编码文本映射文件。")])])])]),t._v(" "),s("p",[t._v("mapping char_filter 示例如下：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[t._v("GET /_analyze\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"keyword"')]),t._v(",\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char_filter"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mapping"')]),t._v(",\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mappings"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٠ => 0"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"١ => 1"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٢ => 2"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٣ => 3"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٤ => 4"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٥ => 5"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٦ => 6"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٧ => 7"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٨ => 8"')]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"٩ => 9"')]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(",\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"My license plate is ٢٥٠١٥"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("分析结果如下：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("[ My license plate is 25015 ]\n")])])]),s("h3",{attrs:{id:"pattern-replace-character-filter"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pattern-replace-character-filter"}},[t._v("#")]),t._v(" Pattern Replace character filter")]),t._v(" "),s("p",[t._v("Pattern Replace character filter 支持如下三个参数：")]),t._v(" "),s("table",[s("thead",[s("tr",[s("th",{staticStyle:{"text-align":"left"}},[t._v("参数名称")]),t._v(" "),s("th",{staticStyle:{"text-align":"left"}},[t._v("参数说明")])])]),t._v(" "),s("tbody",[s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[s("em",[s("code",[t._v("pattern")])])])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("必填参数，一个 java 的正则表达式。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[s("em",[s("code",[t._v("replacement")])])])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("替换字符串，可以使用 "),s("strong",[s("em",[s("code",[t._v("$1 ... $9")])])]),t._v(" 语法来引用捕获组。")])]),t._v(" "),s("tr",[s("td",{staticStyle:{"text-align":"left"}},[s("strong",[s("em",[s("code",[t._v("flags")])])])]),t._v(" "),s("td",{staticStyle:{"text-align":"left"}},[t._v("Java 正则表达式的标志，具体参考 java 的 java.util.regex.Pattern 类的标志属性。")])])])]),t._v(" "),s("p",[t._v("如将输入的 text 中大于一个的空格都转变为一个空格，在 settings 时，配置示例如下：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char_filter"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"multi_space_2_one"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pattern"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"[ ]+"')]),t._v(",\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pattern_replace"')]),t._v(",\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"replacement"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("..")]),t._v(".\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h2",{attrs:{id:"分词器-tokenizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#分词器-tokenizer"}},[t._v("#")]),t._v(" 分词器（Tokenizer）")]),t._v(" "),s("p",[t._v("tokenizer 即分词器，也是 analyzer 最重要的组件，它对文本进行分词；"),s("strong",[t._v("一个 analyzer 必需且只可包含一个 tokenizer")]),t._v("。")]),t._v(" "),s("p",[t._v("ES 自带默认的分词器是 standard tokenizer，标准分词器提供基于语法的分词（基于 Unicode 文本分割算法），并且适用于大多数语言。")]),t._v(" "),s("p",[t._v("此外有很多第三方的分词插件，如中文分词界最经典的 ik 分词器，它对应的 tokenizer 分为 ik_smart 和 ik_max_word，一个是智能分词（针对搜索侧），一个是全切分词（针对索引侧）。")]),t._v(" "),s("p",[t._v("ES 默认提供的分词器 standard 对中文分词不优化，效果差，一般会安装第三方中文分词插件，通常首先 "),s("a",{attrs:{href:"https://github.com/medcl/elasticsearch-analysis-ik",target:"_blank",rel:"noopener noreferrer"}},[t._v("elasticsearch-analysis-ik"),s("OutboundLink")],1),t._v(" 插件，它其实是 ik 针对的 ES 的定制版。")]),t._v(" "),s("h3",{attrs:{id:"elasticsearch-plugin-使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#elasticsearch-plugin-使用"}},[t._v("#")]),t._v(" elasticsearch-plugin 使用")]),t._v(" "),s("p",[t._v("在安装 elasticsearch-analysis-ik 第三方之前，我们首先要了解 es 的插件管理工具 "),s("strong",[s("em",[s("code",[t._v("elasticsearch-plugin")])])]),t._v(" 的使用。")]),t._v(" "),s("p",[t._v("现在的 elasticsearch 安装完后，在安装目录的 bin 目录下会存在 elasticsearch-plugin 命令工具，用它来对 es 插件进行管理。")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("bin/elasticsearch-plugin\n")])])]),s("p",[t._v("其实该命令的是软连接，原始路径是：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("libexec/bin/elasticsearch-plugin\n")])])]),s("p",[t._v("再进一步看脚本代码，你会发现，它是通过 "),s("strong",[s("em",[s("code",[t._v("elasticsearch-cli")])])]),t._v(" 执行 "),s("code",[t._v("libexec/lib/tools/plugin-cli/elasticsearch-plugin-cli-x.x.x.jar")]),t._v("。")]),t._v(" "),s("p",[t._v("但一般使用者了解 elasticsearch-plugin 命令使用就可：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  安装指定的插件到当前 ES 节点中")]),t._v("\nelasticsearch-plugin "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("plugin_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  显示当前 ES 节点已经安装的插件列表")]),t._v("\nelasticsearch-plugin list\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  删除已安装的插件")]),t._v("\nelasticsearch-plugin remove "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("plugin_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("在安装插件时，要保证安装的插件与 ES 版本一致。")])]),t._v(" "),s("h3",{attrs:{id:"elasticsearch-analysis-ik-安装"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#elasticsearch-analysis-ik-安装"}},[t._v("#")]),t._v(" elasticsearch-analysis-ik 安装")]),t._v(" "),s("p",[t._v("在确定要安装的 ik 版本之后，执行如下命令：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[t._v("./bin/elasticsearch-plugin "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("install")]),t._v(" https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("X.X.X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("/elasticsearch-analysis-ik-"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("X.X.X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(".zip\n")])])]),s("p",[t._v("执行完安装命令后，我们会发现在 plugins 中多了 analysis-ik 目录，这里面主要存放的是源码 jar 包，此外，在 config 文件里也多了 analysis-ik 目录，里面主要是 ik 相关的配置，如 IKAnalyzer.cfg.xml 配置、词典文件等。")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  两个新增目录路径")]),t._v("\nlibexec/plugins/analysis-ik/\nlibexec/config/analysis-ik/\n")])])]),s("h3",{attrs:{id:"elasticsearch-analysis-ik-使用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#elasticsearch-analysis-ik-使用"}},[t._v("#")]),t._v(" elasticsearch-analysis-ik 使用")]),t._v(" "),s("p",[t._v("ES 5.X 版本开始安装完的 elasticsearch-analysis-ik 提供了两个分词器，分别对应名称是 "),s("strong",[s("em",[t._v("ik_max_word")])]),t._v(" 和 "),s("strong",[s("em",[t._v("ik_smart")])]),t._v("，ik_max_word 是索引侧的分词器，走全切模式，ik_smart 是搜索侧的分词器，走智能分词，属于搜索模式。")]),t._v(" "),s("h4",{attrs:{id:"索引-mapping-设置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#索引-mapping-设置"}},[t._v("#")]),t._v(" 索引 mapping 设置")]),t._v(" "),s("p",[t._v("安装完 elasticsearch-analysis-ik 后，我们可以指定索引及指定字段设置可用的分析器（analyzer），示例如下：")]),t._v(" "),s("div",{staticClass:"language-json extra-class"},[s("pre",{pre:!0,attrs:{class:"language-json"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"qa"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"mappings"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"qa"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"_all"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"enabled"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"properties"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"question"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"store"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"similarity"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"BM25"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"analyzer"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ik_max_word"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"search_analyzer"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ik_smart"')]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"answer"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"store"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"similarity"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"BM25"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"analyzer"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ik_max_word"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"search_analyzer"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ik_smart"')]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          ...\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("如上示例中，analyzer 指定 ik_max_word，即索引侧使用 ik 全切模式，search_analyzer 设置 ik_smart，即搜索侧使用 ik 智能分词模式。")]),t._v(" "),s("h4",{attrs:{id:"查看-ik-分词结果"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#查看-ik-分词结果"}},[t._v("#")]),t._v(" 查看 ik 分词结果")]),t._v(" "),s("p",[t._v("es 提供了查看分词结果的 api "),s("strong",[s("code",[t._v("analyze")])]),t._v("，具体示例如下：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[t._v("GET "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("/_analyze\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"analyzer"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ik_smart"')]),t._v(",\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es 中文分词器安装"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("输出如下：")]),t._v(" "),s("div",{staticClass:"language-json extra-class"},[s("pre",{pre:!0,attrs:{class:"language-json"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"tokens"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"token"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"es"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"start_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"end_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CN_WORD"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"position"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"token"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"中文"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"start_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"end_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CN_WORD"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"position"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"token"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"分词器"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"start_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"end_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CN_WORD"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"position"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"token"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"安装"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"start_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"end_offset"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CN_WORD"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token property"}},[t._v('"position"')]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h4",{attrs:{id:"elasticsearch-analysis-ik-自定义词典"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#elasticsearch-analysis-ik-自定义词典"}},[t._v("#")]),t._v(" elasticsearch-analysis-ik 自定义词典")]),t._v(" "),s("p",[t._v("elasticsearch-analysis-ik 本质是 ik 分词器，使用者根据实际需求可以扩展自定义的词典，具体主要分为如下 2 大类，每类又分为本地配置和远程配置 2 种：")]),t._v(" "),s("ol",[s("li",[t._v("自定义扩展词典；")]),t._v(" "),s("li",[t._v("自定义扩展停用词典；")])]),t._v(" "),s("p",[t._v("elasticsearch-analysis-ik 配置文件为 "),s("code",[t._v("IKAnalyzer.cfg.xml")]),t._v("，它位于 "),s("code",[t._v("libexec/config/analysis-ik")]),t._v(" 目录下，具体配置结构如下：")]),t._v(" "),s("div",{staticClass:"language-xml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-xml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token prolog"}},[t._v('<?xml version="1.0" encoding="UTF-8"?>')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token doctype"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<!")]),s("span",{pre:!0,attrs:{class:"token doctype-tag"}},[t._v("DOCTYPE")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token name"}},[t._v("properties")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token name"}},[t._v("SYSTEM")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"http://java.sun.com/dtd/properties.dtd"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("properties")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("comment")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("IK Analyzer 扩展配置"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("comment")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!--用户可以在这里配置自己的扩展字典 --\x3e")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("entry")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("key")]),s("span",{pre:!0,attrs:{class:"token attr-value"}},[s("span",{pre:!0,attrs:{class:"token punctuation attr-equals"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v('"')]),t._v("ext_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("entry")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!--用户可以在这里配置自己的扩展停止词字典--\x3e")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),t._v("entry")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token attr-name"}},[t._v("key")]),s("span",{pre:!0,attrs:{class:"token attr-value"}},[s("span",{pre:!0,attrs:{class:"token punctuation attr-equals"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v('"')]),t._v("ext_stopwords"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v('"')])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("entry")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!--用户可以在这里配置远程扩展字典 --\x3e")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('\x3c!-- <entry key="remote_ext_dict">words_location</entry> --\x3e')]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("\x3c!--用户可以在这里配置远程扩展停止词字典--\x3e")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('\x3c!-- <entry key="remote_ext_stopwords">words_location</entry> --\x3e')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token tag"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("</")]),t._v("properties")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v("\n")])])]),s("blockquote",[s("p",[t._v("当然，如果开发者认为 ik 默认的词表有问题，也可以进行调整，文件都在 "),s("code",[t._v("libexec/config/analysis-ik")]),t._v(" 下，如 main.dic 为主词典，stopword.dic 为停用词表。")])]),t._v(" "),s("h2",{attrs:{id:"词元过滤器-token-filters"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#词元过滤器-token-filters"}},[t._v("#")]),t._v(" 词元过滤器（Token Filters）")]),t._v(" "),s("p",[t._v("token filters 叫词元过滤器，或词项过滤器，对 tokenizer 分出的词进行过滤处理。常用的有转小写、停用词处理、同义词处理等等。"),s("strong",[t._v("一个 analyzer 可包含 0 个或多个词项过滤器，按配置顺序进行过滤")]),t._v("。")]),t._v(" "),s("p",[t._v("以同义词过滤器的使用示例，具体如下：")]),t._v(" "),s("div",{staticClass:"language-bash extra-class"},[s("pre",{pre:!0,attrs:{class:"language-bash"}},[s("code",[t._v("PUT /test_index\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"settings"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"index"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"analysis"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"analyzer"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"synonym"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"standard"')]),t._v(",\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"filter"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_stop"')]),t._v(", "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"synonym"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"filter"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_stop"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"stop"')]),t._v(",\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"stopwords"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bar"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(",\n          "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"synonym"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"synonym"')]),t._v(",\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lenient"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" true,\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"synonyms"')]),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"foo, bar => baz"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n          "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("h3",{attrs:{id:"同义词"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#同义词"}},[t._v("#")]),t._v(" 同义词")]),t._v(" "),s("p",[t._v("Elasticsearch 同义词通过专有的同义词过滤器（synonym token filter）来进行工作，它允许在分析（analysis）过程中方便地处理同义词，一般是通过配置文件配置同义词。此外，同义词可以再建索引时（index-time synonyms）或者检索时（search-time synonyms）使用。")]),t._v(" "),s("h4",{attrs:{id:"同义词-synonym-配置语法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#同义词-synonym-配置语法"}},[t._v("#")]),t._v(" 同义词（synonym）配置语法")]),t._v(" "),s("p",[t._v("如上例子所示，es 同义词配置的 filter 语法具体如下选项：")]),t._v(" "),s("ul",[s("li",[s("p",[s("strong",[s("em",[s("code",[t._v("type")])])]),t._v("：指定 synonym，表示同义词 filter；")])]),t._v(" "),s("li",[s("p",[s("strong",[s("em",[s("code",[t._v("synonyms_path")])])]),t._v("：指定同义词配置文件路径；")])]),t._v(" "),s("li",[s("p",[s("strong",[s("code",[t._v("expand")])]),t._v("：该参数决定映射行为的模式，默认为 true，表示扩展模式，具体示例如下：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("当 "),s("strong",[s("code",[t._v("expand == true")])]),t._v(" 时，")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("ipod, i-pod, i pod\n")])])]),s("p",[t._v("等价于：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("ipod, i-pod, i pod => ipod, i-pod, i pod\n")])])]),s("p",[t._v("当 "),s("strong",[s("em",[s("code",[t._v("expand == false")])])]),t._v(" 时，")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("ipod, i-pod, i pod\n")])])]),s("p",[t._v("仅映射第一个单词，等价于：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("ipod, i-pod, i pod => ipod\n")])])])])])]),t._v(" "),s("li",[s("p",[s("strong",[s("em",[s("code",[t._v("lenient")])])]),t._v("：如果值为 true 时，遇到那些无法解析的同义词规则时，忽略异常。默认为 false。")])])]),t._v(" "),s("h4",{attrs:{id:"同义词文档格式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#同义词文档格式"}},[t._v("#")]),t._v(" 同义词文档格式")]),t._v(" "),s("p",[t._v("elasticsearch 的同义词有如下两种形式：")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("单向同义词：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("ipod, i-pod, i pod => ipod\n")])])])]),t._v(" "),s("li",[s("p",[t._v("双向同义词：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("马铃薯, 土豆, potato\n")])])])])]),t._v(" "),s("p",[t._v("单向同义词不管索引还是检索时，箭头左侧的词都会映射成箭头右侧的词；")]),t._v(" "),s("p",[t._v("双向同义词是索引时，都建立同义词的倒排索引，检索时，同义词之间都会进行倒排索引的匹配。")]),t._v(" "),s("blockquote",[s("p",[t._v("同义词的文档化时，需要注意的是，同一个词在不同的同义词关系中出现时，其它同义词之间不具有传递性，这点需要注意。")])]),t._v(" "),s("p",[t._v("假设如上示例中，如果“马铃薯”和其它两个同义词分成两行写：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("马铃薯,土豆\n马铃薯,potato\n")])])]),s("p",[t._v("此时，elasticsearch 中不会将“土豆”和“potato”视为同义词关系，所以多个同义词要写在一起，这往往是开发中经常容易疏忽的点。")]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://www.knowledgedict.com/tutorial/elasticsearch-intro.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Elasticsearch 教程"),s("OutboundLink")],1)])])])}),[],!1,null,null,null);s.default=e.exports}}]);