(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(n){function e(e){for(var a,r,o=e[0],l=e[1],c=e[2],u=0,m=[];u<o.length;u++)r=o[u],Object.prototype.hasOwnProperty.call(s,r)&&s[r]&&m.push(s[r][0]),s[r]=0;for(a in l)Object.prototype.hasOwnProperty.call(l,a)&&(n[a]=l[a]);for(d&&d(e);m.length;)m.shift()();return i.push.apply(i,c||[]),t()}function t(){for(var n,e=0;e<i.length;e++){for(var t=i[e],a=!0,o=1;o<t.length;o++){var l=t[o];0!==s[l]&&(a=!1)}a&&(i.splice(e--,1),n=r(r.s=t[0]))}return n}var a={},s={1:0},i=[];function r(e){if(a[e])return a[e].exports;var t=a[e]={i:e,l:!1,exports:{}};return n[e].call(t.exports,t,t.exports,r),t.l=!0,t.exports}r.e=function(n){var e=[],t=s[n];if(0!==t)if(t)e.push(t[2]);else{var a=new Promise((function(e,a){t=s[n]=[e,a]}));e.push(t[2]=a);var i,o=document.createElement("script");o.charset="utf-8",o.timeout=120,r.nc&&o.setAttribute("nonce",r.nc),o.src=function(n){return r.p+"assets/js/"+({}[n]||n)+"."+{2:"cc645ea8",3:"07f08211",4:"9edd854b",5:"3850a653",6:"ec598f68",7:"c455f857",8:"acea45c9",9:"0ef39ce9",10:"67ceec29",11:"917853ce",12:"3c8f3021",13:"4aa0bc50",14:"8a375258",15:"edbef757",16:"fa4cea11",17:"b27d12d2",18:"8dbef247",19:"16d06c34",20:"349782f5",21:"0c4cb722",22:"08afd969",23:"f6078302",24:"2c37bceb",25:"612350a3",26:"2ce86b41",27:"a0af278e",28:"e76641c1",29:"16bcbce6",30:"5afae00a",31:"ff50dd9e",32:"736e210d",33:"024aa2b1",34:"347dc5ce",35:"4ed56603",36:"b279277d",37:"13f1b7b5",38:"61897c23",39:"2852a032",40:"aea83b8f",41:"74c5f6d3",42:"4fb8ac6e",43:"78dbfcb5",44:"2883f73e",45:"c764f565",46:"627a475d",47:"8017a53d",48:"9e2f9da9",49:"9c5fec27",50:"dc175bf2",51:"98a34fca",52:"f06a8d1c",53:"b3848a77",54:"fe0c9cd5",55:"1116689c",56:"77e96777",57:"261e7cab",58:"0aa67c7c",59:"7712af8f",60:"b1e26e7c",61:"8eec515e",62:"95c5ace4",63:"947996fd",64:"78f59f12",65:"61319496",66:"f5a6ec28",67:"f951881a",68:"737c984b",69:"a3149896",70:"4e169493",71:"39d3a931",72:"7353c48a",73:"f05c8db8",74:"900c3e27",75:"9af5cc6e",76:"e5498672",77:"97d6e3e6",78:"12c069f8",79:"5c8ca0d8",80:"1ba70bec",81:"8d6502ea",82:"040529c2",83:"4ccae056",84:"c8813d8a",85:"947070c9"}[n]+".js"}(n);var l=new Error;i=function(e){o.onerror=o.onload=null,clearTimeout(c);var t=s[n];if(0!==t){if(t){var a=e&&("load"===e.type?"missing":e.type),i=e&&e.target&&e.target.src;l.message="Loading chunk "+n+" failed.\n("+a+": "+i+")",l.name="ChunkLoadError",l.type=a,l.request=i,t[1](l)}s[n]=void 0}};var c=setTimeout((function(){i({type:"timeout",target:o})}),12e4);o.onerror=o.onload=i,document.head.appendChild(o)}return Promise.all(e)},r.m=n,r.c=a,r.d=function(n,e,t){r.o(n,e)||Object.defineProperty(n,e,{enumerable:!0,get:t})},r.r=function(n){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})},r.t=function(n,e){if(1&e&&(n=r(n)),8&e)return n;if(4&e&&"object"==typeof n&&n&&n.__esModule)return n;var t=Object.create(null);if(r.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:n}),2&e&&"string"!=typeof n)for(var a in n)r.d(t,a,function(e){return n[e]}.bind(null,a));return t},r.n=function(n){var e=n&&n.__esModule?function(){return n.default}:function(){return n};return r.d(e,"a",e),e},r.o=function(n,e){return Object.prototype.hasOwnProperty.call(n,e)},r.p="/db-tutorial/",r.oe=function(n){throw console.error(n),n};var o=window.webpackJsonp=window.webpackJsonp||[],l=o.push.bind(o);o.push=e,o=o.slice();for(var c=0;c<o.length;c++)e(o[c]);var d=l;i.push([104,0]),t()}([function(n,e,t){var a=t(56),s=a.all;n.exports=a.IS_HTMLDDA?function(n){return"function"==typeof n||n===s}:function(n){return"function"==typeof n}},function(n,e,t){var a=t(29),s=Function.prototype,i=s.call,r=a&&s.bind.bind(i,i);n.exports=a?r:function(n){return function(){return i.apply(n,arguments)}}},function(n,e){var t=function(n){return n&&n.Math==Math&&n};n.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(n,e){n.exports=function(n){try{return!!n()}catch(n){return!0}}},function(n,e,t){"use strict";function a(n,e,t,a,s,i,r,o){var l,c="function"==typeof n?n.options:n;if(e&&(c.render=e,c.staticRenderFns=t,c._compiled=!0),a&&(c.functional=!0),i&&(c._scopeId="data-v-"+i),r?(l=function(n){(n=n||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(n=__VUE_SSR_CONTEXT__),s&&s.call(this,n),n&&n._registeredComponents&&n._registeredComponents.add(r)},c._ssrRegister=l):s&&(l=o?function(){s.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:s),l)if(c.functional){c._injectStyles=l;var d=c.render;c.render=function(n,e){return l.call(e),d(n,e)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,l):[l]}return{exports:n,options:c}}t.d(e,"a",(function(){return a}))},function(n,e,t){var a=t(3);n.exports=!a((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(n,e){var t=Array.isArray;n.exports=t},function(n,e,t){var a=t(1),s=t(32),i=a({}.hasOwnProperty);n.exports=Object.hasOwn||function(n,e){return i(s(n),e)}},function(n,e,t){var a=t(70),s="object"==typeof self&&self&&self.Object===Object&&self,i=a||s||Function("return this")();n.exports=i},function(n,e,t){var a=t(0),s=t(56),i=s.all;n.exports=s.IS_HTMLDDA?function(n){return"object"==typeof n?null!==n:a(n)||n===i}:function(n){return"object"==typeof n?null!==n:a(n)}},function(n,e,t){var a=t(164),s=t(167);n.exports=function(n,e){var t=s(n,e);return a(t)?t:void 0}},function(n,e,t){"use strict";t.d(e,"e",(function(){return a})),t.d(e,"b",(function(){return i})),t.d(e,"j",(function(){return r})),t.d(e,"g",(function(){return l})),t.d(e,"h",(function(){return c})),t.d(e,"i",(function(){return d})),t.d(e,"c",(function(){return u})),t.d(e,"f",(function(){return m})),t.d(e,"l",(function(){return p})),t.d(e,"m",(function(){return h})),t.d(e,"d",(function(){return f})),t.d(e,"k",(function(){return b})),t.d(e,"n",(function(){return y})),t.d(e,"a",(function(){return _}));t(17);const a=/#.*$/,s=/\.(md|html)$/,i=/\/$/,r=/^[a-z]+:/i;function o(n){return decodeURI(n).replace(a,"").replace(s,"")}function l(n){return r.test(n)}function c(n){return/^mailto:/.test(n)}function d(n){return/^tel:/.test(n)}function u(n){if(l(n))return n;if(!n)return"404";const e=n.match(a),t=e?e[0]:"",s=o(n);return i.test(s)?n:s+".html"+t}function m(n,e){const t=n.hash,s=function(n){const e=n&&n.match(a);if(e)return e[0]}(e);if(s&&t!==s)return!1;return o(n.path)===o(e)}function p(n,e,t){if(l(e))return{type:"external",path:e};t&&(e=function(n,e,t){const a=n.charAt(0);if("/"===a)return n;if("?"===a||"#"===a)return e+n;const s=e.split("/");t&&s[s.length-1]||s.pop();const i=n.replace(/^\//,"").split("/");for(let n=0;n<i.length;n++){const e=i[n];".."===e?s.pop():"."!==e&&s.push(e)}""!==s[0]&&s.unshift("");return s.join("/")}(e,t));const a=o(e);for(let e=0;e<n.length;e++)if(o(n[e].regularPath)===a)return Object.assign({},n[e],{type:"page",path:u(n[e].path)});return console.error(`[vuepress] No matching page found for sidebar item "${e}"`),{}}function h(n,e,t,a){const{pages:s,themeConfig:i}=t,r=a&&i.locales&&i.locales[a]||i;if("auto"===(n.frontmatter.sidebar||r.sidebar||i.sidebar))return g(n);const o=r.sidebar||i.sidebar;if(o){const{base:t,config:a}=function(n,e){if(Array.isArray(e))return{base:"/",config:e};for(const a in e)if(0===(t=n,/(\.html|\/)$/.test(t)?t:t+"/").indexOf(encodeURI(a)))return{base:a,config:e[a]};var t;return{}}(e,o);return"auto"===a?g(n):a?a.map(n=>function n(e,t,a,s=1){if("string"==typeof e)return p(t,e,a);if(Array.isArray(e))return Object.assign(p(t,e[0],a),{title:e[1]});{s>3&&console.error("[vuepress] detected a too deep nested sidebar group.");const i=e.children||[];return 0===i.length&&e.path?Object.assign(p(t,e.path,a),{title:e.title}):{type:"group",path:e.path,title:e.title,sidebarDepth:e.sidebarDepth,initialOpenGroupIndex:e.initialOpenGroupIndex,children:i.map(e=>n(e,t,a,s+1)),collapsable:!1!==e.collapsable}}}(n,s,t)):[]}return[]}function g(n){const e=f(n.headers||[]);return[{type:"group",collapsable:!1,title:n.title,path:null,children:e.map(e=>({type:"auto",title:e.title,basePath:n.path,path:n.path+"#"+e.slug,children:e.children||[]}))}]}function f(n){let e;return(n=n.map(n=>Object.assign({},n))).forEach(n=>{2===n.level?e=n:e&&(e.children||(e.children=[])).push(n)}),n.filter(n=>2===n.level)}function b(n){return Object.assign(n,{type:n.items&&n.items.length?"links":"link"})}function y(n){return Object.prototype.toString.call(n).match(/\[object (.*?)\]/)[1].toLowerCase()}function v(n){let e=n.frontmatter.date||n.lastUpdated||new Date,t=new Date(e);return"Invalid Date"==t&&e&&(t=new Date(e.replace(/-/g,"/"))),t.getTime()}function _(n,e){return v(e)-v(n)}},function(n,e){n.exports=function(n){return null!=n&&"object"==typeof n}},function(n,e,t){var a=t(5),s=t(65),i=t(100),r=t(28),o=t(55),l=TypeError,c=Object.defineProperty,d=Object.getOwnPropertyDescriptor;e.f=a?i?function(n,e,t){if(r(n),e=o(e),r(t),"function"==typeof n&&"prototype"===e&&"value"in t&&"writable"in t&&!t.writable){var a=d(n,e);a&&a.writable&&(n[e]=t.value,t={configurable:"configurable"in t?t.configurable:a.configurable,enumerable:"enumerable"in t?t.enumerable:a.enumerable,writable:!1})}return c(n,e,t)}:c:function(n,e,t){if(r(n),e=o(e),r(t),s)try{return c(n,e,t)}catch(n){}if("get"in t||"set"in t)throw l("Accessors not supported");return"value"in t&&(n[e]=t.value),n}},function(n,e,t){var a=t(16),s=t(149),i=t(150),r=a?a.toStringTag:void 0;n.exports=function(n){return null==n?void 0===n?"[object Undefined]":"[object Null]":r&&r in Object(n)?s(n):i(n)}},function(n,e,t){var a=t(5),s=t(13),i=t(35);n.exports=a?function(n,e,t){return s.f(n,e,i(1,t))}:function(n,e,t){return n[e]=t,n}},function(n,e,t){var a=t(8).Symbol;n.exports=a},function(n,e,t){"use strict";var a=t(18),s=t(32),i=t(33),r=t(129),o=t(131);a({target:"Array",proto:!0,arity:1,forced:t(3)((function(){return 4294967297!==[].push.call({length:4294967296},1)}))||!function(){try{Object.defineProperty([],"length",{writable:!1}).push()}catch(n){return n instanceof TypeError}}()},{push:function(n){var e=s(this),t=i(e),a=arguments.length;o(t+a);for(var l=0;l<a;l++)e[t]=arguments[l],t++;return r(e,t),t}})},function(n,e,t){var a=t(2),s=t(52).f,i=t(15),r=t(112),o=t(37),l=t(66),c=t(125);n.exports=function(n,e){var t,d,u,m,p,h=n.target,g=n.global,f=n.stat;if(t=g?a:f?a[h]||o(h,{}):(a[h]||{}).prototype)for(d in e){if(m=e[d],u=n.dontCallGetSet?(p=s(t,d))&&p.value:t[d],!c(g?d:h+(f?".":"#")+d,n.forced)&&void 0!==u){if(typeof m==typeof u)continue;l(m,u)}(n.sham||u&&u.sham)&&i(m,"sham",!0),r(t,d,m,n)}}},function(n,e,t){var a=t(1),s=a({}.toString),i=a("".slice);n.exports=function(n){return i(s(n),8,-1)}},function(n,e,t){var a=t(2),s=t(62),i=t(7),r=t(64),o=t(60),l=t(59),c=a.Symbol,d=s("wks"),u=l?c.for||c:c&&c.withoutSetter||r;n.exports=function(n){return i(d,n)||(d[n]=o&&i(c,n)?c[n]:u("Symbol."+n)),d[n]}},function(n,e,t){var a=t(154),s=t(155),i=t(156),r=t(157),o=t(158);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=s,l.prototype.get=i,l.prototype.has=r,l.prototype.set=o,n.exports=l},function(n,e,t){var a=t(72);n.exports=function(n,e){for(var t=n.length;t--;)if(a(n[t][0],e))return t;return-1}},function(n,e,t){var a=t(10)(Object,"create");n.exports=a},function(n,e,t){var a=t(176);n.exports=function(n,e){var t=n.__data__;return a(e)?t["string"==typeof e?"string":"hash"]:t.map}},function(n,e,t){var a=t(45);n.exports=function(n){if("string"==typeof n||a(n))return n;var e=n+"";return"0"==e&&1/n==-1/0?"-0":e}},function(n,e,t){var a,s;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(s="function"==typeof(a=function(){var n,e,t={version:"0.2.0"},a=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function s(n,e,t){return n<e?e:n>t?t:n}function i(n){return 100*(-1+n)}t.configure=function(n){var e,t;for(e in n)void 0!==(t=n[e])&&n.hasOwnProperty(e)&&(a[e]=t);return this},t.status=null,t.set=function(n){var e=t.isStarted();n=s(n,a.minimum,1),t.status=1===n?null:n;var l=t.render(!e),c=l.querySelector(a.barSelector),d=a.speed,u=a.easing;return l.offsetWidth,r((function(e){""===a.positionUsing&&(a.positionUsing=t.getPositioningCSS()),o(c,function(n,e,t){var s;return(s="translate3d"===a.positionUsing?{transform:"translate3d("+i(n)+"%,0,0)"}:"translate"===a.positionUsing?{transform:"translate("+i(n)+"%,0)"}:{"margin-left":i(n)+"%"}).transition="all "+e+"ms "+t,s}(n,d,u)),1===n?(o(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){o(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){t.remove(),e()}),d)}),d)):setTimeout(e,d)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var n=function(){setTimeout((function(){t.status&&(t.trickle(),n())}),a.trickleSpeed)};return a.trickle&&n(),this},t.done=function(n){return n||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(n){var e=t.status;return e?("number"!=typeof n&&(n=(1-e)*s(Math.random()*e,.1,.95)),e=s(e+n,0,.994),t.set(e)):t.start()},t.trickle=function(){return t.inc(Math.random()*a.trickleRate)},n=0,e=0,t.promise=function(a){return a&&"resolved"!==a.state()?(0===e&&t.start(),n++,e++,a.always((function(){0==--e?(n=0,t.done()):t.set((n-e)/n)})),this):this},t.render=function(n){if(t.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var e=document.createElement("div");e.id="nprogress",e.innerHTML=a.template;var s,r=e.querySelector(a.barSelector),l=n?"-100":i(t.status||0),d=document.querySelector(a.parent);return o(r,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),a.showSpinner||(s=e.querySelector(a.spinnerSelector))&&m(s),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(e),e},t.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(a.parent),"nprogress-custom-parent");var n=document.getElementById("nprogress");n&&m(n)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var n=document.body.style,e="WebkitTransform"in n?"Webkit":"MozTransform"in n?"Moz":"msTransform"in n?"ms":"OTransform"in n?"O":"";return e+"Perspective"in n?"translate3d":e+"Transform"in n?"translate":"margin"};var r=function(){var n=[];function e(){var t=n.shift();t&&t(e)}return function(t){n.push(t),1==n.length&&e()}}(),o=function(){var n=["Webkit","O","Moz","ms"],e={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(n,e){return e.toUpperCase()})),e[t]||(e[t]=function(e){var t=document.body.style;if(e in t)return e;for(var a,s=n.length,i=e.charAt(0).toUpperCase()+e.slice(1);s--;)if((a=n[s]+i)in t)return a;return e}(t))}function a(n,e,a){e=t(e),n.style[e]=a}return function(n,e){var t,s,i=arguments;if(2==i.length)for(t in e)void 0!==(s=e[t])&&e.hasOwnProperty(t)&&a(n,t,s);else a(n,i[1],i[2])}}();function l(n,e){return("string"==typeof n?n:u(n)).indexOf(" "+e+" ")>=0}function c(n,e){var t=u(n),a=t+e;l(t,e)||(n.className=a.substring(1))}function d(n,e){var t,a=u(n);l(n,e)&&(t=a.replace(" "+e+" "," "),n.className=t.substring(1,t.length-1))}function u(n){return(" "+(n.className||"")+" ").replace(/\s+/gi," ")}function m(n){n&&n.parentNode&&n.parentNode.removeChild(n)}return t})?a.call(e,t,e,n):a)||(n.exports=s)},function(n){n.exports=JSON.parse('{"_from":"vuepress-plugin-comment@^0.7.3","_id":"vuepress-plugin-comment@0.7.3","_inBundle":false,"_integrity":"sha512-CvuBST37snmQGzGD5lMxyw0u2eXWKVPjegfuwlI2+CTs2qWarKX16dVHHLR8DVa/yL5UGT3VuoNETH/zEexl8A==","_location":"/vuepress-plugin-comment","_phantomChildren":{},"_requested":{"type":"range","registry":true,"raw":"vuepress-plugin-comment@^0.7.3","name":"vuepress-plugin-comment","escapedName":"vuepress-plugin-comment","rawSpec":"^0.7.3","saveSpec":null,"fetchSpec":"^0.7.3"},"_requiredBy":["#DEV:/"],"_resolved":"https://registry.npmjs.org/vuepress-plugin-comment/-/vuepress-plugin-comment-0.7.3.tgz","_shasum":"2cff36b8f90896a7f88d494d78458398b1510249","_spec":"vuepress-plugin-comment@^0.7.3","_where":"/home/runner/work/db-tutorial/db-tutorial","author":{"name":"dongyuanxin"},"bugs":{"url":"https://github.com/dongyuanxin/vuepress-plugin-comment/issues"},"bundleDependencies":false,"dependencies":{"ejs":"^2.6.1","gitalk":"^1.5.0","gitalk-fix":"^1.5.2","i":"^0.3.6","npm":"^6.9.0","valine":"^1.3.9"},"deprecated":false,"description":"Comment plugin in vuepress, such as Gitalk, Valine...","homepage":"https://github.com/dongyuanxin/vuepress-plugin-comment#readme","keywords":["vuepress","comment","plugin","vue","gitalk","valine"],"license":"MIT","main":"index.js","name":"vuepress-plugin-comment","repository":{"type":"git","url":"git+ssh://git@github.com/dongyuanxin/vuepress-plugin-comment.git"},"scripts":{"test":"echo \\"Error: no test specified\\" && exit 1"},"version":"0.7.3"}')},function(n,e,t){var a=t(9),s=String,i=TypeError;n.exports=function(n){if(a(n))return n;throw i(s(n)+" is not an object")}},function(n,e,t){var a=t(3);n.exports=!a((function(){var n=function(){}.bind();return"function"!=typeof n||n.hasOwnProperty("prototype")}))},function(n,e,t){var a=t(47),s=t(53);n.exports=function(n){return a(s(n))}},function(n,e,t){var a=t(2),s=t(0),i=function(n){return s(n)?n:void 0};n.exports=function(n,e){return arguments.length<2?i(a[n]):a[n]&&a[n][e]}},function(n,e,t){var a=t(53),s=Object;n.exports=function(n){return s(a(n))}},function(n,e,t){var a=t(123);n.exports=function(n){return a(n.length)}},function(n,e,t){var a=t(29),s=Function.prototype.call;n.exports=a?s.bind(s):function(){return s.apply(s,arguments)}},function(n,e){n.exports=function(n,e){return{enumerable:!(1&n),configurable:!(2&n),writable:!(4&n),value:e}}},function(n,e,t){var a=t(2),s=t(37),i=a["__core-js_shared__"]||s("__core-js_shared__",{});n.exports=i},function(n,e,t){var a=t(2),s=Object.defineProperty;n.exports=function(n,e){try{s(a,n,{value:e,configurable:!0,writable:!0})}catch(t){a[n]=e}return e}},function(n,e,t){var a=t(148),s=t(12),i=Object.prototype,r=i.hasOwnProperty,o=i.propertyIsEnumerable,l=a(function(){return arguments}())?a:function(n){return s(n)&&r.call(n,"callee")&&!o.call(n,"callee")};n.exports=l},function(n,e,t){var a=t(10)(t(8),"Map");n.exports=a},function(n,e){n.exports=function(n){var e=typeof n;return null!=n&&("object"==e||"function"==e)}},function(n,e,t){var a=t(168),s=t(175),i=t(177),r=t(178),o=t(179);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=s,l.prototype.get=i,l.prototype.has=r,l.prototype.set=o,n.exports=l},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n){t[++e]=n})),t}},function(n,e){n.exports=function(n){return"number"==typeof n&&n>-1&&n%1==0&&n<=9007199254740991}},function(n,e,t){var a=t(6),s=t(45),i=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,r=/^\w*$/;n.exports=function(n,e){if(a(n))return!1;var t=typeof n;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=n&&!s(n))||(r.test(n)||!i.test(n)||null!=e&&n in Object(e))}},function(n,e,t){var a=t(14),s=t(12);n.exports=function(n){return"symbol"==typeof n||s(n)&&"[object Symbol]"==a(n)}},function(n,e){n.exports=function(n){return n}},function(n,e,t){var a=t(1),s=t(3),i=t(19),r=Object,o=a("".split);n.exports=s((function(){return!r("z").propertyIsEnumerable(0)}))?function(n){return"String"==i(n)?o(n,""):r(n)}:r},function(n,e,t){var a=t(0),s=t(110),i=TypeError;n.exports=function(n){if(a(n))return n;throw i(s(n)+" is not a function")}},function(n,e){n.exports={}},function(n,e){n.exports=function(n){return n.webpackPolyfill||(n.deprecate=function(){},n.paths=[],n.children||(n.children=[]),Object.defineProperty(n,"loaded",{enumerable:!0,get:function(){return n.l}}),Object.defineProperty(n,"id",{enumerable:!0,get:function(){return n.i}}),n.webpackPolyfill=1),n}},function(n,e){var t=/^\s+|\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,i=/^0o[0-7]+$/i,r=parseInt,o="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=o||l||Function("return this")(),d=Object.prototype.toString,u=Math.max,m=Math.min,p=function(){return c.Date.now()};function h(n){var e=typeof n;return!!n&&("object"==e||"function"==e)}function g(n){if("number"==typeof n)return n;if(function(n){return"symbol"==typeof n||function(n){return!!n&&"object"==typeof n}(n)&&"[object Symbol]"==d.call(n)}(n))return NaN;if(h(n)){var e="function"==typeof n.valueOf?n.valueOf():n;n=h(e)?e+"":e}if("string"!=typeof n)return 0===n?n:+n;n=n.replace(t,"");var o=s.test(n);return o||i.test(n)?r(n.slice(2),o?2:8):a.test(n)?NaN:+n}n.exports=function(n,e,t){var a,s,i,r,o,l,c=0,d=!1,f=!1,b=!0;if("function"!=typeof n)throw new TypeError("Expected a function");function y(e){var t=a,i=s;return a=s=void 0,c=e,r=n.apply(i,t)}function v(n){return c=n,o=setTimeout(E,e),d?y(n):r}function _(n){var t=n-l;return void 0===l||t>=e||t<0||f&&n-c>=i}function E(){var n=p();if(_(n))return x(n);o=setTimeout(E,function(n){var t=e-(n-l);return f?m(t,i-(n-c)):t}(n))}function x(n){return o=void 0,b&&a?y(n):(a=s=void 0,r)}function T(){var n=p(),t=_(n);if(a=arguments,s=this,l=n,t){if(void 0===o)return v(l);if(f)return o=setTimeout(E,e),y(l)}return void 0===o&&(o=setTimeout(E,e)),r}return e=g(e)||0,h(t)&&(d=!!t.leading,i=(f="maxWait"in t)?u(g(t.maxWait)||0,e):i,b="trailing"in t?!!t.trailing:b),T.cancel=function(){void 0!==o&&clearTimeout(o),c=0,a=l=s=o=void 0},T.flush=function(){return void 0===o?r:x(p())},T}},function(n,e,t){var a=t(5),s=t(34),i=t(106),r=t(35),o=t(30),l=t(55),c=t(7),d=t(65),u=Object.getOwnPropertyDescriptor;e.f=a?u:function(n,e){if(n=o(n),e=l(e),d)try{return u(n,e)}catch(n){}if(c(n,e))return r(!s(i.f,n,e),n[e])}},function(n,e,t){var a=t(54),s=TypeError;n.exports=function(n){if(a(n))throw s("Can't call method on "+n);return n}},function(n,e){n.exports=function(n){return null==n}},function(n,e,t){var a=t(107),s=t(57);n.exports=function(n){var e=a(n,"string");return s(e)?e:e+""}},function(n,e){var t="object"==typeof document&&document.all,a=void 0===t&&void 0!==t;n.exports={all:t,IS_HTMLDDA:a}},function(n,e,t){var a=t(31),s=t(0),i=t(58),r=t(59),o=Object;n.exports=r?function(n){return"symbol"==typeof n}:function(n){var e=a("Symbol");return s(e)&&i(e.prototype,o(n))}},function(n,e,t){var a=t(1);n.exports=a({}.isPrototypeOf)},function(n,e,t){var a=t(60);n.exports=a&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(n,e,t){var a=t(61),s=t(3);n.exports=!!Object.getOwnPropertySymbols&&!s((function(){var n=Symbol();return!String(n)||!(Object(n)instanceof Symbol)||!Symbol.sham&&a&&a<41}))},function(n,e,t){var a,s,i=t(2),r=t(108),o=i.process,l=i.Deno,c=o&&o.versions||l&&l.version,d=c&&c.v8;d&&(s=(a=d.split("."))[0]>0&&a[0]<4?1:+(a[0]+a[1])),!s&&r&&(!(a=r.match(/Edge\/(\d+)/))||a[1]>=74)&&(a=r.match(/Chrome\/(\d+)/))&&(s=+a[1]),n.exports=s},function(n,e,t){var a=t(63),s=t(36);(n.exports=function(n,e){return s[n]||(s[n]=void 0!==e?e:{})})("versions",[]).push({version:"3.27.2",mode:a?"pure":"global",copyright:"© 2014-2023 Denis Pushkarev (zloirock.ru)",license:"https://github.com/zloirock/core-js/blob/v3.27.2/LICENSE",source:"https://github.com/zloirock/core-js"})},function(n,e){n.exports=!1},function(n,e,t){var a=t(1),s=0,i=Math.random(),r=a(1..toString);n.exports=function(n){return"Symbol("+(void 0===n?"":n)+")_"+r(++s+i,36)}},function(n,e,t){var a=t(5),s=t(3),i=t(99);n.exports=!a&&!s((function(){return 7!=Object.defineProperty(i("div"),"a",{get:function(){return 7}}).a}))},function(n,e,t){var a=t(7),s=t(118),i=t(52),r=t(13);n.exports=function(n,e,t){for(var o=s(e),l=r.f,c=i.f,d=0;d<o.length;d++){var u=o[d];a(n,u)||t&&a(t,u)||l(n,u,c(e,u))}}},function(n,e,t){var a=t(122);n.exports=function(n){var e=+n;return e!=e||0===e?0:a(e)}},function(n,e,t){var a=t(1),s=t(28),i=t(135);n.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var n,e=!1,t={};try{(n=a(Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set))(t,[]),e=t instanceof Array}catch(n){}return function(t,a){return s(t),i(a),e?n(t,a):t.__proto__=a,t}}():void 0)},function(n,e){n.exports=function(n,e){for(var t=-1,a=e.length,s=n.length;++t<a;)n[s+t]=e[t];return n}},function(n,e){var t="object"==typeof global&&global&&global.Object===Object&&global;n.exports=t},function(n,e,t){var a=t(21),s=t(159),i=t(160),r=t(161),o=t(162),l=t(163);function c(n){var e=this.__data__=new a(n);this.size=e.size}c.prototype.clear=s,c.prototype.delete=i,c.prototype.get=r,c.prototype.has=o,c.prototype.set=l,n.exports=c},function(n,e){n.exports=function(n,e){return n===e||n!=n&&e!=e}},function(n,e,t){var a=t(14),s=t(40);n.exports=function(n){if(!s(n))return!1;var e=a(n);return"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e}},function(n,e){var t=Function.prototype.toString;n.exports=function(n){if(null!=n){try{return t.call(n)}catch(n){}try{return n+""}catch(n){}}return""}},function(n,e,t){var a=t(180),s=t(12);n.exports=function n(e,t,i,r,o){return e===t||(null==e||null==t||!s(e)&&!s(t)?e!=e&&t!=t:a(e,t,i,r,n,o))}},function(n,e,t){var a=t(77),s=t(183),i=t(78);n.exports=function(n,e,t,r,o,l){var c=1&t,d=n.length,u=e.length;if(d!=u&&!(c&&u>d))return!1;var m=l.get(n),p=l.get(e);if(m&&p)return m==e&&p==n;var h=-1,g=!0,f=2&t?new a:void 0;for(l.set(n,e),l.set(e,n);++h<d;){var b=n[h],y=e[h];if(r)var v=c?r(y,b,h,e,n,l):r(b,y,h,n,e,l);if(void 0!==v){if(v)continue;g=!1;break}if(f){if(!s(e,(function(n,e){if(!i(f,e)&&(b===n||o(b,n,t,r,l)))return f.push(e)}))){g=!1;break}}else if(b!==y&&!o(b,y,t,r,l)){g=!1;break}}return l.delete(n),l.delete(e),g}},function(n,e,t){var a=t(41),s=t(181),i=t(182);function r(n){var e=-1,t=null==n?0:n.length;for(this.__data__=new a;++e<t;)this.add(n[e])}r.prototype.add=r.prototype.push=s,r.prototype.has=i,n.exports=r},function(n,e){n.exports=function(n,e){return n.has(e)}},function(n,e,t){var a=t(193),s=t(199),i=t(83);n.exports=function(n){return i(n)?a(n):s(n)}},function(n,e,t){(function(n){var a=t(8),s=t(195),i=e&&!e.nodeType&&e,r=i&&"object"==typeof n&&n&&!n.nodeType&&n,o=r&&r.exports===i?a.Buffer:void 0,l=(o?o.isBuffer:void 0)||s;n.exports=l}).call(this,t(50)(n))},function(n,e){var t=/^(?:0|[1-9]\d*)$/;n.exports=function(n,e){var a=typeof n;return!!(e=null==e?9007199254740991:e)&&("number"==a||"symbol"!=a&&t.test(n))&&n>-1&&n%1==0&&n<e}},function(n,e,t){var a=t(196),s=t(197),i=t(198),r=i&&i.isTypedArray,o=r?s(r):a;n.exports=o},function(n,e,t){var a=t(73),s=t(43);n.exports=function(n){return null!=n&&s(n.length)&&!a(n)}},function(n,e,t){var a=t(10)(t(8),"Set");n.exports=a},function(n,e,t){var a=t(40);n.exports=function(n){return n==n&&!a(n)}},function(n,e){n.exports=function(n,e){return function(t){return null!=t&&(t[n]===e&&(void 0!==e||n in Object(t)))}}},function(n,e,t){var a=t(88),s=t(25);n.exports=function(n,e){for(var t=0,i=(e=a(e,n)).length;null!=n&&t<i;)n=n[s(e[t++])];return t&&t==i?n:void 0}},function(n,e,t){var a=t(6),s=t(44),i=t(210),r=t(213);n.exports=function(n,e){return a(n)?n:s(n,e)?[n]:i(r(n))}},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(146),s=t(151),i=t(222),r=t(230),o=t(239),l=t(103),c=i((function(n){var e=l(n);return o(e)&&(e=void 0),r(a(n,1,o,!0),s(e,2))}));n.exports=c},function(n,e,t){"use strict";
/*!
 * escape-html
 * Copyright(c) 2012-2013 TJ Holowaychuk
 * Copyright(c) 2015 Andreas Lubbe
 * Copyright(c) 2015 Tiancheng "Timothy" Gu
 * MIT Licensed
 */var a=/["'&<>]/;n.exports=function(n){var e,t=""+n,s=a.exec(t);if(!s)return t;var i="",r=0,o=0;for(r=s.index;r<t.length;r++){switch(t.charCodeAt(r)){case 34:e="&quot;";break;case 38:e="&amp;";break;case 39:e="&#39;";break;case 60:e="&lt;";break;case 62:e="&gt;";break;default:continue}o!==r&&(i+=t.substring(o,r)),o=r+1,i+=e}return o!==r?i+t.substring(o,r):i}},function(n,e,t){"use strict";
/**
 * @file Embedded JavaScript templating engine. {@link http://ejs.co}
 * @author Matthew Eernisse <mde@fleegix.org>
 * @author Tiancheng "Timothy" Gu <timothygu99@gmail.com>
 * @project EJS
 * @license {@link http://www.apache.org/licenses/LICENSE-2.0 Apache License, Version 2.0}
 */var a=t(248),s=t(249),i=t(250),r=!1,o=t(251).version,l=["delimiter","scope","context","debug","compileDebug","client","_with","rmWhitespace","strict","filename","async"],c=l.concat("cache"),d=/^\uFEFF/;function u(n,t){var s,i,r=t.views,o=/^[A-Za-z]+:\\|^\//.exec(n);if(o&&o.length)s=e.resolveInclude(n.replace(/^\/*/,""),t.root||"/",!0);else if(t.filename&&(i=e.resolveInclude(n,t.filename),a.existsSync(i)&&(s=i)),s||Array.isArray(r)&&r.some((function(t){return i=e.resolveInclude(n,t,!0),a.existsSync(i)}))&&(s=i),!s)throw new Error('Could not find the include file "'+t.escapeFunction(n)+'"');return s}function m(n,t){var a,s=n.filename,i=arguments.length>1;if(n.cache){if(!s)throw new Error("cache option requires a filename");if(a=e.cache.get(s))return a;i||(t=h(s).toString().replace(d,""))}else if(!i){if(!s)throw new Error("Internal EJS error: no file name or template provided");t=h(s).toString().replace(d,"")}return a=e.compile(t,n),n.cache&&e.cache.set(s,a),a}function p(n,t,a){var s;if(!a){if("function"==typeof e.promiseImpl)return new e.promiseImpl((function(e,a){try{e(s=m(n)(t))}catch(n){a(n)}}));throw new Error("Please provide a callback function")}try{s=m(n)(t)}catch(n){return a(n)}a(null,s)}function h(n){return e.fileLoader(n)}function g(n,e,t,a,s){var i=e.split("\n"),r=Math.max(a-3,0),o=Math.min(i.length,a+3),l=s(t),c=i.slice(r,o).map((function(n,e){var t=e+r+1;return(t==a?" >> ":"    ")+t+"| "+n})).join("\n");throw n.path=l,n.message=(l||"ejs")+":"+a+"\n"+c+"\n\n"+n.message,n}function f(n){return n.replace(/;(\s*$)/,"$1")}function b(n,t){t=t||{};var a={};this.templateText=n,this.mode=null,this.truncate=!1,this.currentLine=1,this.source="",this.dependencies=[],a.client=t.client||!1,a.escapeFunction=t.escape||t.escapeFunction||i.escapeXML,a.compileDebug=!1!==t.compileDebug,a.debug=!!t.debug,a.filename=t.filename,a.openDelimiter=t.openDelimiter||e.openDelimiter||"<",a.closeDelimiter=t.closeDelimiter||e.closeDelimiter||">",a.delimiter=t.delimiter||e.delimiter||"%",a.strict=t.strict||!1,a.context=t.context,a.cache=t.cache||!1,a.rmWhitespace=t.rmWhitespace,a.root=t.root,a.outputFunctionName=t.outputFunctionName,a.localsName=t.localsName||e.localsName||"locals",a.views=t.views,a.async=t.async,a.destructuredLocals=t.destructuredLocals,a.legacyInclude=void 0===t.legacyInclude||!!t.legacyInclude,a.strict?a._with=!1:a._with=void 0===t._with||t._with,this.opts=a,this.regex=this.createRegex()}e.cache=i.cache,e.fileLoader=a.readFileSync,e.localsName="locals",e.promiseImpl=new Function("return this;")().Promise,e.resolveInclude=function(n,e,t){var a=s.dirname,i=s.extname,r=(0,s.resolve)(t?e:a(e),n);return i(n)||(r+=".ejs"),r},e.compile=function(n,e){return e&&e.scope&&(r||(console.warn("`scope` option is deprecated and will be removed in EJS 3"),r=!0),e.context||(e.context=e.scope),delete e.scope),new b(n,e).compile()},e.render=function(n,e,t){var a=e||{},s=t||{};return 2==arguments.length&&i.shallowCopyFromList(s,a,l),m(s,n)(a)},e.renderFile=function(){var n,e,t,a=Array.prototype.slice.call(arguments),s=a.shift(),r={filename:s};return"function"==typeof arguments[arguments.length-1]&&(n=a.pop()),a.length?(e=a.shift(),a.length?i.shallowCopy(r,a.pop()):(e.settings&&(e.settings.views&&(r.views=e.settings.views),e.settings["view cache"]&&(r.cache=!0),(t=e.settings["view options"])&&i.shallowCopy(r,t)),i.shallowCopyFromList(r,e,c)),r.filename=s):e={},p(r,e,n)},e.Template=b,e.clearCache=function(){e.cache.reset()},b.modes={EVAL:"eval",ESCAPED:"escaped",RAW:"raw",COMMENT:"comment",LITERAL:"literal"},b.prototype={createRegex:function(){var n="(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)",e=i.escapeRegExpChars(this.opts.delimiter),t=i.escapeRegExpChars(this.opts.openDelimiter),a=i.escapeRegExpChars(this.opts.closeDelimiter);return n=n.replace(/%/g,e).replace(/</g,t).replace(/>/g,a),new RegExp(n)},compile:function(){var n,e,t,a=this.opts,r="",o="",l=a.escapeFunction;if(!this.source){if(this.generateSource(),r+='  var __output = "";\n  function __append(s) { if (s !== undefined && s !== null) __output += s }\n',a.outputFunctionName&&(r+="  var "+a.outputFunctionName+" = __append;\n"),a.destructuredLocals&&a.destructuredLocals.length){for(var c="  var __locals = ("+a.localsName+" || {}),\n",d=0;d<a.destructuredLocals.length;d++){var p=a.destructuredLocals[d];d>0&&(c+=",\n  "),c+=p+" = __locals."+p}r+=c+";\n"}!1!==a._with&&(r+="  with ("+a.localsName+" || {}) {\n",o+="  }\n"),o+="  return __output;\n",this.source=r+this.source+o}n=a.compileDebug?"var __line = 1\n  , __lines = "+JSON.stringify(this.templateText)+"\n  , __filename = "+(a.filename?JSON.stringify(a.filename):"undefined")+";\ntry {\n"+this.source+"} catch (e) {\n  rethrow(e, __lines, __filename, __line, escapeFn);\n}\n":this.source,a.client&&(n="escapeFn = escapeFn || "+l.toString()+";\n"+n,a.compileDebug&&(n="rethrow = rethrow || "+g.toString()+";\n"+n)),a.strict&&(n='"use strict";\n'+n),a.debug&&console.log(n),a.compileDebug&&a.filename&&(n=n+"\n//# sourceURL="+a.filename+"\n");try{if(a.async)try{t=new Function("return (async function(){}).constructor;")()}catch(n){throw n instanceof SyntaxError?new Error("This environment does not support async/await"):n}else t=Function;e=new t(a.localsName+", escapeFn, include, rethrow",n)}catch(n){throw n instanceof SyntaxError&&(a.filename&&(n.message+=" in "+a.filename),n.message+=" while compiling ejs\n\n",n.message+="If the above error is not helpful, you may want to try EJS-Lint:\n",n.message+="https://github.com/RyanZim/EJS-Lint",a.async||(n.message+="\n",n.message+="Or, if you meant to create an async function, pass `async: true` as an option.")),n}var h=a.client?e:function(n){return e.apply(a.context,[n||{},l,function(e,t){var s=i.shallowCopy({},n);return t&&(s=i.shallowCopy(s,t)),function(n,e){var t=i.shallowCopy({},e);return t.filename=u(n,t),m(t)}(e,a)(s)},g])};if(h.dependencies=this.dependencies,a.filename&&"function"==typeof Object.defineProperty){var f=a.filename,b=s.basename(f,s.extname(f));try{Object.defineProperty(h,"name",{value:b,writable:!1,enumerable:!1,configurable:!0})}catch(n){}}return h},generateSource:function(){var n=this.opts;n.rmWhitespace&&(this.templateText=this.templateText.replace(/[\r\n]+/g,"\n").replace(/^\s+|\s+$/gm,"")),this.templateText=this.templateText.replace(/[ \t]*<%_/gm,"<%_").replace(/_%>[ \t]*/gm,"_%>");var t=this,a=this.parseTemplateText(),s=this.opts.delimiter,r=this.opts.openDelimiter,o=this.opts.closeDelimiter;a&&a.length&&a.forEach((function(l,c){var m,p,g,f,y,v;if(0===l.indexOf(r+s)&&0!==l.indexOf(r+s+s)&&(p=a[c+2])!=s+o&&p!="-"+s+o&&p!="_"+s+o)throw new Error('Could not find matching close tag for "'+l+'".');if(n.legacyInclude&&(g=l.match(/^\s*include\s+(\S+)/))&&(m=a[c-1])&&(m==r+s||m==r+s+"-"||m==r+s+"_"))return f=i.shallowCopy({},t.opts),y=function(n,e){var t,a,s=i.shallowCopy({},e);a=h(t=u(n,s)).toString().replace(d,""),s.filename=t;var r=new b(a,s);return r.generateSource(),{source:r.source,filename:t,template:a}}(g[1],f),v=t.opts.compileDebug?"    ; (function(){\n      var __line = 1\n      , __lines = "+JSON.stringify(y.template)+"\n      , __filename = "+JSON.stringify(y.filename)+";\n      try {\n"+y.source+"      } catch (e) {\n        rethrow(e, __lines, __filename, __line, escapeFn);\n      }\n    ; }).call(this)\n":"    ; (function(){\n"+y.source+"    ; }).call(this)\n",t.source+=v,void t.dependencies.push(e.resolveInclude(g[1],f.filename));t.scanLine(l)}))},parseTemplateText:function(){for(var n,e=this.templateText,t=this.regex,a=t.exec(e),s=[];a;)0!==(n=a.index)&&(s.push(e.substring(0,n)),e=e.slice(n)),s.push(a[0]),e=e.slice(a[0].length),a=t.exec(e);return e&&s.push(e),s},_addOutput:function(n){if(this.truncate&&(n=n.replace(/^(?:\r\n|\r|\n)/,""),this.truncate=!1),!n)return n;n=(n=(n=(n=n.replace(/\\/g,"\\\\")).replace(/\n/g,"\\n")).replace(/\r/g,"\\r")).replace(/"/g,'\\"'),this.source+='    ; __append("'+n+'")\n'},scanLine:function(n){var e,t=this.opts.delimiter,a=this.opts.openDelimiter,s=this.opts.closeDelimiter;switch(e=n.split("\n").length-1,n){case a+t:case a+t+"_":this.mode=b.modes.EVAL;break;case a+t+"=":this.mode=b.modes.ESCAPED;break;case a+t+"-":this.mode=b.modes.RAW;break;case a+t+"#":this.mode=b.modes.COMMENT;break;case a+t+t:this.mode=b.modes.LITERAL,this.source+='    ; __append("'+n.replace(a+t+t,a+t)+'")\n';break;case t+t+s:this.mode=b.modes.LITERAL,this.source+='    ; __append("'+n.replace(t+t+s,t+s)+'")\n';break;case t+s:case"-"+t+s:case"_"+t+s:this.mode==b.modes.LITERAL&&this._addOutput(n),this.mode=null,this.truncate=0===n.indexOf("-")||0===n.indexOf("_");break;default:if(this.mode){switch(this.mode){case b.modes.EVAL:case b.modes.ESCAPED:case b.modes.RAW:n.lastIndexOf("//")>n.lastIndexOf("\n")&&(n+="\n")}switch(this.mode){case b.modes.EVAL:this.source+="    ; "+n+"\n";break;case b.modes.ESCAPED:this.source+="    ; __append(escapeFn("+f(n)+"))\n";break;case b.modes.RAW:this.source+="    ; __append("+f(n)+")\n";break;case b.modes.COMMENT:break;case b.modes.LITERAL:this._addOutput(n)}}else this._addOutput(n)}this.opts.compileDebug&&e&&(this.currentLine+=e,this.source+="    ; __line = "+this.currentLine+"\n")}},e.escapeXML=i.escapeXML,e.__express=e.renderFile,e.VERSION=o,e.name="ejs","undefined"!=typeof window&&(window.ejs=e)},function(n,e,t){"use strict";t.r(e);var a={name:"CodeBlock",props:{title:{type:String,required:!0},active:{type:Boolean,default:!1}}},s=(t(242),t(4)),i=Object(s.a)(a,(function(){return(0,this._self._c)("div",{staticClass:"theme-code-block",class:{"theme-code-block__active":this.active}},[this._t("default")],2)}),[],!1,null,"4f1e9d0c",null);e.default=i.exports},function(n,e,t){"use strict";t.r(e);var a={name:"CodeGroup",data:()=>({codeTabs:[],activeCodeTabIndex:-1}),watch:{activeCodeTabIndex(n){this.codeTabs.forEach(n=>{n.elm.classList.remove("theme-code-block__active")}),this.codeTabs[n].elm.classList.add("theme-code-block__active")}},mounted(){this.codeTabs=(this.$slots.default||[]).filter(n=>Boolean(n.componentOptions)).map((n,e)=>(""===n.componentOptions.propsData.active&&(this.activeCodeTabIndex=e),{title:n.componentOptions.propsData.title,elm:n.elm})),-1===this.activeCodeTabIndex&&this.codeTabs.length>0&&(this.activeCodeTabIndex=0)},methods:{changeCodeTab(n){this.activeCodeTabIndex=n}}},s=(t(243),t(4)),i=Object(s.a)(a,(function(){var n=this,e=n._self._c;return e("div",{staticClass:"theme-code-group"},[e("div",{staticClass:"theme-code-group__nav"},[e("ul",{staticClass:"theme-code-group__ul"},n._l(n.codeTabs,(function(t,a){return e("li",{key:t.title,staticClass:"theme-code-group__li"},[e("button",{staticClass:"theme-code-group__nav-tab",class:{"theme-code-group__nav-tab-active":a===n.activeCodeTabIndex},on:{click:function(e){return n.changeCodeTab(a)}}},[n._v("\n            "+n._s(t.title)+"\n          ")])])})),0)]),n._v(" "),n._t("default"),n._v(" "),n.codeTabs.length<1?e("pre",{staticClass:"pre-blank"},[n._v("// Make sure to add code blocks to your code group")]):n._e()],2)}),[],!1,null,"2f5f1757",null);e.default=i.exports},function(n,e){n.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(n,e,t){var a=t(2),s=t(9),i=a.document,r=s(i)&&s(i.createElement);n.exports=function(n){return r?i.createElement(n):{}}},function(n,e,t){var a=t(5),s=t(3);n.exports=a&&s((function(){return 42!=Object.defineProperty((function(){}),"prototype",{value:42,writable:!1}).prototype}))},function(n,e,t){var a=t(62),s=t(64),i=a("keys");n.exports=function(n){return i[n]||(i[n]=s(n))}},function(n,e,t){var a=t(1),s=t(7),i=t(30),r=t(120).indexOf,o=t(49),l=a([].push);n.exports=function(n,e){var t,a=i(n),c=0,d=[];for(t in a)!s(o,t)&&s(a,t)&&l(d,t);for(;e.length>c;)s(a,t=e[c++])&&(~r(d,t)||l(d,t));return d}},function(n,e){n.exports=function(n){var e=null==n?0:n.length;return e?n[e-1]:void 0}},function(n,e,t){n.exports=t(254)},function(n,e,t){"use strict";var a=t(18),s=t(126).left,i=t(127),r=t(61);a({target:"Array",proto:!0,forced:!t(128)&&r>79&&r<83||!i("reduce")},{reduce:function(n){var e=arguments.length;return s(this,n,e,e>1?arguments[1]:void 0)}})},function(n,e,t){"use strict";var a={}.propertyIsEnumerable,s=Object.getOwnPropertyDescriptor,i=s&&!a.call({1:2},1);e.f=i?function(n){var e=s(this,n);return!!e&&e.enumerable}:a},function(n,e,t){var a=t(34),s=t(9),i=t(57),r=t(109),o=t(111),l=t(20),c=TypeError,d=l("toPrimitive");n.exports=function(n,e){if(!s(n)||i(n))return n;var t,l=r(n,d);if(l){if(void 0===e&&(e="default"),t=a(l,n,e),!s(t)||i(t))return t;throw c("Can't convert object to primitive value")}return void 0===e&&(e="number"),o(n,e)}},function(n,e){n.exports="undefined"!=typeof navigator&&String(navigator.userAgent)||""},function(n,e,t){var a=t(48),s=t(54);n.exports=function(n,e){var t=n[e];return s(t)?void 0:a(t)}},function(n,e){var t=String;n.exports=function(n){try{return t(n)}catch(n){return"Object"}}},function(n,e,t){var a=t(34),s=t(0),i=t(9),r=TypeError;n.exports=function(n,e){var t,o;if("string"===e&&s(t=n.toString)&&!i(o=a(t,n)))return o;if(s(t=n.valueOf)&&!i(o=a(t,n)))return o;if("string"!==e&&s(t=n.toString)&&!i(o=a(t,n)))return o;throw r("Can't convert object to primitive value")}},function(n,e,t){var a=t(0),s=t(13),i=t(113),r=t(37);n.exports=function(n,e,t,o){o||(o={});var l=o.enumerable,c=void 0!==o.name?o.name:e;if(a(t)&&i(t,c,o),o.global)l?n[e]=t:r(e,t);else{try{o.unsafe?n[e]&&(l=!0):delete n[e]}catch(n){}l?n[e]=t:s.f(n,e,{value:t,enumerable:!1,configurable:!o.nonConfigurable,writable:!o.nonWritable})}return n}},function(n,e,t){var a=t(1),s=t(3),i=t(0),r=t(7),o=t(5),l=t(114).CONFIGURABLE,c=t(115),d=t(116),u=d.enforce,m=d.get,p=String,h=Object.defineProperty,g=a("".slice),f=a("".replace),b=a([].join),y=o&&!s((function(){return 8!==h((function(){}),"length",{value:8}).length})),v=String(String).split("String"),_=n.exports=function(n,e,t){"Symbol("===g(p(e),0,7)&&(e="["+f(p(e),/^Symbol\(([^)]*)\)/,"$1")+"]"),t&&t.getter&&(e="get "+e),t&&t.setter&&(e="set "+e),(!r(n,"name")||l&&n.name!==e)&&(o?h(n,"name",{value:e,configurable:!0}):n.name=e),y&&t&&r(t,"arity")&&n.length!==t.arity&&h(n,"length",{value:t.arity});try{t&&r(t,"constructor")&&t.constructor?o&&h(n,"prototype",{writable:!1}):n.prototype&&(n.prototype=void 0)}catch(n){}var a=u(n);return r(a,"source")||(a.source=b(v,"string"==typeof e?e:"")),n};Function.prototype.toString=_((function(){return i(this)&&m(this).source||c(this)}),"toString")},function(n,e,t){var a=t(5),s=t(7),i=Function.prototype,r=a&&Object.getOwnPropertyDescriptor,o=s(i,"name"),l=o&&"something"===function(){}.name,c=o&&(!a||a&&r(i,"name").configurable);n.exports={EXISTS:o,PROPER:l,CONFIGURABLE:c}},function(n,e,t){var a=t(1),s=t(0),i=t(36),r=a(Function.toString);s(i.inspectSource)||(i.inspectSource=function(n){return r(n)}),n.exports=i.inspectSource},function(n,e,t){var a,s,i,r=t(117),o=t(2),l=t(9),c=t(15),d=t(7),u=t(36),m=t(101),p=t(49),h=o.TypeError,g=o.WeakMap;if(r||u.state){var f=u.state||(u.state=new g);f.get=f.get,f.has=f.has,f.set=f.set,a=function(n,e){if(f.has(n))throw h("Object already initialized");return e.facade=n,f.set(n,e),e},s=function(n){return f.get(n)||{}},i=function(n){return f.has(n)}}else{var b=m("state");p[b]=!0,a=function(n,e){if(d(n,b))throw h("Object already initialized");return e.facade=n,c(n,b,e),e},s=function(n){return d(n,b)?n[b]:{}},i=function(n){return d(n,b)}}n.exports={set:a,get:s,has:i,enforce:function(n){return i(n)?s(n):a(n,{})},getterFor:function(n){return function(e){var t;if(!l(e)||(t=s(e)).type!==n)throw h("Incompatible receiver, "+n+" required");return t}}}},function(n,e,t){var a=t(2),s=t(0),i=a.WeakMap;n.exports=s(i)&&/native code/.test(String(i))},function(n,e,t){var a=t(31),s=t(1),i=t(119),r=t(124),o=t(28),l=s([].concat);n.exports=a("Reflect","ownKeys")||function(n){var e=i.f(o(n)),t=r.f;return t?l(e,t(n)):e}},function(n,e,t){var a=t(102),s=t(98).concat("length","prototype");e.f=Object.getOwnPropertyNames||function(n){return a(n,s)}},function(n,e,t){var a=t(30),s=t(121),i=t(33),r=function(n){return function(e,t,r){var o,l=a(e),c=i(l),d=s(r,c);if(n&&t!=t){for(;c>d;)if((o=l[d++])!=o)return!0}else for(;c>d;d++)if((n||d in l)&&l[d]===t)return n||d||0;return!n&&-1}};n.exports={includes:r(!0),indexOf:r(!1)}},function(n,e,t){var a=t(67),s=Math.max,i=Math.min;n.exports=function(n,e){var t=a(n);return t<0?s(t+e,0):i(t,e)}},function(n,e){var t=Math.ceil,a=Math.floor;n.exports=Math.trunc||function(n){var e=+n;return(e>0?a:t)(e)}},function(n,e,t){var a=t(67),s=Math.min;n.exports=function(n){return n>0?s(a(n),9007199254740991):0}},function(n,e){e.f=Object.getOwnPropertySymbols},function(n,e,t){var a=t(3),s=t(0),i=/#|\.prototype\./,r=function(n,e){var t=l[o(n)];return t==d||t!=c&&(s(e)?a(e):!!e)},o=r.normalize=function(n){return String(n).replace(i,".").toLowerCase()},l=r.data={},c=r.NATIVE="N",d=r.POLYFILL="P";n.exports=r},function(n,e,t){var a=t(48),s=t(32),i=t(47),r=t(33),o=TypeError,l=function(n){return function(e,t,l,c){a(t);var d=s(e),u=i(d),m=r(d),p=n?m-1:0,h=n?-1:1;if(l<2)for(;;){if(p in u){c=u[p],p+=h;break}if(p+=h,n?p<0:m<=p)throw o("Reduce of empty array with no initial value")}for(;n?p>=0:m>p;p+=h)p in u&&(c=t(c,u[p],p,d));return c}};n.exports={left:l(!1),right:l(!0)}},function(n,e,t){"use strict";var a=t(3);n.exports=function(n,e){var t=[][n];return!!t&&a((function(){t.call(null,e||function(){return 1},1)}))}},function(n,e,t){var a=t(19);n.exports="undefined"!=typeof process&&"process"==a(process)},function(n,e,t){"use strict";var a=t(5),s=t(130),i=TypeError,r=Object.getOwnPropertyDescriptor,o=a&&!function(){if(void 0!==this)return!0;try{Object.defineProperty([],"length",{writable:!1}).length=1}catch(n){return n instanceof TypeError}}();n.exports=o?function(n,e){if(s(n)&&!r(n,"length").writable)throw i("Cannot set read only .length");return n.length=e}:function(n,e){return n.length=e}},function(n,e,t){var a=t(19);n.exports=Array.isArray||function(n){return"Array"==a(n)}},function(n,e){var t=TypeError;n.exports=function(n){if(n>9007199254740991)throw t("Maximum allowed index exceeded");return n}},function(n,e,t){var a=t(18),s=t(2),i=t(133),r=t(134),o=s.WebAssembly,l=7!==Error("e",{cause:7}).cause,c=function(n,e){var t={};t[n]=r(n,e,l),a({global:!0,constructor:!0,arity:1,forced:l},t)},d=function(n,e){if(o&&o[n]){var t={};t[n]=r("WebAssembly."+n,e,l),a({target:"WebAssembly",stat:!0,constructor:!0,arity:1,forced:l},t)}};c("Error",(function(n){return function(e){return i(n,this,arguments)}})),c("EvalError",(function(n){return function(e){return i(n,this,arguments)}})),c("RangeError",(function(n){return function(e){return i(n,this,arguments)}})),c("ReferenceError",(function(n){return function(e){return i(n,this,arguments)}})),c("SyntaxError",(function(n){return function(e){return i(n,this,arguments)}})),c("TypeError",(function(n){return function(e){return i(n,this,arguments)}})),c("URIError",(function(n){return function(e){return i(n,this,arguments)}})),d("CompileError",(function(n){return function(e){return i(n,this,arguments)}})),d("LinkError",(function(n){return function(e){return i(n,this,arguments)}})),d("RuntimeError",(function(n){return function(e){return i(n,this,arguments)}}))},function(n,e,t){var a=t(29),s=Function.prototype,i=s.apply,r=s.call;n.exports="object"==typeof Reflect&&Reflect.apply||(a?r.bind(i):function(){return r.apply(i,arguments)})},function(n,e,t){"use strict";var a=t(31),s=t(7),i=t(15),r=t(58),o=t(68),l=t(66),c=t(136),d=t(137),u=t(138),m=t(142),p=t(143),h=t(5),g=t(63);n.exports=function(n,e,t,f){var b=f?2:1,y=n.split("."),v=y[y.length-1],_=a.apply(null,y);if(_){var E=_.prototype;if(!g&&s(E,"cause")&&delete E.cause,!t)return _;var x=a("Error"),T=e((function(n,e){var t=u(f?e:n,void 0),a=f?new _(n):new _;return void 0!==t&&i(a,"message",t),p(a,T,a.stack,2),this&&r(E,this)&&d(a,this,T),arguments.length>b&&m(a,arguments[b]),a}));if(T.prototype=E,"Error"!==v?o?o(T,x):l(T,x,{name:!0}):h&&"stackTraceLimit"in _&&(c(T,_,"stackTraceLimit"),c(T,_,"prepareStackTrace")),l(T,_),!g)try{E.name!==v&&i(E,"name",v),E.constructor=T}catch(n){}return T}}},function(n,e,t){var a=t(0),s=String,i=TypeError;n.exports=function(n){if("object"==typeof n||a(n))return n;throw i("Can't set "+s(n)+" as a prototype")}},function(n,e,t){var a=t(13).f;n.exports=function(n,e,t){t in n||a(n,t,{configurable:!0,get:function(){return e[t]},set:function(n){e[t]=n}})}},function(n,e,t){var a=t(0),s=t(9),i=t(68);n.exports=function(n,e,t){var r,o;return i&&a(r=e.constructor)&&r!==t&&s(o=r.prototype)&&o!==t.prototype&&i(n,o),n}},function(n,e,t){var a=t(139);n.exports=function(n,e){return void 0===n?arguments.length<2?"":e:a(n)}},function(n,e,t){var a=t(140),s=String;n.exports=function(n){if("Symbol"===a(n))throw TypeError("Cannot convert a Symbol value to a string");return s(n)}},function(n,e,t){var a=t(141),s=t(0),i=t(19),r=t(20)("toStringTag"),o=Object,l="Arguments"==i(function(){return arguments}());n.exports=a?i:function(n){var e,t,a;return void 0===n?"Undefined":null===n?"Null":"string"==typeof(t=function(n,e){try{return n[e]}catch(n){}}(e=o(n),r))?t:l?i(e):"Object"==(a=i(e))&&s(e.callee)?"Arguments":a}},function(n,e,t){var a={};a[t(20)("toStringTag")]="z",n.exports="[object z]"===String(a)},function(n,e,t){var a=t(9),s=t(15);n.exports=function(n,e){a(e)&&"cause"in e&&s(n,"cause",e.cause)}},function(n,e,t){var a=t(15),s=t(144),i=t(145),r=Error.captureStackTrace;n.exports=function(n,e,t,o){i&&(r?r(n,e):a(n,"stack",s(t,o)))}},function(n,e,t){var a=t(1),s=Error,i=a("".replace),r=String(s("zxcasd").stack),o=/\n\s*at [^:]*:[^\n]*/,l=o.test(r);n.exports=function(n,e){if(l&&"string"==typeof n&&!s.prepareStackTrace)for(;e--;)n=i(n,o,"");return n}},function(n,e,t){var a=t(3),s=t(35);n.exports=!a((function(){var n=Error("a");return!("stack"in n)||(Object.defineProperty(n,"stack",s(1,7)),7!==n.stack)}))},function(n,e,t){var a=t(69),s=t(147);n.exports=function n(e,t,i,r,o){var l=-1,c=e.length;for(i||(i=s),o||(o=[]);++l<c;){var d=e[l];t>0&&i(d)?t>1?n(d,t-1,i,r,o):a(o,d):r||(o[o.length]=d)}return o}},function(n,e,t){var a=t(16),s=t(38),i=t(6),r=a?a.isConcatSpreadable:void 0;n.exports=function(n){return i(n)||s(n)||!!(r&&n&&n[r])}},function(n,e,t){var a=t(14),s=t(12);n.exports=function(n){return s(n)&&"[object Arguments]"==a(n)}},function(n,e,t){var a=t(16),s=Object.prototype,i=s.hasOwnProperty,r=s.toString,o=a?a.toStringTag:void 0;n.exports=function(n){var e=i.call(n,o),t=n[o];try{n[o]=void 0;var a=!0}catch(n){}var s=r.call(n);return a&&(e?n[o]=t:delete n[o]),s}},function(n,e){var t=Object.prototype.toString;n.exports=function(n){return t.call(n)}},function(n,e,t){var a=t(152),s=t(208),i=t(46),r=t(6),o=t(219);n.exports=function(n){return"function"==typeof n?n:null==n?i:"object"==typeof n?r(n)?s(n[0],n[1]):a(n):o(n)}},function(n,e,t){var a=t(153),s=t(207),i=t(86);n.exports=function(n){var e=s(n);return 1==e.length&&e[0][2]?i(e[0][0],e[0][1]):function(t){return t===n||a(t,n,e)}}},function(n,e,t){var a=t(71),s=t(75);n.exports=function(n,e,t,i){var r=t.length,o=r,l=!i;if(null==n)return!o;for(n=Object(n);r--;){var c=t[r];if(l&&c[2]?c[1]!==n[c[0]]:!(c[0]in n))return!1}for(;++r<o;){var d=(c=t[r])[0],u=n[d],m=c[1];if(l&&c[2]){if(void 0===u&&!(d in n))return!1}else{var p=new a;if(i)var h=i(u,m,d,n,e,p);if(!(void 0===h?s(m,u,3,i,p):h))return!1}}return!0}},function(n,e){n.exports=function(){this.__data__=[],this.size=0}},function(n,e,t){var a=t(22),s=Array.prototype.splice;n.exports=function(n){var e=this.__data__,t=a(e,n);return!(t<0)&&(t==e.length-1?e.pop():s.call(e,t,1),--this.size,!0)}},function(n,e,t){var a=t(22);n.exports=function(n){var e=this.__data__,t=a(e,n);return t<0?void 0:e[t][1]}},function(n,e,t){var a=t(22);n.exports=function(n){return a(this.__data__,n)>-1}},function(n,e,t){var a=t(22);n.exports=function(n,e){var t=this.__data__,s=a(t,n);return s<0?(++this.size,t.push([n,e])):t[s][1]=e,this}},function(n,e,t){var a=t(21);n.exports=function(){this.__data__=new a,this.size=0}},function(n,e){n.exports=function(n){var e=this.__data__,t=e.delete(n);return this.size=e.size,t}},function(n,e){n.exports=function(n){return this.__data__.get(n)}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e,t){var a=t(21),s=t(39),i=t(41);n.exports=function(n,e){var t=this.__data__;if(t instanceof a){var r=t.__data__;if(!s||r.length<199)return r.push([n,e]),this.size=++t.size,this;t=this.__data__=new i(r)}return t.set(n,e),this.size=t.size,this}},function(n,e,t){var a=t(73),s=t(165),i=t(40),r=t(74),o=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,d=l.toString,u=c.hasOwnProperty,m=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");n.exports=function(n){return!(!i(n)||s(n))&&(a(n)?m:o).test(r(n))}},function(n,e,t){var a,s=t(166),i=(a=/[^.]+$/.exec(s&&s.keys&&s.keys.IE_PROTO||""))?"Symbol(src)_1."+a:"";n.exports=function(n){return!!i&&i in n}},function(n,e,t){var a=t(8)["__core-js_shared__"];n.exports=a},function(n,e){n.exports=function(n,e){return null==n?void 0:n[e]}},function(n,e,t){var a=t(169),s=t(21),i=t(39);n.exports=function(){this.size=0,this.__data__={hash:new a,map:new(i||s),string:new a}}},function(n,e,t){var a=t(170),s=t(171),i=t(172),r=t(173),o=t(174);function l(n){var e=-1,t=null==n?0:n.length;for(this.clear();++e<t;){var a=n[e];this.set(a[0],a[1])}}l.prototype.clear=a,l.prototype.delete=s,l.prototype.get=i,l.prototype.has=r,l.prototype.set=o,n.exports=l},function(n,e,t){var a=t(23);n.exports=function(){this.__data__=a?a(null):{},this.size=0}},function(n,e){n.exports=function(n){var e=this.has(n)&&delete this.__data__[n];return this.size-=e?1:0,e}},function(n,e,t){var a=t(23),s=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;if(a){var t=e[n];return"__lodash_hash_undefined__"===t?void 0:t}return s.call(e,n)?e[n]:void 0}},function(n,e,t){var a=t(23),s=Object.prototype.hasOwnProperty;n.exports=function(n){var e=this.__data__;return a?void 0!==e[n]:s.call(e,n)}},function(n,e,t){var a=t(23);n.exports=function(n,e){var t=this.__data__;return this.size+=this.has(n)?0:1,t[n]=a&&void 0===e?"__lodash_hash_undefined__":e,this}},function(n,e,t){var a=t(24);n.exports=function(n){var e=a(this,n).delete(n);return this.size-=e?1:0,e}},function(n,e){n.exports=function(n){var e=typeof n;return"string"==e||"number"==e||"symbol"==e||"boolean"==e?"__proto__"!==n:null===n}},function(n,e,t){var a=t(24);n.exports=function(n){return a(this,n).get(n)}},function(n,e,t){var a=t(24);n.exports=function(n){return a(this,n).has(n)}},function(n,e,t){var a=t(24);n.exports=function(n,e){var t=a(this,n),s=t.size;return t.set(n,e),this.size+=t.size==s?0:1,this}},function(n,e,t){var a=t(71),s=t(76),i=t(184),r=t(187),o=t(203),l=t(6),c=t(80),d=t(82),u="[object Object]",m=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,p,h,g){var f=l(n),b=l(e),y=f?"[object Array]":o(n),v=b?"[object Array]":o(e),_=(y="[object Arguments]"==y?u:y)==u,E=(v="[object Arguments]"==v?u:v)==u,x=y==v;if(x&&c(n)){if(!c(e))return!1;f=!0,_=!1}if(x&&!_)return g||(g=new a),f||d(n)?s(n,e,t,p,h,g):i(n,e,y,t,p,h,g);if(!(1&t)){var T=_&&m.call(n,"__wrapped__"),k=E&&m.call(e,"__wrapped__");if(T||k){var I=T?n.value():n,S=k?e.value():e;return g||(g=new a),h(I,S,t,p,g)}}return!!x&&(g||(g=new a),r(n,e,t,p,h,g))}},function(n,e){n.exports=function(n){return this.__data__.set(n,"__lodash_hash_undefined__"),this}},function(n,e){n.exports=function(n){return this.__data__.has(n)}},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length;++t<a;)if(e(n[t],t,n))return!0;return!1}},function(n,e,t){var a=t(16),s=t(185),i=t(72),r=t(76),o=t(186),l=t(42),c=a?a.prototype:void 0,d=c?c.valueOf:void 0;n.exports=function(n,e,t,a,c,u,m){switch(t){case"[object DataView]":if(n.byteLength!=e.byteLength||n.byteOffset!=e.byteOffset)return!1;n=n.buffer,e=e.buffer;case"[object ArrayBuffer]":return!(n.byteLength!=e.byteLength||!u(new s(n),new s(e)));case"[object Boolean]":case"[object Date]":case"[object Number]":return i(+n,+e);case"[object Error]":return n.name==e.name&&n.message==e.message;case"[object RegExp]":case"[object String]":return n==e+"";case"[object Map]":var p=o;case"[object Set]":var h=1&a;if(p||(p=l),n.size!=e.size&&!h)return!1;var g=m.get(n);if(g)return g==e;a|=2,m.set(n,e);var f=r(p(n),p(e),a,c,u,m);return m.delete(n),f;case"[object Symbol]":if(d)return d.call(n)==d.call(e)}return!1}},function(n,e,t){var a=t(8).Uint8Array;n.exports=a},function(n,e){n.exports=function(n){var e=-1,t=Array(n.size);return n.forEach((function(n,a){t[++e]=[a,n]})),t}},function(n,e,t){var a=t(188),s=Object.prototype.hasOwnProperty;n.exports=function(n,e,t,i,r,o){var l=1&t,c=a(n),d=c.length;if(d!=a(e).length&&!l)return!1;for(var u=d;u--;){var m=c[u];if(!(l?m in e:s.call(e,m)))return!1}var p=o.get(n),h=o.get(e);if(p&&h)return p==e&&h==n;var g=!0;o.set(n,e),o.set(e,n);for(var f=l;++u<d;){var b=n[m=c[u]],y=e[m];if(i)var v=l?i(y,b,m,e,n,o):i(b,y,m,n,e,o);if(!(void 0===v?b===y||r(b,y,t,i,o):v)){g=!1;break}f||(f="constructor"==m)}if(g&&!f){var _=n.constructor,E=e.constructor;_==E||!("constructor"in n)||!("constructor"in e)||"function"==typeof _&&_ instanceof _&&"function"==typeof E&&E instanceof E||(g=!1)}return o.delete(n),o.delete(e),g}},function(n,e,t){var a=t(189),s=t(190),i=t(79);n.exports=function(n){return a(n,i,s)}},function(n,e,t){var a=t(69),s=t(6);n.exports=function(n,e,t){var i=e(n);return s(n)?i:a(i,t(n))}},function(n,e,t){var a=t(191),s=t(192),i=Object.prototype.propertyIsEnumerable,r=Object.getOwnPropertySymbols,o=r?function(n){return null==n?[]:(n=Object(n),a(r(n),(function(e){return i.call(n,e)})))}:s;n.exports=o},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length,s=0,i=[];++t<a;){var r=n[t];e(r,t,n)&&(i[s++]=r)}return i}},function(n,e){n.exports=function(){return[]}},function(n,e,t){var a=t(194),s=t(38),i=t(6),r=t(80),o=t(81),l=t(82),c=Object.prototype.hasOwnProperty;n.exports=function(n,e){var t=i(n),d=!t&&s(n),u=!t&&!d&&r(n),m=!t&&!d&&!u&&l(n),p=t||d||u||m,h=p?a(n.length,String):[],g=h.length;for(var f in n)!e&&!c.call(n,f)||p&&("length"==f||u&&("offset"==f||"parent"==f)||m&&("buffer"==f||"byteLength"==f||"byteOffset"==f)||o(f,g))||h.push(f);return h}},function(n,e){n.exports=function(n,e){for(var t=-1,a=Array(n);++t<n;)a[t]=e(t);return a}},function(n,e){n.exports=function(){return!1}},function(n,e,t){var a=t(14),s=t(43),i=t(12),r={};r["[object Float32Array]"]=r["[object Float64Array]"]=r["[object Int8Array]"]=r["[object Int16Array]"]=r["[object Int32Array]"]=r["[object Uint8Array]"]=r["[object Uint8ClampedArray]"]=r["[object Uint16Array]"]=r["[object Uint32Array]"]=!0,r["[object Arguments]"]=r["[object Array]"]=r["[object ArrayBuffer]"]=r["[object Boolean]"]=r["[object DataView]"]=r["[object Date]"]=r["[object Error]"]=r["[object Function]"]=r["[object Map]"]=r["[object Number]"]=r["[object Object]"]=r["[object RegExp]"]=r["[object Set]"]=r["[object String]"]=r["[object WeakMap]"]=!1,n.exports=function(n){return i(n)&&s(n.length)&&!!r[a(n)]}},function(n,e){n.exports=function(n){return function(e){return n(e)}}},function(n,e,t){(function(n){var a=t(70),s=e&&!e.nodeType&&e,i=s&&"object"==typeof n&&n&&!n.nodeType&&n,r=i&&i.exports===s&&a.process,o=function(){try{var n=i&&i.require&&i.require("util").types;return n||r&&r.binding&&r.binding("util")}catch(n){}}();n.exports=o}).call(this,t(50)(n))},function(n,e,t){var a=t(200),s=t(201),i=Object.prototype.hasOwnProperty;n.exports=function(n){if(!a(n))return s(n);var e=[];for(var t in Object(n))i.call(n,t)&&"constructor"!=t&&e.push(t);return e}},function(n,e){var t=Object.prototype;n.exports=function(n){var e=n&&n.constructor;return n===("function"==typeof e&&e.prototype||t)}},function(n,e,t){var a=t(202)(Object.keys,Object);n.exports=a},function(n,e){n.exports=function(n,e){return function(t){return n(e(t))}}},function(n,e,t){var a=t(204),s=t(39),i=t(205),r=t(84),o=t(206),l=t(14),c=t(74),d=c(a),u=c(s),m=c(i),p=c(r),h=c(o),g=l;(a&&"[object DataView]"!=g(new a(new ArrayBuffer(1)))||s&&"[object Map]"!=g(new s)||i&&"[object Promise]"!=g(i.resolve())||r&&"[object Set]"!=g(new r)||o&&"[object WeakMap]"!=g(new o))&&(g=function(n){var e=l(n),t="[object Object]"==e?n.constructor:void 0,a=t?c(t):"";if(a)switch(a){case d:return"[object DataView]";case u:return"[object Map]";case m:return"[object Promise]";case p:return"[object Set]";case h:return"[object WeakMap]"}return e}),n.exports=g},function(n,e,t){var a=t(10)(t(8),"DataView");n.exports=a},function(n,e,t){var a=t(10)(t(8),"Promise");n.exports=a},function(n,e,t){var a=t(10)(t(8),"WeakMap");n.exports=a},function(n,e,t){var a=t(85),s=t(79);n.exports=function(n){for(var e=s(n),t=e.length;t--;){var i=e[t],r=n[i];e[t]=[i,r,a(r)]}return e}},function(n,e,t){var a=t(75),s=t(209),i=t(216),r=t(44),o=t(85),l=t(86),c=t(25);n.exports=function(n,e){return r(n)&&o(e)?l(c(n),e):function(t){var r=s(t,n);return void 0===r&&r===e?i(t,n):a(e,r,3)}}},function(n,e,t){var a=t(87);n.exports=function(n,e,t){var s=null==n?void 0:a(n,e);return void 0===s?t:s}},function(n,e,t){var a=t(211),s=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,i=/\\(\\)?/g,r=a((function(n){var e=[];return 46===n.charCodeAt(0)&&e.push(""),n.replace(s,(function(n,t,a,s){e.push(a?s.replace(i,"$1"):t||n)})),e}));n.exports=r},function(n,e,t){var a=t(212);n.exports=function(n){var e=a(n,(function(n){return 500===t.size&&t.clear(),n})),t=e.cache;return e}},function(n,e,t){var a=t(41);function s(n,e){if("function"!=typeof n||null!=e&&"function"!=typeof e)throw new TypeError("Expected a function");var t=function(){var a=arguments,s=e?e.apply(this,a):a[0],i=t.cache;if(i.has(s))return i.get(s);var r=n.apply(this,a);return t.cache=i.set(s,r)||i,r};return t.cache=new(s.Cache||a),t}s.Cache=a,n.exports=s},function(n,e,t){var a=t(214);n.exports=function(n){return null==n?"":a(n)}},function(n,e,t){var a=t(16),s=t(215),i=t(6),r=t(45),o=a?a.prototype:void 0,l=o?o.toString:void 0;n.exports=function n(e){if("string"==typeof e)return e;if(i(e))return s(e,n)+"";if(r(e))return l?l.call(e):"";var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(n,e){n.exports=function(n,e){for(var t=-1,a=null==n?0:n.length,s=Array(a);++t<a;)s[t]=e(n[t],t,n);return s}},function(n,e,t){var a=t(217),s=t(218);n.exports=function(n,e){return null!=n&&s(n,e,a)}},function(n,e){n.exports=function(n,e){return null!=n&&e in Object(n)}},function(n,e,t){var a=t(88),s=t(38),i=t(6),r=t(81),o=t(43),l=t(25);n.exports=function(n,e,t){for(var c=-1,d=(e=a(e,n)).length,u=!1;++c<d;){var m=l(e[c]);if(!(u=null!=n&&t(n,m)))break;n=n[m]}return u||++c!=d?u:!!(d=null==n?0:n.length)&&o(d)&&r(m,d)&&(i(n)||s(n))}},function(n,e,t){var a=t(220),s=t(221),i=t(44),r=t(25);n.exports=function(n){return i(n)?a(r(n)):s(n)}},function(n,e){n.exports=function(n){return function(e){return null==e?void 0:e[n]}}},function(n,e,t){var a=t(87);n.exports=function(n){return function(e){return a(e,n)}}},function(n,e,t){var a=t(46),s=t(223),i=t(225);n.exports=function(n,e){return i(s(n,e,a),n+"")}},function(n,e,t){var a=t(224),s=Math.max;n.exports=function(n,e,t){return e=s(void 0===e?n.length-1:e,0),function(){for(var i=arguments,r=-1,o=s(i.length-e,0),l=Array(o);++r<o;)l[r]=i[e+r];r=-1;for(var c=Array(e+1);++r<e;)c[r]=i[r];return c[e]=t(l),a(n,this,c)}}},function(n,e){n.exports=function(n,e,t){switch(t.length){case 0:return n.call(e);case 1:return n.call(e,t[0]);case 2:return n.call(e,t[0],t[1]);case 3:return n.call(e,t[0],t[1],t[2])}return n.apply(e,t)}},function(n,e,t){var a=t(226),s=t(229)(a);n.exports=s},function(n,e,t){var a=t(227),s=t(228),i=t(46),r=s?function(n,e){return s(n,"toString",{configurable:!0,enumerable:!1,value:a(e),writable:!0})}:i;n.exports=r},function(n,e){n.exports=function(n){return function(){return n}}},function(n,e,t){var a=t(10),s=function(){try{var n=a(Object,"defineProperty");return n({},"",{}),n}catch(n){}}();n.exports=s},function(n,e){var t=Date.now;n.exports=function(n){var e=0,a=0;return function(){var s=t(),i=16-(s-a);if(a=s,i>0){if(++e>=800)return arguments[0]}else e=0;return n.apply(void 0,arguments)}}},function(n,e,t){var a=t(77),s=t(231),i=t(236),r=t(78),o=t(237),l=t(42);n.exports=function(n,e,t){var c=-1,d=s,u=n.length,m=!0,p=[],h=p;if(t)m=!1,d=i;else if(u>=200){var g=e?null:o(n);if(g)return l(g);m=!1,d=r,h=new a}else h=e?[]:p;n:for(;++c<u;){var f=n[c],b=e?e(f):f;if(f=t||0!==f?f:0,m&&b==b){for(var y=h.length;y--;)if(h[y]===b)continue n;e&&h.push(b),p.push(f)}else d(h,b,t)||(h!==p&&h.push(b),p.push(f))}return p}},function(n,e,t){var a=t(232);n.exports=function(n,e){return!!(null==n?0:n.length)&&a(n,e,0)>-1}},function(n,e,t){var a=t(233),s=t(234),i=t(235);n.exports=function(n,e,t){return e==e?i(n,e,t):a(n,s,t)}},function(n,e){n.exports=function(n,e,t,a){for(var s=n.length,i=t+(a?1:-1);a?i--:++i<s;)if(e(n[i],i,n))return i;return-1}},function(n,e){n.exports=function(n){return n!=n}},function(n,e){n.exports=function(n,e,t){for(var a=t-1,s=n.length;++a<s;)if(n[a]===e)return a;return-1}},function(n,e){n.exports=function(n,e,t){for(var a=-1,s=null==n?0:n.length;++a<s;)if(t(e,n[a]))return!0;return!1}},function(n,e,t){var a=t(84),s=t(238),i=t(42),r=a&&1/i(new a([,-0]))[1]==1/0?function(n){return new a(n)}:s;n.exports=r},function(n,e){n.exports=function(){}},function(n,e,t){var a=t(83),s=t(12);n.exports=function(n){return s(n)&&a(n)}},function(n,e,t){},function(n,e,t){},function(n,e,t){"use strict";t(89)},function(n,e,t){"use strict";t(90)},function(n,e,t){},function(n,e,t){},function(n,e,t){var a=t(18),s=t(2),i=t(247);a({global:!0},{Reflect:{}}),i(s.Reflect,"Reflect",!0)},function(n,e,t){var a=t(13).f,s=t(7),i=t(20)("toStringTag");n.exports=function(n,e,t){n&&!t&&(n=n.prototype),n&&!s(n,i)&&a(n,i,{configurable:!0,value:e})}},function(n,e){},function(n,e){function t(n,e){for(var t=0,a=n.length-1;a>=0;a--){var s=n[a];"."===s?n.splice(a,1):".."===s?(n.splice(a,1),t++):t&&(n.splice(a,1),t--)}if(e)for(;t--;t)n.unshift("..");return n}function a(n,e){if(n.filter)return n.filter(e);for(var t=[],a=0;a<n.length;a++)e(n[a],a,n)&&t.push(n[a]);return t}e.resolve=function(){for(var n="",e=!1,s=arguments.length-1;s>=-1&&!e;s--){var i=s>=0?arguments[s]:process.cwd();if("string"!=typeof i)throw new TypeError("Arguments to path.resolve must be strings");i&&(n=i+"/"+n,e="/"===i.charAt(0))}return(e?"/":"")+(n=t(a(n.split("/"),(function(n){return!!n})),!e).join("/"))||"."},e.normalize=function(n){var i=e.isAbsolute(n),r="/"===s(n,-1);return(n=t(a(n.split("/"),(function(n){return!!n})),!i).join("/"))||i||(n="."),n&&r&&(n+="/"),(i?"/":"")+n},e.isAbsolute=function(n){return"/"===n.charAt(0)},e.join=function(){var n=Array.prototype.slice.call(arguments,0);return e.normalize(a(n,(function(n,e){if("string"!=typeof n)throw new TypeError("Arguments to path.join must be strings");return n})).join("/"))},e.relative=function(n,t){function a(n){for(var e=0;e<n.length&&""===n[e];e++);for(var t=n.length-1;t>=0&&""===n[t];t--);return e>t?[]:n.slice(e,t-e+1)}n=e.resolve(n).substr(1),t=e.resolve(t).substr(1);for(var s=a(n.split("/")),i=a(t.split("/")),r=Math.min(s.length,i.length),o=r,l=0;l<r;l++)if(s[l]!==i[l]){o=l;break}var c=[];for(l=o;l<s.length;l++)c.push("..");return(c=c.concat(i.slice(o))).join("/")},e.sep="/",e.delimiter=":",e.dirname=function(n){if("string"!=typeof n&&(n+=""),0===n.length)return".";for(var e=n.charCodeAt(0),t=47===e,a=-1,s=!0,i=n.length-1;i>=1;--i)if(47===(e=n.charCodeAt(i))){if(!s){a=i;break}}else s=!1;return-1===a?t?"/":".":t&&1===a?"/":n.slice(0,a)},e.basename=function(n,e){var t=function(n){"string"!=typeof n&&(n+="");var e,t=0,a=-1,s=!0;for(e=n.length-1;e>=0;--e)if(47===n.charCodeAt(e)){if(!s){t=e+1;break}}else-1===a&&(s=!1,a=e+1);return-1===a?"":n.slice(t,a)}(n);return e&&t.substr(-1*e.length)===e&&(t=t.substr(0,t.length-e.length)),t},e.extname=function(n){"string"!=typeof n&&(n+="");for(var e=-1,t=0,a=-1,s=!0,i=0,r=n.length-1;r>=0;--r){var o=n.charCodeAt(r);if(47!==o)-1===a&&(s=!1,a=r+1),46===o?-1===e?e=r:1!==i&&(i=1):-1!==e&&(i=-1);else if(!s){t=r+1;break}}return-1===e||-1===a||0===i||1===i&&e===a-1&&e===t+1?"":n.slice(e,a)};var s="b"==="ab".substr(-1)?function(n,e,t){return n.substr(e,t)}:function(n,e,t){return e<0&&(e=n.length+e),n.substr(e,t)}},function(n,e,t){"use strict";var a=/[|\\{}()[\]^$+*?.]/g;e.escapeRegExpChars=function(n){return n?String(n).replace(a,"\\$&"):""};var s={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&#34;","'":"&#39;"},i=/[&<>'"]/g;function r(n){return s[n]||n}e.escapeXML=function(n){return null==n?"":String(n).replace(i,r)},e.escapeXML.toString=function(){return Function.prototype.toString.call(this)+';\nvar _ENCODE_HTML_RULES = {\n      "&": "&amp;"\n    , "<": "&lt;"\n    , ">": "&gt;"\n    , \'"\': "&#34;"\n    , "\'": "&#39;"\n    }\n  , _MATCH_HTML = /[&<>\'"]/g;\nfunction encode_char(c) {\n  return _ENCODE_HTML_RULES[c] || c;\n};\n'},e.shallowCopy=function(n,e){for(var t in e=e||{})n[t]=e[t];return n},e.shallowCopyFromList=function(n,e,t){for(var a=0;a<t.length;a++){var s=t[a];void 0!==e[s]&&(n[s]=e[s])}return n},e.cache={_data:{},set:function(n,e){this._data[n]=e},get:function(n){return this._data[n]},remove:function(n){delete this._data[n]},reset:function(){this._data={}}}},function(n){n.exports=JSON.parse('{"_from":"ejs@^2.6.1","_id":"ejs@2.7.4","_inBundle":false,"_integrity":"sha512-7vmuyh5+kuUyJKePhQfRQBhXV5Ce+RnaeeQArKu1EAMpL3WbgMt5WG6uQZpEVvYSSsxMXRKOewtDk9RaTKXRlA==","_location":"/ejs","_phantomChildren":{},"_requested":{"type":"range","registry":true,"raw":"ejs@^2.6.1","name":"ejs","escapedName":"ejs","rawSpec":"^2.6.1","saveSpec":null,"fetchSpec":"^2.6.1"},"_requiredBy":["/vuepress-plugin-comment"],"_resolved":"https://registry.npmjs.org/ejs/-/ejs-2.7.4.tgz","_shasum":"48661287573dcc53e366c7a1ae52c3a120eec9ba","_spec":"ejs@^2.6.1","_where":"/home/runner/work/db-tutorial/db-tutorial/node_modules/vuepress-plugin-comment","author":{"name":"Matthew Eernisse","email":"mde@fleegix.org","url":"http://fleegix.org"},"bugs":{"url":"https://github.com/mde/ejs/issues"},"bundleDependencies":false,"dependencies":{},"deprecated":false,"description":"Embedded JavaScript templates","devDependencies":{"browserify":"^13.1.1","eslint":"^4.14.0","git-directory-deploy":"^1.5.1","jake":"^10.3.1","jsdoc":"^3.4.0","lru-cache":"^4.0.1","mocha":"^5.0.5","uglify-js":"^3.3.16"},"engines":{"node":">=0.10.0"},"homepage":"https://github.com/mde/ejs","keywords":["template","engine","ejs"],"license":"Apache-2.0","main":"./lib/ejs.js","name":"ejs","repository":{"type":"git","url":"git://github.com/mde/ejs.git"},"scripts":{"postinstall":"node ./postinstall.js","test":"mocha"},"version":"2.7.4"}')},function(n,e,t){"use strict";t(91)},function(n,e,t){"use strict";t(92)},function(n,e,t){"use strict";t.r(e);
/*!
 * Vue.js v2.7.14
 * (c) 2014-2022 Evan You
 * Released under the MIT License.
 */
var a=Object.freeze({}),s=Array.isArray;function i(n){return null==n}function r(n){return null!=n}function o(n){return!0===n}function l(n){return"string"==typeof n||"number"==typeof n||"symbol"==typeof n||"boolean"==typeof n}function c(n){return"function"==typeof n}function d(n){return null!==n&&"object"==typeof n}var u=Object.prototype.toString;function m(n){return"[object Object]"===u.call(n)}function p(n){return"[object RegExp]"===u.call(n)}function h(n){var e=parseFloat(String(n));return e>=0&&Math.floor(e)===e&&isFinite(n)}function g(n){return r(n)&&"function"==typeof n.then&&"function"==typeof n.catch}function f(n){return null==n?"":Array.isArray(n)||m(n)&&n.toString===u?JSON.stringify(n,null,2):String(n)}function b(n){var e=parseFloat(n);return isNaN(e)?n:e}function y(n,e){for(var t=Object.create(null),a=n.split(","),s=0;s<a.length;s++)t[a[s]]=!0;return e?function(n){return t[n.toLowerCase()]}:function(n){return t[n]}}y("slot,component",!0);var v=y("key,ref,slot,slot-scope,is");function _(n,e){var t=n.length;if(t){if(e===n[t-1])return void(n.length=t-1);var a=n.indexOf(e);if(a>-1)return n.splice(a,1)}}var E=Object.prototype.hasOwnProperty;function x(n,e){return E.call(n,e)}function T(n){var e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}var k=/-(\w)/g,I=T((function(n){return n.replace(k,(function(n,e){return e?e.toUpperCase():""}))})),S=T((function(n){return n.charAt(0).toUpperCase()+n.slice(1)})),q=/\B([A-Z])/g,w=T((function(n){return n.replace(q,"-$1").toLowerCase()}));var R=Function.prototype.bind?function(n,e){return n.bind(e)}:function(n,e){function t(t){var a=arguments.length;return a?a>1?n.apply(e,arguments):n.call(e,t):n.call(e)}return t._length=n.length,t};function A(n,e){e=e||0;for(var t=n.length-e,a=new Array(t);t--;)a[t]=n[t+e];return a}function z(n,e){for(var t in e)n[t]=e[t];return n}function D(n){for(var e={},t=0;t<n.length;t++)n[t]&&z(e,n[t]);return e}function L(n,e,t){}var B=function(n,e,t){return!1},M=function(n){return n};function O(n,e){if(n===e)return!0;var t=d(n),a=d(e);if(!t||!a)return!t&&!a&&String(n)===String(e);try{var s=Array.isArray(n),i=Array.isArray(e);if(s&&i)return n.length===e.length&&n.every((function(n,t){return O(n,e[t])}));if(n instanceof Date&&e instanceof Date)return n.getTime()===e.getTime();if(s||i)return!1;var r=Object.keys(n),o=Object.keys(e);return r.length===o.length&&r.every((function(t){return O(n[t],e[t])}))}catch(n){return!1}}function C(n,e){for(var t=0;t<n.length;t++)if(O(n[t],e))return t;return-1}function N(n){var e=!1;return function(){e||(e=!0,n.apply(this,arguments))}}function P(n,e){return n===e?0===n&&1/n!=1/e:n==n||e==e}var j=["component","directive","filter"],U=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch","renderTracked","renderTriggered"],F={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:B,isReservedAttr:B,isUnknownElement:B,getTagNamespace:L,parsePlatformTagName:M,mustUseProp:B,async:!0,_lifecycleHooks:U},$=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function Q(n){var e=(n+"").charCodeAt(0);return 36===e||95===e}function H(n,e,t,a){Object.defineProperty(n,e,{value:t,enumerable:!!a,writable:!0,configurable:!0})}var G=new RegExp("[^".concat($.source,".$_\\d]"));var V="__proto__"in{},K="undefined"!=typeof window,W=K&&window.navigator.userAgent.toLowerCase(),J=W&&/msie|trident/.test(W),X=W&&W.indexOf("msie 9.0")>0,Y=W&&W.indexOf("edge/")>0;W&&W.indexOf("android");var Z=W&&/iphone|ipad|ipod|ios/.test(W);W&&/chrome\/\d+/.test(W),W&&/phantomjs/.test(W);var nn,en=W&&W.match(/firefox\/(\d+)/),tn={}.watch,an=!1;if(K)try{var sn={};Object.defineProperty(sn,"passive",{get:function(){an=!0}}),window.addEventListener("test-passive",null,sn)}catch(n){}var rn=function(){return void 0===nn&&(nn=!K&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),nn},on=K&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ln(n){return"function"==typeof n&&/native code/.test(n.toString())}var cn,dn="undefined"!=typeof Symbol&&ln(Symbol)&&"undefined"!=typeof Reflect&&ln(Reflect.ownKeys);cn="undefined"!=typeof Set&&ln(Set)?Set:function(){function n(){this.set=Object.create(null)}return n.prototype.has=function(n){return!0===this.set[n]},n.prototype.add=function(n){this.set[n]=!0},n.prototype.clear=function(){this.set=Object.create(null)},n}();var un=null;function mn(n){void 0===n&&(n=null),n||un&&un._scope.off(),un=n,n&&n._scope.on()}var pn=function(){function n(n,e,t,a,s,i,r,o){this.tag=n,this.data=e,this.children=t,this.text=a,this.elm=s,this.ns=void 0,this.context=i,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=e&&e.key,this.componentOptions=r,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=o,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1}return Object.defineProperty(n.prototype,"child",{get:function(){return this.componentInstance},enumerable:!1,configurable:!0}),n}(),hn=function(n){void 0===n&&(n="");var e=new pn;return e.text=n,e.isComment=!0,e};function gn(n){return new pn(void 0,void 0,void 0,String(n))}function fn(n){var e=new pn(n.tag,n.data,n.children&&n.children.slice(),n.text,n.elm,n.context,n.componentOptions,n.asyncFactory);return e.ns=n.ns,e.isStatic=n.isStatic,e.key=n.key,e.isComment=n.isComment,e.fnContext=n.fnContext,e.fnOptions=n.fnOptions,e.fnScopeId=n.fnScopeId,e.asyncMeta=n.asyncMeta,e.isCloned=!0,e}var bn=0,yn=[],vn=function(){function n(){this._pending=!1,this.id=bn++,this.subs=[]}return n.prototype.addSub=function(n){this.subs.push(n)},n.prototype.removeSub=function(n){this.subs[this.subs.indexOf(n)]=null,this._pending||(this._pending=!0,yn.push(this))},n.prototype.depend=function(e){n.target&&n.target.addDep(this)},n.prototype.notify=function(n){var e=this.subs.filter((function(n){return n}));for(var t=0,a=e.length;t<a;t++){0,e[t].update()}},n}();vn.target=null;var _n=[];function En(n){_n.push(n),vn.target=n}function xn(){_n.pop(),vn.target=_n[_n.length-1]}var Tn=Array.prototype,kn=Object.create(Tn);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(n){var e=Tn[n];H(kn,n,(function(){for(var t=[],a=0;a<arguments.length;a++)t[a]=arguments[a];var s,i=e.apply(this,t),r=this.__ob__;switch(n){case"push":case"unshift":s=t;break;case"splice":s=t.slice(2)}return s&&r.observeArray(s),r.dep.notify(),i}))}));var In=Object.getOwnPropertyNames(kn),Sn={},qn=!0;function wn(n){qn=n}var Rn={notify:L,depend:L,addSub:L,removeSub:L},An=function(){function n(n,e,t){if(void 0===e&&(e=!1),void 0===t&&(t=!1),this.value=n,this.shallow=e,this.mock=t,this.dep=t?Rn:new vn,this.vmCount=0,H(n,"__ob__",this),s(n)){if(!t)if(V)n.__proto__=kn;else for(var a=0,i=In.length;a<i;a++){H(n,o=In[a],kn[o])}e||this.observeArray(n)}else{var r=Object.keys(n);for(a=0;a<r.length;a++){var o;Dn(n,o=r[a],Sn,void 0,e,t)}}}return n.prototype.observeArray=function(n){for(var e=0,t=n.length;e<t;e++)zn(n[e],!1,this.mock)},n}();function zn(n,e,t){return n&&x(n,"__ob__")&&n.__ob__ instanceof An?n.__ob__:!qn||!t&&rn()||!s(n)&&!m(n)||!Object.isExtensible(n)||n.__v_skip||Pn(n)||n instanceof pn?void 0:new An(n,e,t)}function Dn(n,e,t,a,i,r){var o=new vn,l=Object.getOwnPropertyDescriptor(n,e);if(!l||!1!==l.configurable){var c=l&&l.get,d=l&&l.set;c&&!d||t!==Sn&&2!==arguments.length||(t=n[e]);var u=!i&&zn(t,!1,r);return Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){var e=c?c.call(n):t;return vn.target&&(o.depend(),u&&(u.dep.depend(),s(e)&&Mn(e))),Pn(e)&&!i?e.value:e},set:function(e){var a=c?c.call(n):t;if(P(a,e)){if(d)d.call(n,e);else{if(c)return;if(!i&&Pn(a)&&!Pn(e))return void(a.value=e);t=e}u=!i&&zn(e,!1,r),o.notify()}}}),o}}function Ln(n,e,t){if(!Nn(n)){var a=n.__ob__;return s(n)&&h(e)?(n.length=Math.max(n.length,e),n.splice(e,1,t),a&&!a.shallow&&a.mock&&zn(t,!1,!0),t):e in n&&!(e in Object.prototype)?(n[e]=t,t):n._isVue||a&&a.vmCount?t:a?(Dn(a.value,e,t,void 0,a.shallow,a.mock),a.dep.notify(),t):(n[e]=t,t)}}function Bn(n,e){if(s(n)&&h(e))n.splice(e,1);else{var t=n.__ob__;n._isVue||t&&t.vmCount||Nn(n)||x(n,e)&&(delete n[e],t&&t.dep.notify())}}function Mn(n){for(var e=void 0,t=0,a=n.length;t<a;t++)(e=n[t])&&e.__ob__&&e.__ob__.dep.depend(),s(e)&&Mn(e)}function On(n){return Cn(n,!0),H(n,"__v_isShallow",!0),n}function Cn(n,e){if(!Nn(n)){zn(n,e,rn());0}}function Nn(n){return!(!n||!n.__v_isReadonly)}function Pn(n){return!(!n||!0!==n.__v_isRef)}function jn(n,e,t){Object.defineProperty(n,t,{enumerable:!0,configurable:!0,get:function(){var n=e[t];if(Pn(n))return n.value;var a=n&&n.__ob__;return a&&a.dep.depend(),n},set:function(n){var a=e[t];Pn(a)&&!Pn(n)?a.value=n:e[t]=n}})}"".concat("watcher"," callback"),"".concat("watcher"," getter"),"".concat("watcher"," cleanup");var Un;var Fn=function(){function n(n){void 0===n&&(n=!1),this.detached=n,this.active=!0,this.effects=[],this.cleanups=[],this.parent=Un,!n&&Un&&(this.index=(Un.scopes||(Un.scopes=[])).push(this)-1)}return n.prototype.run=function(n){if(this.active){var e=Un;try{return Un=this,n()}finally{Un=e}}else 0},n.prototype.on=function(){Un=this},n.prototype.off=function(){Un=this.parent},n.prototype.stop=function(n){if(this.active){var e=void 0,t=void 0;for(e=0,t=this.effects.length;e<t;e++)this.effects[e].teardown();for(e=0,t=this.cleanups.length;e<t;e++)this.cleanups[e]();if(this.scopes)for(e=0,t=this.scopes.length;e<t;e++)this.scopes[e].stop(!0);if(!this.detached&&this.parent&&!n){var a=this.parent.scopes.pop();a&&a!==this&&(this.parent.scopes[this.index]=a,a.index=this.index)}this.parent=void 0,this.active=!1}},n}();function $n(n){var e=n._provided,t=n.$parent&&n.$parent._provided;return t===e?n._provided=Object.create(t):e}var Qn=T((function(n){var e="&"===n.charAt(0),t="~"===(n=e?n.slice(1):n).charAt(0),a="!"===(n=t?n.slice(1):n).charAt(0);return{name:n=a?n.slice(1):n,once:t,capture:a,passive:e}}));function Hn(n,e){function t(){var n=t.fns;if(!s(n))return qe(n,null,arguments,e,"v-on handler");for(var a=n.slice(),i=0;i<a.length;i++)qe(a[i],null,arguments,e,"v-on handler")}return t.fns=n,t}function Gn(n,e,t,a,s,r){var l,c,d,u;for(l in n)c=n[l],d=e[l],u=Qn(l),i(c)||(i(d)?(i(c.fns)&&(c=n[l]=Hn(c,r)),o(u.once)&&(c=n[l]=s(u.name,c,u.capture)),t(u.name,c,u.capture,u.passive,u.params)):c!==d&&(d.fns=c,n[l]=d));for(l in e)i(n[l])&&a((u=Qn(l)).name,e[l],u.capture)}function Vn(n,e,t){var a;n instanceof pn&&(n=n.data.hook||(n.data.hook={}));var s=n[e];function l(){t.apply(this,arguments),_(a.fns,l)}i(s)?a=Hn([l]):r(s.fns)&&o(s.merged)?(a=s).fns.push(l):a=Hn([s,l]),a.merged=!0,n[e]=a}function Kn(n,e,t,a,s){if(r(e)){if(x(e,t))return n[t]=e[t],s||delete e[t],!0;if(x(e,a))return n[t]=e[a],s||delete e[a],!0}return!1}function Wn(n){return l(n)?[gn(n)]:s(n)?function n(e,t){var a,c,d,u,m=[];for(a=0;a<e.length;a++)i(c=e[a])||"boolean"==typeof c||(d=m.length-1,u=m[d],s(c)?c.length>0&&(Jn((c=n(c,"".concat(t||"","_").concat(a)))[0])&&Jn(u)&&(m[d]=gn(u.text+c[0].text),c.shift()),m.push.apply(m,c)):l(c)?Jn(u)?m[d]=gn(u.text+c):""!==c&&m.push(gn(c)):Jn(c)&&Jn(u)?m[d]=gn(u.text+c.text):(o(e._isVList)&&r(c.tag)&&i(c.key)&&r(t)&&(c.key="__vlist".concat(t,"_").concat(a,"__")),m.push(c)));return m}(n):void 0}function Jn(n){return r(n)&&r(n.text)&&!1===n.isComment}function Xn(n,e){var t,a,i,o,l=null;if(s(n)||"string"==typeof n)for(l=new Array(n.length),t=0,a=n.length;t<a;t++)l[t]=e(n[t],t);else if("number"==typeof n)for(l=new Array(n),t=0;t<n;t++)l[t]=e(t+1,t);else if(d(n))if(dn&&n[Symbol.iterator]){l=[];for(var c=n[Symbol.iterator](),u=c.next();!u.done;)l.push(e(u.value,l.length)),u=c.next()}else for(i=Object.keys(n),l=new Array(i.length),t=0,a=i.length;t<a;t++)o=i[t],l[t]=e(n[o],o,t);return r(l)||(l=[]),l._isVList=!0,l}function Yn(n,e,t,a){var s,i=this.$scopedSlots[n];i?(t=t||{},a&&(t=z(z({},a),t)),s=i(t)||(c(e)?e():e)):s=this.$slots[n]||(c(e)?e():e);var r=t&&t.slot;return r?this.$createElement("template",{slot:r},s):s}function Zn(n){return At(this.$options,"filters",n,!0)||M}function ne(n,e){return s(n)?-1===n.indexOf(e):n!==e}function ee(n,e,t,a,s){var i=F.keyCodes[e]||t;return s&&a&&!F.keyCodes[e]?ne(s,a):i?ne(i,n):a?w(a)!==e:void 0===n}function te(n,e,t,a,i){if(t)if(d(t)){s(t)&&(t=D(t));var r=void 0,o=function(s){if("class"===s||"style"===s||v(s))r=n;else{var o=n.attrs&&n.attrs.type;r=a||F.mustUseProp(e,o,s)?n.domProps||(n.domProps={}):n.attrs||(n.attrs={})}var l=I(s),c=w(s);l in r||c in r||(r[s]=t[s],i&&((n.on||(n.on={}))["update:".concat(s)]=function(n){t[s]=n}))};for(var l in t)o(l)}else;return n}function ae(n,e){var t=this._staticTrees||(this._staticTrees=[]),a=t[n];return a&&!e||ie(a=t[n]=this.$options.staticRenderFns[n].call(this._renderProxy,this._c,this),"__static__".concat(n),!1),a}function se(n,e,t){return ie(n,"__once__".concat(e).concat(t?"_".concat(t):""),!0),n}function ie(n,e,t){if(s(n))for(var a=0;a<n.length;a++)n[a]&&"string"!=typeof n[a]&&re(n[a],"".concat(e,"_").concat(a),t);else re(n,e,t)}function re(n,e,t){n.isStatic=!0,n.key=e,n.isOnce=t}function oe(n,e){if(e)if(m(e)){var t=n.on=n.on?z({},n.on):{};for(var a in e){var s=t[a],i=e[a];t[a]=s?[].concat(s,i):i}}else;return n}function le(n,e,t,a){e=e||{$stable:!t};for(var i=0;i<n.length;i++){var r=n[i];s(r)?le(r,e,t):r&&(r.proxy&&(r.fn.proxy=!0),e[r.key]=r.fn)}return a&&(e.$key=a),e}function ce(n,e){for(var t=0;t<e.length;t+=2){var a=e[t];"string"==typeof a&&a&&(n[e[t]]=e[t+1])}return n}function de(n,e){return"string"==typeof n?e+n:n}function ue(n){n._o=se,n._n=b,n._s=f,n._l=Xn,n._t=Yn,n._q=O,n._i=C,n._m=ae,n._f=Zn,n._k=ee,n._b=te,n._v=gn,n._e=hn,n._u=le,n._g=oe,n._d=ce,n._p=de}function me(n,e){if(!n||!n.length)return{};for(var t={},a=0,s=n.length;a<s;a++){var i=n[a],r=i.data;if(r&&r.attrs&&r.attrs.slot&&delete r.attrs.slot,i.context!==e&&i.fnContext!==e||!r||null==r.slot)(t.default||(t.default=[])).push(i);else{var o=r.slot,l=t[o]||(t[o]=[]);"template"===i.tag?l.push.apply(l,i.children||[]):l.push(i)}}for(var c in t)t[c].every(pe)&&delete t[c];return t}function pe(n){return n.isComment&&!n.asyncFactory||" "===n.text}function he(n){return n.isComment&&n.asyncFactory}function ge(n,e,t,s){var i,r=Object.keys(t).length>0,o=e?!!e.$stable:!r,l=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(o&&s&&s!==a&&l===s.$key&&!r&&!s.$hasNormal)return s;for(var c in i={},e)e[c]&&"$"!==c[0]&&(i[c]=fe(n,t,c,e[c]))}else i={};for(var d in t)d in i||(i[d]=be(t,d));return e&&Object.isExtensible(e)&&(e._normalized=i),H(i,"$stable",o),H(i,"$key",l),H(i,"$hasNormal",r),i}function fe(n,e,t,a){var i=function(){var e=un;mn(n);var t=arguments.length?a.apply(null,arguments):a({}),i=(t=t&&"object"==typeof t&&!s(t)?[t]:Wn(t))&&t[0];return mn(e),t&&(!i||1===t.length&&i.isComment&&!he(i))?void 0:t};return a.proxy&&Object.defineProperty(e,t,{get:i,enumerable:!0,configurable:!0}),i}function be(n,e){return function(){return n[e]}}function ye(n){return{get attrs(){if(!n._attrsProxy){var e=n._attrsProxy={};H(e,"_v_attr_proxy",!0),ve(e,n.$attrs,a,n,"$attrs")}return n._attrsProxy},get listeners(){n._listenersProxy||ve(n._listenersProxy={},n.$listeners,a,n,"$listeners");return n._listenersProxy},get slots(){return function(n){n._slotsProxy||Ee(n._slotsProxy={},n.$scopedSlots);return n._slotsProxy}(n)},emit:R(n.$emit,n),expose:function(e){e&&Object.keys(e).forEach((function(t){return jn(n,e,t)}))}}}function ve(n,e,t,a,s){var i=!1;for(var r in e)r in n?e[r]!==t[r]&&(i=!0):(i=!0,_e(n,r,a,s));for(var r in n)r in e||(i=!0,delete n[r]);return i}function _e(n,e,t,a){Object.defineProperty(n,e,{enumerable:!0,configurable:!0,get:function(){return t[a][e]}})}function Ee(n,e){for(var t in e)n[t]=e[t];for(var t in n)t in e||delete n[t]}var xe=null;function Te(n,e){return(n.__esModule||dn&&"Module"===n[Symbol.toStringTag])&&(n=n.default),d(n)?e.extend(n):n}function ke(n){if(s(n))for(var e=0;e<n.length;e++){var t=n[e];if(r(t)&&(r(t.componentOptions)||he(t)))return t}}function Ie(n,e,t,a,u,m){return(s(t)||l(t))&&(u=a,a=t,t=void 0),o(m)&&(u=2),function(n,e,t,a,l){if(r(t)&&r(t.__ob__))return hn();r(t)&&r(t.is)&&(e=t.is);if(!e)return hn();0;s(a)&&c(a[0])&&((t=t||{}).scopedSlots={default:a[0]},a.length=0);2===l?a=Wn(a):1===l&&(a=function(n){for(var e=0;e<n.length;e++)if(s(n[e]))return Array.prototype.concat.apply([],n);return n}(a));var u,m;if("string"==typeof e){var p=void 0;m=n.$vnode&&n.$vnode.ns||F.getTagNamespace(e),u=F.isReservedTag(e)?new pn(F.parsePlatformTagName(e),t,a,void 0,void 0,n):t&&t.pre||!r(p=At(n.$options,"components",e))?new pn(e,t,a,void 0,void 0,n):_t(p,t,n,a,e)}else u=_t(e,t,n,a);return s(u)?u:r(u)?(r(m)&&function n(e,t,a){e.ns=t,"foreignObject"===e.tag&&(t=void 0,a=!0);if(r(e.children))for(var s=0,l=e.children.length;s<l;s++){var c=e.children[s];r(c.tag)&&(i(c.ns)||o(a)&&"svg"!==c.tag)&&n(c,t,a)}}(u,m),r(t)&&function(n){d(n.style)&&Fe(n.style);d(n.class)&&Fe(n.class)}(t),u):hn()}(n,e,t,a,u)}function Se(n,e,t){En();try{if(e)for(var a=e;a=a.$parent;){var s=a.$options.errorCaptured;if(s)for(var i=0;i<s.length;i++)try{if(!1===s[i].call(a,n,e,t))return}catch(n){we(n,a,"errorCaptured hook")}}we(n,e,t)}finally{xn()}}function qe(n,e,t,a,s){var i;try{(i=t?n.apply(e,t):n.call(e))&&!i._isVue&&g(i)&&!i._handled&&(i.catch((function(n){return Se(n,a,s+" (Promise/async)")})),i._handled=!0)}catch(n){Se(n,a,s)}return i}function we(n,e,t){if(F.errorHandler)try{return F.errorHandler.call(null,n,e,t)}catch(e){e!==n&&Re(e,null,"config.errorHandler")}Re(n,e,t)}function Re(n,e,t){if(!K||"undefined"==typeof console)throw n;console.error(n)}var Ae,ze=!1,De=[],Le=!1;function Be(){Le=!1;var n=De.slice(0);De.length=0;for(var e=0;e<n.length;e++)n[e]()}if("undefined"!=typeof Promise&&ln(Promise)){var Me=Promise.resolve();Ae=function(){Me.then(Be),Z&&setTimeout(L)},ze=!0}else if(J||"undefined"==typeof MutationObserver||!ln(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Ae="undefined"!=typeof setImmediate&&ln(setImmediate)?function(){setImmediate(Be)}:function(){setTimeout(Be,0)};else{var Oe=1,Ce=new MutationObserver(Be),Ne=document.createTextNode(String(Oe));Ce.observe(Ne,{characterData:!0}),Ae=function(){Oe=(Oe+1)%2,Ne.data=String(Oe)},ze=!0}function Pe(n,e){var t;if(De.push((function(){if(n)try{n.call(e)}catch(n){Se(n,e,"nextTick")}else t&&t(e)})),Le||(Le=!0,Ae()),!n&&"undefined"!=typeof Promise)return new Promise((function(n){t=n}))}function je(n){return function(e,t){if(void 0===t&&(t=un),t)return function(n,e,t){var a=n.$options;a[e]=St(a[e],t)}(t,n,e)}}je("beforeMount"),je("mounted"),je("beforeUpdate"),je("updated"),je("beforeDestroy"),je("destroyed"),je("activated"),je("deactivated"),je("serverPrefetch"),je("renderTracked"),je("renderTriggered"),je("errorCaptured");var Ue=new cn;function Fe(n){return function n(e,t){var a,i,r=s(e);if(!r&&!d(e)||e.__v_skip||Object.isFrozen(e)||e instanceof pn)return;if(e.__ob__){var o=e.__ob__.dep.id;if(t.has(o))return;t.add(o)}if(r)for(a=e.length;a--;)n(e[a],t);else if(Pn(e))n(e.value,t);else for(i=Object.keys(e),a=i.length;a--;)n(e[i[a]],t)}(n,Ue),Ue.clear(),n}var $e,Qe=0,He=function(){function n(n,e,t,a,s){var i,r;i=this,void 0===(r=Un&&!Un._vm?Un:n?n._scope:void 0)&&(r=Un),r&&r.active&&r.effects.push(i),(this.vm=n)&&s&&(n._watcher=this),a?(this.deep=!!a.deep,this.user=!!a.user,this.lazy=!!a.lazy,this.sync=!!a.sync,this.before=a.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++Qe,this.active=!0,this.post=!1,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new cn,this.newDepIds=new cn,this.expression="",c(e)?this.getter=e:(this.getter=function(n){if(!G.test(n)){var e=n.split(".");return function(n){for(var t=0;t<e.length;t++){if(!n)return;n=n[e[t]]}return n}}}(e),this.getter||(this.getter=L)),this.value=this.lazy?void 0:this.get()}return n.prototype.get=function(){var n;En(this);var e=this.vm;try{n=this.getter.call(e,e)}catch(n){if(!this.user)throw n;Se(n,e,'getter for watcher "'.concat(this.expression,'"'))}finally{this.deep&&Fe(n),xn(),this.cleanupDeps()}return n},n.prototype.addDep=function(n){var e=n.id;this.newDepIds.has(e)||(this.newDepIds.add(e),this.newDeps.push(n),this.depIds.has(e)||n.addSub(this))},n.prototype.cleanupDeps=function(){for(var n=this.deps.length;n--;){var e=this.deps[n];this.newDepIds.has(e.id)||e.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},n.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():mt(this)},n.prototype.run=function(){if(this.active){var n=this.get();if(n!==this.value||d(n)||this.deep){var e=this.value;if(this.value=n,this.user){var t='callback for watcher "'.concat(this.expression,'"');qe(this.cb,this.vm,[n,e],this.vm,t)}else this.cb.call(this.vm,n,e)}}},n.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},n.prototype.depend=function(){for(var n=this.deps.length;n--;)this.deps[n].depend()},n.prototype.teardown=function(){if(this.vm&&!this.vm._isBeingDestroyed&&_(this.vm._scope.effects,this),this.active){for(var n=this.deps.length;n--;)this.deps[n].removeSub(this);this.active=!1,this.onStop&&this.onStop()}},n}();function Ge(n,e){$e.$on(n,e)}function Ve(n,e){$e.$off(n,e)}function Ke(n,e){var t=$e;return function a(){var s=e.apply(null,arguments);null!==s&&t.$off(n,a)}}function We(n,e,t){$e=n,Gn(e,t||{},Ge,Ve,Ke,n),$e=void 0}var Je=null;function Xe(n){var e=Je;return Je=n,function(){Je=e}}function Ye(n){for(;n&&(n=n.$parent);)if(n._inactive)return!0;return!1}function Ze(n,e){if(e){if(n._directInactive=!1,Ye(n))return}else if(n._directInactive)return;if(n._inactive||null===n._inactive){n._inactive=!1;for(var t=0;t<n.$children.length;t++)Ze(n.$children[t]);nt(n,"activated")}}function nt(n,e,t,a){void 0===a&&(a=!0),En();var s=un;a&&mn(n);var i=n.$options[e],r="".concat(e," hook");if(i)for(var o=0,l=i.length;o<l;o++)qe(i[o],n,t||null,n,r);n._hasHookEvent&&n.$emit("hook:"+e),a&&mn(s),xn()}var et=[],tt=[],at={},st=!1,it=!1,rt=0;var ot=0,lt=Date.now;if(K&&!J){var ct=window.performance;ct&&"function"==typeof ct.now&&lt()>document.createEvent("Event").timeStamp&&(lt=function(){return ct.now()})}var dt=function(n,e){if(n.post){if(!e.post)return 1}else if(e.post)return-1;return n.id-e.id};function ut(){var n,e;for(ot=lt(),it=!0,et.sort(dt),rt=0;rt<et.length;rt++)(n=et[rt]).before&&n.before(),e=n.id,at[e]=null,n.run();var t=tt.slice(),a=et.slice();rt=et.length=tt.length=0,at={},st=it=!1,function(n){for(var e=0;e<n.length;e++)n[e]._inactive=!0,Ze(n[e],!0)}(t),function(n){var e=n.length;for(;e--;){var t=n[e],a=t.vm;a&&a._watcher===t&&a._isMounted&&!a._isDestroyed&&nt(a,"updated")}}(a),function(){for(var n=0;n<yn.length;n++){var e=yn[n];e.subs=e.subs.filter((function(n){return n})),e._pending=!1}yn.length=0}(),on&&F.devtools&&on.emit("flush")}function mt(n){var e=n.id;if(null==at[e]&&(n!==vn.target||!n.noRecurse)){if(at[e]=!0,it){for(var t=et.length-1;t>rt&&et[t].id>n.id;)t--;et.splice(t+1,0,n)}else et.push(n);st||(st=!0,Pe(ut))}}function pt(n,e){if(n){for(var t=Object.create(null),a=dn?Reflect.ownKeys(n):Object.keys(n),s=0;s<a.length;s++){var i=a[s];if("__ob__"!==i){var r=n[i].from;if(r in e._provided)t[i]=e._provided[r];else if("default"in n[i]){var o=n[i].default;t[i]=c(o)?o.call(e):o}else 0}}return t}}function ht(n,e,t,i,r){var l,c=this,d=r.options;x(i,"_uid")?(l=Object.create(i))._original=i:(l=i,i=i._original);var u=o(d._compiled),m=!u;this.data=n,this.props=e,this.children=t,this.parent=i,this.listeners=n.on||a,this.injections=pt(d.inject,i),this.slots=function(){return c.$slots||ge(i,n.scopedSlots,c.$slots=me(t,i)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return ge(i,n.scopedSlots,this.slots())}}),u&&(this.$options=d,this.$slots=this.slots(),this.$scopedSlots=ge(i,n.scopedSlots,this.$slots)),d._scopeId?this._c=function(n,e,t,a){var r=Ie(l,n,e,t,a,m);return r&&!s(r)&&(r.fnScopeId=d._scopeId,r.fnContext=i),r}:this._c=function(n,e,t,a){return Ie(l,n,e,t,a,m)}}function gt(n,e,t,a,s){var i=fn(n);return i.fnContext=t,i.fnOptions=a,e.slot&&((i.data||(i.data={})).slot=e.slot),i}function ft(n,e){for(var t in e)n[I(t)]=e[t]}function bt(n){return n.name||n.__name||n._componentTag}ue(ht.prototype);var yt={init:function(n,e){if(n.componentInstance&&!n.componentInstance._isDestroyed&&n.data.keepAlive){var t=n;yt.prepatch(t,t)}else{(n.componentInstance=function(n,e){var t={_isComponent:!0,_parentVnode:n,parent:e},a=n.data.inlineTemplate;r(a)&&(t.render=a.render,t.staticRenderFns=a.staticRenderFns);return new n.componentOptions.Ctor(t)}(n,Je)).$mount(e?n.elm:void 0,e)}},prepatch:function(n,e){var t=e.componentOptions;!function(n,e,t,s,i){var r=s.data.scopedSlots,o=n.$scopedSlots,l=!!(r&&!r.$stable||o!==a&&!o.$stable||r&&n.$scopedSlots.$key!==r.$key||!r&&n.$scopedSlots.$key),c=!!(i||n.$options._renderChildren||l),d=n.$vnode;n.$options._parentVnode=s,n.$vnode=s,n._vnode&&(n._vnode.parent=s),n.$options._renderChildren=i;var u=s.data.attrs||a;n._attrsProxy&&ve(n._attrsProxy,u,d.data&&d.data.attrs||a,n,"$attrs")&&(c=!0),n.$attrs=u,t=t||a;var m=n.$options._parentListeners;if(n._listenersProxy&&ve(n._listenersProxy,t,m||a,n,"$listeners"),n.$listeners=n.$options._parentListeners=t,We(n,t,m),e&&n.$options.props){wn(!1);for(var p=n._props,h=n.$options._propKeys||[],g=0;g<h.length;g++){var f=h[g],b=n.$options.props;p[f]=zt(f,b,e,n)}wn(!0),n.$options.propsData=e}c&&(n.$slots=me(i,s.context),n.$forceUpdate())}(e.componentInstance=n.componentInstance,t.propsData,t.listeners,e,t.children)},insert:function(n){var e,t=n.context,a=n.componentInstance;a._isMounted||(a._isMounted=!0,nt(a,"mounted")),n.data.keepAlive&&(t._isMounted?((e=a)._inactive=!1,tt.push(e)):Ze(a,!0))},destroy:function(n){var e=n.componentInstance;e._isDestroyed||(n.data.keepAlive?function n(e,t){if(!(t&&(e._directInactive=!0,Ye(e))||e._inactive)){e._inactive=!0;for(var a=0;a<e.$children.length;a++)n(e.$children[a]);nt(e,"deactivated")}}(e,!0):e.$destroy())}},vt=Object.keys(yt);function _t(n,e,t,l,c){if(!i(n)){var u=t.$options._base;if(d(n)&&(n=u.extend(n)),"function"==typeof n){var m;if(i(n.cid)&&void 0===(n=function(n,e){if(o(n.error)&&r(n.errorComp))return n.errorComp;if(r(n.resolved))return n.resolved;var t=xe;if(t&&r(n.owners)&&-1===n.owners.indexOf(t)&&n.owners.push(t),o(n.loading)&&r(n.loadingComp))return n.loadingComp;if(t&&!r(n.owners)){var a=n.owners=[t],s=!0,l=null,c=null;t.$on("hook:destroyed",(function(){return _(a,t)}));var u=function(n){for(var e=0,t=a.length;e<t;e++)a[e].$forceUpdate();n&&(a.length=0,null!==l&&(clearTimeout(l),l=null),null!==c&&(clearTimeout(c),c=null))},m=N((function(t){n.resolved=Te(t,e),s?a.length=0:u(!0)})),p=N((function(e){r(n.errorComp)&&(n.error=!0,u(!0))})),h=n(m,p);return d(h)&&(g(h)?i(n.resolved)&&h.then(m,p):g(h.component)&&(h.component.then(m,p),r(h.error)&&(n.errorComp=Te(h.error,e)),r(h.loading)&&(n.loadingComp=Te(h.loading,e),0===h.delay?n.loading=!0:l=setTimeout((function(){l=null,i(n.resolved)&&i(n.error)&&(n.loading=!0,u(!1))}),h.delay||200)),r(h.timeout)&&(c=setTimeout((function(){c=null,i(n.resolved)&&p(null)}),h.timeout)))),s=!1,n.loading?n.loadingComp:n.resolved}}(m=n,u)))return function(n,e,t,a,s){var i=hn();return i.asyncFactory=n,i.asyncMeta={data:e,context:t,children:a,tag:s},i}(m,e,t,l,c);e=e||{},Ht(n),r(e.model)&&function(n,e){var t=n.model&&n.model.prop||"value",a=n.model&&n.model.event||"input";(e.attrs||(e.attrs={}))[t]=e.model.value;var i=e.on||(e.on={}),o=i[a],l=e.model.callback;r(o)?(s(o)?-1===o.indexOf(l):o!==l)&&(i[a]=[l].concat(o)):i[a]=l}(n.options,e);var p=function(n,e,t){var a=e.options.props;if(!i(a)){var s={},o=n.attrs,l=n.props;if(r(o)||r(l))for(var c in a){var d=w(c);Kn(s,l,c,d,!0)||Kn(s,o,c,d,!1)}return s}}(e,n);if(o(n.options.functional))return function(n,e,t,i,o){var l=n.options,c={},d=l.props;if(r(d))for(var u in d)c[u]=zt(u,d,e||a);else r(t.attrs)&&ft(c,t.attrs),r(t.props)&&ft(c,t.props);var m=new ht(t,c,o,i,n),p=l.render.call(null,m._c,m);if(p instanceof pn)return gt(p,t,m.parent,l,m);if(s(p)){for(var h=Wn(p)||[],g=new Array(h.length),f=0;f<h.length;f++)g[f]=gt(h[f],t,m.parent,l,m);return g}}(n,p,e,t,l);var h=e.on;if(e.on=e.nativeOn,o(n.options.abstract)){var f=e.slot;e={},f&&(e.slot=f)}!function(n){for(var e=n.hook||(n.hook={}),t=0;t<vt.length;t++){var a=vt[t],s=e[a],i=yt[a];s===i||s&&s._merged||(e[a]=s?Et(i,s):i)}}(e);var b=bt(n.options)||c;return new pn("vue-component-".concat(n.cid).concat(b?"-".concat(b):""),e,void 0,void 0,void 0,t,{Ctor:n,propsData:p,listeners:h,tag:c,children:l},m)}}}function Et(n,e){var t=function(t,a){n(t,a),e(t,a)};return t._merged=!0,t}var xt=L,Tt=F.optionMergeStrategies;function kt(n,e,t){if(void 0===t&&(t=!0),!e)return n;for(var a,s,i,r=dn?Reflect.ownKeys(e):Object.keys(e),o=0;o<r.length;o++)"__ob__"!==(a=r[o])&&(s=n[a],i=e[a],t&&x(n,a)?s!==i&&m(s)&&m(i)&&kt(s,i):Ln(n,a,i));return n}function It(n,e,t){return t?function(){var a=c(e)?e.call(t,t):e,s=c(n)?n.call(t,t):n;return a?kt(a,s):s}:e?n?function(){return kt(c(e)?e.call(this,this):e,c(n)?n.call(this,this):n)}:e:n}function St(n,e){var t=e?n?n.concat(e):s(e)?e:[e]:n;return t?function(n){for(var e=[],t=0;t<n.length;t++)-1===e.indexOf(n[t])&&e.push(n[t]);return e}(t):t}function qt(n,e,t,a){var s=Object.create(n||null);return e?z(s,e):s}Tt.data=function(n,e,t){return t?It(n,e,t):e&&"function"!=typeof e?n:It(n,e)},U.forEach((function(n){Tt[n]=St})),j.forEach((function(n){Tt[n+"s"]=qt})),Tt.watch=function(n,e,t,a){if(n===tn&&(n=void 0),e===tn&&(e=void 0),!e)return Object.create(n||null);if(!n)return e;var i={};for(var r in z(i,n),e){var o=i[r],l=e[r];o&&!s(o)&&(o=[o]),i[r]=o?o.concat(l):s(l)?l:[l]}return i},Tt.props=Tt.methods=Tt.inject=Tt.computed=function(n,e,t,a){if(!n)return e;var s=Object.create(null);return z(s,n),e&&z(s,e),s},Tt.provide=function(n,e){return n?function(){var t=Object.create(null);return kt(t,c(n)?n.call(this):n),e&&kt(t,c(e)?e.call(this):e,!1),t}:e};var wt=function(n,e){return void 0===e?n:e};function Rt(n,e,t){if(c(e)&&(e=e.options),function(n,e){var t=n.props;if(t){var a,i,r={};if(s(t))for(a=t.length;a--;)"string"==typeof(i=t[a])&&(r[I(i)]={type:null});else if(m(t))for(var o in t)i=t[o],r[I(o)]=m(i)?i:{type:i};else 0;n.props=r}}(e),function(n,e){var t=n.inject;if(t){var a=n.inject={};if(s(t))for(var i=0;i<t.length;i++)a[t[i]]={from:t[i]};else if(m(t))for(var r in t){var o=t[r];a[r]=m(o)?z({from:r},o):{from:o}}else 0}}(e),function(n){var e=n.directives;if(e)for(var t in e){var a=e[t];c(a)&&(e[t]={bind:a,update:a})}}(e),!e._base&&(e.extends&&(n=Rt(n,e.extends,t)),e.mixins))for(var a=0,i=e.mixins.length;a<i;a++)n=Rt(n,e.mixins[a],t);var r,o={};for(r in n)l(r);for(r in e)x(n,r)||l(r);function l(a){var s=Tt[a]||wt;o[a]=s(n[a],e[a],t,a)}return o}function At(n,e,t,a){if("string"==typeof t){var s=n[e];if(x(s,t))return s[t];var i=I(t);if(x(s,i))return s[i];var r=S(i);return x(s,r)?s[r]:s[t]||s[i]||s[r]}}function zt(n,e,t,a){var s=e[n],i=!x(t,n),r=t[n],o=Mt(Boolean,s.type);if(o>-1)if(i&&!x(s,"default"))r=!1;else if(""===r||r===w(n)){var l=Mt(String,s.type);(l<0||o<l)&&(r=!0)}if(void 0===r){r=function(n,e,t){if(!x(e,"default"))return;var a=e.default;0;if(n&&n.$options.propsData&&void 0===n.$options.propsData[t]&&void 0!==n._props[t])return n._props[t];return c(a)&&"Function"!==Lt(e.type)?a.call(n):a}(a,s,n);var d=qn;wn(!0),zn(r),wn(d)}return r}var Dt=/^\s*function (\w+)/;function Lt(n){var e=n&&n.toString().match(Dt);return e?e[1]:""}function Bt(n,e){return Lt(n)===Lt(e)}function Mt(n,e){if(!s(e))return Bt(e,n)?0:-1;for(var t=0,a=e.length;t<a;t++)if(Bt(e[t],n))return t;return-1}var Ot={enumerable:!0,configurable:!0,get:L,set:L};function Ct(n,e,t){Ot.get=function(){return this[e][t]},Ot.set=function(n){this[e][t]=n},Object.defineProperty(n,t,Ot)}function Nt(n){var e=n.$options;if(e.props&&function(n,e){var t=n.$options.propsData||{},a=n._props=On({}),s=n.$options._propKeys=[];n.$parent&&wn(!1);var i=function(i){s.push(i);var r=zt(i,e,t,n);Dn(a,i,r),i in n||Ct(n,"_props",i)};for(var r in e)i(r);wn(!0)}(n,e.props),function(n){var e=n.$options,t=e.setup;if(t){var a=n._setupContext=ye(n);mn(n),En();var s=qe(t,null,[n._props||On({}),a],n,"setup");if(xn(),mn(),c(s))e.render=s;else if(d(s))if(n._setupState=s,s.__sfc){var i=n._setupProxy={};for(var r in s)"__sfc"!==r&&jn(i,s,r)}else for(var r in s)Q(r)||jn(n,s,r);else 0}}(n),e.methods&&function(n,e){n.$options.props;for(var t in e)n[t]="function"!=typeof e[t]?L:R(e[t],n)}(n,e.methods),e.data)!function(n){var e=n.$options.data;m(e=n._data=c(e)?function(n,e){En();try{return n.call(e,e)}catch(n){return Se(n,e,"data()"),{}}finally{xn()}}(e,n):e||{})||(e={});var t=Object.keys(e),a=n.$options.props,s=(n.$options.methods,t.length);for(;s--;){var i=t[s];0,a&&x(a,i)||Q(i)||Ct(n,"_data",i)}var r=zn(e);r&&r.vmCount++}(n);else{var t=zn(n._data={});t&&t.vmCount++}e.computed&&function(n,e){var t=n._computedWatchers=Object.create(null),a=rn();for(var s in e){var i=e[s],r=c(i)?i:i.get;0,a||(t[s]=new He(n,r||L,L,Pt)),s in n||jt(n,s,i)}}(n,e.computed),e.watch&&e.watch!==tn&&function(n,e){for(var t in e){var a=e[t];if(s(a))for(var i=0;i<a.length;i++)$t(n,t,a[i]);else $t(n,t,a)}}(n,e.watch)}var Pt={lazy:!0};function jt(n,e,t){var a=!rn();c(t)?(Ot.get=a?Ut(e):Ft(t),Ot.set=L):(Ot.get=t.get?a&&!1!==t.cache?Ut(e):Ft(t.get):L,Ot.set=t.set||L),Object.defineProperty(n,e,Ot)}function Ut(n){return function(){var e=this._computedWatchers&&this._computedWatchers[n];if(e)return e.dirty&&e.evaluate(),vn.target&&e.depend(),e.value}}function Ft(n){return function(){return n.call(this,this)}}function $t(n,e,t,a){return m(t)&&(a=t,t=t.handler),"string"==typeof t&&(t=n[t]),n.$watch(e,t,a)}var Qt=0;function Ht(n){var e=n.options;if(n.super){var t=Ht(n.super);if(t!==n.superOptions){n.superOptions=t;var a=function(n){var e,t=n.options,a=n.sealedOptions;for(var s in t)t[s]!==a[s]&&(e||(e={}),e[s]=t[s]);return e}(n);a&&z(n.extendOptions,a),(e=n.options=Rt(t,n.extendOptions)).name&&(e.components[e.name]=n)}}return e}function Gt(n){this._init(n)}function Vt(n){n.cid=0;var e=1;n.extend=function(n){n=n||{};var t=this,a=t.cid,s=n._Ctor||(n._Ctor={});if(s[a])return s[a];var i=bt(n)||bt(t.options);var r=function(n){this._init(n)};return(r.prototype=Object.create(t.prototype)).constructor=r,r.cid=e++,r.options=Rt(t.options,n),r.super=t,r.options.props&&function(n){var e=n.options.props;for(var t in e)Ct(n.prototype,"_props",t)}(r),r.options.computed&&function(n){var e=n.options.computed;for(var t in e)jt(n.prototype,t,e[t])}(r),r.extend=t.extend,r.mixin=t.mixin,r.use=t.use,j.forEach((function(n){r[n]=t[n]})),i&&(r.options.components[i]=r),r.superOptions=t.options,r.extendOptions=n,r.sealedOptions=z({},r.options),s[a]=r,r}}function Kt(n){return n&&(bt(n.Ctor.options)||n.tag)}function Wt(n,e){return s(n)?n.indexOf(e)>-1:"string"==typeof n?n.split(",").indexOf(e)>-1:!!p(n)&&n.test(e)}function Jt(n,e){var t=n.cache,a=n.keys,s=n._vnode;for(var i in t){var r=t[i];if(r){var o=r.name;o&&!e(o)&&Xt(t,i,a,s)}}}function Xt(n,e,t,a){var s=n[e];!s||a&&s.tag===a.tag||s.componentInstance.$destroy(),n[e]=null,_(t,e)}!function(n){n.prototype._init=function(n){var e=this;e._uid=Qt++,e._isVue=!0,e.__v_skip=!0,e._scope=new Fn(!0),e._scope._vm=!0,n&&n._isComponent?function(n,e){var t=n.$options=Object.create(n.constructor.options),a=e._parentVnode;t.parent=e.parent,t._parentVnode=a;var s=a.componentOptions;t.propsData=s.propsData,t._parentListeners=s.listeners,t._renderChildren=s.children,t._componentTag=s.tag,e.render&&(t.render=e.render,t.staticRenderFns=e.staticRenderFns)}(e,n):e.$options=Rt(Ht(e.constructor),n||{},e),e._renderProxy=e,e._self=e,function(n){var e=n.$options,t=e.parent;if(t&&!e.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(n)}n.$parent=t,n.$root=t?t.$root:n,n.$children=[],n.$refs={},n._provided=t?t._provided:Object.create(null),n._watcher=null,n._inactive=null,n._directInactive=!1,n._isMounted=!1,n._isDestroyed=!1,n._isBeingDestroyed=!1}(e),function(n){n._events=Object.create(null),n._hasHookEvent=!1;var e=n.$options._parentListeners;e&&We(n,e)}(e),function(n){n._vnode=null,n._staticTrees=null;var e=n.$options,t=n.$vnode=e._parentVnode,s=t&&t.context;n.$slots=me(e._renderChildren,s),n.$scopedSlots=t?ge(n.$parent,t.data.scopedSlots,n.$slots):a,n._c=function(e,t,a,s){return Ie(n,e,t,a,s,!1)},n.$createElement=function(e,t,a,s){return Ie(n,e,t,a,s,!0)};var i=t&&t.data;Dn(n,"$attrs",i&&i.attrs||a,null,!0),Dn(n,"$listeners",e._parentListeners||a,null,!0)}(e),nt(e,"beforeCreate",void 0,!1),function(n){var e=pt(n.$options.inject,n);e&&(wn(!1),Object.keys(e).forEach((function(t){Dn(n,t,e[t])})),wn(!0))}(e),Nt(e),function(n){var e=n.$options.provide;if(e){var t=c(e)?e.call(n):e;if(!d(t))return;for(var a=$n(n),s=dn?Reflect.ownKeys(t):Object.keys(t),i=0;i<s.length;i++){var r=s[i];Object.defineProperty(a,r,Object.getOwnPropertyDescriptor(t,r))}}}(e),nt(e,"created"),e.$options.el&&e.$mount(e.$options.el)}}(Gt),function(n){var e={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(n.prototype,"$data",e),Object.defineProperty(n.prototype,"$props",t),n.prototype.$set=Ln,n.prototype.$delete=Bn,n.prototype.$watch=function(n,e,t){if(m(e))return $t(this,n,e,t);(t=t||{}).user=!0;var a=new He(this,n,e,t);if(t.immediate){var s='callback for immediate watcher "'.concat(a.expression,'"');En(),qe(e,this,[a.value],this,s),xn()}return function(){a.teardown()}}}(Gt),function(n){var e=/^hook:/;n.prototype.$on=function(n,t){var a=this;if(s(n))for(var i=0,r=n.length;i<r;i++)a.$on(n[i],t);else(a._events[n]||(a._events[n]=[])).push(t),e.test(n)&&(a._hasHookEvent=!0);return a},n.prototype.$once=function(n,e){var t=this;function a(){t.$off(n,a),e.apply(t,arguments)}return a.fn=e,t.$on(n,a),t},n.prototype.$off=function(n,e){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(s(n)){for(var a=0,i=n.length;a<i;a++)t.$off(n[a],e);return t}var r,o=t._events[n];if(!o)return t;if(!e)return t._events[n]=null,t;for(var l=o.length;l--;)if((r=o[l])===e||r.fn===e){o.splice(l,1);break}return t},n.prototype.$emit=function(n){var e=this,t=e._events[n];if(t){t=t.length>1?A(t):t;for(var a=A(arguments,1),s='event handler for "'.concat(n,'"'),i=0,r=t.length;i<r;i++)qe(t[i],e,a,e,s)}return e}}(Gt),function(n){n.prototype._update=function(n,e){var t=this,a=t.$el,s=t._vnode,i=Xe(t);t._vnode=n,t.$el=s?t.__patch__(s,n):t.__patch__(t.$el,n,e,!1),i(),a&&(a.__vue__=null),t.$el&&(t.$el.__vue__=t);for(var r=t;r&&r.$vnode&&r.$parent&&r.$vnode===r.$parent._vnode;)r.$parent.$el=r.$el,r=r.$parent},n.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},n.prototype.$destroy=function(){var n=this;if(!n._isBeingDestroyed){nt(n,"beforeDestroy"),n._isBeingDestroyed=!0;var e=n.$parent;!e||e._isBeingDestroyed||n.$options.abstract||_(e.$children,n),n._scope.stop(),n._data.__ob__&&n._data.__ob__.vmCount--,n._isDestroyed=!0,n.__patch__(n._vnode,null),nt(n,"destroyed"),n.$off(),n.$el&&(n.$el.__vue__=null),n.$vnode&&(n.$vnode.parent=null)}}}(Gt),function(n){ue(n.prototype),n.prototype.$nextTick=function(n){return Pe(n,this)},n.prototype._render=function(){var n,e=this,t=e.$options,a=t.render,i=t._parentVnode;i&&e._isMounted&&(e.$scopedSlots=ge(e.$parent,i.data.scopedSlots,e.$slots,e.$scopedSlots),e._slotsProxy&&Ee(e._slotsProxy,e.$scopedSlots)),e.$vnode=i;try{mn(e),xe=e,n=a.call(e._renderProxy,e.$createElement)}catch(t){Se(t,e,"render"),n=e._vnode}finally{xe=null,mn()}return s(n)&&1===n.length&&(n=n[0]),n instanceof pn||(n=hn()),n.parent=i,n}}(Gt);var Yt=[String,RegExp,Array],Zt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Yt,exclude:Yt,max:[String,Number]},methods:{cacheVNode:function(){var n=this.cache,e=this.keys,t=this.vnodeToCache,a=this.keyToCache;if(t){var s=t.tag,i=t.componentInstance,r=t.componentOptions;n[a]={name:Kt(r),tag:s,componentInstance:i},e.push(a),this.max&&e.length>parseInt(this.max)&&Xt(n,e[0],e,this._vnode),this.vnodeToCache=null}}},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var n in this.cache)Xt(this.cache,n,this.keys)},mounted:function(){var n=this;this.cacheVNode(),this.$watch("include",(function(e){Jt(n,(function(n){return Wt(e,n)}))})),this.$watch("exclude",(function(e){Jt(n,(function(n){return!Wt(e,n)}))}))},updated:function(){this.cacheVNode()},render:function(){var n=this.$slots.default,e=ke(n),t=e&&e.componentOptions;if(t){var a=Kt(t),s=this.include,i=this.exclude;if(s&&(!a||!Wt(s,a))||i&&a&&Wt(i,a))return e;var r=this.cache,o=this.keys,l=null==e.key?t.Ctor.cid+(t.tag?"::".concat(t.tag):""):e.key;r[l]?(e.componentInstance=r[l].componentInstance,_(o,l),o.push(l)):(this.vnodeToCache=e,this.keyToCache=l),e.data.keepAlive=!0}return e||n&&n[0]}}};!function(n){var e={get:function(){return F}};Object.defineProperty(n,"config",e),n.util={warn:xt,extend:z,mergeOptions:Rt,defineReactive:Dn},n.set=Ln,n.delete=Bn,n.nextTick=Pe,n.observable=function(n){return zn(n),n},n.options=Object.create(null),j.forEach((function(e){n.options[e+"s"]=Object.create(null)})),n.options._base=n,z(n.options.components,Zt),function(n){n.use=function(n){var e=this._installedPlugins||(this._installedPlugins=[]);if(e.indexOf(n)>-1)return this;var t=A(arguments,1);return t.unshift(this),c(n.install)?n.install.apply(n,t):c(n)&&n.apply(null,t),e.push(n),this}}(n),function(n){n.mixin=function(n){return this.options=Rt(this.options,n),this}}(n),Vt(n),function(n){j.forEach((function(e){n[e]=function(n,t){return t?("component"===e&&m(t)&&(t.name=t.name||n,t=this.options._base.extend(t)),"directive"===e&&c(t)&&(t={bind:t,update:t}),this.options[e+"s"][n]=t,t):this.options[e+"s"][n]}}))}(n)}(Gt),Object.defineProperty(Gt.prototype,"$isServer",{get:rn}),Object.defineProperty(Gt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Gt,"FunctionalRenderContext",{value:ht}),Gt.version="2.7.14";var na=y("style,class"),ea=y("input,textarea,option,select,progress"),ta=y("contenteditable,draggable,spellcheck"),aa=y("events,caret,typing,plaintext-only"),sa=y("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,truespeed,typemustmatch,visible"),ia="http://www.w3.org/1999/xlink",ra=function(n){return":"===n.charAt(5)&&"xlink"===n.slice(0,5)},oa=function(n){return ra(n)?n.slice(6,n.length):""},la=function(n){return null==n||!1===n};function ca(n){for(var e=n.data,t=n,a=n;r(a.componentInstance);)(a=a.componentInstance._vnode)&&a.data&&(e=da(a.data,e));for(;r(t=t.parent);)t&&t.data&&(e=da(e,t.data));return function(n,e){if(r(n)||r(e))return ua(n,ma(e));return""}(e.staticClass,e.class)}function da(n,e){return{staticClass:ua(n.staticClass,e.staticClass),class:r(n.class)?[n.class,e.class]:e.class}}function ua(n,e){return n?e?n+" "+e:n:e||""}function ma(n){return Array.isArray(n)?function(n){for(var e,t="",a=0,s=n.length;a<s;a++)r(e=ma(n[a]))&&""!==e&&(t&&(t+=" "),t+=e);return t}(n):d(n)?function(n){var e="";for(var t in n)n[t]&&(e&&(e+=" "),e+=t);return e}(n):"string"==typeof n?n:""}var pa={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},ha=y("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),ga=y("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignobject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),fa=function(n){return ha(n)||ga(n)};var ba=Object.create(null);var ya=y("text,number,password,search,email,tel,url");var va=Object.freeze({__proto__:null,createElement:function(n,e){var t=document.createElement(n);return"select"!==n||e.data&&e.data.attrs&&void 0!==e.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(n,e){return document.createElementNS(pa[n],e)},createTextNode:function(n){return document.createTextNode(n)},createComment:function(n){return document.createComment(n)},insertBefore:function(n,e,t){n.insertBefore(e,t)},removeChild:function(n,e){n.removeChild(e)},appendChild:function(n,e){n.appendChild(e)},parentNode:function(n){return n.parentNode},nextSibling:function(n){return n.nextSibling},tagName:function(n){return n.tagName},setTextContent:function(n,e){n.textContent=e},setStyleScope:function(n,e){n.setAttribute(e,"")}}),_a={create:function(n,e){Ea(e)},update:function(n,e){n.data.ref!==e.data.ref&&(Ea(n,!0),Ea(e))},destroy:function(n){Ea(n,!0)}};function Ea(n,e){var t=n.data.ref;if(r(t)){var a=n.context,i=n.componentInstance||n.elm,o=e?null:i,l=e?void 0:i;if(c(t))qe(t,a,[o],a,"template ref function");else{var d=n.data.refInFor,u="string"==typeof t||"number"==typeof t,m=Pn(t),p=a.$refs;if(u||m)if(d){var h=u?p[t]:t.value;e?s(h)&&_(h,i):s(h)?h.includes(i)||h.push(i):u?(p[t]=[i],xa(a,t,p[t])):t.value=[i]}else if(u){if(e&&p[t]!==i)return;p[t]=l,xa(a,t,o)}else if(m){if(e&&t.value!==i)return;t.value=o}else 0}}}function xa(n,e,t){var a=n._setupState;a&&x(a,e)&&(Pn(a[e])?a[e].value=t:a[e]=t)}var Ta=new pn("",{},[]),ka=["create","activate","update","remove","destroy"];function Ia(n,e){return n.key===e.key&&n.asyncFactory===e.asyncFactory&&(n.tag===e.tag&&n.isComment===e.isComment&&r(n.data)===r(e.data)&&function(n,e){if("input"!==n.tag)return!0;var t,a=r(t=n.data)&&r(t=t.attrs)&&t.type,s=r(t=e.data)&&r(t=t.attrs)&&t.type;return a===s||ya(a)&&ya(s)}(n,e)||o(n.isAsyncPlaceholder)&&i(e.asyncFactory.error))}function Sa(n,e,t){var a,s,i={};for(a=e;a<=t;++a)r(s=n[a].key)&&(i[s]=a);return i}var qa={create:wa,update:wa,destroy:function(n){wa(n,Ta)}};function wa(n,e){(n.data.directives||e.data.directives)&&function(n,e){var t,a,s,i=n===Ta,r=e===Ta,o=Aa(n.data.directives,n.context),l=Aa(e.data.directives,e.context),c=[],d=[];for(t in l)a=o[t],s=l[t],a?(s.oldValue=a.value,s.oldArg=a.arg,Da(s,"update",e,n),s.def&&s.def.componentUpdated&&d.push(s)):(Da(s,"bind",e,n),s.def&&s.def.inserted&&c.push(s));if(c.length){var u=function(){for(var t=0;t<c.length;t++)Da(c[t],"inserted",e,n)};i?Vn(e,"insert",u):u()}d.length&&Vn(e,"postpatch",(function(){for(var t=0;t<d.length;t++)Da(d[t],"componentUpdated",e,n)}));if(!i)for(t in o)l[t]||Da(o[t],"unbind",n,n,r)}(n,e)}var Ra=Object.create(null);function Aa(n,e){var t,a,s=Object.create(null);if(!n)return s;for(t=0;t<n.length;t++){if((a=n[t]).modifiers||(a.modifiers=Ra),s[za(a)]=a,e._setupState&&e._setupState.__sfc){var i=a.def||At(e,"_setupState","v-"+a.name);a.def="function"==typeof i?{bind:i,update:i}:i}a.def=a.def||At(e.$options,"directives",a.name)}return s}function za(n){return n.rawName||"".concat(n.name,".").concat(Object.keys(n.modifiers||{}).join("."))}function Da(n,e,t,a,s){var i=n.def&&n.def[e];if(i)try{i(t.elm,n,t,a,s)}catch(a){Se(a,t.context,"directive ".concat(n.name," ").concat(e," hook"))}}var La=[_a,qa];function Ba(n,e){var t=e.componentOptions;if(!(r(t)&&!1===t.Ctor.options.inheritAttrs||i(n.data.attrs)&&i(e.data.attrs))){var a,s,l=e.elm,c=n.data.attrs||{},d=e.data.attrs||{};for(a in(r(d.__ob__)||o(d._v_attr_proxy))&&(d=e.data.attrs=z({},d)),d)s=d[a],c[a]!==s&&Ma(l,a,s,e.data.pre);for(a in(J||Y)&&d.value!==c.value&&Ma(l,"value",d.value),c)i(d[a])&&(ra(a)?l.removeAttributeNS(ia,oa(a)):ta(a)||l.removeAttribute(a))}}function Ma(n,e,t,a){a||n.tagName.indexOf("-")>-1?Oa(n,e,t):sa(e)?la(t)?n.removeAttribute(e):(t="allowfullscreen"===e&&"EMBED"===n.tagName?"true":e,n.setAttribute(e,t)):ta(e)?n.setAttribute(e,function(n,e){return la(e)||"false"===e?"false":"contenteditable"===n&&aa(e)?e:"true"}(e,t)):ra(e)?la(t)?n.removeAttributeNS(ia,oa(e)):n.setAttributeNS(ia,e,t):Oa(n,e,t)}function Oa(n,e,t){if(la(t))n.removeAttribute(e);else{if(J&&!X&&"TEXTAREA"===n.tagName&&"placeholder"===e&&""!==t&&!n.__ieph){var a=function(e){e.stopImmediatePropagation(),n.removeEventListener("input",a)};n.addEventListener("input",a),n.__ieph=!0}n.setAttribute(e,t)}}var Ca={create:Ba,update:Ba};function Na(n,e){var t=e.elm,a=e.data,s=n.data;if(!(i(a.staticClass)&&i(a.class)&&(i(s)||i(s.staticClass)&&i(s.class)))){var o=ca(e),l=t._transitionClasses;r(l)&&(o=ua(o,ma(l))),o!==t._prevClass&&(t.setAttribute("class",o),t._prevClass=o)}}var Pa,ja={create:Na,update:Na};function Ua(n,e,t){var a=Pa;return function s(){var i=e.apply(null,arguments);null!==i&&Qa(n,s,t,a)}}var Fa=ze&&!(en&&Number(en[1])<=53);function $a(n,e,t,a){if(Fa){var s=ot,i=e;e=i._wrapper=function(n){if(n.target===n.currentTarget||n.timeStamp>=s||n.timeStamp<=0||n.target.ownerDocument!==document)return i.apply(this,arguments)}}Pa.addEventListener(n,e,an?{capture:t,passive:a}:t)}function Qa(n,e,t,a){(a||Pa).removeEventListener(n,e._wrapper||e,t)}function Ha(n,e){if(!i(n.data.on)||!i(e.data.on)){var t=e.data.on||{},a=n.data.on||{};Pa=e.elm||n.elm,function(n){if(r(n.__r)){var e=J?"change":"input";n[e]=[].concat(n.__r,n[e]||[]),delete n.__r}r(n.__c)&&(n.change=[].concat(n.__c,n.change||[]),delete n.__c)}(t),Gn(t,a,$a,Qa,Ua,e.context),Pa=void 0}}var Ga,Va={create:Ha,update:Ha,destroy:function(n){return Ha(n,Ta)}};function Ka(n,e){if(!i(n.data.domProps)||!i(e.data.domProps)){var t,a,s=e.elm,l=n.data.domProps||{},c=e.data.domProps||{};for(t in(r(c.__ob__)||o(c._v_attr_proxy))&&(c=e.data.domProps=z({},c)),l)t in c||(s[t]="");for(t in c){if(a=c[t],"textContent"===t||"innerHTML"===t){if(e.children&&(e.children.length=0),a===l[t])continue;1===s.childNodes.length&&s.removeChild(s.childNodes[0])}if("value"===t&&"PROGRESS"!==s.tagName){s._value=a;var d=i(a)?"":String(a);Wa(s,d)&&(s.value=d)}else if("innerHTML"===t&&ga(s.tagName)&&i(s.innerHTML)){(Ga=Ga||document.createElement("div")).innerHTML="<svg>".concat(a,"</svg>");for(var u=Ga.firstChild;s.firstChild;)s.removeChild(s.firstChild);for(;u.firstChild;)s.appendChild(u.firstChild)}else if(a!==l[t])try{s[t]=a}catch(n){}}}}function Wa(n,e){return!n.composing&&("OPTION"===n.tagName||function(n,e){var t=!0;try{t=document.activeElement!==n}catch(n){}return t&&n.value!==e}(n,e)||function(n,e){var t=n.value,a=n._vModifiers;if(r(a)){if(a.number)return b(t)!==b(e);if(a.trim)return t.trim()!==e.trim()}return t!==e}(n,e))}var Ja={create:Ka,update:Ka},Xa=T((function(n){var e={},t=/:(.+)/;return n.split(/;(?![^(]*\))/g).forEach((function(n){if(n){var a=n.split(t);a.length>1&&(e[a[0].trim()]=a[1].trim())}})),e}));function Ya(n){var e=Za(n.style);return n.staticStyle?z(n.staticStyle,e):e}function Za(n){return Array.isArray(n)?D(n):"string"==typeof n?Xa(n):n}var ns,es=/^--/,ts=/\s*!important$/,as=function(n,e,t){if(es.test(e))n.style.setProperty(e,t);else if(ts.test(t))n.style.setProperty(w(e),t.replace(ts,""),"important");else{var a=is(e);if(Array.isArray(t))for(var s=0,i=t.length;s<i;s++)n.style[a]=t[s];else n.style[a]=t}},ss=["Webkit","Moz","ms"],is=T((function(n){if(ns=ns||document.createElement("div").style,"filter"!==(n=I(n))&&n in ns)return n;for(var e=n.charAt(0).toUpperCase()+n.slice(1),t=0;t<ss.length;t++){var a=ss[t]+e;if(a in ns)return a}}));function rs(n,e){var t=e.data,a=n.data;if(!(i(t.staticStyle)&&i(t.style)&&i(a.staticStyle)&&i(a.style))){var s,o,l=e.elm,c=a.staticStyle,d=a.normalizedStyle||a.style||{},u=c||d,m=Za(e.data.style)||{};e.data.normalizedStyle=r(m.__ob__)?z({},m):m;var p=function(n,e){var t,a={};if(e)for(var s=n;s.componentInstance;)(s=s.componentInstance._vnode)&&s.data&&(t=Ya(s.data))&&z(a,t);(t=Ya(n.data))&&z(a,t);for(var i=n;i=i.parent;)i.data&&(t=Ya(i.data))&&z(a,t);return a}(e,!0);for(o in u)i(p[o])&&as(l,o,"");for(o in p)(s=p[o])!==u[o]&&as(l,o,null==s?"":s)}}var os={create:rs,update:rs},ls=/\s+/;function cs(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ls).forEach((function(e){return n.classList.add(e)})):n.classList.add(e);else{var t=" ".concat(n.getAttribute("class")||""," ");t.indexOf(" "+e+" ")<0&&n.setAttribute("class",(t+e).trim())}}function ds(n,e){if(e&&(e=e.trim()))if(n.classList)e.indexOf(" ")>-1?e.split(ls).forEach((function(e){return n.classList.remove(e)})):n.classList.remove(e),n.classList.length||n.removeAttribute("class");else{for(var t=" ".concat(n.getAttribute("class")||""," "),a=" "+e+" ";t.indexOf(a)>=0;)t=t.replace(a," ");(t=t.trim())?n.setAttribute("class",t):n.removeAttribute("class")}}function us(n){if(n){if("object"==typeof n){var e={};return!1!==n.css&&z(e,ms(n.name||"v")),z(e,n),e}return"string"==typeof n?ms(n):void 0}}var ms=T((function(n){return{enterClass:"".concat(n,"-enter"),enterToClass:"".concat(n,"-enter-to"),enterActiveClass:"".concat(n,"-enter-active"),leaveClass:"".concat(n,"-leave"),leaveToClass:"".concat(n,"-leave-to"),leaveActiveClass:"".concat(n,"-leave-active")}})),ps=K&&!X,hs="transition",gs="transitionend",fs="animation",bs="animationend";ps&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(hs="WebkitTransition",gs="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(fs="WebkitAnimation",bs="webkitAnimationEnd"));var ys=K?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(n){return n()};function vs(n){ys((function(){ys(n)}))}function _s(n,e){var t=n._transitionClasses||(n._transitionClasses=[]);t.indexOf(e)<0&&(t.push(e),cs(n,e))}function Es(n,e){n._transitionClasses&&_(n._transitionClasses,e),ds(n,e)}function xs(n,e,t){var a=ks(n,e),s=a.type,i=a.timeout,r=a.propCount;if(!s)return t();var o="transition"===s?gs:bs,l=0,c=function(){n.removeEventListener(o,d),t()},d=function(e){e.target===n&&++l>=r&&c()};setTimeout((function(){l<r&&c()}),i+1),n.addEventListener(o,d)}var Ts=/\b(transform|all)(,|$)/;function ks(n,e){var t,a=window.getComputedStyle(n),s=(a[hs+"Delay"]||"").split(", "),i=(a[hs+"Duration"]||"").split(", "),r=Is(s,i),o=(a[fs+"Delay"]||"").split(", "),l=(a[fs+"Duration"]||"").split(", "),c=Is(o,l),d=0,u=0;return"transition"===e?r>0&&(t="transition",d=r,u=i.length):"animation"===e?c>0&&(t="animation",d=c,u=l.length):u=(t=(d=Math.max(r,c))>0?r>c?"transition":"animation":null)?"transition"===t?i.length:l.length:0,{type:t,timeout:d,propCount:u,hasTransform:"transition"===t&&Ts.test(a[hs+"Property"])}}function Is(n,e){for(;n.length<e.length;)n=n.concat(n);return Math.max.apply(null,e.map((function(e,t){return Ss(e)+Ss(n[t])})))}function Ss(n){return 1e3*Number(n.slice(0,-1).replace(",","."))}function qs(n,e){var t=n.elm;r(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var a=us(n.data.transition);if(!i(a)&&!r(t._enterCb)&&1===t.nodeType){for(var s=a.css,o=a.type,l=a.enterClass,u=a.enterToClass,m=a.enterActiveClass,p=a.appearClass,h=a.appearToClass,g=a.appearActiveClass,f=a.beforeEnter,y=a.enter,v=a.afterEnter,_=a.enterCancelled,E=a.beforeAppear,x=a.appear,T=a.afterAppear,k=a.appearCancelled,I=a.duration,S=Je,q=Je.$vnode;q&&q.parent;)S=q.context,q=q.parent;var w=!S._isMounted||!n.isRootInsert;if(!w||x||""===x){var R=w&&p?p:l,A=w&&g?g:m,z=w&&h?h:u,D=w&&E||f,L=w&&c(x)?x:y,B=w&&T||v,M=w&&k||_,O=b(d(I)?I.enter:I);0;var C=!1!==s&&!X,P=As(L),j=t._enterCb=N((function(){C&&(Es(t,z),Es(t,A)),j.cancelled?(C&&Es(t,R),M&&M(t)):B&&B(t),t._enterCb=null}));n.data.show||Vn(n,"insert",(function(){var e=t.parentNode,a=e&&e._pending&&e._pending[n.key];a&&a.tag===n.tag&&a.elm._leaveCb&&a.elm._leaveCb(),L&&L(t,j)})),D&&D(t),C&&(_s(t,R),_s(t,A),vs((function(){Es(t,R),j.cancelled||(_s(t,z),P||(Rs(O)?setTimeout(j,O):xs(t,o,j)))}))),n.data.show&&(e&&e(),L&&L(t,j)),C||P||j()}}}function ws(n,e){var t=n.elm;r(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var a=us(n.data.transition);if(i(a)||1!==t.nodeType)return e();if(!r(t._leaveCb)){var s=a.css,o=a.type,l=a.leaveClass,c=a.leaveToClass,u=a.leaveActiveClass,m=a.beforeLeave,p=a.leave,h=a.afterLeave,g=a.leaveCancelled,f=a.delayLeave,y=a.duration,v=!1!==s&&!X,_=As(p),E=b(d(y)?y.leave:y);0;var x=t._leaveCb=N((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[n.key]=null),v&&(Es(t,c),Es(t,u)),x.cancelled?(v&&Es(t,l),g&&g(t)):(e(),h&&h(t)),t._leaveCb=null}));f?f(T):T()}function T(){x.cancelled||(!n.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[n.key]=n),m&&m(t),v&&(_s(t,l),_s(t,u),vs((function(){Es(t,l),x.cancelled||(_s(t,c),_||(Rs(E)?setTimeout(x,E):xs(t,o,x)))}))),p&&p(t,x),v||_||x())}}function Rs(n){return"number"==typeof n&&!isNaN(n)}function As(n){if(i(n))return!1;var e=n.fns;return r(e)?As(Array.isArray(e)?e[0]:e):(n._length||n.length)>1}function zs(n,e){!0!==e.data.show&&qs(e)}var Ds=function(n){var e,t,a={},c=n.modules,d=n.nodeOps;for(e=0;e<ka.length;++e)for(a[ka[e]]=[],t=0;t<c.length;++t)r(c[t][ka[e]])&&a[ka[e]].push(c[t][ka[e]]);function u(n){var e=d.parentNode(n);r(e)&&d.removeChild(e,n)}function m(n,e,t,s,i,l,c){if(r(n.elm)&&r(l)&&(n=l[c]=fn(n)),n.isRootInsert=!i,!function(n,e,t,s){var i=n.data;if(r(i)){var l=r(n.componentInstance)&&i.keepAlive;if(r(i=i.hook)&&r(i=i.init)&&i(n,!1),r(n.componentInstance))return p(n,e),h(t,n.elm,s),o(l)&&function(n,e,t,s){var i,o=n;for(;o.componentInstance;)if(o=o.componentInstance._vnode,r(i=o.data)&&r(i=i.transition)){for(i=0;i<a.activate.length;++i)a.activate[i](Ta,o);e.push(o);break}h(t,n.elm,s)}(n,e,t,s),!0}}(n,e,t,s)){var u=n.data,m=n.children,f=n.tag;r(f)?(n.elm=n.ns?d.createElementNS(n.ns,f):d.createElement(f,n),v(n),g(n,m,e),r(u)&&b(n,e),h(t,n.elm,s)):o(n.isComment)?(n.elm=d.createComment(n.text),h(t,n.elm,s)):(n.elm=d.createTextNode(n.text),h(t,n.elm,s))}}function p(n,e){r(n.data.pendingInsert)&&(e.push.apply(e,n.data.pendingInsert),n.data.pendingInsert=null),n.elm=n.componentInstance.$el,f(n)?(b(n,e),v(n)):(Ea(n),e.push(n))}function h(n,e,t){r(n)&&(r(t)?d.parentNode(t)===n&&d.insertBefore(n,e,t):d.appendChild(n,e))}function g(n,e,t){if(s(e)){0;for(var a=0;a<e.length;++a)m(e[a],t,n.elm,null,!0,e,a)}else l(n.text)&&d.appendChild(n.elm,d.createTextNode(String(n.text)))}function f(n){for(;n.componentInstance;)n=n.componentInstance._vnode;return r(n.tag)}function b(n,t){for(var s=0;s<a.create.length;++s)a.create[s](Ta,n);r(e=n.data.hook)&&(r(e.create)&&e.create(Ta,n),r(e.insert)&&t.push(n))}function v(n){var e;if(r(e=n.fnScopeId))d.setStyleScope(n.elm,e);else for(var t=n;t;)r(e=t.context)&&r(e=e.$options._scopeId)&&d.setStyleScope(n.elm,e),t=t.parent;r(e=Je)&&e!==n.context&&e!==n.fnContext&&r(e=e.$options._scopeId)&&d.setStyleScope(n.elm,e)}function _(n,e,t,a,s,i){for(;a<=s;++a)m(t[a],i,n,e,!1,t,a)}function E(n){var e,t,s=n.data;if(r(s))for(r(e=s.hook)&&r(e=e.destroy)&&e(n),e=0;e<a.destroy.length;++e)a.destroy[e](n);if(r(e=n.children))for(t=0;t<n.children.length;++t)E(n.children[t])}function x(n,e,t){for(;e<=t;++e){var a=n[e];r(a)&&(r(a.tag)?(T(a),E(a)):u(a.elm))}}function T(n,e){if(r(e)||r(n.data)){var t,s=a.remove.length+1;for(r(e)?e.listeners+=s:e=function(n,e){function t(){0==--t.listeners&&u(n)}return t.listeners=e,t}(n.elm,s),r(t=n.componentInstance)&&r(t=t._vnode)&&r(t.data)&&T(t,e),t=0;t<a.remove.length;++t)a.remove[t](n,e);r(t=n.data.hook)&&r(t=t.remove)?t(n,e):e()}else u(n.elm)}function k(n,e,t,a){for(var s=t;s<a;s++){var i=e[s];if(r(i)&&Ia(n,i))return s}}function I(n,e,t,s,l,c){if(n!==e){r(e.elm)&&r(s)&&(e=s[l]=fn(e));var u=e.elm=n.elm;if(o(n.isAsyncPlaceholder))r(e.asyncFactory.resolved)?w(n.elm,e,t):e.isAsyncPlaceholder=!0;else if(o(e.isStatic)&&o(n.isStatic)&&e.key===n.key&&(o(e.isCloned)||o(e.isOnce)))e.componentInstance=n.componentInstance;else{var p,h=e.data;r(h)&&r(p=h.hook)&&r(p=p.prepatch)&&p(n,e);var g=n.children,b=e.children;if(r(h)&&f(e)){for(p=0;p<a.update.length;++p)a.update[p](n,e);r(p=h.hook)&&r(p=p.update)&&p(n,e)}i(e.text)?r(g)&&r(b)?g!==b&&function(n,e,t,a,s){var o,l,c,u=0,p=0,h=e.length-1,g=e[0],f=e[h],b=t.length-1,y=t[0],v=t[b],E=!s;for(0;u<=h&&p<=b;)i(g)?g=e[++u]:i(f)?f=e[--h]:Ia(g,y)?(I(g,y,a,t,p),g=e[++u],y=t[++p]):Ia(f,v)?(I(f,v,a,t,b),f=e[--h],v=t[--b]):Ia(g,v)?(I(g,v,a,t,b),E&&d.insertBefore(n,g.elm,d.nextSibling(f.elm)),g=e[++u],v=t[--b]):Ia(f,y)?(I(f,y,a,t,p),E&&d.insertBefore(n,f.elm,g.elm),f=e[--h],y=t[++p]):(i(o)&&(o=Sa(e,u,h)),i(l=r(y.key)?o[y.key]:k(y,e,u,h))?m(y,a,n,g.elm,!1,t,p):Ia(c=e[l],y)?(I(c,y,a,t,p),e[l]=void 0,E&&d.insertBefore(n,c.elm,g.elm)):m(y,a,n,g.elm,!1,t,p),y=t[++p]);u>h?_(n,i(t[b+1])?null:t[b+1].elm,t,p,b,a):p>b&&x(e,u,h)}(u,g,b,t,c):r(b)?(r(n.text)&&d.setTextContent(u,""),_(u,null,b,0,b.length-1,t)):r(g)?x(g,0,g.length-1):r(n.text)&&d.setTextContent(u,""):n.text!==e.text&&d.setTextContent(u,e.text),r(h)&&r(p=h.hook)&&r(p=p.postpatch)&&p(n,e)}}}function S(n,e,t){if(o(t)&&r(n.parent))n.parent.data.pendingInsert=e;else for(var a=0;a<e.length;++a)e[a].data.hook.insert(e[a])}var q=y("attrs,class,staticClass,staticStyle,key");function w(n,e,t,a){var s,i=e.tag,l=e.data,c=e.children;if(a=a||l&&l.pre,e.elm=n,o(e.isComment)&&r(e.asyncFactory))return e.isAsyncPlaceholder=!0,!0;if(r(l)&&(r(s=l.hook)&&r(s=s.init)&&s(e,!0),r(s=e.componentInstance)))return p(e,t),!0;if(r(i)){if(r(c))if(n.hasChildNodes())if(r(s=l)&&r(s=s.domProps)&&r(s=s.innerHTML)){if(s!==n.innerHTML)return!1}else{for(var d=!0,u=n.firstChild,m=0;m<c.length;m++){if(!u||!w(u,c[m],t,a)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else g(e,c,t);if(r(l)){var h=!1;for(var f in l)if(!q(f)){h=!0,b(e,t);break}!h&&l.class&&Fe(l.class)}}else n.data!==e.text&&(n.data=e.text);return!0}return function(n,e,t,s){if(!i(e)){var l,c=!1,u=[];if(i(n))c=!0,m(e,u);else{var p=r(n.nodeType);if(!p&&Ia(n,e))I(n,e,u,null,null,s);else{if(p){if(1===n.nodeType&&n.hasAttribute("data-server-rendered")&&(n.removeAttribute("data-server-rendered"),t=!0),o(t)&&w(n,e,u))return S(e,u,!0),n;l=n,n=new pn(d.tagName(l).toLowerCase(),{},[],void 0,l)}var h=n.elm,g=d.parentNode(h);if(m(e,u,h._leaveCb?null:g,d.nextSibling(h)),r(e.parent))for(var b=e.parent,y=f(e);b;){for(var v=0;v<a.destroy.length;++v)a.destroy[v](b);if(b.elm=e.elm,y){for(var _=0;_<a.create.length;++_)a.create[_](Ta,b);var T=b.data.hook.insert;if(T.merged)for(var k=1;k<T.fns.length;k++)T.fns[k]()}else Ea(b);b=b.parent}r(g)?x([n],0,0):r(n.tag)&&E(n)}}return S(e,u,c),e.elm}r(n)&&E(n)}}({nodeOps:va,modules:[Ca,ja,Va,Ja,os,K?{create:zs,activate:zs,remove:function(n,e){!0!==n.data.show?ws(n,e):e()}}:{}].concat(La)});X&&document.addEventListener("selectionchange",(function(){var n=document.activeElement;n&&n.vmodel&&js(n,"input")}));var Ls={inserted:function(n,e,t,a){"select"===t.tag?(a.elm&&!a.elm._vOptions?Vn(t,"postpatch",(function(){Ls.componentUpdated(n,e,t)})):Bs(n,e,t.context),n._vOptions=[].map.call(n.options,Cs)):("textarea"===t.tag||ya(n.type))&&(n._vModifiers=e.modifiers,e.modifiers.lazy||(n.addEventListener("compositionstart",Ns),n.addEventListener("compositionend",Ps),n.addEventListener("change",Ps),X&&(n.vmodel=!0)))},componentUpdated:function(n,e,t){if("select"===t.tag){Bs(n,e,t.context);var a=n._vOptions,s=n._vOptions=[].map.call(n.options,Cs);if(s.some((function(n,e){return!O(n,a[e])})))(n.multiple?e.value.some((function(n){return Os(n,s)})):e.value!==e.oldValue&&Os(e.value,s))&&js(n,"change")}}};function Bs(n,e,t){Ms(n,e,t),(J||Y)&&setTimeout((function(){Ms(n,e,t)}),0)}function Ms(n,e,t){var a=e.value,s=n.multiple;if(!s||Array.isArray(a)){for(var i,r,o=0,l=n.options.length;o<l;o++)if(r=n.options[o],s)i=C(a,Cs(r))>-1,r.selected!==i&&(r.selected=i);else if(O(Cs(r),a))return void(n.selectedIndex!==o&&(n.selectedIndex=o));s||(n.selectedIndex=-1)}}function Os(n,e){return e.every((function(e){return!O(e,n)}))}function Cs(n){return"_value"in n?n._value:n.value}function Ns(n){n.target.composing=!0}function Ps(n){n.target.composing&&(n.target.composing=!1,js(n.target,"input"))}function js(n,e){var t=document.createEvent("HTMLEvents");t.initEvent(e,!0,!0),n.dispatchEvent(t)}function Us(n){return!n.componentInstance||n.data&&n.data.transition?n:Us(n.componentInstance._vnode)}var Fs={model:Ls,show:{bind:function(n,e,t){var a=e.value,s=(t=Us(t)).data&&t.data.transition,i=n.__vOriginalDisplay="none"===n.style.display?"":n.style.display;a&&s?(t.data.show=!0,qs(t,(function(){n.style.display=i}))):n.style.display=a?i:"none"},update:function(n,e,t){var a=e.value;!a!=!e.oldValue&&((t=Us(t)).data&&t.data.transition?(t.data.show=!0,a?qs(t,(function(){n.style.display=n.__vOriginalDisplay})):ws(t,(function(){n.style.display="none"}))):n.style.display=a?n.__vOriginalDisplay:"none")},unbind:function(n,e,t,a,s){s||(n.style.display=n.__vOriginalDisplay)}}},$s={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function Qs(n){var e=n&&n.componentOptions;return e&&e.Ctor.options.abstract?Qs(ke(e.children)):n}function Hs(n){var e={},t=n.$options;for(var a in t.propsData)e[a]=n[a];var s=t._parentListeners;for(var a in s)e[I(a)]=s[a];return e}function Gs(n,e){if(/\d-keep-alive$/.test(e.tag))return n("keep-alive",{props:e.componentOptions.propsData})}var Vs=function(n){return n.tag||he(n)},Ks=function(n){return"show"===n.name},Ws={name:"transition",props:$s,abstract:!0,render:function(n){var e=this,t=this.$slots.default;if(t&&(t=t.filter(Vs)).length){0;var a=this.mode;0;var s=t[0];if(function(n){for(;n=n.parent;)if(n.data.transition)return!0}(this.$vnode))return s;var i=Qs(s);if(!i)return s;if(this._leaving)return Gs(n,s);var r="__transition-".concat(this._uid,"-");i.key=null==i.key?i.isComment?r+"comment":r+i.tag:l(i.key)?0===String(i.key).indexOf(r)?i.key:r+i.key:i.key;var o=(i.data||(i.data={})).transition=Hs(this),c=this._vnode,d=Qs(c);if(i.data.directives&&i.data.directives.some(Ks)&&(i.data.show=!0),d&&d.data&&!function(n,e){return e.key===n.key&&e.tag===n.tag}(i,d)&&!he(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=z({},o);if("out-in"===a)return this._leaving=!0,Vn(u,"afterLeave",(function(){e._leaving=!1,e.$forceUpdate()})),Gs(n,s);if("in-out"===a){if(he(i))return c;var m,p=function(){m()};Vn(o,"afterEnter",p),Vn(o,"enterCancelled",p),Vn(u,"delayLeave",(function(n){m=n}))}}return s}}},Js=z({tag:String,moveClass:String},$s);function Xs(n){n.elm._moveCb&&n.elm._moveCb(),n.elm._enterCb&&n.elm._enterCb()}function Ys(n){n.data.newPos=n.elm.getBoundingClientRect()}function Zs(n){var e=n.data.pos,t=n.data.newPos,a=e.left-t.left,s=e.top-t.top;if(a||s){n.data.moved=!0;var i=n.elm.style;i.transform=i.WebkitTransform="translate(".concat(a,"px,").concat(s,"px)"),i.transitionDuration="0s"}}delete Js.mode;var ni={Transition:Ws,TransitionGroup:{props:Js,beforeMount:function(){var n=this,e=this._update;this._update=function(t,a){var s=Xe(n);n.__patch__(n._vnode,n.kept,!1,!0),n._vnode=n.kept,s(),e.call(n,t,a)}},render:function(n){for(var e=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),a=this.prevChildren=this.children,s=this.$slots.default||[],i=this.children=[],r=Hs(this),o=0;o<s.length;o++){if((d=s[o]).tag)if(null!=d.key&&0!==String(d.key).indexOf("__vlist"))i.push(d),t[d.key]=d,(d.data||(d.data={})).transition=r;else;}if(a){var l=[],c=[];for(o=0;o<a.length;o++){var d;(d=a[o]).data.transition=r,d.data.pos=d.elm.getBoundingClientRect(),t[d.key]?l.push(d):c.push(d)}this.kept=n(e,null,l),this.removed=c}return n(e,null,i)},updated:function(){var n=this.prevChildren,e=this.moveClass||(this.name||"v")+"-move";n.length&&this.hasMove(n[0].elm,e)&&(n.forEach(Xs),n.forEach(Ys),n.forEach(Zs),this._reflow=document.body.offsetHeight,n.forEach((function(n){if(n.data.moved){var t=n.elm,a=t.style;_s(t,e),a.transform=a.WebkitTransform=a.transitionDuration="",t.addEventListener(gs,t._moveCb=function n(a){a&&a.target!==t||a&&!/transform$/.test(a.propertyName)||(t.removeEventListener(gs,n),t._moveCb=null,Es(t,e))})}})))},methods:{hasMove:function(n,e){if(!ps)return!1;if(this._hasMove)return this._hasMove;var t=n.cloneNode();n._transitionClasses&&n._transitionClasses.forEach((function(n){ds(t,n)})),cs(t,e),t.style.display="none",this.$el.appendChild(t);var a=ks(t);return this.$el.removeChild(t),this._hasMove=a.hasTransform}}}};function ei(n,e){for(var t in e)n[t]=e[t];return n}Gt.config.mustUseProp=function(n,e,t){return"value"===t&&ea(n)&&"button"!==e||"selected"===t&&"option"===n||"checked"===t&&"input"===n||"muted"===t&&"video"===n},Gt.config.isReservedTag=fa,Gt.config.isReservedAttr=na,Gt.config.getTagNamespace=function(n){return ga(n)?"svg":"math"===n?"math":void 0},Gt.config.isUnknownElement=function(n){if(!K)return!0;if(fa(n))return!1;if(n=n.toLowerCase(),null!=ba[n])return ba[n];var e=document.createElement(n);return n.indexOf("-")>-1?ba[n]=e.constructor===window.HTMLUnknownElement||e.constructor===window.HTMLElement:ba[n]=/HTMLUnknownElement/.test(e.toString())},z(Gt.options.directives,Fs),z(Gt.options.components,ni),Gt.prototype.__patch__=K?Ds:L,Gt.prototype.$mount=function(n,e){return function(n,e,t){var a;n.$el=e,n.$options.render||(n.$options.render=hn),nt(n,"beforeMount"),a=function(){n._update(n._render(),t)},new He(n,a,L,{before:function(){n._isMounted&&!n._isDestroyed&&nt(n,"beforeUpdate")}},!0),t=!1;var s=n._preWatchers;if(s)for(var i=0;i<s.length;i++)s[i].run();return null==n.$vnode&&(n._isMounted=!0,nt(n,"mounted")),n}(this,n=n&&K?function(n){if("string"==typeof n){var e=document.querySelector(n);return e||document.createElement("div")}return n}(n):void 0,e)},K&&setTimeout((function(){F.devtools&&on&&on.emit("init",Gt)}),0);var ti=/[!'()*]/g,ai=function(n){return"%"+n.charCodeAt(0).toString(16)},si=/%2C/g,ii=function(n){return encodeURIComponent(n).replace(ti,ai).replace(si,",")};function ri(n){try{return decodeURIComponent(n)}catch(n){0}return n}var oi=function(n){return null==n||"object"==typeof n?n:String(n)};function li(n){var e={};return(n=n.trim().replace(/^(\?|#|&)/,""))?(n.split("&").forEach((function(n){var t=n.replace(/\+/g," ").split("="),a=ri(t.shift()),s=t.length>0?ri(t.join("=")):null;void 0===e[a]?e[a]=s:Array.isArray(e[a])?e[a].push(s):e[a]=[e[a],s]})),e):e}function ci(n){var e=n?Object.keys(n).map((function(e){var t=n[e];if(void 0===t)return"";if(null===t)return ii(e);if(Array.isArray(t)){var a=[];return t.forEach((function(n){void 0!==n&&(null===n?a.push(ii(e)):a.push(ii(e)+"="+ii(n)))})),a.join("&")}return ii(e)+"="+ii(t)})).filter((function(n){return n.length>0})).join("&"):null;return e?"?"+e:""}var di=/\/?$/;function ui(n,e,t,a){var s=a&&a.options.stringifyQuery,i=e.query||{};try{i=mi(i)}catch(n){}var r={name:e.name||n&&n.name,meta:n&&n.meta||{},path:e.path||"/",hash:e.hash||"",query:i,params:e.params||{},fullPath:gi(e,s),matched:n?hi(n):[]};return t&&(r.redirectedFrom=gi(t,s)),Object.freeze(r)}function mi(n){if(Array.isArray(n))return n.map(mi);if(n&&"object"==typeof n){var e={};for(var t in n)e[t]=mi(n[t]);return e}return n}var pi=ui(null,{path:"/"});function hi(n){for(var e=[];n;)e.unshift(n),n=n.parent;return e}function gi(n,e){var t=n.path,a=n.query;void 0===a&&(a={});var s=n.hash;return void 0===s&&(s=""),(t||"/")+(e||ci)(a)+s}function fi(n,e,t){return e===pi?n===e:!!e&&(n.path&&e.path?n.path.replace(di,"")===e.path.replace(di,"")&&(t||n.hash===e.hash&&bi(n.query,e.query)):!(!n.name||!e.name)&&(n.name===e.name&&(t||n.hash===e.hash&&bi(n.query,e.query)&&bi(n.params,e.params))))}function bi(n,e){if(void 0===n&&(n={}),void 0===e&&(e={}),!n||!e)return n===e;var t=Object.keys(n).sort(),a=Object.keys(e).sort();return t.length===a.length&&t.every((function(t,s){var i=n[t];if(a[s]!==t)return!1;var r=e[t];return null==i||null==r?i===r:"object"==typeof i&&"object"==typeof r?bi(i,r):String(i)===String(r)}))}function yi(n){for(var e=0;e<n.matched.length;e++){var t=n.matched[e];for(var a in t.instances){var s=t.instances[a],i=t.enteredCbs[a];if(s&&i){delete t.enteredCbs[a];for(var r=0;r<i.length;r++)s._isBeingDestroyed||i[r](s)}}}}var vi={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(n,e){var t=e.props,a=e.children,s=e.parent,i=e.data;i.routerView=!0;for(var r=s.$createElement,o=t.name,l=s.$route,c=s._routerViewCache||(s._routerViewCache={}),d=0,u=!1;s&&s._routerRoot!==s;){var m=s.$vnode?s.$vnode.data:{};m.routerView&&d++,m.keepAlive&&s._directInactive&&s._inactive&&(u=!0),s=s.$parent}if(i.routerViewDepth=d,u){var p=c[o],h=p&&p.component;return h?(p.configProps&&_i(h,i,p.route,p.configProps),r(h,i,a)):r()}var g=l.matched[d],f=g&&g.components[o];if(!g||!f)return c[o]=null,r();c[o]={component:f},i.registerRouteInstance=function(n,e){var t=g.instances[o];(e&&t!==n||!e&&t===n)&&(g.instances[o]=e)},(i.hook||(i.hook={})).prepatch=function(n,e){g.instances[o]=e.componentInstance},i.hook.init=function(n){n.data.keepAlive&&n.componentInstance&&n.componentInstance!==g.instances[o]&&(g.instances[o]=n.componentInstance),yi(l)};var b=g.props&&g.props[o];return b&&(ei(c[o],{route:l,configProps:b}),_i(f,i,l,b)),r(f,i,a)}};function _i(n,e,t,a){var s=e.props=function(n,e){switch(typeof e){case"undefined":return;case"object":return e;case"function":return e(n);case"boolean":return e?n.params:void 0;default:0}}(t,a);if(s){s=e.props=ei({},s);var i=e.attrs=e.attrs||{};for(var r in s)n.props&&r in n.props||(i[r]=s[r],delete s[r])}}function Ei(n,e,t){var a=n.charAt(0);if("/"===a)return n;if("?"===a||"#"===a)return e+n;var s=e.split("/");t&&s[s.length-1]||s.pop();for(var i=n.replace(/^\//,"").split("/"),r=0;r<i.length;r++){var o=i[r];".."===o?s.pop():"."!==o&&s.push(o)}return""!==s[0]&&s.unshift(""),s.join("/")}function xi(n){return n.replace(/\/(?:\s*\/)+/g,"/")}var Ti=Array.isArray||function(n){return"[object Array]"==Object.prototype.toString.call(n)},ki=Ni,Ii=Ai,Si=function(n,e){return Di(Ai(n,e),e)},qi=Di,wi=Ci,Ri=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function Ai(n,e){for(var t,a=[],s=0,i=0,r="",o=e&&e.delimiter||"/";null!=(t=Ri.exec(n));){var l=t[0],c=t[1],d=t.index;if(r+=n.slice(i,d),i=d+l.length,c)r+=c[1];else{var u=n[i],m=t[2],p=t[3],h=t[4],g=t[5],f=t[6],b=t[7];r&&(a.push(r),r="");var y=null!=m&&null!=u&&u!==m,v="+"===f||"*"===f,_="?"===f||"*"===f,E=t[2]||o,x=h||g;a.push({name:p||s++,prefix:m||"",delimiter:E,optional:_,repeat:v,partial:y,asterisk:!!b,pattern:x?Bi(x):b?".*":"[^"+Li(E)+"]+?"})}}return i<n.length&&(r+=n.substr(i)),r&&a.push(r),a}function zi(n){return encodeURI(n).replace(/[\/?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()}))}function Di(n,e){for(var t=new Array(n.length),a=0;a<n.length;a++)"object"==typeof n[a]&&(t[a]=new RegExp("^(?:"+n[a].pattern+")$",Oi(e)));return function(e,a){for(var s="",i=e||{},r=(a||{}).pretty?zi:encodeURIComponent,o=0;o<n.length;o++){var l=n[o];if("string"!=typeof l){var c,d=i[l.name];if(null==d){if(l.optional){l.partial&&(s+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(Ti(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(c=r(d[u]),!t[o].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");s+=(0===u?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(n){return"%"+n.charCodeAt(0).toString(16).toUpperCase()})):r(d),!t[o].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');s+=l.prefix+c}}else s+=l}return s}}function Li(n){return n.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function Bi(n){return n.replace(/([=!:$\/()])/g,"\\$1")}function Mi(n,e){return n.keys=e,n}function Oi(n){return n&&n.sensitive?"":"i"}function Ci(n,e,t){Ti(e)||(t=e||t,e=[]);for(var a=(t=t||{}).strict,s=!1!==t.end,i="",r=0;r<n.length;r++){var o=n[r];if("string"==typeof o)i+=Li(o);else{var l=Li(o.prefix),c="(?:"+o.pattern+")";e.push(o),o.repeat&&(c+="(?:"+l+c+")*"),i+=c=o.optional?o.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var d=Li(t.delimiter||"/"),u=i.slice(-d.length)===d;return a||(i=(u?i.slice(0,-d.length):i)+"(?:"+d+"(?=$))?"),i+=s?"$":a&&u?"":"(?="+d+"|$)",Mi(new RegExp("^"+i,Oi(t)),e)}function Ni(n,e,t){return Ti(e)||(t=e||t,e=[]),t=t||{},n instanceof RegExp?function(n,e){var t=n.source.match(/\((?!\?)/g);if(t)for(var a=0;a<t.length;a++)e.push({name:a,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return Mi(n,e)}(n,e):Ti(n)?function(n,e,t){for(var a=[],s=0;s<n.length;s++)a.push(Ni(n[s],e,t).source);return Mi(new RegExp("(?:"+a.join("|")+")",Oi(t)),e)}(n,e,t):function(n,e,t){return Ci(Ai(n,t),e,t)}(n,e,t)}ki.parse=Ii,ki.compile=Si,ki.tokensToFunction=qi,ki.tokensToRegExp=wi;var Pi=Object.create(null);function ji(n,e,t){e=e||{};try{var a=Pi[n]||(Pi[n]=ki.compile(n));return"string"==typeof e.pathMatch&&(e[0]=e.pathMatch),a(e,{pretty:!0})}catch(n){return""}finally{delete e[0]}}function Ui(n,e,t,a){var s="string"==typeof n?{path:n}:n;if(s._normalized)return s;if(s.name){var i=(s=ei({},n)).params;return i&&"object"==typeof i&&(s.params=ei({},i)),s}if(!s.path&&s.params&&e){(s=ei({},s))._normalized=!0;var r=ei(ei({},e.params),s.params);if(e.name)s.name=e.name,s.params=r;else if(e.matched.length){var o=e.matched[e.matched.length-1].path;s.path=ji(o,r,e.path)}else 0;return s}var l=function(n){var e="",t="",a=n.indexOf("#");a>=0&&(e=n.slice(a),n=n.slice(0,a));var s=n.indexOf("?");return s>=0&&(t=n.slice(s+1),n=n.slice(0,s)),{path:n,query:t,hash:e}}(s.path||""),c=e&&e.path||"/",d=l.path?Ei(l.path,c,t||s.append):c,u=function(n,e,t){void 0===e&&(e={});var a,s=t||li;try{a=s(n||"")}catch(n){a={}}for(var i in e){var r=e[i];a[i]=Array.isArray(r)?r.map(oi):oi(r)}return a}(l.query,s.query,a&&a.options.parseQuery),m=s.hash||l.hash;return m&&"#"!==m.charAt(0)&&(m="#"+m),{_normalized:!0,path:d,query:u,hash:m}}var Fi,$i=function(){},Qi={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},custom:Boolean,exact:Boolean,exactPath:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(n){var e=this,t=this.$router,a=this.$route,s=t.resolve(this.to,a,this.append),i=s.location,r=s.route,o=s.href,l={},c=t.options.linkActiveClass,d=t.options.linkExactActiveClass,u=null==c?"router-link-active":c,m=null==d?"router-link-exact-active":d,p=null==this.activeClass?u:this.activeClass,h=null==this.exactActiveClass?m:this.exactActiveClass,g=r.redirectedFrom?ui(null,Ui(r.redirectedFrom),null,t):r;l[h]=fi(a,g,this.exactPath),l[p]=this.exact||this.exactPath?l[h]:function(n,e){return 0===n.path.replace(di,"/").indexOf(e.path.replace(di,"/"))&&(!e.hash||n.hash===e.hash)&&function(n,e){for(var t in e)if(!(t in n))return!1;return!0}(n.query,e.query)}(a,g);var f=l[h]?this.ariaCurrentValue:null,b=function(n){Hi(n)&&(e.replace?t.replace(i,$i):t.push(i,$i))},y={click:Hi};Array.isArray(this.event)?this.event.forEach((function(n){y[n]=b})):y[this.event]=b;var v={class:l},_=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:o,route:r,navigate:b,isActive:l[p],isExactActive:l[h]});if(_){if(1===_.length)return _[0];if(_.length>1||!_.length)return 0===_.length?n():n("span",{},_)}if("a"===this.tag)v.on=y,v.attrs={href:o,"aria-current":f};else{var E=function n(e){var t;if(e)for(var a=0;a<e.length;a++){if("a"===(t=e[a]).tag)return t;if(t.children&&(t=n(t.children)))return t}}(this.$slots.default);if(E){E.isStatic=!1;var x=E.data=ei({},E.data);for(var T in x.on=x.on||{},x.on){var k=x.on[T];T in y&&(x.on[T]=Array.isArray(k)?k:[k])}for(var I in y)I in x.on?x.on[I].push(y[I]):x.on[I]=b;var S=E.data.attrs=ei({},E.data.attrs);S.href=o,S["aria-current"]=f}else v.on=y}return n(this.tag,v,this.$slots.default)}};function Hi(n){if(!(n.metaKey||n.altKey||n.ctrlKey||n.shiftKey||n.defaultPrevented||void 0!==n.button&&0!==n.button)){if(n.currentTarget&&n.currentTarget.getAttribute){var e=n.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(e))return}return n.preventDefault&&n.preventDefault(),!0}}var Gi="undefined"!=typeof window;function Vi(n,e,t,a,s){var i=e||[],r=t||Object.create(null),o=a||Object.create(null);n.forEach((function(n){!function n(e,t,a,s,i,r){var o=s.path,l=s.name;0;var c=s.pathToRegexpOptions||{},d=function(n,e,t){t||(n=n.replace(/\/$/,""));if("/"===n[0])return n;if(null==e)return n;return xi(e.path+"/"+n)}(o,i,c.strict);"boolean"==typeof s.caseSensitive&&(c.sensitive=s.caseSensitive);var u={path:d,regex:Ki(d,c),components:s.components||{default:s.component},alias:s.alias?"string"==typeof s.alias?[s.alias]:s.alias:[],instances:{},enteredCbs:{},name:l,parent:i,matchAs:r,redirect:s.redirect,beforeEnter:s.beforeEnter,meta:s.meta||{},props:null==s.props?{}:s.components?s.props:{default:s.props}};s.children&&s.children.forEach((function(s){var i=r?xi(r+"/"+s.path):void 0;n(e,t,a,s,u,i)}));t[u.path]||(e.push(u.path),t[u.path]=u);if(void 0!==s.alias)for(var m=Array.isArray(s.alias)?s.alias:[s.alias],p=0;p<m.length;++p){0;var h={path:m[p],children:s.children};n(e,t,a,h,i,u.path||"/")}l&&(a[l]||(a[l]=u))}(i,r,o,n,s)}));for(var l=0,c=i.length;l<c;l++)"*"===i[l]&&(i.push(i.splice(l,1)[0]),c--,l--);return{pathList:i,pathMap:r,nameMap:o}}function Ki(n,e){return ki(n,[],e)}function Wi(n,e){var t=Vi(n),a=t.pathList,s=t.pathMap,i=t.nameMap;function r(n,t,r){var o=Ui(n,t,!1,e),c=o.name;if(c){var d=i[c];if(!d)return l(null,o);var u=d.regex.keys.filter((function(n){return!n.optional})).map((function(n){return n.name}));if("object"!=typeof o.params&&(o.params={}),t&&"object"==typeof t.params)for(var m in t.params)!(m in o.params)&&u.indexOf(m)>-1&&(o.params[m]=t.params[m]);return o.path=ji(d.path,o.params),l(d,o,r)}if(o.path){o.params={};for(var p=0;p<a.length;p++){var h=a[p],g=s[h];if(Ji(g.regex,o.path,o.params))return l(g,o,r)}}return l(null,o)}function o(n,t){var a=n.redirect,s="function"==typeof a?a(ui(n,t,null,e)):a;if("string"==typeof s&&(s={path:s}),!s||"object"!=typeof s)return l(null,t);var o=s,c=o.name,d=o.path,u=t.query,m=t.hash,p=t.params;if(u=o.hasOwnProperty("query")?o.query:u,m=o.hasOwnProperty("hash")?o.hash:m,p=o.hasOwnProperty("params")?o.params:p,c){i[c];return r({_normalized:!0,name:c,query:u,hash:m,params:p},void 0,t)}if(d){var h=function(n,e){return Ei(n,e.parent?e.parent.path:"/",!0)}(d,n);return r({_normalized:!0,path:ji(h,p),query:u,hash:m},void 0,t)}return l(null,t)}function l(n,t,a){return n&&n.redirect?o(n,a||t):n&&n.matchAs?function(n,e,t){var a=r({_normalized:!0,path:ji(t,e.params)});if(a){var s=a.matched,i=s[s.length-1];return e.params=a.params,l(i,e)}return l(null,e)}(0,t,n.matchAs):ui(n,t,a,e)}return{match:r,addRoute:function(n,e){var t="object"!=typeof n?i[n]:void 0;Vi([e||n],a,s,i,t),t&&t.alias.length&&Vi(t.alias.map((function(n){return{path:n,children:[e]}})),a,s,i,t)},getRoutes:function(){return a.map((function(n){return s[n]}))},addRoutes:function(n){Vi(n,a,s,i)}}}function Ji(n,e,t){var a=e.match(n);if(!a)return!1;if(!t)return!0;for(var s=1,i=a.length;s<i;++s){var r=n.keys[s-1];r&&(t[r.name||"pathMatch"]="string"==typeof a[s]?ri(a[s]):a[s])}return!0}var Xi=Gi&&window.performance&&window.performance.now?window.performance:Date;function Yi(){return Xi.now().toFixed(3)}var Zi=Yi();function nr(){return Zi}function er(n){return Zi=n}var tr=Object.create(null);function ar(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var n=window.location.protocol+"//"+window.location.host,e=window.location.href.replace(n,""),t=ei({},window.history.state);return t.key=nr(),window.history.replaceState(t,"",e),window.addEventListener("popstate",rr),function(){window.removeEventListener("popstate",rr)}}function sr(n,e,t,a){if(n.app){var s=n.options.scrollBehavior;s&&n.app.$nextTick((function(){var i=function(){var n=nr();if(n)return tr[n]}(),r=s.call(n,e,t,a?i:null);r&&("function"==typeof r.then?r.then((function(n){ur(n,i)})).catch((function(n){0})):ur(r,i))}))}}function ir(){var n=nr();n&&(tr[n]={x:window.pageXOffset,y:window.pageYOffset})}function rr(n){ir(),n.state&&n.state.key&&er(n.state.key)}function or(n){return cr(n.x)||cr(n.y)}function lr(n){return{x:cr(n.x)?n.x:window.pageXOffset,y:cr(n.y)?n.y:window.pageYOffset}}function cr(n){return"number"==typeof n}var dr=/^#\d/;function ur(n,e){var t,a="object"==typeof n;if(a&&"string"==typeof n.selector){var s=dr.test(n.selector)?document.getElementById(n.selector.slice(1)):document.querySelector(n.selector);if(s){var i=n.offset&&"object"==typeof n.offset?n.offset:{};e=function(n,e){var t=document.documentElement.getBoundingClientRect(),a=n.getBoundingClientRect();return{x:a.left-t.left-e.x,y:a.top-t.top-e.y}}(s,i={x:cr((t=i).x)?t.x:0,y:cr(t.y)?t.y:0})}else or(n)&&(e=lr(n))}else a&&or(n)&&(e=lr(n));e&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:e.x,top:e.y,behavior:n.behavior}):window.scrollTo(e.x,e.y))}var mr,pr=Gi&&((-1===(mr=window.navigator.userAgent).indexOf("Android 2.")&&-1===mr.indexOf("Android 4.0")||-1===mr.indexOf("Mobile Safari")||-1!==mr.indexOf("Chrome")||-1!==mr.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function hr(n,e){ir();var t=window.history;try{if(e){var a=ei({},t.state);a.key=nr(),t.replaceState(a,"",n)}else t.pushState({key:er(Yi())},"",n)}catch(t){window.location[e?"replace":"assign"](n)}}function gr(n){hr(n,!0)}var fr={redirected:2,aborted:4,cancelled:8,duplicated:16};function br(n,e){return vr(n,e,fr.redirected,'Redirected when going from "'+n.fullPath+'" to "'+function(n){if("string"==typeof n)return n;if("path"in n)return n.path;var e={};return _r.forEach((function(t){t in n&&(e[t]=n[t])})),JSON.stringify(e,null,2)}(e)+'" via a navigation guard.')}function yr(n,e){return vr(n,e,fr.cancelled,'Navigation cancelled from "'+n.fullPath+'" to "'+e.fullPath+'" with a new navigation.')}function vr(n,e,t,a){var s=new Error(a);return s._isRouter=!0,s.from=n,s.to=e,s.type=t,s}var _r=["params","query","hash"];function Er(n){return Object.prototype.toString.call(n).indexOf("Error")>-1}function xr(n,e){return Er(n)&&n._isRouter&&(null==e||n.type===e)}function Tr(n,e,t){var a=function(s){s>=n.length?t():n[s]?e(n[s],(function(){a(s+1)})):a(s+1)};a(0)}function kr(n){return function(e,t,a){var s=!1,i=0,r=null;Ir(n,(function(n,e,t,o){if("function"==typeof n&&void 0===n.cid){s=!0,i++;var l,c=wr((function(e){var s;((s=e).__esModule||qr&&"Module"===s[Symbol.toStringTag])&&(e=e.default),n.resolved="function"==typeof e?e:Fi.extend(e),t.components[o]=e,--i<=0&&a()})),d=wr((function(n){var e="Failed to resolve async component "+o+": "+n;r||(r=Er(n)?n:new Error(e),a(r))}));try{l=n(c,d)}catch(n){d(n)}if(l)if("function"==typeof l.then)l.then(c,d);else{var u=l.component;u&&"function"==typeof u.then&&u.then(c,d)}}})),s||a()}}function Ir(n,e){return Sr(n.map((function(n){return Object.keys(n.components).map((function(t){return e(n.components[t],n.instances[t],n,t)}))})))}function Sr(n){return Array.prototype.concat.apply([],n)}var qr="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function wr(n){var e=!1;return function(){for(var t=[],a=arguments.length;a--;)t[a]=arguments[a];if(!e)return e=!0,n.apply(this,t)}}var Rr=function(n,e){this.router=n,this.base=function(n){if(!n)if(Gi){var e=document.querySelector("base");n=(n=e&&e.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else n="/";"/"!==n.charAt(0)&&(n="/"+n);return n.replace(/\/$/,"")}(e),this.current=pi,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function Ar(n,e,t,a){var s=Ir(n,(function(n,a,s,i){var r=function(n,e){"function"!=typeof n&&(n=Fi.extend(n));return n.options[e]}(n,e);if(r)return Array.isArray(r)?r.map((function(n){return t(n,a,s,i)})):t(r,a,s,i)}));return Sr(a?s.reverse():s)}function zr(n,e){if(e)return function(){return n.apply(e,arguments)}}Rr.prototype.listen=function(n){this.cb=n},Rr.prototype.onReady=function(n,e){this.ready?n():(this.readyCbs.push(n),e&&this.readyErrorCbs.push(e))},Rr.prototype.onError=function(n){this.errorCbs.push(n)},Rr.prototype.transitionTo=function(n,e,t){var a,s=this;try{a=this.router.match(n,this.current)}catch(n){throw this.errorCbs.forEach((function(e){e(n)})),n}var i=this.current;this.confirmTransition(a,(function(){s.updateRoute(a),e&&e(a),s.ensureURL(),s.router.afterHooks.forEach((function(n){n&&n(a,i)})),s.ready||(s.ready=!0,s.readyCbs.forEach((function(n){n(a)})))}),(function(n){t&&t(n),n&&!s.ready&&(xr(n,fr.redirected)&&i===pi||(s.ready=!0,s.readyErrorCbs.forEach((function(e){e(n)}))))}))},Rr.prototype.confirmTransition=function(n,e,t){var a=this,s=this.current;this.pending=n;var i,r,o=function(n){!xr(n)&&Er(n)&&(a.errorCbs.length?a.errorCbs.forEach((function(e){e(n)})):console.error(n)),t&&t(n)},l=n.matched.length-1,c=s.matched.length-1;if(fi(n,s)&&l===c&&n.matched[l]===s.matched[c])return this.ensureURL(),n.hash&&sr(this.router,s,n,!1),o(((r=vr(i=s,n,fr.duplicated,'Avoided redundant navigation to current location: "'+i.fullPath+'".')).name="NavigationDuplicated",r));var d=function(n,e){var t,a=Math.max(n.length,e.length);for(t=0;t<a&&n[t]===e[t];t++);return{updated:e.slice(0,t),activated:e.slice(t),deactivated:n.slice(t)}}(this.current.matched,n.matched),u=d.updated,m=d.deactivated,p=d.activated,h=[].concat(function(n){return Ar(n,"beforeRouteLeave",zr,!0)}(m),this.router.beforeHooks,function(n){return Ar(n,"beforeRouteUpdate",zr)}(u),p.map((function(n){return n.beforeEnter})),kr(p)),g=function(e,t){if(a.pending!==n)return o(yr(s,n));try{e(n,s,(function(e){!1===e?(a.ensureURL(!0),o(function(n,e){return vr(n,e,fr.aborted,'Navigation aborted from "'+n.fullPath+'" to "'+e.fullPath+'" via a navigation guard.')}(s,n))):Er(e)?(a.ensureURL(!0),o(e)):"string"==typeof e||"object"==typeof e&&("string"==typeof e.path||"string"==typeof e.name)?(o(br(s,n)),"object"==typeof e&&e.replace?a.replace(e):a.push(e)):t(e)}))}catch(n){o(n)}};Tr(h,g,(function(){Tr(function(n){return Ar(n,"beforeRouteEnter",(function(n,e,t,a){return function(n,e,t){return function(a,s,i){return n(a,s,(function(n){"function"==typeof n&&(e.enteredCbs[t]||(e.enteredCbs[t]=[]),e.enteredCbs[t].push(n)),i(n)}))}}(n,t,a)}))}(p).concat(a.router.resolveHooks),g,(function(){if(a.pending!==n)return o(yr(s,n));a.pending=null,e(n),a.router.app&&a.router.app.$nextTick((function(){yi(n)}))}))}))},Rr.prototype.updateRoute=function(n){this.current=n,this.cb&&this.cb(n)},Rr.prototype.setupListeners=function(){},Rr.prototype.teardown=function(){this.listeners.forEach((function(n){n()})),this.listeners=[],this.current=pi,this.pending=null};var Dr=function(n){function e(e,t){n.call(this,e,t),this._startLocation=Lr(this.base)}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router,t=e.options.scrollBehavior,a=pr&&t;a&&this.listeners.push(ar());var s=function(){var t=n.current,s=Lr(n.base);n.current===pi&&s===n._startLocation||n.transitionTo(s,(function(n){a&&sr(e,n,t,!0)}))};window.addEventListener("popstate",s),this.listeners.push((function(){window.removeEventListener("popstate",s)}))}},e.prototype.go=function(n){window.history.go(n)},e.prototype.push=function(n,e,t){var a=this,s=this.current;this.transitionTo(n,(function(n){hr(xi(a.base+n.fullPath)),sr(a.router,n,s,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this,s=this.current;this.transitionTo(n,(function(n){gr(xi(a.base+n.fullPath)),sr(a.router,n,s,!1),e&&e(n)}),t)},e.prototype.ensureURL=function(n){if(Lr(this.base)!==this.current.fullPath){var e=xi(this.base+this.current.fullPath);n?hr(e):gr(e)}},e.prototype.getCurrentLocation=function(){return Lr(this.base)},e}(Rr);function Lr(n){var e=window.location.pathname,t=e.toLowerCase(),a=n.toLowerCase();return!n||t!==a&&0!==t.indexOf(xi(a+"/"))||(e=e.slice(n.length)),(e||"/")+window.location.search+window.location.hash}var Br=function(n){function e(e,t,a){n.call(this,e,t),a&&function(n){var e=Lr(n);if(!/^\/#/.test(e))return window.location.replace(xi(n+"/#"+e)),!0}(this.base)||Mr()}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.setupListeners=function(){var n=this;if(!(this.listeners.length>0)){var e=this.router.options.scrollBehavior,t=pr&&e;t&&this.listeners.push(ar());var a=function(){var e=n.current;Mr()&&n.transitionTo(Or(),(function(a){t&&sr(n.router,a,e,!0),pr||Pr(a.fullPath)}))},s=pr?"popstate":"hashchange";window.addEventListener(s,a),this.listeners.push((function(){window.removeEventListener(s,a)}))}},e.prototype.push=function(n,e,t){var a=this,s=this.current;this.transitionTo(n,(function(n){Nr(n.fullPath),sr(a.router,n,s,!1),e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this,s=this.current;this.transitionTo(n,(function(n){Pr(n.fullPath),sr(a.router,n,s,!1),e&&e(n)}),t)},e.prototype.go=function(n){window.history.go(n)},e.prototype.ensureURL=function(n){var e=this.current.fullPath;Or()!==e&&(n?Nr(e):Pr(e))},e.prototype.getCurrentLocation=function(){return Or()},e}(Rr);function Mr(){var n=Or();return"/"===n.charAt(0)||(Pr("/"+n),!1)}function Or(){var n=window.location.href,e=n.indexOf("#");return e<0?"":n=n.slice(e+1)}function Cr(n){var e=window.location.href,t=e.indexOf("#");return(t>=0?e.slice(0,t):e)+"#"+n}function Nr(n){pr?hr(Cr(n)):window.location.hash=n}function Pr(n){pr?gr(Cr(n)):window.location.replace(Cr(n))}var jr=function(n){function e(e,t){n.call(this,e,t),this.stack=[],this.index=-1}return n&&(e.__proto__=n),e.prototype=Object.create(n&&n.prototype),e.prototype.constructor=e,e.prototype.push=function(n,e,t){var a=this;this.transitionTo(n,(function(n){a.stack=a.stack.slice(0,a.index+1).concat(n),a.index++,e&&e(n)}),t)},e.prototype.replace=function(n,e,t){var a=this;this.transitionTo(n,(function(n){a.stack=a.stack.slice(0,a.index).concat(n),e&&e(n)}),t)},e.prototype.go=function(n){var e=this,t=this.index+n;if(!(t<0||t>=this.stack.length)){var a=this.stack[t];this.confirmTransition(a,(function(){var n=e.current;e.index=t,e.updateRoute(a),e.router.afterHooks.forEach((function(e){e&&e(a,n)}))}),(function(n){xr(n,fr.duplicated)&&(e.index=t)}))}},e.prototype.getCurrentLocation=function(){var n=this.stack[this.stack.length-1];return n?n.fullPath:"/"},e.prototype.ensureURL=function(){},e}(Rr),Ur=function(n){void 0===n&&(n={}),this.app=null,this.apps=[],this.options=n,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Wi(n.routes||[],this);var e=n.mode||"hash";switch(this.fallback="history"===e&&!pr&&!1!==n.fallback,this.fallback&&(e="hash"),Gi||(e="abstract"),this.mode=e,e){case"history":this.history=new Dr(this,n.base);break;case"hash":this.history=new Br(this,n.base,this.fallback);break;case"abstract":this.history=new jr(this,n.base);break;default:0}},Fr={currentRoute:{configurable:!0}};Ur.prototype.match=function(n,e,t){return this.matcher.match(n,e,t)},Fr.currentRoute.get=function(){return this.history&&this.history.current},Ur.prototype.init=function(n){var e=this;if(this.apps.push(n),n.$once("hook:destroyed",(function(){var t=e.apps.indexOf(n);t>-1&&e.apps.splice(t,1),e.app===n&&(e.app=e.apps[0]||null),e.app||e.history.teardown()})),!this.app){this.app=n;var t=this.history;if(t instanceof Dr||t instanceof Br){var a=function(n){t.setupListeners(),function(n){var a=t.current,s=e.options.scrollBehavior;pr&&s&&"fullPath"in n&&sr(e,n,a,!1)}(n)};t.transitionTo(t.getCurrentLocation(),a,a)}t.listen((function(n){e.apps.forEach((function(e){e._route=n}))}))}},Ur.prototype.beforeEach=function(n){return Qr(this.beforeHooks,n)},Ur.prototype.beforeResolve=function(n){return Qr(this.resolveHooks,n)},Ur.prototype.afterEach=function(n){return Qr(this.afterHooks,n)},Ur.prototype.onReady=function(n,e){this.history.onReady(n,e)},Ur.prototype.onError=function(n){this.history.onError(n)},Ur.prototype.push=function(n,e,t){var a=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){a.history.push(n,e,t)}));this.history.push(n,e,t)},Ur.prototype.replace=function(n,e,t){var a=this;if(!e&&!t&&"undefined"!=typeof Promise)return new Promise((function(e,t){a.history.replace(n,e,t)}));this.history.replace(n,e,t)},Ur.prototype.go=function(n){this.history.go(n)},Ur.prototype.back=function(){this.go(-1)},Ur.prototype.forward=function(){this.go(1)},Ur.prototype.getMatchedComponents=function(n){var e=n?n.matched?n:this.resolve(n).route:this.currentRoute;return e?[].concat.apply([],e.matched.map((function(n){return Object.keys(n.components).map((function(e){return n.components[e]}))}))):[]},Ur.prototype.resolve=function(n,e,t){var a=Ui(n,e=e||this.history.current,t,this),s=this.match(a,e),i=s.redirectedFrom||s.fullPath;return{location:a,route:s,href:function(n,e,t){var a="hash"===t?"#"+e:e;return n?xi(n+"/"+a):a}(this.history.base,i,this.mode),normalizedTo:a,resolved:s}},Ur.prototype.getRoutes=function(){return this.matcher.getRoutes()},Ur.prototype.addRoute=function(n,e){this.matcher.addRoute(n,e),this.history.current!==pi&&this.history.transitionTo(this.history.getCurrentLocation())},Ur.prototype.addRoutes=function(n){this.matcher.addRoutes(n),this.history.current!==pi&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(Ur.prototype,Fr);var $r=Ur;function Qr(n,e){return n.push(e),function(){var t=n.indexOf(e);t>-1&&n.splice(t,1)}}Ur.install=function n(e){if(!n.installed||Fi!==e){n.installed=!0,Fi=e;var t=function(n){return void 0!==n},a=function(n,e){var a=n.$options._parentVnode;t(a)&&t(a=a.data)&&t(a=a.registerRouteInstance)&&a(n,e)};e.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),e.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,a(this,this)},destroyed:function(){a(this)}}),Object.defineProperty(e.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(e.prototype,"$route",{get:function(){return this._routerRoot._route}}),e.component("RouterView",vi),e.component("RouterLink",Qi);var s=e.config.optionMergeStrategies;s.beforeRouteEnter=s.beforeRouteLeave=s.beforeRouteUpdate=s.created}},Ur.version="3.6.5",Ur.isNavigationFailure=xr,Ur.NavigationFailureType=fr,Ur.START_LOCATION=pi,Gi&&window.Vue&&window.Vue.use(Ur);t(105);t(17),t(132);var Hr={NotFound:()=>Promise.all([t.e(0),t.e(4)]).then(t.bind(null,338)),Layout:()=>Promise.all([t.e(0),t.e(2)]).then(t.bind(null,337))},Gr={"v-01aea2f9":()=>t.e(5).then(t.bind(null,339)),"v-2f525dc4":()=>t.e(6).then(t.bind(null,340)),"v-4c53fba0":()=>t.e(7).then(t.bind(null,341)),"v-909a48e0":()=>t.e(8).then(t.bind(null,342)),"v-e3ca250c":()=>t.e(9).then(t.bind(null,343)),"v-1a0fef38":()=>t.e(10).then(t.bind(null,344)),"v-208168d6":()=>t.e(11).then(t.bind(null,345)),"v-099fc77f":()=>t.e(12).then(t.bind(null,346)),"v-4a7a8afa":()=>t.e(13).then(t.bind(null,347)),"v-49d5bdfa":()=>t.e(14).then(t.bind(null,348)),"v-0752b95e":()=>t.e(15).then(t.bind(null,349)),"v-b19bf61e":()=>t.e(16).then(t.bind(null,350)),"v-e2f7f754":()=>t.e(17).then(t.bind(null,351)),"v-413ab35d":()=>t.e(18).then(t.bind(null,352)),"v-41343aac":()=>t.e(19).then(t.bind(null,353)),"v-54c62a9f":()=>t.e(20).then(t.bind(null,354)),"v-6d5ab7d6":()=>t.e(21).then(t.bind(null,355)),"v-4ef63c90":()=>t.e(22).then(t.bind(null,356)),"v-076936e6":()=>t.e(23).then(t.bind(null,357)),"v-4954d596":()=>t.e(24).then(t.bind(null,358)),"v-74c83129":()=>t.e(25).then(t.bind(null,359)),"v-5a900e2a":()=>t.e(26).then(t.bind(null,360)),"v-7b0ce341":()=>t.e(27).then(t.bind(null,361)),"v-28fda0bf":()=>t.e(28).then(t.bind(null,362)),"v-19684eb4":()=>t.e(29).then(t.bind(null,363)),"v-44b09ccb":()=>t.e(30).then(t.bind(null,364)),"v-62392692":()=>t.e(31).then(t.bind(null,365)),"v-62ac19f1":()=>t.e(32).then(t.bind(null,366)),"v-0f37026c":()=>t.e(33).then(t.bind(null,367)),"v-f8f28ef4":()=>t.e(34).then(t.bind(null,368)),"v-925d1ece":()=>t.e(35).then(t.bind(null,369)),"v-2d5a6152":()=>t.e(36).then(t.bind(null,370)),"v-2310a086":()=>t.e(37).then(t.bind(null,371)),"v-0b02d45b":()=>t.e(38).then(t.bind(null,372)),"v-1211efa8":()=>t.e(39).then(t.bind(null,373)),"v-74c8571a":()=>t.e(40).then(t.bind(null,374)),"v-d98adcc4":()=>t.e(41).then(t.bind(null,375)),"v-cd3bd61e":()=>t.e(42).then(t.bind(null,376)),"v-2a8b7fae":()=>t.e(43).then(t.bind(null,377)),"v-a0623b42":()=>t.e(44).then(t.bind(null,378)),"v-80c82fb0":()=>t.e(45).then(t.bind(null,379)),"v-26fbb292":()=>t.e(46).then(t.bind(null,380)),"v-18f5009d":()=>t.e(47).then(t.bind(null,381)),"v-5b5c3224":()=>t.e(48).then(t.bind(null,382)),"v-2ccee58b":()=>t.e(49).then(t.bind(null,383)),"v-5ed748af":()=>t.e(50).then(t.bind(null,384)),"v-671a08c2":()=>t.e(51).then(t.bind(null,385)),"v-dbc2e0c0":()=>t.e(52).then(t.bind(null,386)),"v-1c459100":()=>t.e(53).then(t.bind(null,387)),"v-04b75150":()=>t.e(54).then(t.bind(null,388)),"v-41d7fd45":()=>t.e(55).then(t.bind(null,389)),"v-83a32830":()=>t.e(56).then(t.bind(null,390)),"v-0f15ab6e":()=>t.e(57).then(t.bind(null,391)),"v-a1e79a52":()=>t.e(58).then(t.bind(null,392)),"v-f6b5c434":()=>t.e(59).then(t.bind(null,393)),"v-957680f8":()=>t.e(60).then(t.bind(null,394)),"v-2ac3b3cb":()=>t.e(61).then(t.bind(null,395)),"v-164e75b2":()=>t.e(62).then(t.bind(null,396)),"v-665de124":()=>t.e(63).then(t.bind(null,397)),"v-9a8c12f8":()=>t.e(64).then(t.bind(null,398)),"v-3c818a28":()=>t.e(65).then(t.bind(null,399)),"v-7c0bbdd0":()=>t.e(66).then(t.bind(null,400)),"v-2a07dc5d":()=>t.e(67).then(t.bind(null,401)),"v-717638d6":()=>t.e(68).then(t.bind(null,402)),"v-2c1e440a":()=>t.e(69).then(t.bind(null,403)),"v-778f3185":()=>t.e(70).then(t.bind(null,404)),"v-ec0e4a94":()=>t.e(71).then(t.bind(null,405)),"v-15d3f40c":()=>t.e(72).then(t.bind(null,406)),"v-6550ec14":()=>t.e(73).then(t.bind(null,407)),"v-db66b0d8":()=>t.e(74).then(t.bind(null,408)),"v-8ad847d4":()=>t.e(75).then(t.bind(null,409)),"v-2afcea96":()=>t.e(76).then(t.bind(null,410)),"v-0d48f15b":()=>t.e(77).then(t.bind(null,411)),"v-72683126":()=>t.e(78).then(t.bind(null,412)),"v-c7c85e2e":()=>t.e(79).then(t.bind(null,413)),"v-03067a49":()=>t.e(80).then(t.bind(null,414)),"v-be824b2e":()=>t.e(81).then(t.bind(null,415)),"v-91a7d35c":()=>t.e(82).then(t.bind(null,416))};function Vr(n){const e=Object.create(null);return function(t){return e[t]||(e[t]=n(t))}}const Kr=/-(\w)/g,Wr=Vr(n=>n.replace(Kr,(n,e)=>e?e.toUpperCase():"")),Jr=/\B([A-Z])/g,Xr=Vr(n=>n.replace(Jr,"-$1").toLowerCase()),Yr=Vr(n=>n.charAt(0).toUpperCase()+n.slice(1));function Zr(n,e){if(!e)return;if(n(e))return n(e);return e.includes("-")?n(Yr(Wr(e))):n(Yr(e))||n(Xr(e))}const no=Object.assign({},Hr,Gr),eo=n=>no[n],to=n=>Gr[n],ao=n=>Hr[n],so=n=>Gt.component(n);function io(n){return Zr(to,n)}function ro(n){return Zr(ao,n)}function oo(n){return Zr(eo,n)}function lo(n){return Zr(so,n)}function co(...n){return Promise.all(n.filter(n=>n).map(async n=>{if(!lo(n)&&oo(n)){const e=await oo(n)();Gt.component(n,e.default)}}))}function uo(n,e){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[n]=e)}var mo=t(93),po=t.n(mo),ho=t(94),go=t.n(ho),fo={created(){if(this.siteMeta=this.$site.headTags.filter(([n])=>"meta"===n).map(([n,e])=>e),this.$ssrContext){const e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map(n=>{let e="<meta";return Object.keys(n).forEach(t=>{e+=` ${t}="${go()(n[t])}"`}),e+">"}).join("\n    "):"",this.$ssrContext.canonicalLink=yo(this.$canonicalUrl)}var n},mounted(){this.currentMetaTags=[...document.querySelectorAll("meta")],this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta(){document.title=this.$title,document.documentElement.lang=this.$lang;const n=this.getMergedMetaTags();this.currentMetaTags=vo(n,this.currentMetaTags)},getMergedMetaTags(){const n=this.$page.frontmatter.meta||[];return po()([{name:"description",content:this.$description}],n,this.siteMeta,_o)},updateCanonicalLink(){bo(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",yo(this.$canonicalUrl))}},watch:{$page(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy(){vo(null,this.currentMetaTags),bo()}};function bo(){const n=document.querySelector("link[rel='canonical']");n&&n.remove()}function yo(n=""){return n?`<link href="${n}" rel="canonical" />`:""}function vo(n,e){if(e&&[...e].filter(n=>n.parentNode===document.head).forEach(n=>document.head.removeChild(n)),n)return n.map(n=>{const e=document.createElement("meta");return Object.keys(n).forEach(t=>{e.setAttribute(t,n[t])}),document.head.appendChild(e),e})}function _o(n){for(const e of["name","property","itemprop"])if(n.hasOwnProperty(e))return n[e]+e;return JSON.stringify(n)}var Eo=t(51),xo={mounted(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:t.n(Eo)()((function(){this.setActiveHash()}),300),setActiveHash(){const n=[].slice.call(document.querySelectorAll(".sidebar-link")),e=[].slice.call(document.querySelectorAll(".header-anchor")).filter(e=>n.some(n=>n.hash===e.hash)),t=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),a=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),s=window.innerHeight+t;for(let n=0;n<e.length;n++){const i=e[n],r=e[n+1],o=0===n&&0===t||t>=i.parentElement.offsetTop+10&&(!r||t<r.parentElement.offsetTop-10),l=decodeURIComponent(this.$route.hash);if(o&&l!==decodeURIComponent(i.hash)){const t=i;if(s===a)for(let t=n+1;t<e.length;t++)if(l===decodeURIComponent(e[t].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(t.hash),()=>{this.$nextTick(()=>{this.$vuepress.$set("disableScrollBehavior",!1)})})}}}},beforeDestroy(){window.removeEventListener("scroll",this.onScroll)}},To=t(26),ko=t.n(To),Io={mounted(){ko.a.configure({showSpinner:!1}),this.$router.beforeEach((n,e,t)=>{n.path===e.path||Gt.component(n.name)||ko.a.start(),t()}),this.$router.afterEach(()=>{ko.a.done(),this.isSidebarOpen=!1})}};t(240),t(241);class So{constructor(){this.containerEl=document.getElementById("message-container"),this.containerEl||(this.containerEl=document.createElement("div"),this.containerEl.id="message-container",document.body.appendChild(this.containerEl))}show({text:n="",duration:e=3e3}){let t=document.createElement("div");t.className="message move-in",t.innerHTML=`\n      <i style="fill: #06a35a;font-size: 14px;display:inline-flex;align-items: center;">\n        <svg style="fill: #06a35a;font-size: 14px;" t="1572421810237" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2323" width="16" height="16"><path d="M822.811993 824.617989c-83.075838 81.99224-188.546032 124.613757-316.049383 127.86455-122.085362-3.250794-223.943563-45.87231-305.935802-127.86455s-124.613757-184.21164-127.86455-305.935802c3.250794-127.503351 45.87231-232.973545 127.86455-316.049383 81.99224-83.075838 184.21164-126.058554 305.935802-129.309347 127.503351 3.250794 232.973545 46.23351 316.049383 129.309347 83.075838 83.075838 126.058554 188.546032 129.309347 316.049383C949.231746 640.406349 905.887831 742.62575 822.811993 824.617989zM432.716755 684.111464c3.973192 3.973192 8.307584 5.779189 13.364374 6.140388 5.05679 0.361199 9.752381-1.444797 13.364374-5.417989l292.571429-287.514638c3.973192-3.973192 5.779189-8.307584 5.779189-13.364374 0-5.05679-1.805996-9.752381-5.779189-13.364374l1.805996 1.805996c-3.973192-3.973192-8.668783-5.779189-14.086772-6.140388-5.417989-0.361199-10.47478 1.444797-14.809171 5.417989l-264.397884 220.33157c-3.973192 3.250794-8.668783 4.695591-14.447972 4.695591-5.779189 0-10.835979-1.444797-15.53157-3.973192l-94.273016-72.962257c-4.334392-3.250794-9.391182-4.334392-14.447972-3.973192s-9.391182 3.250794-12.641975 7.585185l-2.889594 3.973192c-3.250794 4.334392-4.334392 9.391182-3.973192 14.809171 0.722399 5.417989 2.528395 10.11358 5.779189 14.086772L432.716755 684.111464z" p-id="2324"></path></svg>\n      </i>\n      <div class="text">${n}</div>\n    `,this.containerEl.appendChild(t),e>0&&setTimeout(()=>{this.close(t)},e)}close(n){n.className=n.className.replace("move-in",""),n.className+="move-out",n.addEventListener("animationend",()=>{n.remove()})}}var qo={mounted(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},updated(){!!/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)||this.updateCopy()},methods:{updateCopy(){setTimeout(()=>{(['div[class*="language-"] pre','div[class*="aside-code"] aside']instanceof Array||Array.isArray(['div[class*="language-"] pre','div[class*="aside-code"] aside']))&&['div[class*="language-"] pre','div[class*="aside-code"] aside'].forEach(n=>{document.querySelectorAll(n).forEach(this.generateCopyButton)})},1e3)},generateCopyButton(n){if(n.classList.contains("codecopy-enabled"))return;const e=document.createElement("i");e.className="code-copy",e.innerHTML='<svg  style="color:#aaa;font-size:14px" t="1572422231464" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3201" width="14" height="14"><path d="M866.461538 39.384615H354.461538c-43.323077 0-78.769231 35.446154-78.76923 78.769231v39.384616h472.615384c43.323077 0 78.769231 35.446154 78.769231 78.76923v551.384616h39.384615c43.323077 0 78.769231-35.446154 78.769231-78.769231V118.153846c0-43.323077-35.446154-78.769231-78.769231-78.769231z m-118.153846 275.692308c0-43.323077-35.446154-78.769231-78.76923-78.769231H157.538462c-43.323077 0-78.769231 35.446154-78.769231 78.769231v590.769231c0 43.323077 35.446154 78.769231 78.769231 78.769231h512c43.323077 0 78.769231-35.446154 78.76923-78.769231V315.076923z m-354.461538 137.846154c0 11.815385-7.876923 19.692308-19.692308 19.692308h-157.538461c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h157.538461c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z m157.538461 315.076923c0 11.815385-7.876923 19.692308-19.692307 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h315.076923c11.815385 0 19.692308 7.876923 19.692307 19.692308v39.384615z m78.769231-157.538462c0 11.815385-7.876923 19.692308-19.692308 19.692308H216.615385c-11.815385 0-19.692308-7.876923-19.692308-19.692308v-39.384615c0-11.815385 7.876923-19.692308 19.692308-19.692308h393.846153c11.815385 0 19.692308 7.876923 19.692308 19.692308v39.384615z" p-id="3202"></path></svg>',e.title="Copy to clipboard",e.addEventListener("click",()=>{this.copyToClipboard(n.innerText)}),n.appendChild(e),n.classList.add("codecopy-enabled")},copyToClipboard(n){const e=document.createElement("textarea");e.value=n,e.setAttribute("readonly",""),e.style.position="absolute",e.style.left="-9999px",document.body.appendChild(e);const t=document.getSelection().rangeCount>0&&document.getSelection().getRangeAt(0);e.select(),document.execCommand("copy");(new So).show({text:"复制成功",duration:1e3}),document.body.removeChild(e),t&&(document.getSelection().removeAllRanges(),document.getSelection().addRange(t))}}};!function(n,e){void 0===e&&(e={});var t=e.insertAt;if(n&&"undefined"!=typeof document){var a=document.head||document.getElementsByTagName("head")[0],s=document.createElement("style");s.type="text/css","top"===t&&a.firstChild?a.insertBefore(s,a.firstChild):a.appendChild(s),s.styleSheet?s.styleSheet.cssText=n:s.appendChild(document.createTextNode(n))}}("@media (max-width: 1000px) {\n  .vuepress-plugin-demo-block__h_code {\n    display: none;\n  }\n  .vuepress-plugin-demo-block__app {\n    margin-left: auto !important;\n    margin-right: auto !important;\n  }\n}\n.vuepress-plugin-demo-block__wrapper {\n  margin-top: 10px;\n  border: 1px solid #ebebeb;\n  border-radius: 4px;\n  transition: all 0.2s;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display {\n  height: 400px;\n  display: flex;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__app {\n  width: 300px;\n  border: 1px solid #ebebeb;\n  box-shadow: 1px 1px 3px #ebebeb;\n  margin-right: 5px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code {\n  flex: 1;\n  overflow: auto;\n  height: 100%;\n}\n.vuepress-plugin-demo-block__wrapper.vuepress-plugin-demo-block__horizontal .vuepress-plugin-demo-block__display .vuepress-plugin-demo-block__h_code > pre {\n  overflow: visible;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  max-height: 400px;\n  overflow: auto;\n}\n.vuepress-plugin-demo-block__wrapper div {\n  box-sizing: border-box;\n}\n.vuepress-plugin-demo-block__wrapper:hover {\n  box-shadow: 0 0 11px rgba(33, 33, 33, 0.2);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code {\n  overflow: hidden;\n  height: 0;\n  padding: 0 !important;\n  background-color: #282c34;\n  border-radius: 0 !important;\n  transition: height 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__code pre {\n  margin: 0 !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__display {\n  padding: 20px;\n  border-bottom: 1px solid #ebebeb;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer {\n  position: relative;\n  text-align: center;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__codepen {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer.vuepress-plugin-demo-block__show-link .vuepress-plugin-demo-block__expand::before {\n  border-top: none;\n  border-right: 6px solid transparent;\n  border-bottom: 6px solid #ccc;\n  border-left: 6px solid transparent;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__codepen,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand span,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand {\n  opacity: 1;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover .vuepress-plugin-demo-block__expand::before {\n  border-top-color: #3eaf7c !important;\n  border-bottom-color: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer:hover svg {\n  fill: #3eaf7c !important;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand-text {\n  transition: all 0.5s;\n  opacity: 0;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:nth-last-child(2) {\n  right: 50px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer form:last-child {\n  right: 10px;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button {\n  border-color: transparent;\n  background-color: transparent;\n  font-size: 14px;\n  color: #3eaf7c;\n  cursor: pointer;\n  outline: none;\n  margin: 0;\n  width: 46px;\n  position: relative;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::before {\n  content: attr(data-tip);\n  white-space: nowrap;\n  position: absolute;\n  top: -30px;\n  left: 50%;\n  color: #eee;\n  line-height: 1;\n  z-index: 1000;\n  border-radius: 4px;\n  padding: 6px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  background-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button:hover::after {\n  content: '' !important;\n  display: block;\n  position: absolute;\n  left: 50%;\n  top: -5px;\n  -webkit-transform: translateX(-50%);\n          transform: translateX(-50%);\n  border: 5px solid transparent;\n  border-top-color: rgba(0, 0, 0, 0.8);\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__button svg {\n  width: 34px;\n  height: 20px;\n  fill: #ccc;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__jsfiddle,\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__codepen {\n  position: absolute;\n  top: 10px;\n  transition: all 0.5s;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand {\n  position: relative;\n  width: 100px;\n  height: 40px;\n  margin: 0;\n  color: #3eaf7c;\n  font-size: 14px;\n  background-color: transparent;\n  border-color: transparent;\n  outline: none;\n  transition: all 0.5s;\n  cursor: pointer;\n}\n.vuepress-plugin-demo-block__wrapper .vuepress-plugin-demo-block__footer .vuepress-plugin-demo-block__expand::before {\n  content: \"\";\n  position: absolute;\n  top: 50%;\n  left: 50%;\n  width: 0;\n  height: 0;\n  border-top: 6px solid #ccc;\n  border-right: 6px solid transparent;\n  border-left: 6px solid transparent;\n  -webkit-transform: translate(-50%, -50%);\n          transform: translate(-50%, -50%);\n}\n");var wo={jsLib:[],cssLib:[],jsfiddle:!0,codepen:!0,codepenLayout:"left",codepenJsProcessor:"babel",codepenEditors:"101",horizontal:!1,vue:"https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js",react:"https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js",reactDOM:"https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"},Ro={},Ao=function(n){return'<div id="app">\n'.concat(n,"\n</div>")},zo=function(n){return window.$VUEPRESS_DEMO_BLOCK&&void 0!==window.$VUEPRESS_DEMO_BLOCK[n]?window.$VUEPRESS_DEMO_BLOCK[n]:wo[n]},Do=function n(e,t,a){var s=document.createElement(e);return t&&Object.keys(t).forEach((function(n){if(n.indexOf("data"))s[n]=t[n];else{var e=n.replace("data","");s.dataset[e]=t[n]}})),a&&a.forEach((function(e){var t=e.tag,a=e.attrs,i=e.children;s.appendChild(n(t,a,i))})),s},Lo=function(n,e,t){var a,s=(a=n.querySelectorAll(".".concat(e)),Array.prototype.slice.call(a));return 1!==s.length||t?s:s[0]},Bo=function(n,e){var t,a,s=n.match(/<style>([\s\S]+)<\/style>/),i=n.match(/<template>([\s\S]+)<\/template>/),r=n.match(/<script>([\s\S]+)<\/script>/),o={css:s&&s[1].replace(/^\n|\n$/g,""),html:i&&i[1].replace(/^\n|\n$/g,""),js:r&&r[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};o.htmlTpl=Ao(o.html),o.jsTpl=(t=o.js,a=t.replace(/export\s+default\s*?\{\n*/,"").replace(/\n*\}\s*$/,"").trim(),"new Vue({\n  el: '#app',\n  ".concat(a,"\n})")),o.script=function(n,e){var t=n.split(/export\s+default/),a="(function() {".concat(t[0]," ; return ").concat(t[1],"})()"),s=window.Babel?window.Babel.transform(a,{presets:["es2015"]}).code:a,i=[eval][0](s);return i.template=e,i}(o.js,o.html);var l=zo("vue");return o.jsLib.unshift(l),o},Mo=function(n,e){var t,a=n.match(/<style>([\s\S]+)<\/style>/),s=n.match(/<html>([\s\S]+)<\/html>/),i=n.match(/<script>([\s\S]+)<\/script>/),r={css:a&&a[1].replace(/^\n|\n$/g,""),html:s&&s[1].replace(/^\n|\n$/g,""),js:i&&i[1].replace(/^\n|\n$/g,""),jsLib:e.jsLib||[],cssLib:e.cssLib||[]};return r.htmlTpl=r.html,r.jsTpl=r.js,r.script=(t=r.js,window.Babel?window.Babel.transform(t,{presets:["es2015"]}).code:t),r},Oo=function(n){return n=n.replace("export default ","").replace(/App\.__style__(\s*)=(\s*)`([\s\S]*)?`/,""),n+='ReactDOM.render(React.createElement(App), document.getElementById("app"))'};function Co(){var n=Lo(document,"vuepress-plugin-demo-block__wrapper",!0);n.length?n.forEach((function(n){if("true"!==n.dataset.created){n.style.display="block";var e=Lo(n,"vuepress-plugin-demo-block__code"),t=Lo(n,"vuepress-plugin-demo-block__display"),a=Lo(n,"vuepress-plugin-demo-block__footer"),s=Lo(t,"vuepress-plugin-demo-block__app"),i=decodeURIComponent(n.dataset.code),r=decodeURIComponent(n.dataset.config),o=decodeURIComponent(n.dataset.type);r=r?JSON.parse(r):{};var l=e.querySelector("div").clientHeight,c="react"===o?function(n,e){var t=(0,window.Babel.transform)(n,{presets:["es2015","react"]}).code,a="(function(exports){var module={};module.exports=exports;".concat(t,";return module.exports.__esModule?module.exports.default:module.exports;})({})"),s=new Function("return ".concat(a))(),i={js:s,css:s.__style__||"",jsLib:e.jsLib||[],cssLib:e.cssLib||[],jsTpl:Oo(n),htmlTpl:Ao("")},r=zo("react"),o=zo("reactDOM");return i.jsLib.unshift(r,o),i}(i,r):"vanilla"===o?Mo(i,r):Bo(i,r),d=Do("button",{className:"".concat("vuepress-plugin-demo-block__expand")});if(a.appendChild(d),d.addEventListener("click",No.bind(null,d,l,e,a)),zo("jsfiddle")&&a.appendChild(function(n){var e=n.css,t=n.htmlTpl,a=n.jsTpl,s=n.jsLib,i=n.cssLib,r=s.concat(i).concat(zo("cssLib")).concat(zo("jsLib")).join(",");return Do("form",{className:"vuepress-plugin-demo-block__jsfiddle",target:"_blank",action:"https://jsfiddle.net/api/post/library/pure/",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"css",value:e}},{tag:"input",attrs:{type:"hidden",name:"html",value:t}},{tag:"input",attrs:{type:"hidden",name:"js",value:a}},{tag:"input",attrs:{type:"hidden",name:"panel_js",value:3}},{tag:"input",attrs:{type:"hidden",name:"wrap",value:1}},{tag:"input",attrs:{type:"hidden",name:"resources",value:r}},{tag:"button",attrs:{type:"submit",className:"vuepress-plugin-demo-block__button",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088289967" class="icon" style="" viewBox="0 0 1170 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1952" xmlns:xlink="http://www.w3.org/1999/xlink" width="228.515625" height="200"><defs><style type="text/css"></style></defs><path d="M1028.571429 441.142857q63.428571 26.285714 102.571428 83.142857T1170.285714 650.857143q0 93.714286-67.428571 160.285714T940 877.714286q-2.285714 0-6.571429-0.285715t-6-0.285714H232q-97.142857-5.714286-164.571429-71.714286T0 645.142857q0-62.857143 31.428571-116t84-84q-6.857143-22.285714-6.857142-46.857143 0-65.714286 46.857142-112t113.714286-46.285714q54.285714 0 98.285714 33.142857 42.857143-88 127.142858-141.714286t186.571428-53.714285q94.857143 0 174.857143 46T982.571429 248.571429t46.571428 172q0 3.428571-0.285714 10.285714t-0.285714 10.285714zM267.428571 593.142857q0 69.714286 48 110.285714t118.857143 40.571429q78.285714 0 137.142857-56.571429-9.142857-11.428571-27.142857-32.285714T519.428571 626.285714q-38.285714 37.142857-82.285714 37.142857-31.428571 0-53.428571-19.142857T361.714286 594.285714q0-30.285714 22-49.714285t52.285714-19.428572q25.142857 0 48.285714 12t41.714286 31.428572 37.142857 42.857142 39.428572 46.857143 44 42.857143 55.428571 31.428572 69.428571 12q69.142857 0 116.857143-40.857143T936 594.857143q0-69.142857-48-109.714286t-118.285714-40.571428q-81.714286 0-137.714286 55.428571l53.142857 61.714286q37.714286-36.571429 81.142857-36.571429 29.714286 0 52.571429 18.857143t22.857143 48q0 32.571429-21.142857 52.285714t-53.714286 19.714286q-24.571429 0-47.142857-12t-41.142857-31.428571-37.428572-42.857143-39.714286-46.857143-44.285714-42.857143-55.142857-31.428571T434.285714 444.571429q-69.714286 0-118.285714 40.285714T267.428571 593.142857z" p-id="1953"></path></svg>',datatip:"JSFiddle"}}])}(c)),zo("codepen")&&a.appendChild(function(n){var e=n.css,t=n.htmlTpl,a=n.jsTpl,s=n.jsLib,i=n.cssLib,r=JSON.stringify({css:e,html:t,js:a,js_external:s.concat(zo("jsLib")).join(";"),css_external:i.concat(zo("cssLib")).join(";"),layout:zo("codepenLayout"),js_pre_processor:zo("codepenJsProcessor"),editors:zo("codepenEditors")});return Do("form",{className:"vuepress-plugin-demo-block__codepen",target:"_blank",action:"https://codepen.io/pen/define",method:"post"},[{tag:"input",attrs:{type:"hidden",name:"data",value:r}},{tag:"button",attrs:{type:"submit",innerHTML:'<?xml version="1.0" standalone="no"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg t="1547088271207" class="icon" style="" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1737" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><defs><style type="text/css"></style></defs><path d="M123.428571 668l344.571429 229.714286v-205.142857L277.142857 565.142857z m-35.428571-82.285714l110.285714-73.714286-110.285714-73.714286v147.428572z m468 312l344.571429-229.714286-153.714286-102.857143-190.857143 127.428572v205.142857z m-44-281.714286l155.428571-104-155.428571-104-155.428571 104zM277.142857 458.857143l190.857143-127.428572V126.285714L123.428571 356z m548.571429 53.142857l110.285714 73.714286V438.285714z m-78.857143-53.142857l153.714286-102.857143-344.571429-229.714286v205.142857z m277.142857-102.857143v312q0 23.428571-19.428571 36.571429l-468 312q-12 7.428571-24.571429 7.428571t-24.571429-7.428571L19.428571 704.571429q-19.428571-13.142857-19.428571-36.571429V356q0-23.428571 19.428571-36.571429L487.428571 7.428571q12-7.428571 24.571429-7.428571t24.571429 7.428571l468 312q19.428571 13.142857 19.428571 36.571429z" p-id="1738"></path></svg>',className:"vuepress-plugin-demo-block__button",datatip:"Codepen"}}])}(c)),void 0!==r.horizontal?r.horizontal:zo("horizontal")){n.classList.add("vuepress-plugin-demo-block__horizontal");var u=e.firstChild.cloneNode(!0);u.classList.add("vuepress-plugin-demo-block__h_code"),t.appendChild(u)}if(c.css&&function(n){if(!Ro[n]){var e=Do("style",{innerHTML:n});document.body.appendChild(e),Ro[n]=!0}}(c.css),"react"===o)ReactDOM.render(React.createElement(c.js),s);else if("vue"===o){var m=(new(Vue.extend(c.script))).$mount();s.appendChild(m.$el)}else"vanilla"===o&&(s.innerHTML=c.html,new Function("return (function(){".concat(c.script,"})()"))());n.dataset.created="true"}})):setTimeout((function(n){Co()}),300)}function No(n,e,t,a){var s="1"!==n.dataset.isExpand;t.style.height=s?"".concat(e,"px"):0,s?a.classList.add("vuepress-plugin-demo-block__show-link"):a.classList.remove("vuepress-plugin-demo-block__show-link"),n.dataset.isExpand=s?"1":"0"}var Po={mounted:function(){window.$VUEPRESS_DEMO_BLOCK={jsfiddle:!1,codepen:!0,horizontal:!1},Co()},updated:function(){Co()}},jo="auto",Uo="zoom-in",Fo="zoom-out",$o="grab",Qo="move";function Ho(n,e,t){var a=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],s={passive:!1};a?n.addEventListener(e,t,s):n.removeEventListener(e,t,s)}function Go(n,e){if(n){var t=new Image;t.onload=function(){e&&e(t)},t.src=n}}function Vo(n){return n.dataset.original?n.dataset.original:"A"===n.parentNode.tagName?n.parentNode.getAttribute("href"):null}function Ko(n,e,t){!function(n){var e=Wo,t=Jo;if(n.transition){var a=n.transition;delete n.transition,n[e]=a}if(n.transform){var s=n.transform;delete n.transform,n[t]=s}}(e);var a=n.style,s={};for(var i in e)t&&(s[i]=a[i]||""),a[i]=e[i];return s}var Wo="transition",Jo="transform",Xo="transform",Yo="transitionend";var Zo=function(){},nl={enableGrab:!0,preloadImage:!1,closeOnWindowResize:!0,transitionDuration:.4,transitionTimingFunction:"cubic-bezier(0.4, 0, 0, 1)",bgColor:"rgb(255, 255, 255)",bgOpacity:1,scaleBase:1,scaleExtra:.5,scrollThreshold:40,zIndex:998,customSize:null,onOpen:Zo,onClose:Zo,onGrab:Zo,onMove:Zo,onRelease:Zo,onBeforeOpen:Zo,onBeforeClose:Zo,onBeforeGrab:Zo,onBeforeRelease:Zo,onImageLoading:Zo,onImageLoaded:Zo},el={init:function(n){var e,t;e=this,t=n,Object.getOwnPropertyNames(Object.getPrototypeOf(e)).forEach((function(n){e[n]=e[n].bind(t)}))},click:function(n){if(n.preventDefault(),al(n))return window.open(this.target.srcOriginal||n.currentTarget.src,"_blank");this.shown?this.released?this.close():this.release():this.open(n.currentTarget)},scroll:function(){var n=document.documentElement||document.body.parentNode||document.body,e=window.pageXOffset||n.scrollLeft,t=window.pageYOffset||n.scrollTop;null===this.lastScrollPosition&&(this.lastScrollPosition={x:e,y:t});var a=this.lastScrollPosition.x-e,s=this.lastScrollPosition.y-t,i=this.options.scrollThreshold;(Math.abs(s)>=i||Math.abs(a)>=i)&&(this.lastScrollPosition=null,this.close())},keydown:function(n){(function(n){return"Escape"===(n.key||n.code)||27===n.keyCode})(n)&&(this.released?this.close():this.release(this.close))},mousedown:function(n){if(tl(n)&&!al(n)){n.preventDefault();var e=n.clientX,t=n.clientY;this.pressTimer=setTimeout(function(){this.grab(e,t)}.bind(this),200)}},mousemove:function(n){this.released||this.move(n.clientX,n.clientY)},mouseup:function(n){tl(n)&&!al(n)&&(clearTimeout(this.pressTimer),this.released?this.close():this.release())},touchstart:function(n){n.preventDefault();var e=n.touches[0],t=e.clientX,a=e.clientY;this.pressTimer=setTimeout(function(){this.grab(t,a)}.bind(this),200)},touchmove:function(n){if(!this.released){var e=n.touches[0],t=e.clientX,a=e.clientY;this.move(t,a)}},touchend:function(n){(function(n){n.targetTouches.length})(n)||(clearTimeout(this.pressTimer),this.released?this.close():this.release())},clickOverlay:function(){this.close()},resizeWindow:function(){this.close()}};function tl(n){return 0===n.button}function al(n){return n.metaKey||n.ctrlKey}var sl={init:function(n){this.el=document.createElement("div"),this.instance=n,this.parent=document.body,Ko(this.el,{position:"fixed",top:0,left:0,right:0,bottom:0,opacity:0}),this.updateStyle(n.options),Ho(this.el,"click",n.handler.clickOverlay.bind(n))},updateStyle:function(n){Ko(this.el,{zIndex:n.zIndex,backgroundColor:n.bgColor,transition:"opacity\n        "+n.transitionDuration+"s\n        "+n.transitionTimingFunction})},insert:function(){this.parent.appendChild(this.el)},remove:function(){this.parent.removeChild(this.el)},fadeIn:function(){this.el.offsetWidth,this.el.style.opacity=this.instance.options.bgOpacity},fadeOut:function(){this.el.style.opacity=0}},il="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(n){return typeof n}:function(n){return n&&"function"==typeof Symbol&&n.constructor===Symbol&&n!==Symbol.prototype?"symbol":typeof n},rl=function(){function n(n,e){for(var t=0;t<e.length;t++){var a=e[t];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),Object.defineProperty(n,a.key,a)}}return function(e,t,a){return t&&n(e.prototype,t),a&&n(e,a),e}}(),ol=Object.assign||function(n){for(var e=1;e<arguments.length;e++){var t=arguments[e];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(n[a]=t[a])}return n},ll={init:function(n,e){this.el=n,this.instance=e,this.srcThumbnail=this.el.getAttribute("src"),this.srcset=this.el.getAttribute("srcset"),this.srcOriginal=Vo(this.el),this.rect=this.el.getBoundingClientRect(),this.translate=null,this.scale=null,this.styleOpen=null,this.styleClose=null},zoomIn:function(){var n=this.instance.options,e=n.zIndex,t=n.enableGrab,a=n.transitionDuration,s=n.transitionTimingFunction;this.translate=this.calculateTranslate(),this.scale=this.calculateScale(),this.styleOpen={position:"relative",zIndex:e+1,cursor:t?$o:Fo,transition:Xo+"\n        "+a+"s\n        "+s,transform:"translate3d("+this.translate.x+"px, "+this.translate.y+"px, 0px)\n        scale("+this.scale.x+","+this.scale.y+")",height:this.rect.height+"px",width:this.rect.width+"px"},this.el.offsetWidth,this.styleClose=Ko(this.el,this.styleOpen,!0)},zoomOut:function(){this.el.offsetWidth,Ko(this.el,{transform:"none"})},grab:function(n,e,t){var a=cl(),s=a.x-n,i=a.y-e;Ko(this.el,{cursor:Qo,transform:"translate3d(\n        "+(this.translate.x+s)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},move:function(n,e,t){var a=cl(),s=a.x-n,i=a.y-e;Ko(this.el,{transition:Xo,transform:"translate3d(\n        "+(this.translate.x+s)+"px, "+(this.translate.y+i)+"px, 0px)\n        scale("+(this.scale.x+t)+","+(this.scale.y+t)+")"})},restoreCloseStyle:function(){Ko(this.el,this.styleClose)},restoreOpenStyle:function(){Ko(this.el,this.styleOpen)},upgradeSource:function(){if(this.srcOriginal){var n=this.el.parentNode;this.srcset&&this.el.removeAttribute("srcset");var e=this.el.cloneNode(!1);e.setAttribute("src",this.srcOriginal),e.style.position="fixed",e.style.visibility="hidden",n.appendChild(e),setTimeout(function(){this.el.setAttribute("src",this.srcOriginal),n.removeChild(e)}.bind(this),50)}},downgradeSource:function(){this.srcOriginal&&(this.srcset&&this.el.setAttribute("srcset",this.srcset),this.el.setAttribute("src",this.srcThumbnail))},calculateTranslate:function(){var n=cl(),e=this.rect.left+this.rect.width/2,t=this.rect.top+this.rect.height/2;return{x:n.x-e,y:n.y-t}},calculateScale:function(){var n=this.el.dataset,e=n.zoomingHeight,t=n.zoomingWidth,a=this.instance.options,s=a.customSize,i=a.scaleBase;if(!s&&e&&t)return{x:t/this.rect.width,y:e/this.rect.height};if(s&&"object"===(void 0===s?"undefined":il(s)))return{x:s.width/this.rect.width,y:s.height/this.rect.height};var r=this.rect.width/2,o=this.rect.height/2,l=cl(),c={x:l.x-r,y:l.y-o},d=c.x/r,u=c.y/o,m=i+Math.min(d,u);if(s&&"string"==typeof s){var p=t||this.el.naturalWidth,h=e||this.el.naturalHeight,g=parseFloat(s)*p/(100*this.rect.width),f=parseFloat(s)*h/(100*this.rect.height);if(m>g||m>f)return{x:g,y:f}}return{x:m,y:m}}};function cl(){var n=document.documentElement;return{x:Math.min(n.clientWidth,window.innerWidth)/2,y:Math.min(n.clientHeight,window.innerHeight)/2}}function dl(n,e,t){["mousedown","mousemove","mouseup","touchstart","touchmove","touchend"].forEach((function(a){Ho(n,a,e[a],t)}))}var ul=function(){function n(e){!function(n,e){if(!(n instanceof e))throw new TypeError("Cannot call a class as a function")}(this,n),this.target=Object.create(ll),this.overlay=Object.create(sl),this.handler=Object.create(el),this.body=document.body,this.shown=!1,this.lock=!1,this.released=!0,this.lastScrollPosition=null,this.pressTimer=null,this.options=ol({},nl,e),this.overlay.init(this),this.handler.init(this)}return rl(n,[{key:"listen",value:function(n){if("string"==typeof n)for(var e=document.querySelectorAll(n),t=e.length;t--;)this.listen(e[t]);else"IMG"===n.tagName&&(n.style.cursor=Uo,Ho(n,"click",this.handler.click),this.options.preloadImage&&Go(Vo(n)));return this}},{key:"config",value:function(n){return n?(ol(this.options,n),this.overlay.updateStyle(this.options),this):this.options}},{key:"open",value:function(n){var e=this,t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:this.options.onOpen;if(!this.shown&&!this.lock){var a="string"==typeof n?document.querySelector(n):n;if("IMG"===a.tagName){if(this.options.onBeforeOpen(a),this.target.init(a,this),!this.options.preloadImage){var s=this.target.srcOriginal;null!=s&&(this.options.onImageLoading(a),Go(s,this.options.onImageLoaded))}this.shown=!0,this.lock=!0,this.target.zoomIn(),this.overlay.insert(),this.overlay.fadeIn(),Ho(document,"scroll",this.handler.scroll),Ho(document,"keydown",this.handler.keydown),this.options.closeOnWindowResize&&Ho(window,"resize",this.handler.resizeWindow);var i=function n(){Ho(a,Yo,n,!1),e.lock=!1,e.target.upgradeSource(),e.options.enableGrab&&dl(document,e.handler,!0),t(a)};return Ho(a,Yo,i),this}}}},{key:"close",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onClose;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeClose(t),this.lock=!0,this.body.style.cursor=jo,this.overlay.fadeOut(),this.target.zoomOut(),Ho(document,"scroll",this.handler.scroll,!1),Ho(document,"keydown",this.handler.keydown,!1),this.options.closeOnWindowResize&&Ho(window,"resize",this.handler.resizeWindow,!1);var a=function a(){Ho(t,Yo,a,!1),n.shown=!1,n.lock=!1,n.target.downgradeSource(),n.options.enableGrab&&dl(document,n.handler,!1),n.target.restoreCloseStyle(),n.overlay.remove(),e(t)};return Ho(t,Yo,a),this}}},{key:"grab",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onGrab;if(this.shown&&!this.lock){var s=this.target.el;this.options.onBeforeGrab(s),this.released=!1,this.target.grab(n,e,t);var i=function n(){Ho(s,Yo,n,!1),a(s)};return Ho(s,Yo,i),this}}},{key:"move",value:function(n,e){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:this.options.scaleExtra,a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:this.options.onMove;if(this.shown&&!this.lock){this.released=!1,this.body.style.cursor=Qo,this.target.move(n,e,t);var s=this.target.el,i=function n(){Ho(s,Yo,n,!1),a(s)};return Ho(s,Yo,i),this}}},{key:"release",value:function(){var n=this,e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:this.options.onRelease;if(this.shown&&!this.lock){var t=this.target.el;this.options.onBeforeRelease(t),this.lock=!0,this.body.style.cursor=jo,this.target.restoreOpenStyle();var a=function a(){Ho(t,Yo,a,!1),n.lock=!1,n.released=!0,e(t)};return Ho(t,Yo,a),this}}}]),n}();const ml=JSON.parse('{"bgColor":"rgba(0,0,0,0.6)"}'),pl=Number("500");class hl{constructor(){this.instance=new ul(ml)}update(n=".theme-vdoing-content img:not(.no-zoom)"){"undefined"!=typeof window&&this.instance.listen(n)}updateDelay(n=".theme-vdoing-content img:not(.no-zoom)",e=pl){setTimeout(()=>this.update(n),e)}}var gl=[fo,xo,Io,qo,Po,{watch:{"$page.path"(){void 0!==this.$vuepress.zooming&&this.$vuepress.zooming.updateDelay()}},mounted(){this.$vuepress.zooming=new hl,this.$vuepress.zooming.updateDelay()}}],fl={name:"GlobalLayout",computed:{layout(){const n=this.getLayout();return uo("layout",n),Gt.component(n)}},methods:{getLayout(){if(this.$page.path){const n=this.$page.frontmatter.layout;return n&&(this.$vuepress.getLayoutAsyncComponent(n)||this.$vuepress.getVueComponent(n))?n:"Layout"}return"NotFound"}}},bl=t(4),yl=Object(bl.a)(fl,(function(){return(0,this._self._c)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(n,e,t){switch(e){case"components":n[e]||(n[e]={}),Object.assign(n[e],t);break;case"mixins":n[e]||(n[e]=[]),n[e].push(...t);break;default:throw new Error("Unknown option name.")}}(yl,"mixins",gl);const vl=[{name:"v-01aea2f9",path:"/pages/0e1012/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-01aea2f9").then(t)}},{path:"/pages/0e1012/index.html",redirect:"/pages/0e1012/"},{path:"/12.数据库/01.数据库综合/01.Nosql技术选型.html",redirect:"/pages/0e1012/"},{name:"v-2f525dc4",path:"/pages/d7cd88/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2f525dc4").then(t)}},{path:"/pages/d7cd88/index.html",redirect:"/pages/d7cd88/"},{path:"/12.数据库/01.数据库综合/02.数据结构与数据库索引.html",redirect:"/pages/d7cd88/"},{name:"v-4c53fba0",path:"/pages/3c3c45/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-4c53fba0").then(t)}},{path:"/pages/3c3c45/index.html",redirect:"/pages/3c3c45/"},{path:"/12.数据库/01.数据库综合/",redirect:"/pages/3c3c45/"},{name:"v-909a48e0",path:"/pages/5ed2a2/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-909a48e0").then(t)}},{path:"/pages/5ed2a2/index.html",redirect:"/pages/5ed2a2/"},{path:"/12.数据库/02.数据库中间件/01.Shardingsphere/01.ShardingSphere简介.html",redirect:"/pages/5ed2a2/"},{name:"v-e3ca250c",path:"/pages/8448de/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-e3ca250c").then(t)}},{path:"/pages/8448de/index.html",redirect:"/pages/8448de/"},{path:"/12.数据库/02.数据库中间件/01.Shardingsphere/02.ShardingSphereJdbc.html",redirect:"/pages/8448de/"},{name:"v-1a0fef38",path:"/pages/e2648c/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-1a0fef38").then(t)}},{path:"/pages/e2648c/index.html",redirect:"/pages/e2648c/"},{path:"/12.数据库/02.数据库中间件/02.Flyway.html",redirect:"/pages/e2648c/"},{name:"v-208168d6",path:"/pages/addb05/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-208168d6").then(t)}},{path:"/pages/addb05/index.html",redirect:"/pages/addb05/"},{path:"/12.数据库/02.数据库中间件/",redirect:"/pages/addb05/"},{name:"v-099fc77f",path:"/pages/9bb28f/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-099fc77f").then(t)}},{path:"/pages/9bb28f/index.html",redirect:"/pages/9bb28f/"},{path:"/12.数据库/03.关系型数据库/01.综合/01.关系型数据库面试.html",redirect:"/pages/9bb28f/"},{name:"v-4a7a8afa",path:"/pages/b71c9e/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-4a7a8afa").then(t)}},{path:"/pages/b71c9e/index.html",redirect:"/pages/b71c9e/"},{path:"/12.数据库/03.关系型数据库/01.综合/02.SqlCheatSheet.html",redirect:"/pages/b71c9e/"},{name:"v-49d5bdfa",path:"/pages/55e9a7/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-49d5bdfa").then(t)}},{path:"/pages/55e9a7/index.html",redirect:"/pages/55e9a7/"},{path:"/12.数据库/03.关系型数据库/01.综合/03.扩展SQL.html",redirect:"/pages/55e9a7/"},{name:"v-0752b95e",path:"/pages/22f2e3/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-0752b95e").then(t)}},{path:"/pages/22f2e3/index.html",redirect:"/pages/22f2e3/"},{path:"/12.数据库/03.关系型数据库/01.综合/",redirect:"/pages/22f2e3/"},{name:"v-b19bf61e",path:"/pages/5fe0f3/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-b19bf61e").then(t)}},{path:"/pages/5fe0f3/index.html",redirect:"/pages/5fe0f3/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/01.Mysql应用指南.html",redirect:"/pages/5fe0f3/"},{name:"v-e2f7f754",path:"/pages/8262aa/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-e2f7f754").then(t)}},{path:"/pages/8262aa/index.html",redirect:"/pages/8262aa/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/02.MySQL工作流.html",redirect:"/pages/8262aa/"},{name:"v-413ab35d",path:"/pages/00b04d/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-413ab35d").then(t)}},{path:"/pages/00b04d/index.html",redirect:"/pages/00b04d/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/03.Mysql事务.html",redirect:"/pages/00b04d/"},{name:"v-41343aac",path:"/pages/f1f151/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-41343aac").then(t)}},{path:"/pages/f1f151/index.html",redirect:"/pages/f1f151/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/04.Mysql锁.html",redirect:"/pages/f1f151/"},{name:"v-54c62a9f",path:"/pages/fcb19c/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-54c62a9f").then(t)}},{path:"/pages/fcb19c/index.html",redirect:"/pages/fcb19c/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/05.Mysql索引.html",redirect:"/pages/fcb19c/"},{name:"v-6d5ab7d6",path:"/pages/396816/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-6d5ab7d6").then(t)}},{path:"/pages/396816/index.html",redirect:"/pages/396816/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/06.Mysql性能优化.html",redirect:"/pages/396816/"},{name:"v-4ef63c90",path:"/pages/e33b92/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-4ef63c90").then(t)}},{path:"/pages/e33b92/index.html",redirect:"/pages/e33b92/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/20.Mysql运维.html",redirect:"/pages/e33b92/"},{name:"v-076936e6",path:"/pages/5da42d/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-076936e6").then(t)}},{path:"/pages/5da42d/index.html",redirect:"/pages/5da42d/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/21.Mysql配置.html",redirect:"/pages/5da42d/"},{name:"v-4954d596",path:"/pages/7b0caf/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-4954d596").then(t)}},{path:"/pages/7b0caf/index.html",redirect:"/pages/7b0caf/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/99.Mysql常见问题.html",redirect:"/pages/7b0caf/"},{name:"v-74c83129",path:"/pages/a5b63b/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-74c83129").then(t)}},{path:"/pages/a5b63b/index.html",redirect:"/pages/a5b63b/"},{path:"/12.数据库/03.关系型数据库/02.Mysql/",redirect:"/pages/a5b63b/"},{name:"v-5a900e2a",path:"/pages/52609d/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-5a900e2a").then(t)}},{path:"/pages/52609d/index.html",redirect:"/pages/52609d/"},{path:"/12.数据库/03.关系型数据库/99.其他/01.PostgreSQL.html",redirect:"/pages/52609d/"},{name:"v-7b0ce341",path:"/pages/f27c0c/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-7b0ce341").then(t)}},{path:"/pages/f27c0c/index.html",redirect:"/pages/f27c0c/"},{path:"/12.数据库/03.关系型数据库/99.其他/02.H2.html",redirect:"/pages/f27c0c/"},{name:"v-28fda0bf",path:"/pages/bdcd7e/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-28fda0bf").then(t)}},{path:"/pages/bdcd7e/index.html",redirect:"/pages/bdcd7e/"},{path:"/12.数据库/03.关系型数据库/99.其他/03.Sqlite.html",redirect:"/pages/bdcd7e/"},{name:"v-19684eb4",path:"/pages/ca9888/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-19684eb4").then(t)}},{path:"/pages/ca9888/index.html",redirect:"/pages/ca9888/"},{path:"/12.数据库/03.关系型数据库/99.其他/",redirect:"/pages/ca9888/"},{name:"v-44b09ccb",path:"/pages/bb43eb/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-44b09ccb").then(t)}},{path:"/pages/bb43eb/index.html",redirect:"/pages/bb43eb/"},{path:"/12.数据库/03.关系型数据库/",redirect:"/pages/bb43eb/"},{name:"v-62392692",path:"/pages/3288f3/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-62392692").then(t)}},{path:"/pages/3288f3/index.html",redirect:"/pages/3288f3/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/01.MongoDB应用指南.html",redirect:"/pages/3288f3/"},{name:"v-62ac19f1",path:"/pages/7efbac/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-62ac19f1").then(t)}},{path:"/pages/7efbac/index.html",redirect:"/pages/7efbac/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/02.MongoDB的CRUD操作.html",redirect:"/pages/7efbac/"},{name:"v-0f37026c",path:"/pages/75daa5/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-0f37026c").then(t)}},{path:"/pages/75daa5/index.html",redirect:"/pages/75daa5/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/03.MongoDB的聚合操作.html",redirect:"/pages/75daa5/"},{name:"v-f8f28ef4",path:"/pages/4574fe/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-f8f28ef4").then(t)}},{path:"/pages/4574fe/index.html",redirect:"/pages/4574fe/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/04.MongoDB事务.html",redirect:"/pages/4574fe/"},{name:"v-925d1ece",path:"/pages/562f99/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-925d1ece").then(t)}},{path:"/pages/562f99/index.html",redirect:"/pages/562f99/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/05.MongoDB建模.html",redirect:"/pages/562f99/"},{name:"v-2d5a6152",path:"/pages/88c7d3/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2d5a6152").then(t)}},{path:"/pages/88c7d3/index.html",redirect:"/pages/88c7d3/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/06.MongoDB建模示例.html",redirect:"/pages/88c7d3/"},{name:"v-2310a086",path:"/pages/10c674/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2310a086").then(t)}},{path:"/pages/10c674/index.html",redirect:"/pages/10c674/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/07.MongoDB索引.html",redirect:"/pages/10c674/"},{name:"v-0b02d45b",path:"/pages/505407/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-0b02d45b").then(t)}},{path:"/pages/505407/index.html",redirect:"/pages/505407/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/08.MongoDB复制.html",redirect:"/pages/505407/"},{name:"v-1211efa8",path:"/pages/ad08f5/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-1211efa8").then(t)}},{path:"/pages/ad08f5/index.html",redirect:"/pages/ad08f5/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/09.MongoDB分片.html",redirect:"/pages/ad08f5/"},{name:"v-74c8571a",path:"/pages/5e3c30/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-74c8571a").then(t)}},{path:"/pages/5e3c30/index.html",redirect:"/pages/5e3c30/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/20.MongoDB运维.html",redirect:"/pages/5e3c30/"},{name:"v-d98adcc4",path:"/pages/b1a116/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-d98adcc4").then(t)}},{path:"/pages/b1a116/index.html",redirect:"/pages/b1a116/"},{path:"/12.数据库/04.文档数据库/01.MongoDB/",redirect:"/pages/b1a116/"},{name:"v-cd3bd61e",path:"/pages/451b73/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-cd3bd61e").then(t)}},{path:"/pages/451b73/index.html",redirect:"/pages/451b73/"},{path:"/12.数据库/05.KV数据库/01.Redis/01.Redis面试总结.html",redirect:"/pages/451b73/"},{name:"v-2a8b7fae",path:"/pages/94e9d6/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2a8b7fae").then(t)}},{path:"/pages/94e9d6/index.html",redirect:"/pages/94e9d6/"},{path:"/12.数据库/05.KV数据库/01.Redis/02.Redis应用指南.html",redirect:"/pages/94e9d6/"},{name:"v-a0623b42",path:"/pages/ed757c/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-a0623b42").then(t)}},{path:"/pages/ed757c/index.html",redirect:"/pages/ed757c/"},{path:"/12.数据库/05.KV数据库/01.Redis/03.Redis数据类型和应用.html",redirect:"/pages/ed757c/"},{name:"v-80c82fb0",path:"/pages/4de901/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-80c82fb0").then(t)}},{path:"/pages/4de901/index.html",redirect:"/pages/4de901/"},{path:"/12.数据库/05.KV数据库/01.Redis/04.Redis持久化.html",redirect:"/pages/4de901/"},{name:"v-26fbb292",path:"/pages/379cd8/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-26fbb292").then(t)}},{path:"/pages/379cd8/index.html",redirect:"/pages/379cd8/"},{path:"/12.数据库/05.KV数据库/01.Redis/05.Redis复制.html",redirect:"/pages/379cd8/"},{name:"v-18f5009d",path:"/pages/615afe/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-18f5009d").then(t)}},{path:"/pages/615afe/index.html",redirect:"/pages/615afe/"},{path:"/12.数据库/05.KV数据库/01.Redis/06.Redis哨兵.html",redirect:"/pages/615afe/"},{name:"v-5b5c3224",path:"/pages/77dfbe/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-5b5c3224").then(t)}},{path:"/pages/77dfbe/index.html",redirect:"/pages/77dfbe/"},{path:"/12.数据库/05.KV数据库/01.Redis/07.Redis集群.html",redirect:"/pages/77dfbe/"},{name:"v-2ccee58b",path:"/pages/1fc9c4/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2ccee58b").then(t)}},{path:"/pages/1fc9c4/index.html",redirect:"/pages/1fc9c4/"},{path:"/12.数据库/05.KV数据库/01.Redis/08.Redis实战.html",redirect:"/pages/1fc9c4/"},{name:"v-5ed748af",path:"/pages/537098/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-5ed748af").then(t)}},{path:"/pages/537098/index.html",redirect:"/pages/537098/"},{path:"/12.数据库/05.KV数据库/01.Redis/20.Redis运维.html",redirect:"/pages/537098/"},{name:"v-671a08c2",path:"/pages/fe3808/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-671a08c2").then(t)}},{path:"/pages/fe3808/index.html",redirect:"/pages/fe3808/"},{path:"/12.数据库/05.KV数据库/01.Redis/",redirect:"/pages/fe3808/"},{name:"v-dbc2e0c0",path:"/pages/7ab03c/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-dbc2e0c0").then(t)}},{path:"/pages/7ab03c/index.html",redirect:"/pages/7ab03c/"},{path:"/12.数据库/06.列式数据库/01.Hbase.html",redirect:"/pages/7ab03c/"},{name:"v-1c459100",path:"/pages/ca3ca5/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-1c459100").then(t)}},{path:"/pages/ca3ca5/index.html",redirect:"/pages/ca3ca5/"},{path:"/12.数据库/06.列式数据库/02.Cassandra.html",redirect:"/pages/ca3ca5/"},{name:"v-04b75150",path:"/pages/0cb563/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-04b75150").then(t)}},{path:"/pages/0cb563/index.html",redirect:"/pages/0cb563/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/01.Elasticsearch面试总结.html",redirect:"/pages/0cb563/"},{name:"v-41d7fd45",path:"/pages/98c3a5/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-41d7fd45").then(t)}},{path:"/pages/98c3a5/index.html",redirect:"/pages/98c3a5/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/02.Elasticsearch快速入门.html",redirect:"/pages/98c3a5/"},{name:"v-83a32830",path:"/pages/0fb506/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-83a32830").then(t)}},{path:"/pages/0fb506/index.html",redirect:"/pages/0fb506/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/03.Elasticsearch简介.html",redirect:"/pages/0fb506/"},{name:"v-0f15ab6e",path:"/pages/293175/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-0f15ab6e").then(t)}},{path:"/pages/293175/index.html",redirect:"/pages/293175/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/04.Elasticsearch索引.html",redirect:"/pages/293175/"},{name:"v-a1e79a52",path:"/pages/d1bae4/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-a1e79a52").then(t)}},{path:"/pages/d1bae4/index.html",redirect:"/pages/d1bae4/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/05.Elasticsearch映射.html",redirect:"/pages/d1bae4/"},{name:"v-f6b5c434",path:"/pages/83bd15/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-f6b5c434").then(t)}},{path:"/pages/83bd15/index.html",redirect:"/pages/83bd15/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/05.Elasticsearch查询.html",redirect:"/pages/83bd15/"},{name:"v-957680f8",path:"/pages/e1b769/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-957680f8").then(t)}},{path:"/pages/e1b769/index.html",redirect:"/pages/e1b769/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/06.Elasticsearch高亮.html",redirect:"/pages/e1b769/"},{name:"v-2ac3b3cb",path:"/pages/24baff/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2ac3b3cb").then(t)}},{path:"/pages/24baff/index.html",redirect:"/pages/24baff/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/07.Elasticsearch排序.html",redirect:"/pages/24baff/"},{name:"v-164e75b2",path:"/pages/f89f66/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-164e75b2").then(t)}},{path:"/pages/f89f66/index.html",redirect:"/pages/f89f66/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/08.Elasticsearch聚合.html",redirect:"/pages/f89f66/"},{name:"v-665de124",path:"/pages/a5a001/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-665de124").then(t)}},{path:"/pages/a5a001/index.html",redirect:"/pages/a5a001/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/09.Elasticsearch分析器.html",redirect:"/pages/a5a001/"},{name:"v-9a8c12f8",path:"/pages/2d95ce/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-9a8c12f8").then(t)}},{path:"/pages/2d95ce/index.html",redirect:"/pages/2d95ce/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/10.Elasticsearch性能优化.html",redirect:"/pages/2d95ce/"},{name:"v-3c818a28",path:"/pages/4b1907/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-3c818a28").then(t)}},{path:"/pages/4b1907/index.html",redirect:"/pages/4b1907/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/11.ElasticsearchRestApi.html",redirect:"/pages/4b1907/"},{name:"v-7c0bbdd0",path:"/pages/201e43/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-7c0bbdd0").then(t)}},{path:"/pages/201e43/index.html",redirect:"/pages/201e43/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/12.ElasticsearchHighLevelRestJavaApi.html",redirect:"/pages/201e43/"},{name:"v-2a07dc5d",path:"/pages/9a2546/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2a07dc5d").then(t)}},{path:"/pages/9a2546/index.html",redirect:"/pages/9a2546/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/13.Elasticsearch集群和分片.html",redirect:"/pages/9a2546/"},{name:"v-717638d6",path:"/pages/fdaf15/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-717638d6").then(t)}},{path:"/pages/fdaf15/index.html",redirect:"/pages/fdaf15/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/20.Elasticsearch运维.html",redirect:"/pages/fdaf15/"},{name:"v-2c1e440a",path:"/pages/74675e/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2c1e440a").then(t)}},{path:"/pages/74675e/index.html",redirect:"/pages/74675e/"},{path:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/",redirect:"/pages/74675e/"},{name:"v-778f3185",path:"/pages/553160/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-778f3185").then(t)}},{path:"/pages/553160/index.html",redirect:"/pages/553160/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/01.Elastic快速入门.html",redirect:"/pages/553160/"},{name:"v-ec0e4a94",path:"/pages/b7f079/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-ec0e4a94").then(t)}},{path:"/pages/b7f079/index.html",redirect:"/pages/b7f079/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/02.Elastic技术栈之Filebeat.html",redirect:"/pages/b7f079/"},{name:"v-15d3f40c",path:"/pages/7c067f/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-15d3f40c").then(t)}},{path:"/pages/7c067f/index.html",redirect:"/pages/7c067f/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/03.Filebeat运维.html",redirect:"/pages/7c067f/"},{name:"v-6550ec14",path:"/pages/002159/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-6550ec14").then(t)}},{path:"/pages/002159/index.html",redirect:"/pages/002159/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/04.Elastic技术栈之Kibana.html",redirect:"/pages/002159/"},{name:"v-db66b0d8",path:"/pages/fc47af/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-db66b0d8").then(t)}},{path:"/pages/fc47af/index.html",redirect:"/pages/fc47af/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/05.Kibana运维.html",redirect:"/pages/fc47af/"},{name:"v-8ad847d4",path:"/pages/55ce99/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-8ad847d4").then(t)}},{path:"/pages/55ce99/index.html",redirect:"/pages/55ce99/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/06.Elastic技术栈之Logstash.html",redirect:"/pages/55ce99/"},{name:"v-2afcea96",path:"/pages/92df30/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-2afcea96").then(t)}},{path:"/pages/92df30/index.html",redirect:"/pages/92df30/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/07.Logstash运维.html",redirect:"/pages/92df30/"},{name:"v-0d48f15b",path:"/pages/7bf7f7/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-0d48f15b").then(t)}},{path:"/pages/7bf7f7/index.html",redirect:"/pages/7bf7f7/"},{path:"/12.数据库/07.搜索引擎数据库/02.Elastic/",redirect:"/pages/7bf7f7/"},{name:"v-72683126",path:"/pages/012488/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-72683126").then(t)}},{path:"/pages/012488/index.html",redirect:"/pages/012488/"},{path:"/12.数据库/",redirect:"/pages/012488/"},{name:"v-c7c85e2e",path:"/archives/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-c7c85e2e").then(t)}},{path:"/archives/index.html",redirect:"/archives/"},{path:"/@pages/archivesPage.html",redirect:"/archives/"},{name:"v-03067a49",path:"/categories/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-03067a49").then(t)}},{path:"/categories/index.html",redirect:"/categories/"},{path:"/@pages/categoriesPage.html",redirect:"/categories/"},{name:"v-be824b2e",path:"/tags/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-be824b2e").then(t)}},{path:"/tags/index.html",redirect:"/tags/"},{path:"/@pages/tagsPage.html",redirect:"/tags/"},{name:"v-91a7d35c",path:"/",component:yl,beforeEnter:(n,e,t)=>{co("Layout","v-91a7d35c").then(t)}},{path:"/index.html",redirect:"/"},{path:"*",component:yl}],_l={title:"DB-TUTORIAL",description:"☕ db-tutorial 是一个数据库教程。",base:"/db-tutorial/",headTags:[["link",{rel:"icon",href:"/db-tutorial/img/favicon.ico"}],["meta",{name:"keywords",content:"vuepress,theme,blog,vdoing"}],["meta",{name:"theme-color",content:"#11a8cd"}]],pages:[{title:"Nosql技术选型",frontmatter:{title:"Nosql技术选型",date:"2020-02-09T02:18:58.000Z",categories:["数据库","数据库综合"],tags:["数据库","综合","Nosql"],permalink:"/pages/0e1012/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BB%BC%E5%90%88/01.Nosql%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B.html",relativePath:"12.数据库/01.数据库综合/01.Nosql技术选型.md",key:"v-01aea2f9",path:"/pages/0e1012/",headers:[{level:2,title:"一、Nosql 简介",slug:"一、nosql-简介",normalizedTitle:"一、nosql 简介",charIndex:19},{level:2,title:"二、列式数据库",slug:"二、列式数据库",normalizedTitle:"二、列式数据库",charIndex:1155},{level:3,title:"列式数据库原理",slug:"列式数据库原理",normalizedTitle:"列式数据库原理",charIndex:1316},{level:3,title:"列式数据库产品",slug:"列式数据库产品",normalizedTitle:"列式数据库产品",charIndex:1485},{level:3,title:"列式数据库特性",slug:"列式数据库特性",normalizedTitle:"列式数据库特性",charIndex:1807},{level:3,title:"列式数据库使用场景",slug:"列式数据库使用场景",normalizedTitle:"列式数据库使用场景",charIndex:2495},{level:2,title:"三、K-V 数据库",slug:"三、k-v-数据库",normalizedTitle:"三、k-v 数据库",charIndex:2885},{level:3,title:"K-V 数据库产品",slug:"k-v-数据库产品",normalizedTitle:"k-v 数据库产品",charIndex:3043},{level:3,title:"K-V 数据库特性",slug:"k-v-数据库特性",normalizedTitle:"k-v 数据库特性",charIndex:3713},{level:3,title:"K-V 数据库使用场景",slug:"k-v-数据库使用场景",normalizedTitle:"k-v 数据库使用场景",charIndex:4154},{level:2,title:"四、文档数据库",slug:"四、文档数据库",normalizedTitle:"四、文档数据库",charIndex:4383},{level:3,title:"文档数据库产品",slug:"文档数据库产品",normalizedTitle:"文档数据库产品",charIndex:4598},{level:3,title:"文档数据库特性",slug:"文档数据库特性",normalizedTitle:"文档数据库特性",charIndex:4957},{level:3,title:"文档数据库使用场景",slug:"文档数据库使用场景",normalizedTitle:"文档数据库使用场景",charIndex:5447},{level:2,title:"五、全文搜索引擎",slug:"五、全文搜索引擎",normalizedTitle:"五、全文搜索引擎",charIndex:5620},{level:3,title:"搜索引擎原理",slug:"搜索引擎原理",normalizedTitle:"搜索引擎原理",charIndex:5814},{level:3,title:"搜索引擎产品",slug:"搜索引擎产品",normalizedTitle:"搜索引擎产品",charIndex:6029},{level:3,title:"搜索引擎特性",slug:"搜索引擎特性",normalizedTitle:"搜索引擎特性",charIndex:6422},{level:3,title:"搜索引擎场景",slug:"搜索引擎场景",normalizedTitle:"搜索引擎场景",charIndex:6848},{level:2,title:"六、图数据库",slug:"六、图数据库",normalizedTitle:"六、图数据库",charIndex:6979},{level:3,title:"图数据库产品",slug:"图数据库产品",normalizedTitle:"图数据库产品",charIndex:7123},{level:3,title:"图数据库特性",slug:"图数据库特性",normalizedTitle:"图数据库特性",charIndex:7592},{level:3,title:"图数据库场景",slug:"图数据库场景",normalizedTitle:"图数据库场景",charIndex:8339},{level:2,title:"七、总结",slug:"七、总结",normalizedTitle:"七、总结",charIndex:8504},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:8990}],headersStr:"一、Nosql 简介 二、列式数据库 列式数据库原理 列式数据库产品 列式数据库特性 列式数据库使用场景 三、K-V 数据库 K-V 数据库产品 K-V 数据库特性 K-V 数据库使用场景 四、文档数据库 文档数据库产品 文档数据库特性 文档数据库使用场景 五、全文搜索引擎 搜索引擎原理 搜索引擎产品 搜索引擎特性 搜索引擎场景 六、图数据库 图数据库产品 图数据库特性 图数据库场景 七、总结 参考资料",content:'# Nosql 技术选型\n\n\n\n\n# 一、Nosql 简介\n\n传统的关系型数据库存在以下缺点：\n\n * 大数据场景下 I/O 较高 - 因为数据是按行存储，即使只针对其中某一列进行运算，关系型数据库也会将整行数据从存储设备中读入内存，导致 I/O 较高。\n * 存储的是行记录，无法存储数据结构。\n * 表结构 schema 扩展不方便 - 如要需要修改表结构，需要执行执行 DDL(data definition language)，语句修改，修改期间会导致锁表，部分服务不可用。\n * 全文搜索功能较弱 - 关系型数据库下只能够进行子字符串的匹配查询，当表的数据逐渐变大的时候，LIKE 查询的匹配会非常慢，即使在有索引的情况下。况且关系型数据库也不应该对文本字段进行索引。\n * 存储和处理复杂关系型数据功能较弱 - 许多应用程序需要了解和导航高度连接数据之间的关系，才能启用社交应用程序、推荐引擎、欺诈检测、知识图谱、生命科学和 IT/网络等用例。然而传统的关系数据库并不善于处理数据点之间的关系。它们的表格数据模型和严格的模式使它们很难添加新的或不同种类的关联信息。\n\n随着大数据时代的到来，越来越多的网站、应用系统需要支撑海量数据存储，高并发请求、高可用、高可扩展性等特性要求。传统的关系型数据库在应付这些调整已经显得力不从心，暴露了许多能以克服的问题。由此，各种各样的 NoSQL（Not Only SQL）数据库作为传统关系型数据的一个有力补充得到迅猛发展。\n\n\n\nNoSQL，泛指非关系型的数据库，可以理解为 SQL 的一个有力补充。\n\n在 NoSQL 许多方面性能大大优于非关系型数据库的同时，往往也伴随一些特性的缺失，比较常见的，是事务库事务功能的缺失。 数据库事务正确执行的四个基本要素：ACID 如下：\n\n    名称                描述\nA   Atomicity (原子性)   一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。\n                      事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。\nC   Consistency 一致性   在事务开始之前和事务结束以后，数据的数据的一致性约束没有被破坏。\nI   Isolation 隔离性     数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。\nD   Durability 持久性    事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。\n\n下面介绍 5 大类 NoSQL 数据针对传统关系型数据库的缺点提供的解决方案：\n\n\n# 二、列式数据库\n\n列式数据库是以列相关存储架构进行数据存储的数据库，主要适合于批量数据处理和即时查询。\n\n相对应的是行式数据库，数据以行相关的存储体系架构进行空间分配，主要适合于小批量的数据处理，常用于联机事务型数据处理。\n\n基于列式数据库的列列存储特性，可以解决某些特定场景下关系型数据库 I/O 较高的问题。\n\n\n# 列式数据库原理\n\n传统关系型数据库是按照行来存储数据库，称为“行式数据库”，而列式数据库是按照列来存储数据。\n\n将表放入存储系统中有两种方法，而我们绝大部分是采用行存储的。 行存储法是将各行放入连续的物理位置，这很像传统的记录和文件系统。 列存储法是将数据按照列存储到数据库中，与行存储类似，下图是两种存储方法的图形化解释：\n\n\n\n\n# 列式数据库产品\n\n * HBase\n   \n   \n   \n   HBase 是一个开源的非关系型分布式数据库（NoSQL），它参考了谷歌的 BigTable 建模，实现的编程语言为 Java。它是 Apache 软件基金会的 Hadoop 项目的一部分，运行于 HDFS 文件系统之上，为 Hadoop 提供类似于 BigTable 规模的服务。因此，它可以容错地存储海量稀疏的数据。\n\n * BigTable\n   \n   \n   \n   BigTable 是一种压缩的、高性能的、高可扩展性的，基于 Google 文件系统（Google File System，GFS）的数据存储系统，用于存储大规模结构化数据，适用于云端计算。\n\n\n# 列式数据库特性\n\n优点如下：\n\n * 高效的储存空间利用率\n\n列式数据库由于其针对不同列的数据特征而发明的不同算法，使其往往有比行式数据库高的多的压缩率，普通的行式数据库一般压缩率在 3：1 到 5：1 左右，而列式数据库的压缩率一般在 8：1 到 30：1 左右。 比较常见的，通过字典表压缩数据： 下面中才是那张表本来的样子。经过字典表进行数据压缩后，表中的字符串才都变成数字了。正因为每个字符串在字典表里只出现一次了，所以达到了压缩的目的(有点像规范化和非规范化 Normalize 和 Denomalize)\n\n\n\n * 查询效率高\n\n读取多条数据的同一列效率高，因为这些列都是存储在一起的，一次磁盘操作可以数据的指定列全部读取到内存中。 下图通过一条查询的执行过程说明列式存储(以及数据压缩)的优点\n\n\n\n执行步骤如下：\ni. 去字典表里找到字符串对应数字(只进行一次字符串比较)。\nii. 用数字去列表里匹配，匹配上的位置设为1。\niii. 把不同列的匹配结果进行位运算得到符合所有条件的记录下标。\niv. 使用这个下标组装出最终的结果集。\n复制代码\n\n\n * 适合做聚合操作\n * 适合大量的数据而不是小数据\n\n缺点如下：\n\n * 不适合扫描小量数据\n * 不适合随机的更新\n * 不适合做含有删除和更新的实时操作\n * 单行的数据是 ACID 的，多行的事务时，不支持事务的正常回滚，支持 I(Isolation)隔离性(事务串行提交)，D(Durability)持久性，不能保证 A(Atomicity)原子性， C(Consistency)一致性\n\n\n# 列式数据库使用场景\n\n以 HBase 为例说明：\n\n * 大数据量 （100s TB 级数据） 且有快速随机访问的需求。增长量无法预估的应用，需要进行优雅的数据扩展的 HBase 支持在线扩展，即使在一段时间内数据量呈井喷式增长，也可以通过 HBase 横向扩展来满足功能。\n * 写密集型应用，每天写入量巨大，而相对读数量较小的应用 比如 IM 的历史消息，游戏的日志等等\n * 不需要复杂查询条件来查询数据的应用 HBase 只支持基于 rowkey 的查询，对于 HBase 来说，单条记录或者小范围的查询是可以接受的，大范围的查询由于分布式的原因，可能在性能上有点影响，HBase 不适用于有 join，多级索引，表关系复杂的数据模型。\n * 对性能和可靠性要求非常高的应用，由于 HBase 本身没有单点故障，可用性非常高。\n * 存储结构化和半结构化的数据。\n\n\n# 三、K-V 数据库\n\nK-V 数据库指的是使用键值(key-value)存储的数据库，其数据按照键值对的形式进行组织、索引和存储。\n\nKV 存储非常适合存储不涉及过多数据关系业务关系的数据，同时能有效减少读写磁盘的次数，比 SQL 数据库存储拥有更好的读写性能，能够解决关系型数据库无法存储数据结构的问题。\n\n\n# K-V 数据库产品\n\n * Redis\n   \n   \n   \n   Redis 是一个使用 ANSI C 编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从 2015 年 6 月开始，Redis 的开发由 Redis Labs 赞助，而 2013 年 5 月至 2015 年 6 月期间，其开发由 Pivotal 赞助。在 2013 年 5 月之前，其开发由 VMware 赞助。根据月度排行网站 DB-Engines.com 的数据显示，Redis 是最流行的键值对存储数据库。\n\n * Cassandra\n   \n   \n   \n   Apache Cassandra（社区内一般简称为 C*）是一套开源分布式 NoSQL 数据库系统。它最初由 Facebook 开发，用于储存收件箱等简单格式数据，集 Google BigTable 的数据模型与 Amazon Dynamo 的完全分布式架构于一身。Facebook 于 2008 将 Cassandra 开源，此后，由于 Cassandra 良好的可扩展性和性能，被 Apple, Comcast,Instagram, Spotify, eBay, Rackspace, Netflix 等知名网站所采用，成为了一种流行的分布式结构化数据存储方案。\n\n * LevelDB\n   \n   \n   \n   LevelDB 是一个由 Google 公司所研发的键／值对（Key/Value Pair）嵌入式数据库管理系统编程库， 以开源的 BSD 许可证发布。\n\n\n# K-V 数据库特性\n\n以 Redis 为例：\n\n优点如下：\n\n * 性能极高 - Redis 能支持超过 10W 的 TPS。\n * 丰富的数据类型 - Redis 支持包括 String，Hash，List，Set，Sorted Set，Bitmap 和 hyperloglog。\n * 丰富的特性 - Redis 还支持 publish/subscribe、通知、key 过期等等特性。\n\n缺点如下： 针对 ACID，Redis 事务不能支持原子性和持久性(A 和 D)，只支持隔离性和一致性(I 和 C) 特别说明一下，这里所说的无法保证原子性，是针对 Redis 的事务操作，因为事务是不支持回滚（roll back），而因为 Redis 的单线程模型，Redis 的普通操作是原子性的。\n\n大部分业务不需要严格遵循 ACID 原则，例如游戏实时排行榜，粉丝关注等场景，即使部分数据持久化失败，其实业务影响也非常小。因此在设计方案时，需要根据业务特征和要求来做选择\n\n\n# K-V 数据库使用场景\n\n * 适用场景 - 储存用户信息(比如会话)、配置文件、参数、购物车等等。这些信息一般都和 ID（键）挂钩。\n\n * 不适用场景\n   \n   * 需要通过值来查询，而不是键来查询。Key-Value 数据库中根本没有通过值查询的途径。\n   * 需要储存数据之间的关系。在 Key-Value 数据库中不能通过两个或以上的键来关联数据\n   * 需要事务的支持。在 Key-Value 数据库中故障产生时不可以进行回滚。\n\n\n# 四、文档数据库\n\n文档数据库（也称为文档型数据库）是旨在将半结构化数据存储为文档的一种数据库，它可以解决关系型数据库表结构 schema 扩展不方便的问题。文档数据库通常以 JSON 或 XML 格式存储数据。\n\n由于文档数据库的 no-schema 特性，可以存储和读取任意数据。由于使用的数据格式是 JSON 或者 XML，无需在使用前定义字段，读取一个 JSON 中不存在的字段也不会导致 SQL 那样的语法错误。\n\n\n# 文档数据库产品\n\n * MongoDB\n   \n   \n   \n   MongoDB是一种面向文档的数据库管理系统，由 C++ 撰写而成，以此来解决应用程序开发社区中的大量现实问题。2007 年 10 月，MongoDB 由 10gen 团队所发展。2009 年 2 月首度推出。\n\n * CouchDB\n   \n   \n   \n   Apache CouchDB 是一个开源数据库，专注于易用性和成为"完全拥抱 web 的数据库"。它是一个使用 JSON 作为存储格式，JavaScript 作为查询语言，MapReduce 和 HTTP 作为 API 的 NoSQL 数据库。其中一个显著的功能就是多主复制。CouchDB 的第一个版本发布在 2005 年，在 2008 年成为了 Apache 的项目。\n\n\n# 文档数据库特性\n\n以 MongoDB 为例进行说明\n\n优点如下：\n\n * 容易存储复杂数据结构 - JSON 是一种强大的描述语言，能够描述复杂的数据结构。\n * 容易变更数据结构 - 无需像关系型数据库一样先执行 DDL 语句修改表结构，程序代码直接读写即可。\n * 容易兼容历史数据 - 对于历史数据，即使没有新增的字段，也不会导致错误，只会返回空值，此时代码兼容处理即可。\n\n缺点如下：\n\n * 部分支持事务\n   * Atomicity(原子性) 仅支持单行/文档级原子性，不支持多行、多文档、多语句原子性。\n   * Isolation(隔离性) 隔离级别仅支持已提交读（Read committed）级别，可能导致不可重复读，幻读的问题。\n * 不支持复杂查询 - 例如 join 查询，如果需要 join 查询，需要多次操作数据库。\n\nMongonDB 还是支持多文档事务的 Consistency(一致性)和 Durability(持久性)\n\n虽然官方宣布 MongoDB 将在 4.0 版本中正式推出多文档 ACID 事务支持，最后落地情况还有待见证。\n\n\n# 文档数据库使用场景\n\n适用场景：\n\n * 大数据量，且未来数据增长很快\n * 表结构不明确，且字段在不断增加，例如内容管理系统，信息管理系统\n\n不适用场景：\n\n * 支持事务 - 在不同的文档上需要添加事务。Document-Oriented 数据库并不支持文档间的事务\n * 支持复杂查询 - 多个文档直接需要复杂查询，例如 join\n\n\n# 五、全文搜索引擎\n\n传统关系型数据库主要通过索引来达到快速查询的目的，在全文搜索的业务下，索引也无能为力，主要体现在：\n\n * 全文搜索的条件可以随意排列组合，如果通过索引来满足，则索引的数量非常多\n * 全文搜索的模糊匹配方式，索引无法满足，只能用 LIKE 查询，而 LIKE 查询是整表扫描，效率非常低\n\n而全文搜索引擎的出现，正是解决关系型数据库全文搜索功能较弱的问题。\n\n\n# 搜索引擎原理\n\n全文搜索引擎的技术原理称为 倒排索引（inverted index），是一种索引方法，其基本原理是建立单词到文档的索引。与之相对是，是“正排索引”，其基本原理是建立文档到单词的索引。\n\n现在有如下文档集合：\n\n\n\n正排索引得到索引如下：\n\n\n\n可见，正排索引适用于根据文档名称查询文档内容\n\n简单的倒排索引如下：\n\n\n\n带有单词频率信息的倒排索引如下：\n\n\n\n可见，倒排索引适用于根据关键词来查询文档内容\n\n\n# 搜索引擎产品\n\n * Elasticsearch\n   \n   \n   \n   Elasticsearch 是一个基于 Lucene 的搜索引擎。它提供了一个分布式，多租户 -能够全文搜索与发动机 HTTP Web 界面和无架构 JSON 文件。Elasticsearch 是用 Java 开发的，并根据 Apache License 的条款作为开源发布。根据 DB-Engines 排名，Elasticsearch 是最受欢迎的企业搜索引擎，后面是基于 Lucene 的 Apache Solr。\n\n * Solr\n   \n   \n   \n   Solr 是 Apache Lucene 项目的开源企业搜索平台。其主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本（如 Word、PDF）的处理。Solr 是高度可扩展的，并提供了分布式搜索和索引复制\n\n\n# 搜索引擎特性\n\n以 Elasticsearch 为例： 优点如下：\n\n * 查询效率高 - 对海量数据进行近实时的处理\n * 可扩展性 - 基于集群环境可以方便横向扩展，可以承载 PB 级数据\n * 高可用 - Elasticsearch 集群弹性-他们将发现新的或失败的节点，重组和重新平衡数据，确保数据是安全的和可访问的\n\n缺点如下：\n\n * 部分支持事务 - 单一文档的数据是 ACID 的，包含多个文档的事务时不支持事务的正常回滚，支持 I(Isolation)隔离性（基于乐观锁机制的），D(Durability)持久性，不支持 A(Atomicity)原子性，C(Consistency)一致性\n * 对类似数据库中通过外键的复杂的多表关联操作支持较弱。\n * 读写有一定延时，写入的数据，最快 1s 中能被检索到\n * 更新性能较低，底层实现是先删数据，再插入新数据\n * 内存占用大，因为 Lucene 将索引部分加载到内存中\n\n\n# 搜索引擎场景\n\n适用场景如下：\n\n * 搜索引擎和数据分析引擎 - 全文检索，结构化检索，数据分析\n * 对海量数据进行近实时的处理 - 可以将海量数据分散到多台服务器上去存储和检索\n\n不适用场景如下：\n\n * 数据需要频繁更新\n * 需要复杂关联查询\n\n\n# 六、图数据库\n\n\n\n图形数据库应用图论存储实体之间的关系信息。最常见例子就是社会网络中人与人之间的关系。关系型数据库用于存储“关系型”数据的效果并不好，其查询复杂、缓慢、超出预期，而图形数据库的独特设计恰恰弥补了这个缺陷，解决关系型数据库存储和处理复杂关系型数据功能较弱的问题。\n\n\n# 图数据库产品\n\n * Neo4j\n   \n   \n   \n   Neo4j 是由 Neo4j，Inc。开发的图形数据库管理系统。由其开发人员描述为具有原生图存储和处理的符合 ACID 的事务数据库，根据 DB-Engines 排名， Neo4j 是最流行的图形数据库。\n\n * ArangoDB\n   \n   \n   \n   ArangoDB 是由 triAGENS GmbH 开发的原生多模型数据库系统。数据库系统支持三个重要的数据模型（键/值，文档，图形），其中包含一个数据库核心和统一查询语言 AQL（ArangoDB 查询语言）。查询语言是声明性的，允许在单个查询中组合不同的数据访问模式。ArangoDB 是一个 NoSQL 数据库系统，但 AQL 在很多方面与 SQL 类似。\n\n * Titan\n   \n   \n   \n   Titan 是一个可扩展的图形数据库，针对存储和查询包含分布在多机群集中的数百亿个顶点和边缘的图形进行了优化。Titan 是一个事务性数据库，可以支持数千个并发用户实时执行复杂的图形遍历。\n\n\n# 图数据库特性\n\n以 Neo4j 为例：\n\nNeo4j 使用数据结构中图（graph）的概念来进行建模。 Neo4j 中两个最基本的概念是节点和边。节点表示实体，边则表示实体之间的关系。节点和边都可以有自己的属性。不同实体通过各种不同的关系关联起来，形成复杂的对象图。\n\n针对关系数据，2 种 2 数据库的存储结构不同：\n\n\n\nNeo4j 中，存储节点时使用了”index-free adjacency”，即每个节点都有指向其邻居节点的指针，可以让我们在 O(1)的时间内找到邻居节点。另外，按照官方的说法，在 Neo4j 中边是最重要的,是”first-class entities”，所以单独存储，这有利于在图遍历的时候提高速度，也可以很方便地以任何方向进行遍历\n\n\n\n如下优点：\n\n * 高性能 - 图的遍历是图数据结构所具有的独特算法，即从一个节点开始，根据其连接的关系，可以快速和方便地找出它的邻近节点。这种查找数据的方法并不受数据量的大小所影响，因为邻近查询始终查找的是有限的局部数据，不会对整个数据库进行搜索\n * 设计的灵活性 - 数据结构的自然伸展特性及其非结构化的数据格式，让图数据库设计可以具有很大的伸缩性和灵活性。因为随着需求的变化而增加的节点、关系及其属性并不会影响到原来数据的正常使用\n * 开发的敏捷性 - 直观明了的数据模型，从需求的讨论开始，到程序开发和实现，以及最终保存在数据库中的样子，它的模样似乎没有什么变化，甚至可以说本来就是一模一样的\n * 完全支持 ACID - 不像别的 NoSQL 数据库 Neo4j 还具有完全事务管理特性，完全支持 ACID 事务管理\n\n缺点如下：\n\n * 存在支持节点，关系和属性的数量的限制。\n * 不支持拆分。\n\n\n# 图数据库场景\n\n适用场景如下：\n\n * 关系性强的数据中，如社交网络\n * 推荐引擎。如果我们将数据以图的形式表现，那么将会非常有益于推荐的制定\n\n不适用场景如下：\n\n * 记录大量基于事件的数据（例如日志条目或传感器数据）\n * 对大规模分布式数据进行处理\n * 保存在关系型数据库中的结构化数据\n * 二进制数据存储\n\n\n# 七、总结\n\n关系型数据库和 NoSQL 数据库的选型，往往需要考虑几个指标：\n\n * 数据量\n * 并发量\n * 实时性\n * 一致性要求\n * 读写分布和类型\n * 安全性\n * 运维成本\n\n常见软件系统数据库选型参考如下：\n\n * 中后台管理型系统 - 如运营系统，数据量少，并发量小，首选关系型数据库。\n * 大流量系统 - 如电商单品页，后台考虑选关系型数据库，前台考虑选内存型数据库。\n * 日志型系统 - 原始数据考虑选列式数据库，日志搜索考虑选搜索引擎。\n * 搜索型系统 - 例如站内搜索，非通用搜索，如商品搜索，后台考虑选关系型数据库，前台考虑选搜索引擎。\n * 事务型系统 - 如库存，交易，记账，考虑选关系型数据库+K-V 数据库（作为缓存）+分布式事务。\n * 离线计算 - 如大量数据分析，考虑选列式数据库或关系型数据。\n * 实时计算 - 如实时监控，可以考虑选内存型数据库或者列式数据库。\n\n设计实践中，要基于需求、业务驱动架构，无论选用 RDB/NoSQL/DRDB,一定是以需求为导向，最终数据存储方案必然是各种权衡的综合性设计\n\n\n# 参考资料\n\n * NoSQL 还是 SQL ？这一篇讲清楚',normalizedContent:'# nosql 技术选型\n\n\n\n\n# 一、nosql 简介\n\n传统的关系型数据库存在以下缺点：\n\n * 大数据场景下 i/o 较高 - 因为数据是按行存储，即使只针对其中某一列进行运算，关系型数据库也会将整行数据从存储设备中读入内存，导致 i/o 较高。\n * 存储的是行记录，无法存储数据结构。\n * 表结构 schema 扩展不方便 - 如要需要修改表结构，需要执行执行 ddl(data definition language)，语句修改，修改期间会导致锁表，部分服务不可用。\n * 全文搜索功能较弱 - 关系型数据库下只能够进行子字符串的匹配查询，当表的数据逐渐变大的时候，like 查询的匹配会非常慢，即使在有索引的情况下。况且关系型数据库也不应该对文本字段进行索引。\n * 存储和处理复杂关系型数据功能较弱 - 许多应用程序需要了解和导航高度连接数据之间的关系，才能启用社交应用程序、推荐引擎、欺诈检测、知识图谱、生命科学和 it/网络等用例。然而传统的关系数据库并不善于处理数据点之间的关系。它们的表格数据模型和严格的模式使它们很难添加新的或不同种类的关联信息。\n\n随着大数据时代的到来，越来越多的网站、应用系统需要支撑海量数据存储，高并发请求、高可用、高可扩展性等特性要求。传统的关系型数据库在应付这些调整已经显得力不从心，暴露了许多能以克服的问题。由此，各种各样的 nosql（not only sql）数据库作为传统关系型数据的一个有力补充得到迅猛发展。\n\n\n\nnosql，泛指非关系型的数据库，可以理解为 sql 的一个有力补充。\n\n在 nosql 许多方面性能大大优于非关系型数据库的同时，往往也伴随一些特性的缺失，比较常见的，是事务库事务功能的缺失。 数据库事务正确执行的四个基本要素：acid 如下：\n\n    名称                描述\na   atomicity (原子性)   一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。\n                      事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。\nc   consistency 一致性   在事务开始之前和事务结束以后，数据的数据的一致性约束没有被破坏。\ni   isolation 隔离性     数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。\nd   durability 持久性    事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。\n\n下面介绍 5 大类 nosql 数据针对传统关系型数据库的缺点提供的解决方案：\n\n\n# 二、列式数据库\n\n列式数据库是以列相关存储架构进行数据存储的数据库，主要适合于批量数据处理和即时查询。\n\n相对应的是行式数据库，数据以行相关的存储体系架构进行空间分配，主要适合于小批量的数据处理，常用于联机事务型数据处理。\n\n基于列式数据库的列列存储特性，可以解决某些特定场景下关系型数据库 i/o 较高的问题。\n\n\n# 列式数据库原理\n\n传统关系型数据库是按照行来存储数据库，称为“行式数据库”，而列式数据库是按照列来存储数据。\n\n将表放入存储系统中有两种方法，而我们绝大部分是采用行存储的。 行存储法是将各行放入连续的物理位置，这很像传统的记录和文件系统。 列存储法是将数据按照列存储到数据库中，与行存储类似，下图是两种存储方法的图形化解释：\n\n\n\n\n# 列式数据库产品\n\n * hbase\n   \n   \n   \n   hbase 是一个开源的非关系型分布式数据库（nosql），它参考了谷歌的 bigtable 建模，实现的编程语言为 java。它是 apache 软件基金会的 hadoop 项目的一部分，运行于 hdfs 文件系统之上，为 hadoop 提供类似于 bigtable 规模的服务。因此，它可以容错地存储海量稀疏的数据。\n\n * bigtable\n   \n   \n   \n   bigtable 是一种压缩的、高性能的、高可扩展性的，基于 google 文件系统（google file system，gfs）的数据存储系统，用于存储大规模结构化数据，适用于云端计算。\n\n\n# 列式数据库特性\n\n优点如下：\n\n * 高效的储存空间利用率\n\n列式数据库由于其针对不同列的数据特征而发明的不同算法，使其往往有比行式数据库高的多的压缩率，普通的行式数据库一般压缩率在 3：1 到 5：1 左右，而列式数据库的压缩率一般在 8：1 到 30：1 左右。 比较常见的，通过字典表压缩数据： 下面中才是那张表本来的样子。经过字典表进行数据压缩后，表中的字符串才都变成数字了。正因为每个字符串在字典表里只出现一次了，所以达到了压缩的目的(有点像规范化和非规范化 normalize 和 denomalize)\n\n\n\n * 查询效率高\n\n读取多条数据的同一列效率高，因为这些列都是存储在一起的，一次磁盘操作可以数据的指定列全部读取到内存中。 下图通过一条查询的执行过程说明列式存储(以及数据压缩)的优点\n\n\n\n执行步骤如下：\ni. 去字典表里找到字符串对应数字(只进行一次字符串比较)。\nii. 用数字去列表里匹配，匹配上的位置设为1。\niii. 把不同列的匹配结果进行位运算得到符合所有条件的记录下标。\niv. 使用这个下标组装出最终的结果集。\n复制代码\n\n\n * 适合做聚合操作\n * 适合大量的数据而不是小数据\n\n缺点如下：\n\n * 不适合扫描小量数据\n * 不适合随机的更新\n * 不适合做含有删除和更新的实时操作\n * 单行的数据是 acid 的，多行的事务时，不支持事务的正常回滚，支持 i(isolation)隔离性(事务串行提交)，d(durability)持久性，不能保证 a(atomicity)原子性， c(consistency)一致性\n\n\n# 列式数据库使用场景\n\n以 hbase 为例说明：\n\n * 大数据量 （100s tb 级数据） 且有快速随机访问的需求。增长量无法预估的应用，需要进行优雅的数据扩展的 hbase 支持在线扩展，即使在一段时间内数据量呈井喷式增长，也可以通过 hbase 横向扩展来满足功能。\n * 写密集型应用，每天写入量巨大，而相对读数量较小的应用 比如 im 的历史消息，游戏的日志等等\n * 不需要复杂查询条件来查询数据的应用 hbase 只支持基于 rowkey 的查询，对于 hbase 来说，单条记录或者小范围的查询是可以接受的，大范围的查询由于分布式的原因，可能在性能上有点影响，hbase 不适用于有 join，多级索引，表关系复杂的数据模型。\n * 对性能和可靠性要求非常高的应用，由于 hbase 本身没有单点故障，可用性非常高。\n * 存储结构化和半结构化的数据。\n\n\n# 三、k-v 数据库\n\nk-v 数据库指的是使用键值(key-value)存储的数据库，其数据按照键值对的形式进行组织、索引和存储。\n\nkv 存储非常适合存储不涉及过多数据关系业务关系的数据，同时能有效减少读写磁盘的次数，比 sql 数据库存储拥有更好的读写性能，能够解决关系型数据库无法存储数据结构的问题。\n\n\n# k-v 数据库产品\n\n * redis\n   \n   \n   \n   redis 是一个使用 ansi c 编写的开源、支持网络、基于内存、可选持久性的键值对存储数据库。从 2015 年 6 月开始，redis 的开发由 redis labs 赞助，而 2013 年 5 月至 2015 年 6 月期间，其开发由 pivotal 赞助。在 2013 年 5 月之前，其开发由 vmware 赞助。根据月度排行网站 db-engines.com 的数据显示，redis 是最流行的键值对存储数据库。\n\n * cassandra\n   \n   \n   \n   apache cassandra（社区内一般简称为 c*）是一套开源分布式 nosql 数据库系统。它最初由 facebook 开发，用于储存收件箱等简单格式数据，集 google bigtable 的数据模型与 amazon dynamo 的完全分布式架构于一身。facebook 于 2008 将 cassandra 开源，此后，由于 cassandra 良好的可扩展性和性能，被 apple, comcast,instagram, spotify, ebay, rackspace, netflix 等知名网站所采用，成为了一种流行的分布式结构化数据存储方案。\n\n * leveldb\n   \n   \n   \n   leveldb 是一个由 google 公司所研发的键／值对（key/value pair）嵌入式数据库管理系统编程库， 以开源的 bsd 许可证发布。\n\n\n# k-v 数据库特性\n\n以 redis 为例：\n\n优点如下：\n\n * 性能极高 - redis 能支持超过 10w 的 tps。\n * 丰富的数据类型 - redis 支持包括 string，hash，list，set，sorted set，bitmap 和 hyperloglog。\n * 丰富的特性 - redis 还支持 publish/subscribe、通知、key 过期等等特性。\n\n缺点如下： 针对 acid，redis 事务不能支持原子性和持久性(a 和 d)，只支持隔离性和一致性(i 和 c) 特别说明一下，这里所说的无法保证原子性，是针对 redis 的事务操作，因为事务是不支持回滚（roll back），而因为 redis 的单线程模型，redis 的普通操作是原子性的。\n\n大部分业务不需要严格遵循 acid 原则，例如游戏实时排行榜，粉丝关注等场景，即使部分数据持久化失败，其实业务影响也非常小。因此在设计方案时，需要根据业务特征和要求来做选择\n\n\n# k-v 数据库使用场景\n\n * 适用场景 - 储存用户信息(比如会话)、配置文件、参数、购物车等等。这些信息一般都和 id（键）挂钩。\n\n * 不适用场景\n   \n   * 需要通过值来查询，而不是键来查询。key-value 数据库中根本没有通过值查询的途径。\n   * 需要储存数据之间的关系。在 key-value 数据库中不能通过两个或以上的键来关联数据\n   * 需要事务的支持。在 key-value 数据库中故障产生时不可以进行回滚。\n\n\n# 四、文档数据库\n\n文档数据库（也称为文档型数据库）是旨在将半结构化数据存储为文档的一种数据库，它可以解决关系型数据库表结构 schema 扩展不方便的问题。文档数据库通常以 json 或 xml 格式存储数据。\n\n由于文档数据库的 no-schema 特性，可以存储和读取任意数据。由于使用的数据格式是 json 或者 xml，无需在使用前定义字段，读取一个 json 中不存在的字段也不会导致 sql 那样的语法错误。\n\n\n# 文档数据库产品\n\n * mongodb\n   \n   \n   \n   mongodb是一种面向文档的数据库管理系统，由 c++ 撰写而成，以此来解决应用程序开发社区中的大量现实问题。2007 年 10 月，mongodb 由 10gen 团队所发展。2009 年 2 月首度推出。\n\n * couchdb\n   \n   \n   \n   apache couchdb 是一个开源数据库，专注于易用性和成为"完全拥抱 web 的数据库"。它是一个使用 json 作为存储格式，javascript 作为查询语言，mapreduce 和 http 作为 api 的 nosql 数据库。其中一个显著的功能就是多主复制。couchdb 的第一个版本发布在 2005 年，在 2008 年成为了 apache 的项目。\n\n\n# 文档数据库特性\n\n以 mongodb 为例进行说明\n\n优点如下：\n\n * 容易存储复杂数据结构 - json 是一种强大的描述语言，能够描述复杂的数据结构。\n * 容易变更数据结构 - 无需像关系型数据库一样先执行 ddl 语句修改表结构，程序代码直接读写即可。\n * 容易兼容历史数据 - 对于历史数据，即使没有新增的字段，也不会导致错误，只会返回空值，此时代码兼容处理即可。\n\n缺点如下：\n\n * 部分支持事务\n   * atomicity(原子性) 仅支持单行/文档级原子性，不支持多行、多文档、多语句原子性。\n   * isolation(隔离性) 隔离级别仅支持已提交读（read committed）级别，可能导致不可重复读，幻读的问题。\n * 不支持复杂查询 - 例如 join 查询，如果需要 join 查询，需要多次操作数据库。\n\nmongondb 还是支持多文档事务的 consistency(一致性)和 durability(持久性)\n\n虽然官方宣布 mongodb 将在 4.0 版本中正式推出多文档 acid 事务支持，最后落地情况还有待见证。\n\n\n# 文档数据库使用场景\n\n适用场景：\n\n * 大数据量，且未来数据增长很快\n * 表结构不明确，且字段在不断增加，例如内容管理系统，信息管理系统\n\n不适用场景：\n\n * 支持事务 - 在不同的文档上需要添加事务。document-oriented 数据库并不支持文档间的事务\n * 支持复杂查询 - 多个文档直接需要复杂查询，例如 join\n\n\n# 五、全文搜索引擎\n\n传统关系型数据库主要通过索引来达到快速查询的目的，在全文搜索的业务下，索引也无能为力，主要体现在：\n\n * 全文搜索的条件可以随意排列组合，如果通过索引来满足，则索引的数量非常多\n * 全文搜索的模糊匹配方式，索引无法满足，只能用 like 查询，而 like 查询是整表扫描，效率非常低\n\n而全文搜索引擎的出现，正是解决关系型数据库全文搜索功能较弱的问题。\n\n\n# 搜索引擎原理\n\n全文搜索引擎的技术原理称为 倒排索引（inverted index），是一种索引方法，其基本原理是建立单词到文档的索引。与之相对是，是“正排索引”，其基本原理是建立文档到单词的索引。\n\n现在有如下文档集合：\n\n\n\n正排索引得到索引如下：\n\n\n\n可见，正排索引适用于根据文档名称查询文档内容\n\n简单的倒排索引如下：\n\n\n\n带有单词频率信息的倒排索引如下：\n\n\n\n可见，倒排索引适用于根据关键词来查询文档内容\n\n\n# 搜索引擎产品\n\n * elasticsearch\n   \n   \n   \n   elasticsearch 是一个基于 lucene 的搜索引擎。它提供了一个分布式，多租户 -能够全文搜索与发动机 http web 界面和无架构 json 文件。elasticsearch 是用 java 开发的，并根据 apache license 的条款作为开源发布。根据 db-engines 排名，elasticsearch 是最受欢迎的企业搜索引擎，后面是基于 lucene 的 apache solr。\n\n * solr\n   \n   \n   \n   solr 是 apache lucene 项目的开源企业搜索平台。其主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本（如 word、pdf）的处理。solr 是高度可扩展的，并提供了分布式搜索和索引复制\n\n\n# 搜索引擎特性\n\n以 elasticsearch 为例： 优点如下：\n\n * 查询效率高 - 对海量数据进行近实时的处理\n * 可扩展性 - 基于集群环境可以方便横向扩展，可以承载 pb 级数据\n * 高可用 - elasticsearch 集群弹性-他们将发现新的或失败的节点，重组和重新平衡数据，确保数据是安全的和可访问的\n\n缺点如下：\n\n * 部分支持事务 - 单一文档的数据是 acid 的，包含多个文档的事务时不支持事务的正常回滚，支持 i(isolation)隔离性（基于乐观锁机制的），d(durability)持久性，不支持 a(atomicity)原子性，c(consistency)一致性\n * 对类似数据库中通过外键的复杂的多表关联操作支持较弱。\n * 读写有一定延时，写入的数据，最快 1s 中能被检索到\n * 更新性能较低，底层实现是先删数据，再插入新数据\n * 内存占用大，因为 lucene 将索引部分加载到内存中\n\n\n# 搜索引擎场景\n\n适用场景如下：\n\n * 搜索引擎和数据分析引擎 - 全文检索，结构化检索，数据分析\n * 对海量数据进行近实时的处理 - 可以将海量数据分散到多台服务器上去存储和检索\n\n不适用场景如下：\n\n * 数据需要频繁更新\n * 需要复杂关联查询\n\n\n# 六、图数据库\n\n\n\n图形数据库应用图论存储实体之间的关系信息。最常见例子就是社会网络中人与人之间的关系。关系型数据库用于存储“关系型”数据的效果并不好，其查询复杂、缓慢、超出预期，而图形数据库的独特设计恰恰弥补了这个缺陷，解决关系型数据库存储和处理复杂关系型数据功能较弱的问题。\n\n\n# 图数据库产品\n\n * neo4j\n   \n   \n   \n   neo4j 是由 neo4j，inc。开发的图形数据库管理系统。由其开发人员描述为具有原生图存储和处理的符合 acid 的事务数据库，根据 db-engines 排名， neo4j 是最流行的图形数据库。\n\n * arangodb\n   \n   \n   \n   arangodb 是由 triagens gmbh 开发的原生多模型数据库系统。数据库系统支持三个重要的数据模型（键/值，文档，图形），其中包含一个数据库核心和统一查询语言 aql（arangodb 查询语言）。查询语言是声明性的，允许在单个查询中组合不同的数据访问模式。arangodb 是一个 nosql 数据库系统，但 aql 在很多方面与 sql 类似。\n\n * titan\n   \n   \n   \n   titan 是一个可扩展的图形数据库，针对存储和查询包含分布在多机群集中的数百亿个顶点和边缘的图形进行了优化。titan 是一个事务性数据库，可以支持数千个并发用户实时执行复杂的图形遍历。\n\n\n# 图数据库特性\n\n以 neo4j 为例：\n\nneo4j 使用数据结构中图（graph）的概念来进行建模。 neo4j 中两个最基本的概念是节点和边。节点表示实体，边则表示实体之间的关系。节点和边都可以有自己的属性。不同实体通过各种不同的关系关联起来，形成复杂的对象图。\n\n针对关系数据，2 种 2 数据库的存储结构不同：\n\n\n\nneo4j 中，存储节点时使用了”index-free adjacency”，即每个节点都有指向其邻居节点的指针，可以让我们在 o(1)的时间内找到邻居节点。另外，按照官方的说法，在 neo4j 中边是最重要的,是”first-class entities”，所以单独存储，这有利于在图遍历的时候提高速度，也可以很方便地以任何方向进行遍历\n\n\n\n如下优点：\n\n * 高性能 - 图的遍历是图数据结构所具有的独特算法，即从一个节点开始，根据其连接的关系，可以快速和方便地找出它的邻近节点。这种查找数据的方法并不受数据量的大小所影响，因为邻近查询始终查找的是有限的局部数据，不会对整个数据库进行搜索\n * 设计的灵活性 - 数据结构的自然伸展特性及其非结构化的数据格式，让图数据库设计可以具有很大的伸缩性和灵活性。因为随着需求的变化而增加的节点、关系及其属性并不会影响到原来数据的正常使用\n * 开发的敏捷性 - 直观明了的数据模型，从需求的讨论开始，到程序开发和实现，以及最终保存在数据库中的样子，它的模样似乎没有什么变化，甚至可以说本来就是一模一样的\n * 完全支持 acid - 不像别的 nosql 数据库 neo4j 还具有完全事务管理特性，完全支持 acid 事务管理\n\n缺点如下：\n\n * 存在支持节点，关系和属性的数量的限制。\n * 不支持拆分。\n\n\n# 图数据库场景\n\n适用场景如下：\n\n * 关系性强的数据中，如社交网络\n * 推荐引擎。如果我们将数据以图的形式表现，那么将会非常有益于推荐的制定\n\n不适用场景如下：\n\n * 记录大量基于事件的数据（例如日志条目或传感器数据）\n * 对大规模分布式数据进行处理\n * 保存在关系型数据库中的结构化数据\n * 二进制数据存储\n\n\n# 七、总结\n\n关系型数据库和 nosql 数据库的选型，往往需要考虑几个指标：\n\n * 数据量\n * 并发量\n * 实时性\n * 一致性要求\n * 读写分布和类型\n * 安全性\n * 运维成本\n\n常见软件系统数据库选型参考如下：\n\n * 中后台管理型系统 - 如运营系统，数据量少，并发量小，首选关系型数据库。\n * 大流量系统 - 如电商单品页，后台考虑选关系型数据库，前台考虑选内存型数据库。\n * 日志型系统 - 原始数据考虑选列式数据库，日志搜索考虑选搜索引擎。\n * 搜索型系统 - 例如站内搜索，非通用搜索，如商品搜索，后台考虑选关系型数据库，前台考虑选搜索引擎。\n * 事务型系统 - 如库存，交易，记账，考虑选关系型数据库+k-v 数据库（作为缓存）+分布式事务。\n * 离线计算 - 如大量数据分析，考虑选列式数据库或关系型数据。\n * 实时计算 - 如实时监控，可以考虑选内存型数据库或者列式数据库。\n\n设计实践中，要基于需求、业务驱动架构，无论选用 rdb/nosql/drdb,一定是以需求为导向，最终数据存储方案必然是各种权衡的综合性设计\n\n\n# 参考资料\n\n * nosql 还是 sql ？这一篇讲清楚',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"数据结构与数据库索引",frontmatter:{title:"数据结构与数据库索引",date:"2022-03-27T23:39:10.000Z",categories:["数据库","数据库综合"],tags:["数据库","综合","数据结构","索引"],permalink:"/pages/d7cd88/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BB%BC%E5%90%88/02.%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95.html",relativePath:"12.数据库/01.数据库综合/02.数据结构与数据库索引.md",key:"v-2f525dc4",path:"/pages/d7cd88/",headers:[{level:2,title:"引言",slug:"引言",normalizedTitle:"引言",charIndex:52},{level:2,title:"数组和链表",slug:"数组和链表",normalizedTitle:"数组和链表",charIndex:660},{level:2,title:"哈希索引",slug:"哈希索引",normalizedTitle:"哈希索引",charIndex:1314},{level:2,title:"B-Tree 索引",slug:"b-tree-索引",normalizedTitle:"b-tree 索引",charIndex:2193},{level:3,title:"二叉搜索树",slug:"二叉搜索树",normalizedTitle:"二叉搜索树",charIndex:2517},{level:3,title:"B+Tree 索引",slug:"b-tree-索引-2",normalizedTitle:"b+tree 索引",charIndex:2921},{level:2,title:"LSM 树",slug:"lsm-树",normalizedTitle:"lsm 树",charIndex:39},{level:2,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:6740},{level:2,title:"索引的维护",slug:"索引的维护",normalizedTitle:"索引的维护",charIndex:6936},{level:3,title:"创建索引",slug:"创建索引",normalizedTitle:"创建索引",charIndex:6946},{level:3,title:"更新索引",slug:"更新索引",normalizedTitle:"更新索引",charIndex:582},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7478}],headersStr:"引言 数组和链表 哈希索引 B-Tree 索引 二叉搜索树 B+Tree 索引 LSM 树 倒排索引 索引的维护 创建索引 更新索引 参考资料",content:"# 数据结构与数据库索引\n\n> 关键词：链表、数组、散列表、红黑树、B+ 树、LSM 树、跳表\n\n\n# 引言\n\n数据库是“按照 数据结构 来组织、存储和管理数据的仓库”。是一个长期存储在计算机内的、有组织的、可共享的、统一管理的大量数据的集合。\n\n——上面这句定义对数据库的定义来自百度百科。通过这个定义，我们也能明显看出数据结构是实现数据库的基石。\n\n从本质来看，数据库只负责两件事：读数据、写数据；而数据结构研究的是如何合理组织数据，尽可能提升读、写数据的效率，这恰好是数据库的核心问题。因此，数据结构与数据库这两个领域有非常多的交集。其中，数据库索引最能体现二者的紧密关联。\n\n索引是数据库为了提高查找效率的一种数据结构。索引基于原始数据衍生而来，它的主要作用是缩小检索的数据范围，提升查询性能。通俗来说，索引在数据库中的作用就像是一本书的目录索引。索引对于良好的性能非常关键，在数据量小且负载较低时，不恰当的索引对于性能的影响可能还不明显；但随着数据量逐渐增大，性能则会急剧下降。因此，索引优化应该是查询性能优化的最有效手段。\n\n很多数据库允许单独添加和删除索引，而不影响数据库的内容，它只会影响查询性能。维护额外的结构势必会引入开销，特别是在新数据写入时。对于写入，它很难超过简单地追加文件方式的性能，因为那已经是最简单的写操作了。由于每次写数据时，需要更新索引，因此任何类型的索引通常都会降低写的速度。\n\n本文以一些常见的数据库为例，分析它们的索引采用了什么样的数据结构，有什么利弊，为何如此设计。\n\n\n# 数组和链表\n\n数组和链表分别代表了连续空间和不连续空间的存储方式，它们是线性表（Linear List）的典型代表。其他所有的数据结构，比如栈、队列、二叉树、B+ 树等，实际上都是这两者的结合和变化。\n\n数组用连续的内存空间来存储数据。数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。但这并不代表数组的查找时间复杂度也是 O(1)。\n\n * 对于无序数组，只能顺序查找，其时间复杂度为 O(n)。\n * 对于有序数组，可以应用二分查找法，其时间复杂度为 O(log n)。\n\n在有序数组上应用二分查找法如此高效，为什么几乎没有数据库直接使用数组作为索引？这是因为它的限制条件：数据有序——为了保证数据有序，每次添加、删除数组数据时，都必须要进行数据调整，来保证其有序，而 数组的插入/删除操作，时间复杂度为 O(n)。此外，由于数组空间大小固定，每次扩容只能采用复制数组的方式。数组的这些特性，决定了它不适合用于数据频繁变化的应用场景。\n\n\n\n链表用不连续的内存空间来存储数据；并通过一个指针按顺序将这些空间串起来，形成一条链。\n\n区别于数组，链表中的元素不是存储在内存中连续的一片区域，链表中的数据存储在每一个称之为「结点」复合区域里，在每一个结点除了存储数据以外，还保存了到下一个节点的指针（Pointer）。由于不必按顺序存储，链表的插入/删除操作，时间复杂度为 O(1)，但是，链表只支持顺序访问，其 查找时间复杂度为 O(n)。其低效的查找方式，决定了链表不适合作为索引。\n\n\n\n\n# 哈希索引\n\n哈希表是一种以键 - 值（key-value）对形式存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 Value。\n\n哈希表 使用 哈希函数 组织数据，以支持快速插入和搜索的数据结构。哈希表的本质是一个数组，其思路是：使用 Hash 函数将 Key 转换为数组下标，利用数组的随机访问特性，使得我们能在 O(1) 的时间代价内完成检索。\n\n\n\n有两种不同类型的哈希表：哈希集合 和 哈希映射。\n\n * 哈希集合 是集合数据结构的实现之一，用于存储非重复值。\n * 哈希映射 是映射 数据结构的实现之一，用于存储键值对。\n\n哈希索引基于哈希表实现，只适用于等值查询。对于每一行数据，哈希索引都会将所有的索引列计算一个哈希码（hashcode），哈希码是一个较小的值。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。\n\n✔ 哈希索引的优点：\n\n * 因为索引数据结构紧凑，所以查询速度非常快。\n\n❌ 哈希索引的缺点：\n\n * 哈希索引值包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能影响不大。\n * 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。\n * 哈希索引不支持部分索引匹配查找，因为哈希索引时使用索引列的全部内容来进行哈希计算的。如，在数据列 (A,B) 上建立哈希索引，如果查询只有数据列 A，无法使用该索引。\n * 哈希索引只支持等值比较查询，包括 =、IN()、<=>；不支持任何范围查询，如 WHERE price > 100。\n * 哈希索引有可能出现哈希冲突\n   * 出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。\n   * 如果哈希冲突多的话，维护索引的代价会很高。\n\n> 因为种种限制，所以哈希索引只适用于特定的场合。而一旦使用哈希索引，则它带来的性能提升会非常显著。例如，Mysql 中的 Memory 存储引擎就显示的支持哈希索引。\n\n\n# B-Tree 索引\n\n通常我们所说的 B 树索引是指 B-Tree 索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用 B-Tree 这个术语，是因为 MySQL 在 CREATE TABLE 或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 InnoDB 使用的是 B+Tree索引；而 MyISAM 使用的是 B-Tree索引。\n\nB-Tree 索引中的 B 是指 balance，意为平衡。需要注意的是，B-Tree 索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。\n\n\n# 二叉搜索树\n\n二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。其查询时间复杂度是 O(log n)。\n\n当然为了维持 O(log n) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log n)。\n\n随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗，相对于内存存取，I/O 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 I/O 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 I/O 存取次数？\n\n一种行之有效的解决方法是减少树的深度，将二叉树变为 N 叉树（多路搜索树），而 B+ 树就是一种多路搜索树。\n\n\n# B+Tree 索引\n\nB+ 树索引适用于全键值查找、键值范围查找和键前缀查找，其中键前缀查找只适用于最左前缀查找。\n\n理解 B+Tree，只需要理解其最重要的两个特征即可：\n\n * 第一，所有的关键字（可以理解为数据）都存储在叶子节点，非叶子节点并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。\n * 其次，所有的叶子节点由指针连接。如下图为简化了的B+Tree。\n\n\n\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n * 聚簇索引（clustered）：又称为主键索引，其叶子节点存的是整行数据。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。InnoDB 的聚簇索引实际是在同一个结构中保存了 B 树的索引和数据行。\n * 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary）。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。\n\n聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n聚簇索引和非聚簇索引的查询有什么区别\n\n * 如果语句是 select * from T where ID=500，即聚簇索引查询方式，则只需要搜索 ID 这棵 B+ 树；\n * 如果语句是 select * from T where k=5，即非聚簇索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非聚簇索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n\n显然，主键长度越小，非聚簇索引的叶子节点就越小，非聚簇索引占用的空间也就越小。\n\n自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。从性能和存储空间方面考量，自增主键往往是更合理的选择。有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：\n\n * 只有一个索引；\n * 该索引必须是唯一索引。\n\n由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。\n\n----------------------------------------\n\n内存是半导体元件。对于内存而言，只要给出了内存地址，我们就可以直接访问该地址取出数据。这个过程具有高效的随机访问特性，因此内存也叫随机访问存储器（Random Access Memory，即 RAM）。内存的访问速度很快，但是价格相对较昂贵，因此一般的计算机内存空间都相对较小。\n\n而磁盘是机械器件。磁盘访问数据时，需要等磁盘盘片旋转到磁头下，才能读取相应的数据。尽管磁盘的旋转速度很快，但是和内存的随机访问相比，性能差距非常大。一般来说，如果是随机读写，会有 10 万到 100 万倍左右的差距。但如果是顺序访问大批量数据的话，磁盘的性能和内存就是一个数量级的。\n\n磁盘的最小读写单位是扇区，较早期的磁盘一个扇区是 512 字节。随着磁盘技术的发展，目前常见的磁盘扇区是 4K 个字节。操作系统一次会读写多个扇区，所以操作系统的最小读写单位是块（Block），也叫作簇（Cluster）。当我们要从磁盘中读取一个数据时，操作系统会一次性将整个块都读出来。因此，对于大批量的顺序读写来说，磁盘的效率会比随机读写高许多。\n\n假设有一个有序数组存储在硬盘中，如果它足够大，那么它会存储在多个块中。当我们要对这个数组使用二分查找时，需要先找到中间元素所在的块，将这个块从磁盘中读到内存里，然后在内存中进行二分查找。如果下一步要读的元素在其他块中，则需要再将相应块从磁盘中读入内存。直到查询结束，这个过程可能会多次访问磁盘。我们可以看到，这样的检索性能非常低。\n\n由于磁盘相对于内存而言访问速度实在太慢，因此，对于磁盘上数据的高效检索，我们有一个极其重要的原则：对磁盘的访问次数要尽可能的少！\n\n将索引和数据分离就是一种常见的设计思路。在数据频繁变化的场景中，有序数组并不是一个最好的选择，二叉检索树或者哈希表往往更有普适性。但是，哈希表由于缺乏范围检索的能力，在一些场合也不适用。因此，二叉检索树这种树形结构是许多常见检索系统的实施方案。\n\n随着索引数据越来越大，直到无法完全加载到内存中，这是需要将索引数据也存入磁盘中。B+ 树给出了将树形索引的所有节点都存在磁盘上的高效检索方案。操作系统对磁盘数据的访问是以块为单位的。因此，如果我们想将树型索引的一个节点从磁盘中读出，即使该节点的数据量很小（比如说只有几个字节），但磁盘依然会将整个块的数据全部读出来，而不是只读这一小部分数据，这会让有效读取效率很低。B+ 树的一个关键设计，就是让一个节点的大小等于一个块的大小。节点内存储的数据，不是一个元素，而是一个可以装 m 个元素的有序数组。这样一来，我们就可以将磁盘一次读取的数据全部利用起来，使得读取效率最大化。\n\nB+ 树还有另一个设计，就是将所有的节点分为内部节点和叶子节点。内部节点仅存储 key 和维持树形结构的指针，并不存储 key 对应的数据（无论是具体数据还是文件位置信息）。这样内部节点就能存储更多的索引数据，我们也就可以使用最少的内部节点，将所有数据组织起来了。而叶子节点仅存储 key 和对应数据，不存储维持树形结构的指针。通过这样的设计，B+ 树就能做到节点的空间利用率最大化。此外，B+ 树还将同一层的所有节点串成了有序的双向链表，这样一来，B+ 树就同时具备了良好的范围查询能力和灵活调整的能力了。\n\n因此，B+ 树是一棵完全平衡的 m 阶多叉树。所谓的 m 阶，指的是每个节点最多有 m 个子节点，并且每个节点里都存了一个紧凑的可包含 m 个元素的数组。\n\n即使是复杂的 B+ 树，我们将它拆解开来，其实也是由简单的数组、链表和树组成的，而且 B+ 树的检索过程其实也是二分查找。因此，如果 B+ 树完全加载在内存中的话，它的检索效率其实并不会比有序数组或者二叉检索树更 高，也还是二分查找的 log(n) 的效率。并且，它还比数组和二叉检索树更加复杂，还会带来额外的开销。\n\n另外，这一节还有一个很重要的设计思想需要你掌握，那就是将索引和数据分离。通过这样的方式，我们能将索引的数组大小保持在一个较小的范围内，让它能加载在内存中。在许多大规模系统中，都是使用这个设计思想来精简索引的。而且，B+ 树的内部节点和叶子节点的区分，其实也是索引和数据分离的一次实践。\n\nMySQL 中的 B+ 树实现其实有两种，一种是 MyISAM 引擎，另一种是 InnoDB 引擎。它们的核心区别就在于，数据和索引是否是分离的。\n\n在 MyISAM 引擎中，B+ 树的叶子节点仅存储了数据的位置指针，这是一种索引和数据分离的设计方案，叫作非聚集索引。如果要保证 MyISAM 的数据一致性，那我们需要在表级别上进行加锁处理。\n\n在 InnoDB 中，B+ 树的叶子节点直接存储了具体数据，这是一种索引和数据一体的方案。叫作聚集索引。由于数据直接就存在索引的叶子节点中，因此 InnoDB 不需要给全表加锁来保证一致性，它只需要支持行级的锁就可以了。\n\n\n# LSM 树\n\nB+ 树的数据都存储在叶子节点中，而叶子节点一般都存储在磁盘中。因此，每次插入的新数据都需要随机写入磁盘，而随机写入的性能非常慢。如果是一个日志系统，每秒钟要写入上千条甚至上万条数据，这样的磁盘操作代价会使得系统性能急剧下降，甚至无法使用。\n\n操作系统对磁盘的读写是以块为单位的，我们能否以块为单位写入，而不是每次插入一个数据都要随机写入磁盘呢？这样是不是就可以大幅度减少写入操作了呢？解决方案就是：LSM 树（Log Structured Merge Trees）。\n\nLSM 树就是根据这个思路设计了这样一个机制：当数据写入时，延迟写磁盘，将数据先存放在内存中的树里，进行常规的存储和查询。当内存中的树持续变大达到阈值时，再批量地以块为单位写入磁盘的树中。因此，LSM 树至少需要由两棵树组成，一棵是存储在内存中较小的 C0 树，另一棵是存储在磁盘中较大的 C1 树。\n\nLSM 树具有以下 3 个特点：\n\n 1. 将索引分为内存和磁盘两部分，并在内存达到阈值时启动树合并（Merge Trees）；\n 2. 用批量写入代替随机写入，并且用预写日志 WAL 技术（Write AheadLog，预写日志技术）保证内存数据，在系统崩溃后可以被恢复；\n 3. 数据采取类似日志追加写的方式写入（Log Structured）磁盘，以顺序写的方式提高写 入效率。\n\nLSM 树的这些特点，使得它相对于 B+ 树，在写入性能上有大幅提升。所以，许多 NoSQL 系统都使用 LSM 树作为检索引擎，而且还对 LSM 树进行了优化以提升检索性能。\n\n\n# 倒排索引\n\n倒排索引的核心其实并不复杂，它的具体实现其实是哈希表，只是它不是将文档 ID 或者题目作为 key，而是反过来，通过将内容或者属性作为 key 来存储对应的文档列表，使得我们能在 O(1) 的时间代价内完成查询。\n\n尽管原理并不复杂，但是倒排索引是许多检索引擎的核心。比如说，数据库的全文索引功能、搜索引擎的索引、广告引擎和推荐引擎，都使用了倒排索引技术来实现检索功能。\n\n\n# 索引的维护\n\n\n# 创建索引\n\n * 数据压缩：一个是尽可能地将数据加载到内存中，因为内存的检索效率大大高于磁盘。那为了将数据更多地加载到内存中，索引压缩是一个重要的研究方向。\n * 分支处理：另一个是将大数据集合拆成多个小数据集合来处理。这其实就是分布式系统的核心思想。\n\n\n# 更新索引\n\n（1）Double Buffer（双缓冲）机制\n\n就是在内存中同时保存两份一样的索引，一个是索引 A，一个是索引 B。两个索引保持一个读、一个写，并且来回切换，最终完成高性能的索引更新。\n\n优点：简单高效\n\n缺点：达到一定数据量级后，会带来翻倍的内存开销，甚至有些索引存储在磁盘上的情况下，更是无法使用此机制。\n\n（2）全量索引和增量索引\n\n将新接收到的数据单独建立一个可以存在内存中的倒排索引，也就是增量索引。当查询发生的时候，我们会同时查询全量索引和增量索引，将合并的结果作为总的结果输出。\n\n因为增量索引相对全量索引而言会小很多，内存资源消耗在可承受范围，所以我们可以使用 Double Buffer 机制 对增量索引进行索引更新。这样一来，增量索引就可以做到无锁访问。而全量索引本身就是只读的，也不需要加锁。因此，整个检索过程都可以做到无锁访问，也就提高了系统的检索效率。\n\n\n# 参考资料\n\n * 《数据密集型应用系统设计》\n * 数据结构与算法之美\n * 检索技术核心 20 讲\n * Data Structures for Databases\n * Data Structures and Algorithms for Big Databases",normalizedContent:"# 数据结构与数据库索引\n\n> 关键词：链表、数组、散列表、红黑树、b+ 树、lsm 树、跳表\n\n\n# 引言\n\n数据库是“按照 数据结构 来组织、存储和管理数据的仓库”。是一个长期存储在计算机内的、有组织的、可共享的、统一管理的大量数据的集合。\n\n——上面这句定义对数据库的定义来自百度百科。通过这个定义，我们也能明显看出数据结构是实现数据库的基石。\n\n从本质来看，数据库只负责两件事：读数据、写数据；而数据结构研究的是如何合理组织数据，尽可能提升读、写数据的效率，这恰好是数据库的核心问题。因此，数据结构与数据库这两个领域有非常多的交集。其中，数据库索引最能体现二者的紧密关联。\n\n索引是数据库为了提高查找效率的一种数据结构。索引基于原始数据衍生而来，它的主要作用是缩小检索的数据范围，提升查询性能。通俗来说，索引在数据库中的作用就像是一本书的目录索引。索引对于良好的性能非常关键，在数据量小且负载较低时，不恰当的索引对于性能的影响可能还不明显；但随着数据量逐渐增大，性能则会急剧下降。因此，索引优化应该是查询性能优化的最有效手段。\n\n很多数据库允许单独添加和删除索引，而不影响数据库的内容，它只会影响查询性能。维护额外的结构势必会引入开销，特别是在新数据写入时。对于写入，它很难超过简单地追加文件方式的性能，因为那已经是最简单的写操作了。由于每次写数据时，需要更新索引，因此任何类型的索引通常都会降低写的速度。\n\n本文以一些常见的数据库为例，分析它们的索引采用了什么样的数据结构，有什么利弊，为何如此设计。\n\n\n# 数组和链表\n\n数组和链表分别代表了连续空间和不连续空间的存储方式，它们是线性表（linear list）的典型代表。其他所有的数据结构，比如栈、队列、二叉树、b+ 树等，实际上都是这两者的结合和变化。\n\n数组用连续的内存空间来存储数据。数组支持随机访问，根据下标随机访问的时间复杂度为 o(1)。但这并不代表数组的查找时间复杂度也是 o(1)。\n\n * 对于无序数组，只能顺序查找，其时间复杂度为 o(n)。\n * 对于有序数组，可以应用二分查找法，其时间复杂度为 o(log n)。\n\n在有序数组上应用二分查找法如此高效，为什么几乎没有数据库直接使用数组作为索引？这是因为它的限制条件：数据有序——为了保证数据有序，每次添加、删除数组数据时，都必须要进行数据调整，来保证其有序，而 数组的插入/删除操作，时间复杂度为 o(n)。此外，由于数组空间大小固定，每次扩容只能采用复制数组的方式。数组的这些特性，决定了它不适合用于数据频繁变化的应用场景。\n\n\n\n链表用不连续的内存空间来存储数据；并通过一个指针按顺序将这些空间串起来，形成一条链。\n\n区别于数组，链表中的元素不是存储在内存中连续的一片区域，链表中的数据存储在每一个称之为「结点」复合区域里，在每一个结点除了存储数据以外，还保存了到下一个节点的指针（pointer）。由于不必按顺序存储，链表的插入/删除操作，时间复杂度为 o(1)，但是，链表只支持顺序访问，其 查找时间复杂度为 o(n)。其低效的查找方式，决定了链表不适合作为索引。\n\n\n\n\n# 哈希索引\n\n哈希表是一种以键 - 值（key-value）对形式存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 value。\n\n哈希表 使用 哈希函数 组织数据，以支持快速插入和搜索的数据结构。哈希表的本质是一个数组，其思路是：使用 hash 函数将 key 转换为数组下标，利用数组的随机访问特性，使得我们能在 o(1) 的时间代价内完成检索。\n\n\n\n有两种不同类型的哈希表：哈希集合 和 哈希映射。\n\n * 哈希集合 是集合数据结构的实现之一，用于存储非重复值。\n * 哈希映射 是映射 数据结构的实现之一，用于存储键值对。\n\n哈希索引基于哈希表实现，只适用于等值查询。对于每一行数据，哈希索引都会将所有的索引列计算一个哈希码（hashcode），哈希码是一个较小的值。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。\n\n✔ 哈希索引的优点：\n\n * 因为索引数据结构紧凑，所以查询速度非常快。\n\n❌ 哈希索引的缺点：\n\n * 哈希索引值包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能影响不大。\n * 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。\n * 哈希索引不支持部分索引匹配查找，因为哈希索引时使用索引列的全部内容来进行哈希计算的。如，在数据列 (a,b) 上建立哈希索引，如果查询只有数据列 a，无法使用该索引。\n * 哈希索引只支持等值比较查询，包括 =、in()、<=>；不支持任何范围查询，如 where price > 100。\n * 哈希索引有可能出现哈希冲突\n   * 出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。\n   * 如果哈希冲突多的话，维护索引的代价会很高。\n\n> 因为种种限制，所以哈希索引只适用于特定的场合。而一旦使用哈希索引，则它带来的性能提升会非常显著。例如，mysql 中的 memory 存储引擎就显示的支持哈希索引。\n\n\n# b-tree 索引\n\n通常我们所说的 b 树索引是指 b-tree 索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用 b-tree 这个术语，是因为 mysql 在 create table 或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 innodb 使用的是 b+tree索引；而 myisam 使用的是 b-tree索引。\n\nb-tree 索引中的 b 是指 balance，意为平衡。需要注意的是，b-tree 索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。\n\n\n# 二叉搜索树\n\n二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。其查询时间复杂度是 o(log n)。\n\n当然为了维持 o(log n) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 o(log n)。\n\n随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 i/o 消耗，相对于内存存取，i/o 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 i/o 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 i/o 存取次数？\n\n一种行之有效的解决方法是减少树的深度，将二叉树变为 n 叉树（多路搜索树），而 b+ 树就是一种多路搜索树。\n\n\n# b+tree 索引\n\nb+ 树索引适用于全键值查找、键值范围查找和键前缀查找，其中键前缀查找只适用于最左前缀查找。\n\n理解 b+tree，只需要理解其最重要的两个特征即可：\n\n * 第一，所有的关键字（可以理解为数据）都存储在叶子节点，非叶子节点并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。\n * 其次，所有的叶子节点由指针连接。如下图为简化了的b+tree。\n\n\n\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n * 聚簇索引（clustered）：又称为主键索引，其叶子节点存的是整行数据。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。innodb 的聚簇索引实际是在同一个结构中保存了 b 树的索引和数据行。\n * 非主键索引的叶子节点内容是主键的值。在 innodb 里，非主键索引也被称为二级索引（secondary）。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。\n\n聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n聚簇索引和非聚簇索引的查询有什么区别\n\n * 如果语句是 select * from t where id=500，即聚簇索引查询方式，则只需要搜索 id 这棵 b+ 树；\n * 如果语句是 select * from t where k=5，即非聚簇索引查询方式，则需要先搜索 k 索引树，得到 id 的值为 500，再到 id 索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非聚簇索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n\n显然，主键长度越小，非聚簇索引的叶子节点就越小，非聚簇索引占用的空间也就越小。\n\n自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： not null primary key auto_increment。从性能和存储空间方面考量，自增主键往往是更合理的选择。有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：\n\n * 只有一个索引；\n * 该索引必须是唯一索引。\n\n由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。\n\n----------------------------------------\n\n内存是半导体元件。对于内存而言，只要给出了内存地址，我们就可以直接访问该地址取出数据。这个过程具有高效的随机访问特性，因此内存也叫随机访问存储器（random access memory，即 ram）。内存的访问速度很快，但是价格相对较昂贵，因此一般的计算机内存空间都相对较小。\n\n而磁盘是机械器件。磁盘访问数据时，需要等磁盘盘片旋转到磁头下，才能读取相应的数据。尽管磁盘的旋转速度很快，但是和内存的随机访问相比，性能差距非常大。一般来说，如果是随机读写，会有 10 万到 100 万倍左右的差距。但如果是顺序访问大批量数据的话，磁盘的性能和内存就是一个数量级的。\n\n磁盘的最小读写单位是扇区，较早期的磁盘一个扇区是 512 字节。随着磁盘技术的发展，目前常见的磁盘扇区是 4k 个字节。操作系统一次会读写多个扇区，所以操作系统的最小读写单位是块（block），也叫作簇（cluster）。当我们要从磁盘中读取一个数据时，操作系统会一次性将整个块都读出来。因此，对于大批量的顺序读写来说，磁盘的效率会比随机读写高许多。\n\n假设有一个有序数组存储在硬盘中，如果它足够大，那么它会存储在多个块中。当我们要对这个数组使用二分查找时，需要先找到中间元素所在的块，将这个块从磁盘中读到内存里，然后在内存中进行二分查找。如果下一步要读的元素在其他块中，则需要再将相应块从磁盘中读入内存。直到查询结束，这个过程可能会多次访问磁盘。我们可以看到，这样的检索性能非常低。\n\n由于磁盘相对于内存而言访问速度实在太慢，因此，对于磁盘上数据的高效检索，我们有一个极其重要的原则：对磁盘的访问次数要尽可能的少！\n\n将索引和数据分离就是一种常见的设计思路。在数据频繁变化的场景中，有序数组并不是一个最好的选择，二叉检索树或者哈希表往往更有普适性。但是，哈希表由于缺乏范围检索的能力，在一些场合也不适用。因此，二叉检索树这种树形结构是许多常见检索系统的实施方案。\n\n随着索引数据越来越大，直到无法完全加载到内存中，这是需要将索引数据也存入磁盘中。b+ 树给出了将树形索引的所有节点都存在磁盘上的高效检索方案。操作系统对磁盘数据的访问是以块为单位的。因此，如果我们想将树型索引的一个节点从磁盘中读出，即使该节点的数据量很小（比如说只有几个字节），但磁盘依然会将整个块的数据全部读出来，而不是只读这一小部分数据，这会让有效读取效率很低。b+ 树的一个关键设计，就是让一个节点的大小等于一个块的大小。节点内存储的数据，不是一个元素，而是一个可以装 m 个元素的有序数组。这样一来，我们就可以将磁盘一次读取的数据全部利用起来，使得读取效率最大化。\n\nb+ 树还有另一个设计，就是将所有的节点分为内部节点和叶子节点。内部节点仅存储 key 和维持树形结构的指针，并不存储 key 对应的数据（无论是具体数据还是文件位置信息）。这样内部节点就能存储更多的索引数据，我们也就可以使用最少的内部节点，将所有数据组织起来了。而叶子节点仅存储 key 和对应数据，不存储维持树形结构的指针。通过这样的设计，b+ 树就能做到节点的空间利用率最大化。此外，b+ 树还将同一层的所有节点串成了有序的双向链表，这样一来，b+ 树就同时具备了良好的范围查询能力和灵活调整的能力了。\n\n因此，b+ 树是一棵完全平衡的 m 阶多叉树。所谓的 m 阶，指的是每个节点最多有 m 个子节点，并且每个节点里都存了一个紧凑的可包含 m 个元素的数组。\n\n即使是复杂的 b+ 树，我们将它拆解开来，其实也是由简单的数组、链表和树组成的，而且 b+ 树的检索过程其实也是二分查找。因此，如果 b+ 树完全加载在内存中的话，它的检索效率其实并不会比有序数组或者二叉检索树更 高，也还是二分查找的 log(n) 的效率。并且，它还比数组和二叉检索树更加复杂，还会带来额外的开销。\n\n另外，这一节还有一个很重要的设计思想需要你掌握，那就是将索引和数据分离。通过这样的方式，我们能将索引的数组大小保持在一个较小的范围内，让它能加载在内存中。在许多大规模系统中，都是使用这个设计思想来精简索引的。而且，b+ 树的内部节点和叶子节点的区分，其实也是索引和数据分离的一次实践。\n\nmysql 中的 b+ 树实现其实有两种，一种是 myisam 引擎，另一种是 innodb 引擎。它们的核心区别就在于，数据和索引是否是分离的。\n\n在 myisam 引擎中，b+ 树的叶子节点仅存储了数据的位置指针，这是一种索引和数据分离的设计方案，叫作非聚集索引。如果要保证 myisam 的数据一致性，那我们需要在表级别上进行加锁处理。\n\n在 innodb 中，b+ 树的叶子节点直接存储了具体数据，这是一种索引和数据一体的方案。叫作聚集索引。由于数据直接就存在索引的叶子节点中，因此 innodb 不需要给全表加锁来保证一致性，它只需要支持行级的锁就可以了。\n\n\n# lsm 树\n\nb+ 树的数据都存储在叶子节点中，而叶子节点一般都存储在磁盘中。因此，每次插入的新数据都需要随机写入磁盘，而随机写入的性能非常慢。如果是一个日志系统，每秒钟要写入上千条甚至上万条数据，这样的磁盘操作代价会使得系统性能急剧下降，甚至无法使用。\n\n操作系统对磁盘的读写是以块为单位的，我们能否以块为单位写入，而不是每次插入一个数据都要随机写入磁盘呢？这样是不是就可以大幅度减少写入操作了呢？解决方案就是：lsm 树（log structured merge trees）。\n\nlsm 树就是根据这个思路设计了这样一个机制：当数据写入时，延迟写磁盘，将数据先存放在内存中的树里，进行常规的存储和查询。当内存中的树持续变大达到阈值时，再批量地以块为单位写入磁盘的树中。因此，lsm 树至少需要由两棵树组成，一棵是存储在内存中较小的 c0 树，另一棵是存储在磁盘中较大的 c1 树。\n\nlsm 树具有以下 3 个特点：\n\n 1. 将索引分为内存和磁盘两部分，并在内存达到阈值时启动树合并（merge trees）；\n 2. 用批量写入代替随机写入，并且用预写日志 wal 技术（write aheadlog，预写日志技术）保证内存数据，在系统崩溃后可以被恢复；\n 3. 数据采取类似日志追加写的方式写入（log structured）磁盘，以顺序写的方式提高写 入效率。\n\nlsm 树的这些特点，使得它相对于 b+ 树，在写入性能上有大幅提升。所以，许多 nosql 系统都使用 lsm 树作为检索引擎，而且还对 lsm 树进行了优化以提升检索性能。\n\n\n# 倒排索引\n\n倒排索引的核心其实并不复杂，它的具体实现其实是哈希表，只是它不是将文档 id 或者题目作为 key，而是反过来，通过将内容或者属性作为 key 来存储对应的文档列表，使得我们能在 o(1) 的时间代价内完成查询。\n\n尽管原理并不复杂，但是倒排索引是许多检索引擎的核心。比如说，数据库的全文索引功能、搜索引擎的索引、广告引擎和推荐引擎，都使用了倒排索引技术来实现检索功能。\n\n\n# 索引的维护\n\n\n# 创建索引\n\n * 数据压缩：一个是尽可能地将数据加载到内存中，因为内存的检索效率大大高于磁盘。那为了将数据更多地加载到内存中，索引压缩是一个重要的研究方向。\n * 分支处理：另一个是将大数据集合拆成多个小数据集合来处理。这其实就是分布式系统的核心思想。\n\n\n# 更新索引\n\n（1）double buffer（双缓冲）机制\n\n就是在内存中同时保存两份一样的索引，一个是索引 a，一个是索引 b。两个索引保持一个读、一个写，并且来回切换，最终完成高性能的索引更新。\n\n优点：简单高效\n\n缺点：达到一定数据量级后，会带来翻倍的内存开销，甚至有些索引存储在磁盘上的情况下，更是无法使用此机制。\n\n（2）全量索引和增量索引\n\n将新接收到的数据单独建立一个可以存在内存中的倒排索引，也就是增量索引。当查询发生的时候，我们会同时查询全量索引和增量索引，将合并的结果作为总的结果输出。\n\n因为增量索引相对全量索引而言会小很多，内存资源消耗在可承受范围，所以我们可以使用 double buffer 机制 对增量索引进行索引更新。这样一来，增量索引就可以做到无锁访问。而全量索引本身就是只读的，也不需要加锁。因此，整个检索过程都可以做到无锁访问，也就提高了系统的检索效率。\n\n\n# 参考资料\n\n * 《数据密集型应用系统设计》\n * 数据结构与算法之美\n * 检索技术核心 20 讲\n * data structures for databases\n * data structures and algorithms for big databases",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"数据库综合",frontmatter:{title:"数据库综合",date:"2022-04-11T16:52:35.000Z",categories:["数据库","数据库综合"],tags:["数据库","综合"],permalink:"/pages/3c3c45/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BB%BC%E5%90%88/",relativePath:"12.数据库/01.数据库综合/README.md",key:"v-4c53fba0",path:"/pages/3c3c45/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:12},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:51},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:61}],headersStr:"📖 内容 📚 资料 🚪 传送",content:"# 数据库综合\n\n\n# 📖 内容\n\n * Nosql 技术选型\n * 数据结构与数据库索引\n\n\n# 📚 资料\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# 数据库综合\n\n\n# 📖 内容\n\n * nosql 技术选型\n * 数据结构与数据库索引\n\n\n# 📚 资料\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"ShardingSphere 简介",frontmatter:{title:"ShardingSphere 简介",date:"2020-10-08T20:30:30.000Z",categories:["数据库","数据库中间件","Shardingsphere"],tags:["数据库","中间件","分库分表"],permalink:"/pages/5ed2a2/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/02.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6/01.Shardingsphere/01.ShardingSphere%E7%AE%80%E4%BB%8B.html",relativePath:"12.数据库/02.数据库中间件/01.Shardingsphere/01.ShardingSphere简介.md",key:"v-909a48e0",path:"/pages/5ed2a2/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:17},{level:3,title:"ShardingSphere 组件",slug:"shardingsphere-组件",normalizedTitle:"shardingsphere 组件",charIndex:31},{level:4,title:"ShardingSphere-JDBC",slug:"shardingsphere-jdbc",normalizedTitle:"shardingsphere-jdbc",charIndex:230},{level:4,title:"Sharding-Proxy",slug:"sharding-proxy",normalizedTitle:"sharding-proxy",charIndex:108},{level:4,title:"Sharding-Sidecar（TODO）",slug:"sharding-sidecar-todo",normalizedTitle:"sharding-sidecar（todo）",charIndex:875},{level:4,title:"混合架构",slug:"混合架构",normalizedTitle:"混合架构",charIndex:1555},{level:3,title:"功能列表",slug:"功能列表",normalizedTitle:"功能列表",charIndex:1846},{level:4,title:"数据分片",slug:"数据分片",normalizedTitle:"数据分片",charIndex:171},{level:4,title:"分布式事务",slug:"分布式事务",normalizedTitle:"分布式事务",charIndex:176},{level:4,title:"数据库治理",slug:"数据库治理",normalizedTitle:"数据库治理",charIndex:182},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1994}],headersStr:"简介 ShardingSphere 组件 ShardingSphere-JDBC Sharding-Proxy Sharding-Sidecar（TODO） 混合架构 功能列表 数据分片 分布式事务 数据库治理 参考资料",content:"# ShardingSphere 简介\n\n\n# 简介\n\n\n# ShardingSphere 组件\n\nShardingSphere 是一套开源的分布式数据库中间件解决方案组成的生态圈，它由 Sharding-JDBC、Sharding-Proxy 和 Sharding-Sidecar（计划中）这 3 款相互独立的产品组成。 他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如 Java 同构、异构语言、云原生等各种多样化的应用场景。\n\n\n\n# ShardingSphere-JDBC\n\n定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。\n\n * 适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC。\n * 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等。\n * 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库。\n\n\n\n# Sharding-Proxy\n\n定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 MySQL 和 PostgreSQL 版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端(如：MySQL Command Client, MySQL Workbench, Navicat 等)操作数据，对 DBA 更加友好。\n\n * 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用。\n * 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端。\n\n\n\n# Sharding-Sidecar（TODO）\n\n定位为 Kubernetes 的云原生数据库代理，以 Sidecar 的形式代理所有对数据库的访问。 通过无中心、零侵入的方案提供与数据库交互的的啮合层，即 Database Mesh，又可称数据库网格。\n\nDatabase Mesh 的关注重点在于如何将分布式的数据访问应用与数据库有机串联起来，它更加关注的是交互，是将杂乱无章的应用与数据库之间的交互进行有效地梳理。 使用 Database Mesh，访问数据库的应用和数据库终将形成一个巨大的网格体系，应用和数据库只需在网格体系中对号入座即可，它们都是被啮合层所治理的对象。\n\n\n\nSHARDING-JDBC   SHARDING-PROXY   SHARDING-SIDECAR   \n数据库             任意               MySQL              MySQL\n连接消耗数           高                低                  高\n异构语言            仅 Java           任意                 任意\n性能              损耗低              损耗略高               损耗低\n无中心化            是                否                  是\n静态入口            无                有                  无\n\n# 混合架构\n\nShardingSphere-JDBC 采用无中心化架构，适用于 Java 开发的高性能的轻量级 OLTP 应用；ShardingSphere-Proxy 提供静态入口以及异构语言的支持，适用于 OLAP 应用以及对分片数据库进行管理和运维的场景。\n\nApache ShardingSphere 是多接入端共同组成的生态圈。 通过混合使用 ShardingSphere-JDBC 和 ShardingSphere-Proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，使得架构师更加自由地调整适合与当前业务的最佳系统架构。\n\n\n\n\n# 功能列表\n\n# 数据分片\n\n * 分库 & 分表\n * 读写分离\n * 分片策略定制化\n * 无中心化分布式主键\n\n# 分布式事务\n\n * 标准化事务接口\n * XA 强一致事务\n * 柔性事务\n\n# 数据库治理\n\n * 分布式治理\n * 弹性伸缩\n * 可视化链路追踪\n * 数据加密\n\n\n# 参考资料\n\n * shardingsphere Github\n * shardingsphere 官方文档",normalizedContent:"# shardingsphere 简介\n\n\n# 简介\n\n\n# shardingsphere 组件\n\nshardingsphere 是一套开源的分布式数据库中间件解决方案组成的生态圈，它由 sharding-jdbc、sharding-proxy 和 sharding-sidecar（计划中）这 3 款相互独立的产品组成。 他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如 java 同构、异构语言、云原生等各种多样化的应用场景。\n\n\n\n# shardingsphere-jdbc\n\n定位为轻量级 java 框架，在 java 的 jdbc 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 jdbc 驱动，完全兼容 jdbc 和各种 orm 框架。\n\n * 适用于任何基于 jdbc 的 orm 框架，如：jpa, hibernate, mybatis, spring jdbc template 或直接使用 jdbc。\n * 支持任何第三方的数据库连接池，如：dbcp, c3p0, bonecp, druid, hikaricp 等。\n * 支持任意实现 jdbc 规范的数据库，目前支持 mysql，oracle，sqlserver，postgresql 以及任何遵循 sql92 标准的数据库。\n\n\n\n# sharding-proxy\n\n定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 mysql 和 postgresql 版本，它可以使用任何兼容 mysql/postgresql 协议的访问客户端(如：mysql command client, mysql workbench, navicat 等)操作数据，对 dba 更加友好。\n\n * 向应用程序完全透明，可直接当做 mysql/postgresql 使用。\n * 适用于任何兼容 mysql/postgresql 协议的的客户端。\n\n\n\n# sharding-sidecar（todo）\n\n定位为 kubernetes 的云原生数据库代理，以 sidecar 的形式代理所有对数据库的访问。 通过无中心、零侵入的方案提供与数据库交互的的啮合层，即 database mesh，又可称数据库网格。\n\ndatabase mesh 的关注重点在于如何将分布式的数据访问应用与数据库有机串联起来，它更加关注的是交互，是将杂乱无章的应用与数据库之间的交互进行有效地梳理。 使用 database mesh，访问数据库的应用和数据库终将形成一个巨大的网格体系，应用和数据库只需在网格体系中对号入座即可，它们都是被啮合层所治理的对象。\n\n\n\nsharding-jdbc   sharding-proxy   sharding-sidecar   \n数据库             任意               mysql              mysql\n连接消耗数           高                低                  高\n异构语言            仅 java           任意                 任意\n性能              损耗低              损耗略高               损耗低\n无中心化            是                否                  是\n静态入口            无                有                  无\n\n# 混合架构\n\nshardingsphere-jdbc 采用无中心化架构，适用于 java 开发的高性能的轻量级 oltp 应用；shardingsphere-proxy 提供静态入口以及异构语言的支持，适用于 olap 应用以及对分片数据库进行管理和运维的场景。\n\napache shardingsphere 是多接入端共同组成的生态圈。 通过混合使用 shardingsphere-jdbc 和 shardingsphere-proxy，并采用同一注册中心统一配置分片策略，能够灵活的搭建适用于各种场景的应用系统，使得架构师更加自由地调整适合与当前业务的最佳系统架构。\n\n\n\n\n# 功能列表\n\n# 数据分片\n\n * 分库 & 分表\n * 读写分离\n * 分片策略定制化\n * 无中心化分布式主键\n\n# 分布式事务\n\n * 标准化事务接口\n * xa 强一致事务\n * 柔性事务\n\n# 数据库治理\n\n * 分布式治理\n * 弹性伸缩\n * 可视化链路追踪\n * 数据加密\n\n\n# 参考资料\n\n * shardingsphere github\n * shardingsphere 官方文档",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"ShardingSphere Jdbc",frontmatter:{title:"ShardingSphere Jdbc",date:"2020-12-28T00:01:28.000Z",categories:["数据库","数据库中间件","Shardingsphere"],tags:["数据库","中间件","分库分表"],permalink:"/pages/8448de/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/02.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6/01.Shardingsphere/02.ShardingSphereJdbc.html",relativePath:"12.数据库/02.数据库中间件/01.Shardingsphere/02.ShardingSphereJdbc.md",key:"v-e3ca250c",path:"/pages/8448de/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:26},{level:2,title:"快速入门",slug:"快速入门",normalizedTitle:"快速入门",charIndex:397},{level:3,title:"引入 maven 依赖",slug:"引入-maven-依赖",normalizedTitle:"引入 maven 依赖",charIndex:406},{level:3,title:"规则配置",slug:"规则配置",normalizedTitle:"规则配置",charIndex:647},{level:3,title:"创建数据源",slug:"创建数据源",normalizedTitle:"创建数据源",charIndex:767},{level:2,title:"概念和功能",slug:"概念和功能",normalizedTitle:"概念和功能",charIndex:1041},{level:3,title:"垂直分片",slug:"垂直分片",normalizedTitle:"垂直分片",charIndex:1646},{level:3,title:"水平分片",slug:"水平分片",normalizedTitle:"水平分片",charIndex:1651},{level:3,title:"数据分片带来的问题",slug:"数据分片带来的问题",normalizedTitle:"数据分片带来的问题",charIndex:2157},{level:2,title:"ShardingSphere 内核剖析",slug:"shardingsphere-内核剖析",normalizedTitle:"shardingsphere 内核剖析",charIndex:2419},{level:3,title:"解析引擎",slug:"解析引擎",normalizedTitle:"解析引擎",charIndex:2914},{level:4,title:"抽象语法树",slug:"抽象语法树",normalizedTitle:"抽象语法树",charIndex:2922},{level:4,title:"SQL 解析引擎",slug:"sql-解析引擎",normalizedTitle:"sql 解析引擎",charIndex:3502},{level:3,title:"路由引擎",slug:"路由引擎",normalizedTitle:"路由引擎",charIndex:4289},{level:3,title:"改写引擎",slug:"改写引擎",normalizedTitle:"改写引擎",charIndex:4298},{level:3,title:"执行引擎",slug:"执行引擎",normalizedTitle:"执行引擎",charIndex:4307},{level:3,title:"归并引擎",slug:"归并引擎",normalizedTitle:"归并引擎",charIndex:4316}],headersStr:"简介 快速入门 引入 maven 依赖 规则配置 创建数据源 概念和功能 垂直分片 水平分片 数据分片带来的问题 ShardingSphere 内核剖析 解析引擎 抽象语法树 SQL 解析引擎 路由引擎 改写引擎 执行引擎 归并引擎",content:"# ShardingSphere Jdbc\n\n\n# 简介\n\nshardingsphere-jdbc 定位为轻量级 Java 框架，在 Java 的 JDBC 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 JDBC 驱动，完全兼容 JDBC 和各种 ORM 框架。\n\n * 适用于任何基于 JDBC 的 ORM 框架，如：JPA, Hibernate, Mybatis, Spring JDBC Template 或直接使用 JDBC。\n * 支持任何第三方的数据库连接池，如：DBCP, C3P0, BoneCP, Druid, HikariCP 等。\n * 支持任意实现 JDBC 规范的数据库，目前支持 MySQL，Oracle，SQLServer，PostgreSQL 以及任何遵循 SQL92 标准的数据库。\n\n\n\n\n# 快速入门\n\n\n# 引入 maven 依赖\n\n<dependency>\n    <groupId>org.apache.shardingsphere</groupId>\n    <artifactId>shardingsphere-jdbc-core</artifactId>\n    <version>${latest.release.version}</version>\n</dependency>\n\n\n注意：请将 ${latest.release.version} 更改为实际的版本号。\n\n\n# 规则配置\n\nShardingSphere-JDBC 可以通过 Java，YAML，Spring 命名空间和 Spring Boot Starter 这 4 种方式进行配置，开发者可根据场景选择适合的配置方式。 详情请参见配置手册。\n\n\n# 创建数据源\n\n通过 ShardingSphereDataSourceFactory 工厂和规则配置对象获取 ShardingSphereDataSource。 该对象实现自 JDBC 的标准 DataSource 接口，可用于原生 JDBC 开发，或使用 JPA, MyBatis 等 ORM 类库。\n\nDataSource dataSource = ShardingSphereDataSourceFactory.createDataSource(dataSourceMap, configurations, properties);\n\n\n\n# 概念和功能\n\n单一数据节点难于满足互联网的海量数据场景。\n\n从性能方面来说，由于关系型数据库大多采用 B+ 树类型的索引，在数据量超过阈值的情况下，索引深度的增加也将使得磁盘访问的 IO 次数增加，进而导致查询性能的下降；同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈。\n\n在传统的关系型数据库无法满足互联网场景需要的情况下，将数据存储至原生支持分布式的 NoSQL 的尝试越来越多。 但 NoSQL 对 SQL 的不兼容性以及生态圈的不完善，使得它们在与关系型数据库的博弈中始终无法完成致命一击，而关系型数据库的地位却依然不可撼动。\n\n数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中以达到提升性能瓶颈以及可用性的效果。数据分片的有效手段是对关系型数据库进行分库和分表。分库和分表均可以有效的避免由数据量超过可承受阈值而产生的查询瓶颈。 除此之外，分库还能够用于有效的分散对数据库单点的访问量；分表虽然无法缓解数据库压力，但却能够提供尽量将分布式事务转化为本地事务的可能，一旦涉及到跨库的更新操作，分布式事务往往会使问题变得复杂。 使用多主多从的分片方式，可以有效的避免数据单点，从而提升数据架构的可用性。\n\n通过分库和分表进行数据的拆分来使得各个表的数据量保持在阈值以下，以及对流量进行疏导应对高访问量，是应对高并发和海量数据系统的有效手段。 数据分片的拆分方式又分为垂直分片和水平分片。\n\n\n# 垂直分片\n\n按照业务拆分的方式称为垂直分片，又称为纵向拆分，它的核心理念是专库专用。 在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库。 下图展示了根据业务需要，将用户表和订单表垂直分片到不同的数据库的方案。\n\n\n\n垂直分片往往需要对架构和设计进行调整。通常来讲，是来不及应对互联网业务需求快速变化的；而且，它也并无法真正的解决单点瓶颈。 垂直拆分可以缓解数据量和访问量带来的问题，但无法根治。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理。\n\n\n# 水平分片\n\n水平分片又称为横向拆分。 相对于垂直分片，它不再将数据根据业务逻辑分类，而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分。 例如：根据主键分片，偶数主键的记录放入 0 库（或表），奇数主键的记录放入 1 库（或表），如下图所示。\n\n\n\n水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案。\n\n\n# 数据分片带来的问题\n\n * 数据路由：需要知道数据需要从哪个具体的数据库的分表中获取。\n * SQL 不兼容：分表导致表名称的修改，或者分页、排序、聚合、分组等操作的不正确处理。\n * 跨库事务：合理采用分表，可以在降低单表数据量的情况下，尽量使用本地事务，善于使用同库不同表可有效避免分布式事务带来的麻烦。 在不能避免跨库事务的场景，有些业务仍然需要保持事务的一致性。 而基于 XA 的分布式事务由于在并发度高的场景中性能无法满足需要，并未被互联网巨头大规模使用，他们大多采用最终一致性的柔性事务代替强一致事务。\n\n\n# ShardingSphere 内核剖析\n\nShardingSphere 的 3 个产品的数据分片主要流程是完全一致的。 核心由 SQL 解析 => 执行器优化 => SQL 路由 => SQL 改写 => SQL 执行 => 结果归并的流程组成。\n\n\n\n * QL 解析：分为词法解析和语法解析。 先通过词法解析器将 SQL 拆分为一个个不可再分的单词。再使用语法解析器对 SQL 进行理解，并最终提炼出解析上下文。 解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符的标记。\n * 执行器优化：合并和优化分片条件，如 OR 等。\n * SQL 路由：根据解析上下文匹配用户配置的分片策略，并生成路由路径。目前支持分片路由和广播路由。\n * SQL 改写：将 SQL 改写为在真实数据库中可以正确执行的语句。SQL 改写分为正确性改写和优化改写。\n * SQL 执行：通过多线程执行器异步执行。\n * 结果归并：将多个执行结果集归并以便于通过统一的 JDBC 接口输出。结果归并包括流式归并、内存归并和使用装饰模式的追加归并这几种方式。\n\n\n# 解析引擎\n\n# 抽象语法树\n\n解析过程分为词法解析和语法解析。 词法解析器用于将 SQL 拆解为不可再分的原子符号，称为 Token。并根据不同数据库方言所提供的字典，将其归类为关键字，表达式，字面量和操作符。 再使用语法解析器将 SQL 转换为抽象语法树。\n\n例如，以下 SQL：\n\nSELECT id, name FROM t_user WHERE status = 'ACTIVE' AND age > 18\n\n\n解析之后的为抽象语法树见下图。\n\n\n\n为了便于理解，抽象语法树中的关键字的 Token 用绿色表示，变量的 Token 用红色表示，灰色表示需要进一步拆分。\n\n最后，通过对抽象语法树的遍历去提炼分片所需的上下文，并标记有可能需要改写的位置。 供分片使用的解析上下文包含查询选择项（Select Items）、表信息（Table）、分片条件（Sharding Condition）、自增主键信息（Auto increment Primary Key）、排序信息（Order By）、分组信息（Group By）以及分页信息（Limit、Rownum、Top）。 SQL 的一次解析过程是不可逆的，一个个 Token 按 SQL 原本的顺序依次进行解析，性能很高。 考虑到各种数据库 SQL 方言的异同，在解析模块提供了各类数据库的 SQL 方言字典。\n\n# SQL 解析引擎\n\nSQL 解析作为分库分表类产品的核心，其性能和兼容性是最重要的衡量指标。 ShardingSphere 的 SQL 解析器经历了 3 代产品的更新迭代。\n\n第一代 SQL 解析器为了追求性能与快速实现，在 1.4.x 之前的版本使用 Druid 作为 SQL 解析器。经实际测试，它的性能远超其它解析器。\n\n第二代 SQL 解析器从 1.5.x 版本开始，ShardingSphere 采用完全自研的 SQL 解析引擎。 由于目的不同，ShardingSphere 并不需要将 SQL 转为一颗完全的抽象语法树，也无需通过访问器模式进行二次遍历。它采用对 SQL 半理解的方式，仅提炼数据分片需要关注的上下文，因此 SQL 解析的性能和兼容性得到了进一步的提高。\n\n第三代 SQL 解析器则从 3.0.x 版本开始，ShardingSphere 尝试使用 ANTLR 作为 SQL 解析的引擎，并计划根据 DDL -> TCL -> DAL –> DCL -> DML –>DQL 这个顺序，依次替换原有的解析引擎，目前仍处于替换迭代中。 使用 ANTLR 的原因是希望 ShardingSphere 的解析引擎能够更好的对 SQL 进行兼容。对于复杂的表达式、递归、子查询等语句，虽然 ShardingSphere 的分片核心并不关注，但是会影响对于 SQL 理解的友好度。 经过实例测试，ANTLR 解析 SQL 的性能比自研的 SQL 解析引擎慢 3-10 倍左右。为了弥补这一差距，ShardingSphere 将使用 PreparedStatement 的 SQL 解析的语法树放入缓存。 因此建议采用 PreparedStatement 这种 SQL 预编译的方式提升性能。\n\n第三代 SQL 解析引擎的整体结构划分如下图所示。\n\n\n\n\n# 路由引擎\n\n\n# 改写引擎\n\n\n# 执行引擎\n\n\n# 归并引擎",normalizedContent:"# shardingsphere jdbc\n\n\n# 简介\n\nshardingsphere-jdbc 定位为轻量级 java 框架，在 java 的 jdbc 层提供的额外服务。 它使用客户端直连数据库，以 jar 包形式提供服务，无需额外部署和依赖，可理解为增强版的 jdbc 驱动，完全兼容 jdbc 和各种 orm 框架。\n\n * 适用于任何基于 jdbc 的 orm 框架，如：jpa, hibernate, mybatis, spring jdbc template 或直接使用 jdbc。\n * 支持任何第三方的数据库连接池，如：dbcp, c3p0, bonecp, druid, hikaricp 等。\n * 支持任意实现 jdbc 规范的数据库，目前支持 mysql，oracle，sqlserver，postgresql 以及任何遵循 sql92 标准的数据库。\n\n\n\n\n# 快速入门\n\n\n# 引入 maven 依赖\n\n<dependency>\n    <groupid>org.apache.shardingsphere</groupid>\n    <artifactid>shardingsphere-jdbc-core</artifactid>\n    <version>${latest.release.version}</version>\n</dependency>\n\n\n注意：请将 ${latest.release.version} 更改为实际的版本号。\n\n\n# 规则配置\n\nshardingsphere-jdbc 可以通过 java，yaml，spring 命名空间和 spring boot starter 这 4 种方式进行配置，开发者可根据场景选择适合的配置方式。 详情请参见配置手册。\n\n\n# 创建数据源\n\n通过 shardingspheredatasourcefactory 工厂和规则配置对象获取 shardingspheredatasource。 该对象实现自 jdbc 的标准 datasource 接口，可用于原生 jdbc 开发，或使用 jpa, mybatis 等 orm 类库。\n\ndatasource datasource = shardingspheredatasourcefactory.createdatasource(datasourcemap, configurations, properties);\n\n\n\n# 概念和功能\n\n单一数据节点难于满足互联网的海量数据场景。\n\n从性能方面来说，由于关系型数据库大多采用 b+ 树类型的索引，在数据量超过阈值的情况下，索引深度的增加也将使得磁盘访问的 io 次数增加，进而导致查询性能的下降；同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈。\n\n在传统的关系型数据库无法满足互联网场景需要的情况下，将数据存储至原生支持分布式的 nosql 的尝试越来越多。 但 nosql 对 sql 的不兼容性以及生态圈的不完善，使得它们在与关系型数据库的博弈中始终无法完成致命一击，而关系型数据库的地位却依然不可撼动。\n\n数据分片指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中以达到提升性能瓶颈以及可用性的效果。数据分片的有效手段是对关系型数据库进行分库和分表。分库和分表均可以有效的避免由数据量超过可承受阈值而产生的查询瓶颈。 除此之外，分库还能够用于有效的分散对数据库单点的访问量；分表虽然无法缓解数据库压力，但却能够提供尽量将分布式事务转化为本地事务的可能，一旦涉及到跨库的更新操作，分布式事务往往会使问题变得复杂。 使用多主多从的分片方式，可以有效的避免数据单点，从而提升数据架构的可用性。\n\n通过分库和分表进行数据的拆分来使得各个表的数据量保持在阈值以下，以及对流量进行疏导应对高访问量，是应对高并发和海量数据系统的有效手段。 数据分片的拆分方式又分为垂直分片和水平分片。\n\n\n# 垂直分片\n\n按照业务拆分的方式称为垂直分片，又称为纵向拆分，它的核心理念是专库专用。 在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库。 下图展示了根据业务需要，将用户表和订单表垂直分片到不同的数据库的方案。\n\n\n\n垂直分片往往需要对架构和设计进行调整。通常来讲，是来不及应对互联网业务需求快速变化的；而且，它也并无法真正的解决单点瓶颈。 垂直拆分可以缓解数据量和访问量带来的问题，但无法根治。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理。\n\n\n# 水平分片\n\n水平分片又称为横向拆分。 相对于垂直分片，它不再将数据根据业务逻辑分类，而是通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分。 例如：根据主键分片，偶数主键的记录放入 0 库（或表），奇数主键的记录放入 1 库（或表），如下图所示。\n\n\n\n水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案。\n\n\n# 数据分片带来的问题\n\n * 数据路由：需要知道数据需要从哪个具体的数据库的分表中获取。\n * sql 不兼容：分表导致表名称的修改，或者分页、排序、聚合、分组等操作的不正确处理。\n * 跨库事务：合理采用分表，可以在降低单表数据量的情况下，尽量使用本地事务，善于使用同库不同表可有效避免分布式事务带来的麻烦。 在不能避免跨库事务的场景，有些业务仍然需要保持事务的一致性。 而基于 xa 的分布式事务由于在并发度高的场景中性能无法满足需要，并未被互联网巨头大规模使用，他们大多采用最终一致性的柔性事务代替强一致事务。\n\n\n# shardingsphere 内核剖析\n\nshardingsphere 的 3 个产品的数据分片主要流程是完全一致的。 核心由 sql 解析 => 执行器优化 => sql 路由 => sql 改写 => sql 执行 => 结果归并的流程组成。\n\n\n\n * ql 解析：分为词法解析和语法解析。 先通过词法解析器将 sql 拆分为一个个不可再分的单词。再使用语法解析器对 sql 进行理解，并最终提炼出解析上下文。 解析上下文包括表、选择项、排序项、分组项、聚合函数、分页信息、查询条件以及可能需要修改的占位符的标记。\n * 执行器优化：合并和优化分片条件，如 or 等。\n * sql 路由：根据解析上下文匹配用户配置的分片策略，并生成路由路径。目前支持分片路由和广播路由。\n * sql 改写：将 sql 改写为在真实数据库中可以正确执行的语句。sql 改写分为正确性改写和优化改写。\n * sql 执行：通过多线程执行器异步执行。\n * 结果归并：将多个执行结果集归并以便于通过统一的 jdbc 接口输出。结果归并包括流式归并、内存归并和使用装饰模式的追加归并这几种方式。\n\n\n# 解析引擎\n\n# 抽象语法树\n\n解析过程分为词法解析和语法解析。 词法解析器用于将 sql 拆解为不可再分的原子符号，称为 token。并根据不同数据库方言所提供的字典，将其归类为关键字，表达式，字面量和操作符。 再使用语法解析器将 sql 转换为抽象语法树。\n\n例如，以下 sql：\n\nselect id, name from t_user where status = 'active' and age > 18\n\n\n解析之后的为抽象语法树见下图。\n\n\n\n为了便于理解，抽象语法树中的关键字的 token 用绿色表示，变量的 token 用红色表示，灰色表示需要进一步拆分。\n\n最后，通过对抽象语法树的遍历去提炼分片所需的上下文，并标记有可能需要改写的位置。 供分片使用的解析上下文包含查询选择项（select items）、表信息（table）、分片条件（sharding condition）、自增主键信息（auto increment primary key）、排序信息（order by）、分组信息（group by）以及分页信息（limit、rownum、top）。 sql 的一次解析过程是不可逆的，一个个 token 按 sql 原本的顺序依次进行解析，性能很高。 考虑到各种数据库 sql 方言的异同，在解析模块提供了各类数据库的 sql 方言字典。\n\n# sql 解析引擎\n\nsql 解析作为分库分表类产品的核心，其性能和兼容性是最重要的衡量指标。 shardingsphere 的 sql 解析器经历了 3 代产品的更新迭代。\n\n第一代 sql 解析器为了追求性能与快速实现，在 1.4.x 之前的版本使用 druid 作为 sql 解析器。经实际测试，它的性能远超其它解析器。\n\n第二代 sql 解析器从 1.5.x 版本开始，shardingsphere 采用完全自研的 sql 解析引擎。 由于目的不同，shardingsphere 并不需要将 sql 转为一颗完全的抽象语法树，也无需通过访问器模式进行二次遍历。它采用对 sql 半理解的方式，仅提炼数据分片需要关注的上下文，因此 sql 解析的性能和兼容性得到了进一步的提高。\n\n第三代 sql 解析器则从 3.0.x 版本开始，shardingsphere 尝试使用 antlr 作为 sql 解析的引擎，并计划根据 ddl -> tcl -> dal –> dcl -> dml –>dql 这个顺序，依次替换原有的解析引擎，目前仍处于替换迭代中。 使用 antlr 的原因是希望 shardingsphere 的解析引擎能够更好的对 sql 进行兼容。对于复杂的表达式、递归、子查询等语句，虽然 shardingsphere 的分片核心并不关注，但是会影响对于 sql 理解的友好度。 经过实例测试，antlr 解析 sql 的性能比自研的 sql 解析引擎慢 3-10 倍左右。为了弥补这一差距，shardingsphere 将使用 preparedstatement 的 sql 解析的语法树放入缓存。 因此建议采用 preparedstatement 这种 sql 预编译的方式提升性能。\n\n第三代 sql 解析引擎的整体结构划分如下图所示。\n\n\n\n\n# 路由引擎\n\n\n# 改写引擎\n\n\n# 执行引擎\n\n\n# 归并引擎",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"版本管理中间件 Flyway",frontmatter:{title:"版本管理中间件 Flyway",date:"2019-08-22T09:02:39.000Z",categories:["数据库","数据库中间件"],tags:["数据库","中间件","版本管理"],permalink:"/pages/e2648c/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/02.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6/02.Flyway.html",relativePath:"12.数据库/02.数据库中间件/02.Flyway.md",key:"v-1a0fef38",path:"/pages/e2648c/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:42},{level:3,title:"什么是 Flyway",slug:"什么是-flyway",normalizedTitle:"什么是 flyway",charIndex:49},{level:3,title:"为什么要使用数据迁移",slug:"为什么要使用数据迁移",normalizedTitle:"为什么要使用数据迁移",charIndex:87},{level:3,title:"Flyway 如何工作？",slug:"flyway-如何工作",normalizedTitle:"flyway 如何工作？",charIndex:599},{level:2,title:"快速上手",slug:"快速上手",normalizedTitle:"快速上手",charIndex:1123},{level:3,title:"命令行",slug:"命令行",normalizedTitle:"命令行",charIndex:1151},{level:3,title:"JAVA API",slug:"java-api",normalizedTitle:"java api",charIndex:1158},{level:3,title:"Maven",slug:"maven",normalizedTitle:"maven",charIndex:1170},{level:3,title:"Gradle",slug:"gradle",normalizedTitle:"gradle",charIndex:1179},{level:2,title:"入门篇",slug:"入门篇",normalizedTitle:"入门篇",charIndex:5358},{level:3,title:"概念",slug:"概念",normalizedTitle:"概念",charIndex:5366},{level:4,title:"Migrations",slug:"migrations",normalizedTitle:"migrations",charIndex:5372},{level:5,title:"Versioned migrations",slug:"versioned-migrations",normalizedTitle:"versioned migrations",charIndex:5439},{level:5,title:"Undo migrations",slug:"undo-migrations",normalizedTitle:"undo migrations",charIndex:6025},{level:5,title:"Repeatable migrations",slug:"repeatable-migrations",normalizedTitle:"repeatable migrations",charIndex:5462},{level:5,title:"基于 SQL 的 migrations",slug:"基于-sql-的-migrations",normalizedTitle:"基于 sql 的 migrations",charIndex:6563},{level:5,title:"基于 JAVA 的 migrations",slug:"基于-java-的-migrations",normalizedTitle:"基于 java 的 migrations",charIndex:7073},{level:4,title:"Callbacks",slug:"callbacks",normalizedTitle:"callbacks",charIndex:7529},{level:5,title:"SQL Callbacks",slug:"sql-callbacks",normalizedTitle:"sql callbacks",charIndex:7736},{level:5,title:"JAVA Callbacks",slug:"java-callbacks",normalizedTitle:"java callbacks",charIndex:8083},{level:4,title:"Error Handlers",slug:"error-handlers",normalizedTitle:"error handlers",charIndex:8571},{level:4,title:"Dry Runs",slug:"dry-runs",normalizedTitle:"dry runs",charIndex:8607},{level:3,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:1151},{level:3,title:"支持的数据库",slug:"支持的数据库",normalizedTitle:"支持的数据库",charIndex:8744},{level:2,title:"资料",slug:"资料",normalizedTitle:"资料",charIndex:8921},{level:2,title:"🚪 传送门",slug:"传送门",normalizedTitle:"🚪 传送门",charIndex:8947}],headersStr:"简介 什么是 Flyway 为什么要使用数据迁移 Flyway 如何工作？ 快速上手 命令行 JAVA API Maven Gradle 入门篇 概念 Migrations Versioned migrations Undo migrations Repeatable migrations 基于 SQL 的 migrations 基于 JAVA 的 migrations Callbacks SQL Callbacks JAVA Callbacks Error Handlers Dry Runs 命令 支持的数据库 资料 🚪 传送门",content:'# 版本管理中间件 Flyway\n\n> Flyway 是一个数据迁移工具。\n\n\n# 简介\n\n\n# 什么是 Flyway\n\nFlyway 是一个开源的数据库迁移工具。\n\n\n# 为什么要使用数据迁移\n\n为了说明数据迁移的作用，我们来举一个示例：\n\n（1）假设，有一个叫做 Shiny 的项目，它的架构是一个叫做 Shiny Soft 的 App 连接叫做 Shiny DB 的数据库。\n\n（2）对于大多数项目而言，最简单的持续集成场景如下所示：\n\n\n\n这意味着，我们不仅仅要处理一份环境中的修改，由此会引入一些版本冲突问题：\n\n在代码侧（即应用软件）的版本问题比较容易解决：\n\n * 有方便的版本控制工具\n * 有可复用的构建和持续集成\n * 规范的发布和部署过程\n\n那么，数据库层面的版本问题如何解决呢？\n\n目前仍然没有方便的数据库版本工具。许多项目仍使用 sql 脚本来解决版本冲突，甚至是遇到冲突问题时才想起用 sql 语句去解决。\n\n由此，引发一些问题：\n\n * 机器上的数据库是什么状态？\n * 脚本到底生效没有？\n * 生产环境修复的问题是否也在测试环境修复了？\n * 如何建立一个新的数据库实例？\n\n数据迁移就是用来搞定这些混乱的问题：\n\n * 通过草稿重建一个数据库。\n * 在任何时候都可以清楚的了解数据库的状态。\n * 以一种明确的方式将数据库从当前版本迁移到一个新版本。\n\n\n# Flyway 如何工作？\n\n最简单的场景是指定 Flyway 迁移到一个空的数据库。\n\n\n\nFlyway 会尝试查找它的 schema 历史表，如果数据库是空的，Flyway 就不再查找，而是直接创建数据库。\n\n现再你就有了一个仅包含一张空表的数据库，默认情况下，这张表叫 flyway_schema_history。\n\n\n\n这张表将被用于追踪数据库的状态。\n\n然后，Flyway 将开始扫描文件系统或应用 classpath 中的 migrations。这些 migrations 可以是 sql 或 java。\n\n这些 migrations 将根据他们的版本号进行排序。\n\n\n\n任意 migration 应用后，schema 历史表将更新。当元数据和初始状态替换后，可以称之为：迁移到新版本。\n\nFlyway 一旦扫描了文件系统或应用 classpath 下的 migrations，这些 migrations 会检查 schema 历史表。如果它们的版本号低于或等于当前的版本，将被忽略。保留下来的 migrations 是等待的 migrations，有效但没有应用。\n\n\n\nmigrations 将根据版本号排序并按序执行。\n\n\n\n\n# 快速上手\n\nFlyway 有 4 种使用方式：\n\n * 命令行\n * JAVA API\n * Maven\n * Gradle\n\n\n# 命令行\n\n适用于非 Java 用户，无需构建。\n\n> flyway migrate -url=... -user=... -password=...\n\n\n（1）下载解压\n\n进入官方下载页面，选择合适版本，下载并解压到本地。\n\n（2）配置 flyway\n\n编辑 /conf/flyway.conf：\n\nflyway.url=jdbc:h2:file:./foobardb\nflyway.user=SA\nflyway.password=\n\n\n（3）创建第一个 migration\n\n在 /sql 目录下创建 V1__Create_person_table.sql 文件，内容如下：\n\ncreate table PERSON (\n    ID int not null,\n    NAME varchar(100) not null\n);\n\n\n（4）迁移数据库\n\n运行 Flyway 来迁移数据库：\n\nflyway-5.1.4> flyway migrate\n\n\n运行正常的情况下，应该可以看到如下结果：\n\nDatabase: jdbc:h2:file:./foobardb (H2 1.4)\nSuccessfully validated 1 migration (execution time 00:00.008s)\nCreating Schema History table: "PUBLIC"."flyway_schema_history"\nCurrent version of schema "PUBLIC": << Empty Schema >>\nMigrating schema "PUBLIC" to version 1 - Create person table\nSuccessfully applied 1 migration to schema "PUBLIC" (execution time 00:00.033s)\n\n\n（5）添加第二个 migration\n\n在 /sql 目录下创建 V2__Add_people.sql 文件，内容如下：\n\ninsert into PERSON (ID, NAME) values (1, \'Axel\');\ninsert into PERSON (ID, NAME) values (2, \'Mr. Foo\');\ninsert into PERSON (ID, NAME) values (3, \'Ms. Bar\');\n\n\n运行 Flyway\n\nflyway-5.1.4> flyway migrate\n\n\n运行正常的情况下，应该可以看到如下结果：\n\nDatabase: jdbc:h2:file:./foobardb (H2 1.4)\nSuccessfully validated 2 migrations (execution time 00:00.018s)\nCurrent version of schema "PUBLIC": 1\nMigrating schema "PUBLIC" to version 2 - Add people\nSuccessfully applied 1 migration to schema "PUBLIC" (execution time 00:00.016s)\n\n\n\n# JAVA API\n\n（1）准备\n\n * Java8+\n * Maven 3.x\n\n（2）添加依赖\n\n在 pom.xml 中添加依赖：\n\n<project ...>\n    ...\n    <dependencies>\n        <dependency>\n            <groupId>org.flywaydb</groupId>\n            <artifactId>flyway-core</artifactId>\n            <version>5.1.4</version>\n        </dependency>\n        <dependency>\n            <groupId>com.h2database</groupId>\n            <artifactId>h2</artifactId>\n            <version>1.3.170</version>\n        </dependency>\n        ...\n    </dependencies>\n    ...\n</project>\n\n\n（3）集成 Flyway\n\n添加 App.java 文件，内容如下：\n\nimport org.flywaydb.core.Flyway;\n\npublic class App {\n    public static void main(String[] args) {\n        // Create the Flyway instance\n        Flyway flyway = new Flyway();\n\n        // Point it to the database\n        flyway.setDataSource("jdbc:h2:file:./target/foobar", "sa", null);\n\n        // Start the migration\n        flyway.migrate();\n    }\n}\n\n\n（4）创建第一个 migration\n\n添加 src/main/resources/db/migration/V1__Create_person_table.sql 文件，内容如下：\n\ncreate table PERSON (\n    ID int not null,\n    NAME varchar(100) not null\n);\n\n\n（5）执行程序\n\n执行 App#main：\n\n运行正常的情况下，应该可以看到如下结果：\n\nINFO: Creating schema history table: "PUBLIC"."flyway_schema_history"\nINFO: Current version of schema "PUBLIC": << Empty Schema >>\nINFO: Migrating schema "PUBLIC" to version 1 - Create person table\nINFO: Successfully applied 1 migration to schema "PUBLIC" (execution time 00:00.062s).\n\n\n（6）添加第二个 migration\n\n添加 src/main/resources/db/migration/V2__Add_people.sql 文件，内容如下：\n\ninsert into PERSON (ID, NAME) values (1, \'Axel\');\ninsert into PERSON (ID, NAME) values (2, \'Mr. Foo\');\ninsert into PERSON (ID, NAME) values (3, \'Ms. Bar\');\n\n\n运行正常的情况下，应该可以看到如下结果：\n\nINFO: Current version of schema "PUBLIC": 1\nINFO: Migrating schema "PUBLIC" to version 2 - Add people\nINFO: Successfully applied 1 migration to schema "PUBLIC" (execution time 00:00.090s).\n\n\n\n# Maven\n\n与 Java API 方式大体相同，区别在 集成 Flyway 步骤：\n\nMaven 方式使用插件来集成 Flyway：\n\n<project xmlns="...">\n    ...\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.flywaydb</groupId>\n                <artifactId>flyway-maven-plugin</artifactId>\n                <version>5.1.4</version>\n                <configuration>\n                    <url>jdbc:h2:file:./target/foobar</url>\n                    <user>sa</user>\n                </configuration>\n                <dependencies>\n                    <dependency>\n                        <groupId>com.h2database</groupId>\n                        <artifactId>h2</artifactId>\n                        <version>1.4.191</version>\n                    </dependency>\n                </dependencies>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n因为用的是插件，所以执行方式不再是运行 Java 类，而是执行 maven 插件：\n\n> mvn flyway:migrate\n\n\n> 👉 参考：示例源码\n\n\n# Gradle\n\n本人不用 Gradle，略。\n\n\n# 入门篇\n\n\n# 概念\n\n# Migrations\n\n在 Flyway 中，对于数据库的任何改变都称之为 Migrations。\n\nMigrations 可以分为 Versioned migrations 和 Repeatable migrations。\n\nVersioned migrations 有 2 种形式：regular 和 undo。\n\nVersioned migrations 和 Repeatable migrations 都可以使用 SQL 或 JAVA 来编写。\n\n# Versioned migrations\n\n由一个版本号（version）、一段描述（description）、一个校验（checksum）组成。版本号必须是惟一的。Versioned migrations 只能按顺序执行一次。\n\n一般用于：\n\n * 增删改 tables/indexes/foreign keys/enums/UDTs。\n * 引用数据更新\n * 用户数据校正\n\nRegular 示例：\n\nCREATE TABLE car (\n    id INT NOT NULL PRIMARY KEY,\n    license_plate VARCHAR NOT NULL,\n    color VARCHAR NOT NULL\n);\n\nALTER TABLE owner ADD driver_license_id VARCHAR;\n\nINSERT INTO brand (name) VALUES (\'DeLorean\');\n\n\n# Undo migrations\n\n> 注：仅专业版支持\n\nUndo Versioned Migrations 负责撤销 Regular Versioned migrations 的影响。\n\nUndo 示例：\n\nDELETE FROM brand WHERE name=\'DeLorean\';\n\nALTER TABLE owner DROP driver_license_id;\n\nDROP TABLE car;\n\n\n# Repeatable migrations\n\n由一段描述（description）、一个校验（checksum）组成。Versioned migrations 每次执行后，校验（checksum）会更新。\n\nRepeatable migrations 用于管理可以通过一个文件来维护版本控制的数据库对象。\n\n一般用于：\n\n * 创建（重建）views/procedures/functions/packages 等。\n * 大量引用数据重新插入\n\n示例：\n\nCREATE OR REPLACE VIEW blue_cars AS\n    SELECT id, license_plate FROM cars WHERE color=\'blue\';\n\n\n# 基于 SQL 的 migrations\n\nmigrations 最常用的编写形式就是 SQL。\n\n基于 SQL 的 migrations 一般用于：\n\n * DDL 变更（针对 TABLES,VIEWS,TRIGGERS,SEQUENCES 等的 CREATE/ALTER/DROP 操作）\n * 简单的引用数据变更（引用数据表中的 CRUD）\n * 简单的大量数据变更（常规数据表中的 CRUD）\n\n命名规则\n\n为了被 Flyway 自动识别，SQL migrations 的文件命名必须遵循规定的模式：\n\n\n\n * Prefix - V 代表 versioned migrations (可配置), U 代表 undo migrations (可配置)、 R 代表 repeatable migrations (可配置)\n * Version - 版本号通过.(点)或_(下划线)分隔 (repeatable migrations 不需要)\n * Separator - __ (两个下划线) (可配置)\n * Description - 下划线或空格分隔的单词\n * Suffix - .sql (可配置)\n\n# 基于 JAVA 的 migrations\n\n基于 JAVA 的 migrations 适用于使用 SQL 不容易表达的场景：\n\n * BLOB 和 CLOB 变更\n * 大量数据的高级变更（重新计算、高级格式变更）\n\n命名规则\n\n为了被 Flyway 自动识别，JAVA migrations 的文件命名必须遵循规定的模式：\n\n\n\n * Prefix - V 代表 versioned migrations (可配置), U 代表 undo migrations (可配置)、 R 代表 repeatable migrations (可配置)\n * Version - 版本号通过.(点)或_(下划线)分隔 (repeatable migrations 不需要)\n * Separator - __ (两个下划线) (可配置)\n * Description - 下划线或空格分隔的单词\n\n> 👉 更多细节请参考：https://flywaydb.org/documentation/migrations\n\n# Callbacks\n\n> 注：部分 events 仅专业版支持。\n\n尽管 Migrations 可能已经满足绝大部分场景的需要，但是某些情况下需要你一遍又一遍的执行相同的行为。这可能会重新编译存储过程，更新视图以及许多其他类型的开销。\n\n因为以上原因，Flyway 提供了 Callbacks，用于在 Migrations 生命周期中添加钩子。\n\nCallbacks 可以用 SQL 或 JAVA 来实现。\n\n# SQL Callbacks\n\nSQL Callbacks 的命名规则为：event 名 + SQL migration。\n\n如： beforeMigrate.sql, beforeEachMigrate.sql, afterEachMigrate.sql 等。\n\nSQL Callbacks 也可以包含描述（description）。这种情况下，SQL Callbacks 文件名 = event 名 + 分隔符 + 描述 + 后缀。例：beforeRepair__vacuum.sql\n\n当同一个 event 有多个 SQL callbacks，将按照它们描述（description）的顺序执行。\n\n> 注： Flyway 也支持你配置的 sqlMigrationSuffixes。\n\n# JAVA Callbacks\n\n> 当 SQL Callbacks 不够方便时，才应考虑 JAVA Callbacks。\n\nJAVA Callbacks 有 3 种形式：\n\n 1. 基于 Java 的 Migrations - 实现 JdbcMigration、SpringJdbcMigration、MigrationInfoProvider、MigrationChecksumProvider、ConfigurationAware、FlywayConfiguration\n 2. 基于 Java 的 Callbacks - 实现 org.flywaydb.core.api.callback 接口。\n 3. 自定义 Migration resolvers 和 executors - 实现 MigrationResolver、MigrationExecutor、ConfigurationAware、FlywayConfiguration 接口。\n\n> 👉 更多细节请参考：https://flywaydb.org/documentation/callbacks\n\n# Error Handlers\n\n> 注：仅专业版支持。\n\n（略）\n\n# Dry Runs\n\n> 注：仅专业版支持。\n\n（略）\n\n\n# 命令\n\nFlyway 的功能主要围绕着 7 个基本命令：Migrate、Clean、Info、Validate、Undo、Baseline 和 Repair。\n\n注：各命令的使用方法细节请查阅官方文档。\n\n\n# 支持的数据库\n\n * Oracle\n * SQL Server\n * DB2\n * MySQL\n * MariaDB\n * PostgreSQL\n * Redshift\n * CockroachDB\n * SAP HANA\n * Sybase ASE\n * Informix\n * H2\n * HSQLDB\n * Derby\n * SQLite\n\n\n# 资料\n\n| Github | 官方文档 |\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |',normalizedContent:'# 版本管理中间件 flyway\n\n> flyway 是一个数据迁移工具。\n\n\n# 简介\n\n\n# 什么是 flyway\n\nflyway 是一个开源的数据库迁移工具。\n\n\n# 为什么要使用数据迁移\n\n为了说明数据迁移的作用，我们来举一个示例：\n\n（1）假设，有一个叫做 shiny 的项目，它的架构是一个叫做 shiny soft 的 app 连接叫做 shiny db 的数据库。\n\n（2）对于大多数项目而言，最简单的持续集成场景如下所示：\n\n\n\n这意味着，我们不仅仅要处理一份环境中的修改，由此会引入一些版本冲突问题：\n\n在代码侧（即应用软件）的版本问题比较容易解决：\n\n * 有方便的版本控制工具\n * 有可复用的构建和持续集成\n * 规范的发布和部署过程\n\n那么，数据库层面的版本问题如何解决呢？\n\n目前仍然没有方便的数据库版本工具。许多项目仍使用 sql 脚本来解决版本冲突，甚至是遇到冲突问题时才想起用 sql 语句去解决。\n\n由此，引发一些问题：\n\n * 机器上的数据库是什么状态？\n * 脚本到底生效没有？\n * 生产环境修复的问题是否也在测试环境修复了？\n * 如何建立一个新的数据库实例？\n\n数据迁移就是用来搞定这些混乱的问题：\n\n * 通过草稿重建一个数据库。\n * 在任何时候都可以清楚的了解数据库的状态。\n * 以一种明确的方式将数据库从当前版本迁移到一个新版本。\n\n\n# flyway 如何工作？\n\n最简单的场景是指定 flyway 迁移到一个空的数据库。\n\n\n\nflyway 会尝试查找它的 schema 历史表，如果数据库是空的，flyway 就不再查找，而是直接创建数据库。\n\n现再你就有了一个仅包含一张空表的数据库，默认情况下，这张表叫 flyway_schema_history。\n\n\n\n这张表将被用于追踪数据库的状态。\n\n然后，flyway 将开始扫描文件系统或应用 classpath 中的 migrations。这些 migrations 可以是 sql 或 java。\n\n这些 migrations 将根据他们的版本号进行排序。\n\n\n\n任意 migration 应用后，schema 历史表将更新。当元数据和初始状态替换后，可以称之为：迁移到新版本。\n\nflyway 一旦扫描了文件系统或应用 classpath 下的 migrations，这些 migrations 会检查 schema 历史表。如果它们的版本号低于或等于当前的版本，将被忽略。保留下来的 migrations 是等待的 migrations，有效但没有应用。\n\n\n\nmigrations 将根据版本号排序并按序执行。\n\n\n\n\n# 快速上手\n\nflyway 有 4 种使用方式：\n\n * 命令行\n * java api\n * maven\n * gradle\n\n\n# 命令行\n\n适用于非 java 用户，无需构建。\n\n> flyway migrate -url=... -user=... -password=...\n\n\n（1）下载解压\n\n进入官方下载页面，选择合适版本，下载并解压到本地。\n\n（2）配置 flyway\n\n编辑 /conf/flyway.conf：\n\nflyway.url=jdbc:h2:file:./foobardb\nflyway.user=sa\nflyway.password=\n\n\n（3）创建第一个 migration\n\n在 /sql 目录下创建 v1__create_person_table.sql 文件，内容如下：\n\ncreate table person (\n    id int not null,\n    name varchar(100) not null\n);\n\n\n（4）迁移数据库\n\n运行 flyway 来迁移数据库：\n\nflyway-5.1.4> flyway migrate\n\n\n运行正常的情况下，应该可以看到如下结果：\n\ndatabase: jdbc:h2:file:./foobardb (h2 1.4)\nsuccessfully validated 1 migration (execution time 00:00.008s)\ncreating schema history table: "public"."flyway_schema_history"\ncurrent version of schema "public": << empty schema >>\nmigrating schema "public" to version 1 - create person table\nsuccessfully applied 1 migration to schema "public" (execution time 00:00.033s)\n\n\n（5）添加第二个 migration\n\n在 /sql 目录下创建 v2__add_people.sql 文件，内容如下：\n\ninsert into person (id, name) values (1, \'axel\');\ninsert into person (id, name) values (2, \'mr. foo\');\ninsert into person (id, name) values (3, \'ms. bar\');\n\n\n运行 flyway\n\nflyway-5.1.4> flyway migrate\n\n\n运行正常的情况下，应该可以看到如下结果：\n\ndatabase: jdbc:h2:file:./foobardb (h2 1.4)\nsuccessfully validated 2 migrations (execution time 00:00.018s)\ncurrent version of schema "public": 1\nmigrating schema "public" to version 2 - add people\nsuccessfully applied 1 migration to schema "public" (execution time 00:00.016s)\n\n\n\n# java api\n\n（1）准备\n\n * java8+\n * maven 3.x\n\n（2）添加依赖\n\n在 pom.xml 中添加依赖：\n\n<project ...>\n    ...\n    <dependencies>\n        <dependency>\n            <groupid>org.flywaydb</groupid>\n            <artifactid>flyway-core</artifactid>\n            <version>5.1.4</version>\n        </dependency>\n        <dependency>\n            <groupid>com.h2database</groupid>\n            <artifactid>h2</artifactid>\n            <version>1.3.170</version>\n        </dependency>\n        ...\n    </dependencies>\n    ...\n</project>\n\n\n（3）集成 flyway\n\n添加 app.java 文件，内容如下：\n\nimport org.flywaydb.core.flyway;\n\npublic class app {\n    public static void main(string[] args) {\n        // create the flyway instance\n        flyway flyway = new flyway();\n\n        // point it to the database\n        flyway.setdatasource("jdbc:h2:file:./target/foobar", "sa", null);\n\n        // start the migration\n        flyway.migrate();\n    }\n}\n\n\n（4）创建第一个 migration\n\n添加 src/main/resources/db/migration/v1__create_person_table.sql 文件，内容如下：\n\ncreate table person (\n    id int not null,\n    name varchar(100) not null\n);\n\n\n（5）执行程序\n\n执行 app#main：\n\n运行正常的情况下，应该可以看到如下结果：\n\ninfo: creating schema history table: "public"."flyway_schema_history"\ninfo: current version of schema "public": << empty schema >>\ninfo: migrating schema "public" to version 1 - create person table\ninfo: successfully applied 1 migration to schema "public" (execution time 00:00.062s).\n\n\n（6）添加第二个 migration\n\n添加 src/main/resources/db/migration/v2__add_people.sql 文件，内容如下：\n\ninsert into person (id, name) values (1, \'axel\');\ninsert into person (id, name) values (2, \'mr. foo\');\ninsert into person (id, name) values (3, \'ms. bar\');\n\n\n运行正常的情况下，应该可以看到如下结果：\n\ninfo: current version of schema "public": 1\ninfo: migrating schema "public" to version 2 - add people\ninfo: successfully applied 1 migration to schema "public" (execution time 00:00.090s).\n\n\n\n# maven\n\n与 java api 方式大体相同，区别在 集成 flyway 步骤：\n\nmaven 方式使用插件来集成 flyway：\n\n<project xmlns="...">\n    ...\n    <build>\n        <plugins>\n            <plugin>\n                <groupid>org.flywaydb</groupid>\n                <artifactid>flyway-maven-plugin</artifactid>\n                <version>5.1.4</version>\n                <configuration>\n                    <url>jdbc:h2:file:./target/foobar</url>\n                    <user>sa</user>\n                </configuration>\n                <dependencies>\n                    <dependency>\n                        <groupid>com.h2database</groupid>\n                        <artifactid>h2</artifactid>\n                        <version>1.4.191</version>\n                    </dependency>\n                </dependencies>\n            </plugin>\n        </plugins>\n    </build>\n</project>\n\n\n因为用的是插件，所以执行方式不再是运行 java 类，而是执行 maven 插件：\n\n> mvn flyway:migrate\n\n\n> 👉 参考：示例源码\n\n\n# gradle\n\n本人不用 gradle，略。\n\n\n# 入门篇\n\n\n# 概念\n\n# migrations\n\n在 flyway 中，对于数据库的任何改变都称之为 migrations。\n\nmigrations 可以分为 versioned migrations 和 repeatable migrations。\n\nversioned migrations 有 2 种形式：regular 和 undo。\n\nversioned migrations 和 repeatable migrations 都可以使用 sql 或 java 来编写。\n\n# versioned migrations\n\n由一个版本号（version）、一段描述（description）、一个校验（checksum）组成。版本号必须是惟一的。versioned migrations 只能按顺序执行一次。\n\n一般用于：\n\n * 增删改 tables/indexes/foreign keys/enums/udts。\n * 引用数据更新\n * 用户数据校正\n\nregular 示例：\n\ncreate table car (\n    id int not null primary key,\n    license_plate varchar not null,\n    color varchar not null\n);\n\nalter table owner add driver_license_id varchar;\n\ninsert into brand (name) values (\'delorean\');\n\n\n# undo migrations\n\n> 注：仅专业版支持\n\nundo versioned migrations 负责撤销 regular versioned migrations 的影响。\n\nundo 示例：\n\ndelete from brand where name=\'delorean\';\n\nalter table owner drop driver_license_id;\n\ndrop table car;\n\n\n# repeatable migrations\n\n由一段描述（description）、一个校验（checksum）组成。versioned migrations 每次执行后，校验（checksum）会更新。\n\nrepeatable migrations 用于管理可以通过一个文件来维护版本控制的数据库对象。\n\n一般用于：\n\n * 创建（重建）views/procedures/functions/packages 等。\n * 大量引用数据重新插入\n\n示例：\n\ncreate or replace view blue_cars as\n    select id, license_plate from cars where color=\'blue\';\n\n\n# 基于 sql 的 migrations\n\nmigrations 最常用的编写形式就是 sql。\n\n基于 sql 的 migrations 一般用于：\n\n * ddl 变更（针对 tables,views,triggers,sequences 等的 create/alter/drop 操作）\n * 简单的引用数据变更（引用数据表中的 crud）\n * 简单的大量数据变更（常规数据表中的 crud）\n\n命名规则\n\n为了被 flyway 自动识别，sql migrations 的文件命名必须遵循规定的模式：\n\n\n\n * prefix - v 代表 versioned migrations (可配置), u 代表 undo migrations (可配置)、 r 代表 repeatable migrations (可配置)\n * version - 版本号通过.(点)或_(下划线)分隔 (repeatable migrations 不需要)\n * separator - __ (两个下划线) (可配置)\n * description - 下划线或空格分隔的单词\n * suffix - .sql (可配置)\n\n# 基于 java 的 migrations\n\n基于 java 的 migrations 适用于使用 sql 不容易表达的场景：\n\n * blob 和 clob 变更\n * 大量数据的高级变更（重新计算、高级格式变更）\n\n命名规则\n\n为了被 flyway 自动识别，java migrations 的文件命名必须遵循规定的模式：\n\n\n\n * prefix - v 代表 versioned migrations (可配置), u 代表 undo migrations (可配置)、 r 代表 repeatable migrations (可配置)\n * version - 版本号通过.(点)或_(下划线)分隔 (repeatable migrations 不需要)\n * separator - __ (两个下划线) (可配置)\n * description - 下划线或空格分隔的单词\n\n> 👉 更多细节请参考：https://flywaydb.org/documentation/migrations\n\n# callbacks\n\n> 注：部分 events 仅专业版支持。\n\n尽管 migrations 可能已经满足绝大部分场景的需要，但是某些情况下需要你一遍又一遍的执行相同的行为。这可能会重新编译存储过程，更新视图以及许多其他类型的开销。\n\n因为以上原因，flyway 提供了 callbacks，用于在 migrations 生命周期中添加钩子。\n\ncallbacks 可以用 sql 或 java 来实现。\n\n# sql callbacks\n\nsql callbacks 的命名规则为：event 名 + sql migration。\n\n如： beforemigrate.sql, beforeeachmigrate.sql, aftereachmigrate.sql 等。\n\nsql callbacks 也可以包含描述（description）。这种情况下，sql callbacks 文件名 = event 名 + 分隔符 + 描述 + 后缀。例：beforerepair__vacuum.sql\n\n当同一个 event 有多个 sql callbacks，将按照它们描述（description）的顺序执行。\n\n> 注： flyway 也支持你配置的 sqlmigrationsuffixes。\n\n# java callbacks\n\n> 当 sql callbacks 不够方便时，才应考虑 java callbacks。\n\njava callbacks 有 3 种形式：\n\n 1. 基于 java 的 migrations - 实现 jdbcmigration、springjdbcmigration、migrationinfoprovider、migrationchecksumprovider、configurationaware、flywayconfiguration\n 2. 基于 java 的 callbacks - 实现 org.flywaydb.core.api.callback 接口。\n 3. 自定义 migration resolvers 和 executors - 实现 migrationresolver、migrationexecutor、configurationaware、flywayconfiguration 接口。\n\n> 👉 更多细节请参考：https://flywaydb.org/documentation/callbacks\n\n# error handlers\n\n> 注：仅专业版支持。\n\n（略）\n\n# dry runs\n\n> 注：仅专业版支持。\n\n（略）\n\n\n# 命令\n\nflyway 的功能主要围绕着 7 个基本命令：migrate、clean、info、validate、undo、baseline 和 repair。\n\n注：各命令的使用方法细节请查阅官方文档。\n\n\n# 支持的数据库\n\n * oracle\n * sql server\n * db2\n * mysql\n * mariadb\n * postgresql\n * redshift\n * cockroachdb\n * sap hana\n * sybase ase\n * informix\n * h2\n * hsqldb\n * derby\n * sqlite\n\n\n# 资料\n\n| github | 官方文档 |\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"数据库中间件和代理",frontmatter:{title:"数据库中间件和代理",date:"2022-04-11T16:52:35.000Z",categories:["数据库","数据库中间件"],tags:["数据库","中间件"],permalink:"/pages/addb05/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/02.%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6/",relativePath:"12.数据库/02.数据库中间件/README.md",key:"v-208168d6",path:"/pages/addb05/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:16},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:89},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:311}],headersStr:"📖 内容 📚 资料 🚪 传送",content:"# 数据库中间件和代理\n\n\n# 📖 内容\n\n * ShardingSphere 简介\n * ShardingSphere Jdbc\n * 版本管理中间件 Flyway\n\n\n# 📚 资料\n\n * Seata - 分布式事务中间件。\n * ShardingSphere - 关系型数据库读写分离、分库分表中间件。\n * Flyway - 关系型数据库版本管理中间件。\n * Canal - 基于 MySQL 的 binlog，提供增量数据订阅和消费。\n * Twemproxy - Twitter 开源的一个 Redis 和 Memcache 的中间代理服务。\n * Codis - Redis 分布式集群方案。\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# 数据库中间件和代理\n\n\n# 📖 内容\n\n * shardingsphere 简介\n * shardingsphere jdbc\n * 版本管理中间件 flyway\n\n\n# 📚 资料\n\n * seata - 分布式事务中间件。\n * shardingsphere - 关系型数据库读写分离、分库分表中间件。\n * flyway - 关系型数据库版本管理中间件。\n * canal - 基于 mysql 的 binlog，提供增量数据订阅和消费。\n * twemproxy - twitter 开源的一个 redis 和 memcache 的中间代理服务。\n * codis - redis 分布式集群方案。\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"关系型数据库面试",frontmatter:{title:"关系型数据库面试",date:"2020-01-15T23:21:02.000Z",categories:["数据库","关系型数据库","综合"],tags:["数据库","关系型数据库"],permalink:"/pages/9bb28f/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E7%BB%BC%E5%90%88/01.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%A2%E8%AF%95.html",relativePath:"12.数据库/03.关系型数据库/01.综合/01.关系型数据库面试.md",key:"v-099fc77f",path:"/pages/9bb28f/",headers:[{level:2,title:"一、索引和约束",slug:"一、索引和约束",normalizedTitle:"一、索引和约束",charIndex:15},{level:3,title:"什么是索引",slug:"什么是索引",normalizedTitle:"什么是索引",charIndex:27},{level:3,title:"索引的优缺点",slug:"索引的优缺点",normalizedTitle:"索引的优缺点",charIndex:81},{level:3,title:"何时使用索引",slug:"何时使用索引",normalizedTitle:"何时使用索引",charIndex:462},{level:3,title:"索引的类型",slug:"索引的类型",normalizedTitle:"索引的类型",charIndex:769},{level:3,title:"索引的数据结构",slug:"索引的数据结构",normalizedTitle:"索引的数据结构",charIndex:1211},{level:4,title:"B 树",slug:"b-树",normalizedTitle:"b 树",charIndex:1239},{level:4,title:"B+ 树",slug:"b-树-2",normalizedTitle:"b+ 树",charIndex:1243},{level:4,title:"B 树 vs. B+ 树",slug:"b-树-vs-b-树",normalizedTitle:"b 树 vs. b+ 树",charIndex:2198},{level:4,title:"Hash",slug:"hash",normalizedTitle:"hash",charIndex:2456},{level:3,title:"索引策略",slug:"索引策略",normalizedTitle:"索引策略",charIndex:2822},{level:4,title:"索引基本原则",slug:"索引基本原则",normalizedTitle:"索引基本原则",charIndex:2830},{level:4,title:"独立的列",slug:"独立的列",normalizedTitle:"独立的列",charIndex:2947},{level:4,title:"前缀索引和索引选择性",slug:"前缀索引和索引选择性",normalizedTitle:"前缀索引和索引选择性",charIndex:3145},{level:4,title:"多列索引",slug:"多列索引",normalizedTitle:"多列索引",charIndex:3633},{level:4,title:"聚簇索引",slug:"聚簇索引",normalizedTitle:"聚簇索引",charIndex:3830},{level:4,title:"覆盖索引",slug:"覆盖索引",normalizedTitle:"覆盖索引",charIndex:4025},{level:4,title:"使用索引扫描来做排序",slug:"使用索引扫描来做排序",normalizedTitle:"使用索引扫描来做排序",charIndex:4212},{level:4,title:"最左前缀匹配原则",slug:"最左前缀匹配原则",normalizedTitle:"最左前缀匹配原则",charIndex:4301},{level:4,title:"= 和 in 可以乱序",slug:"和-in-可以乱序",normalizedTitle:"= 和 in 可以乱序",charIndex:4654},{level:3,title:"约束",slug:"约束",normalizedTitle:"约束",charIndex:20},{level:2,title:"二、并发控制",slug:"二、并发控制",normalizedTitle:"二、并发控制",charIndex:5187},{level:3,title:"乐观锁和悲观锁",slug:"乐观锁和悲观锁",normalizedTitle:"乐观锁和悲观锁",charIndex:5198},{level:3,title:"行级锁和表级锁",slug:"行级锁和表级锁",normalizedTitle:"行级锁和表级锁",charIndex:5525},{level:3,title:"读写锁",slug:"读写锁",normalizedTitle:"读写锁",charIndex:5902},{level:3,title:"意向锁",slug:"意向锁",normalizedTitle:"意向锁",charIndex:6118},{level:3,title:"MVCC",slug:"mvcc",normalizedTitle:"mvcc",charIndex:6271},{level:3,title:"Next-key 锁",slug:"next-key-锁",normalizedTitle:"next-key 锁",charIndex:6714},{level:2,title:"三、事务",slug:"三、事务",normalizedTitle:"三、事务",charIndex:7447},{level:3,title:"ACID",slug:"acid",normalizedTitle:"acid",charIndex:7510},{level:3,title:"并发一致性问题",slug:"并发一致性问题",normalizedTitle:"并发一致性问题",charIndex:7779},{level:3,title:"事务隔离",slug:"事务隔离",normalizedTitle:"事务隔离",charIndex:8235},{level:3,title:"分布式事务",slug:"分布式事务",normalizedTitle:"分布式事务",charIndex:8599},{level:4,title:"两阶段提交",slug:"两阶段提交",normalizedTitle:"两阶段提交",charIndex:8734},{level:4,title:"柔性事务",slug:"柔性事务",normalizedTitle:"柔性事务",charIndex:8979},{level:4,title:"事务方案对比",slug:"事务方案对比",normalizedTitle:"事务方案对比",charIndex:9415},{level:2,title:"四、分库分表",slug:"四、分库分表",normalizedTitle:"四、分库分表",charIndex:9645},{level:3,title:"什么是分库分表",slug:"什么是分库分表",normalizedTitle:"什么是分库分表",charIndex:9656},{level:4,title:"垂直切分",slug:"垂直切分",normalizedTitle:"垂直切分",charIndex:9830},{level:4,title:"水平拆分",slug:"水平拆分",normalizedTitle:"水平拆分",charIndex:9686},{level:4,title:"分库分表的优点",slug:"分库分表的优点",normalizedTitle:"分库分表的优点",charIndex:10408},{level:4,title:"分库分表策略",slug:"分库分表策略",normalizedTitle:"分库分表策略",charIndex:10609},{level:3,title:"分库分表中间件",slug:"分库分表中间件",normalizedTitle:"分库分表中间件",charIndex:10881},{level:4,title:"常见的分库分表中间件",slug:"常见的分库分表中间件",normalizedTitle:"常见的分库分表中间件",charIndex:10989},{level:4,title:"分库分表中间件技术选型",slug:"分库分表中间件技术选型",normalizedTitle:"分库分表中间件技术选型",charIndex:11778},{level:3,title:"分库分表的问题",slug:"分库分表的问题",normalizedTitle:"分库分表的问题",charIndex:12285},{level:4,title:"分布式事务",slug:"分布式事务-2",normalizedTitle:"分布式事务",charIndex:8599},{level:4,title:"跨节点 Join",slug:"跨节点-join",normalizedTitle:"跨节点 join",charIndex:12585},{level:4,title:"跨节点的 count,order by,group by 以及聚合函数",slug:"跨节点的-count-order-by-group-by-以及聚合函数",normalizedTitle:"跨节点的 count,order by,group by 以及聚合函数",charIndex:12713},{level:4,title:"分布式 ID",slug:"分布式-id",normalizedTitle:"分布式 id",charIndex:10866},{level:4,title:"数据迁移，容量规划，扩容等问题",slug:"数据迁移-容量规划-扩容等问题",normalizedTitle:"数据迁移，容量规划，扩容等问题",charIndex:13360},{level:2,title:"五、集群",slug:"五、集群",normalizedTitle:"五、集群",charIndex:13548},{level:3,title:"复制机制",slug:"复制机制",normalizedTitle:"复制机制",charIndex:13653},{level:3,title:"读写分离",slug:"读写分离",normalizedTitle:"读写分离",charIndex:11185},{level:2,title:"六、数据库优化",slug:"六、数据库优化",normalizedTitle:"六、数据库优化",charIndex:14147},{level:3,title:"SQL 优化",slug:"sql-优化",normalizedTitle:"sql 优化",charIndex:14168},{level:4,title:"执行计划",slug:"执行计划",normalizedTitle:"执行计划",charIndex:14317},{level:4,title:"访问数据优化",slug:"访问数据优化",normalizedTitle:"访问数据优化",charIndex:14549},{level:4,title:"重构查询方式",slug:"重构查询方式",normalizedTitle:"重构查询方式",charIndex:14805},{level:5,title:"切分查询",slug:"切分查询",normalizedTitle:"切分查询",charIndex:14815},{level:5,title:"分解关联查询",slug:"分解关联查询",normalizedTitle:"分解关联查询",charIndex:15124},{level:4,title:"SQL 语句细节",slug:"sql-语句细节",normalizedTitle:"sql 语句细节",charIndex:15715},{level:5,title:"选择最有效率的表名顺序",slug:"选择最有效率的表名顺序",normalizedTitle:"选择最有效率的表名顺序",charIndex:15727},{level:5,title:"WHERE 子句中的连接顺序",slug:"where-子句中的连接顺序",normalizedTitle:"where 子句中的连接顺序",charIndex:16190},{level:5,title:"SELECT 子句中避免使用 * 号",slug:"select-子句中避免使用-号",normalizedTitle:"select 子句中避免使用 * 号",charIndex:16447},{level:5,title:"用 TRUNCATE 替代 DELETE",slug:"用-truncate-替代-delete",normalizedTitle:"用 truncate 替代 delete",charIndex:16602},{level:5,title:"使用内部函数提高 SQL 效率",slug:"使用内部函数提高-sql-效率",normalizedTitle:"使用内部函数提高 sql 效率",charIndex:16712},{level:5,title:"使用表或列的别名",slug:"使用表或列的别名",normalizedTitle:"使用表或列的别名",charIndex:16800},{level:5,title:"SQL 关键字大写",slug:"sql-关键字大写",normalizedTitle:"sql 关键字大写",charIndex:16867},{level:5,title:"用 >= 替代 >",slug:"用-替代",normalizedTitle:"用 &gt;= 替代 &gt;",charIndex:null},{level:5,title:"用 IN 替代 OR",slug:"用-in-替代-or",normalizedTitle:"用 in 替代 or",charIndex:17107},{level:5,title:"总是使用索引的第一个列",slug:"总是使用索引的第一个列",normalizedTitle:"总是使用索引的第一个列",charIndex:17254},{level:5,title:"SQL 关键字尽量大写",slug:"sql-关键字尽量大写",normalizedTitle:"sql 关键字尽量大写",charIndex:17486},{level:3,title:"结构优化",slug:"结构优化",normalizedTitle:"结构优化",charIndex:14175},{level:4,title:"数据类型优化原则",slug:"数据类型优化原则",normalizedTitle:"数据类型优化原则",charIndex:17643},{level:4,title:"范式和反范式",slug:"范式和反范式",normalizedTitle:"范式和反范式",charIndex:17588},{level:3,title:"配置优化",slug:"配置优化",normalizedTitle:"配置优化",charIndex:14180},{level:3,title:"硬件优化",slug:"硬件优化",normalizedTitle:"硬件优化",charIndex:14185},{level:2,title:"七、数据库理论",slug:"七、数据库理论",normalizedTitle:"七、数据库理论",charIndex:18211},{level:3,title:"函数依赖",slug:"函数依赖",normalizedTitle:"函数依赖",charIndex:18223},{level:3,title:"异常",slug:"异常",normalizedTitle:"异常",charIndex:18433},{level:3,title:"范式",slug:"范式",normalizedTitle:"范式",charIndex:17588},{level:4,title:"第一范式 (1NF)",slug:"第一范式-1nf",normalizedTitle:"第一范式 (1nf)",charIndex:19013},{level:4,title:"第二范式 (2NF)",slug:"第二范式-2nf",normalizedTitle:"第二范式 (2nf)",charIndex:19035},{level:4,title:"第三范式 (3NF)",slug:"第三范式-3nf",normalizedTitle:"第三范式 (3nf)",charIndex:19820},{level:2,title:"八、Mysql 存储引擎",slug:"八、mysql-存储引擎",normalizedTitle:"八、mysql 存储引擎",charIndex:20038},{level:3,title:"InnoDB vs. MyISAM",slug:"innodb-vs-myisam",normalizedTitle:"innodb vs. myisam",charIndex:20340},{level:2,title:"九、数据库比较",slug:"九、数据库比较",normalizedTitle:"九、数据库比较",charIndex:20709},{level:3,title:"常见数据库比较",slug:"常见数据库比较",normalizedTitle:"常见数据库比较",charIndex:20721},{level:3,title:"Oracle vs. Mysql",slug:"oracle-vs-mysql",normalizedTitle:"oracle vs. mysql",charIndex:21127},{level:4,title:"数据库对象差异",slug:"数据库对象差异",normalizedTitle:"数据库对象差异",charIndex:21203},{level:4,title:"SQL 差异",slug:"sql-差异",normalizedTitle:"sql 差异",charIndex:21370},{level:4,title:"事务差异",slug:"事务差异",normalizedTitle:"事务差异",charIndex:21764},{level:3,title:"数据类型比较",slug:"数据类型比较",normalizedTitle:"数据类型比较",charIndex:21999},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:22664}],headersStr:"一、索引和约束 什么是索引 索引的优缺点 何时使用索引 索引的类型 索引的数据结构 B 树 B+ 树 B 树 vs. B+ 树 Hash 索引策略 索引基本原则 独立的列 前缀索引和索引选择性 多列索引 聚簇索引 覆盖索引 使用索引扫描来做排序 最左前缀匹配原则 = 和 in 可以乱序 约束 二、并发控制 乐观锁和悲观锁 行级锁和表级锁 读写锁 意向锁 MVCC Next-key 锁 三、事务 ACID 并发一致性问题 事务隔离 分布式事务 两阶段提交 柔性事务 事务方案对比 四、分库分表 什么是分库分表 垂直切分 水平拆分 分库分表的优点 分库分表策略 分库分表中间件 常见的分库分表中间件 分库分表中间件技术选型 分库分表的问题 分布式事务 跨节点 Join 跨节点的 count,order by,group by 以及聚合函数 分布式 ID 数据迁移，容量规划，扩容等问题 五、集群 复制机制 读写分离 六、数据库优化 SQL 优化 执行计划 访问数据优化 重构查询方式 切分查询 分解关联查询 SQL 语句细节 选择最有效率的表名顺序 WHERE 子句中的连接顺序 SELECT 子句中避免使用 * 号 用 TRUNCATE 替代 DELETE 使用内部函数提高 SQL 效率 使用表或列的别名 SQL 关键字大写 用 >= 替代 > 用 IN 替代 OR 总是使用索引的第一个列 SQL 关键字尽量大写 结构优化 数据类型优化原则 范式和反范式 配置优化 硬件优化 七、数据库理论 函数依赖 异常 范式 第一范式 (1NF) 第二范式 (2NF) 第三范式 (3NF) 八、Mysql 存储引擎 InnoDB vs. MyISAM 九、数据库比较 常见数据库比较 Oracle vs. Mysql 数据库对象差异 SQL 差异 事务差异 数据类型比较 参考资料",content:"# 关系型数据库面试\n\n\n# 一、索引和约束\n\n\n# 什么是索引\n\n索引是对数据库表中一或多个列的值进行排序的结构，是帮助数据库高效查询数据的数据结构。\n\n\n# 索引的优缺点\n\n✔ 索引的优点：\n\n * 索引大大减少了服务器需要扫描的数据量，从而加快检索速度。\n * 支持行级锁的数据库，如 InnoDB 会在访问行的时候加锁。使用索引可以减少访问的行数，从而减少锁的竞争，提高并发。\n * 索引可以帮助服务器避免排序和临时表。\n * 索引可以将随机 I/O 变为顺序 I/O。\n * 唯一索引可以确保每一行数据的唯一性，通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能。\n\n❌ 索引的缺点：\n\n * 创建和维护索引要耗费时间，这会随着数据量的增加而增加。\n * 索引需要占用额外的物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。\n * 写操作（INSERT/UPDATE/DELETE）时很可能需要更新索引，导致数据库的写操作性能降低。\n\n\n# 何时使用索引\n\n索引能够轻易将查询性能提升几个数量级。\n\n✔ 什么情况适用索引：\n\n * 表经常进行 SELECT 操作；\n * 表的数据量比较大；\n * 列名经常出现在 WHERE 或连接（JOIN）条件中\n\n❌ 什么情况不适用索引：\n\n * 频繁写操作（ INSERT/UPDATE/DELETE ）- 需要更新索引空间；\n * 非常小的表 - 对于非常小的表，大部分情况下简单的全表扫描更高效。\n * 列名不经常出现在 WHERE 或连接（JOIN）条件中 - 索引就会经常不命中，没有意义，还增加空间开销。\n * 对于特大型表，建立和使用索引的代价将随之增长。可以考虑使用分区技术或 Nosql。\n\n\n# 索引的类型\n\n主流的关系型数据库一般都支持以下索引类型：\n\n从逻辑类型上划分（即一般创建表时设置的索引类型）：\n\n * 唯一索引（UNIQUE）：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。\n * 主键索引（PRIMARY）：一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引。\n * 普通索引（INDEX）：最基本的索引，没有任何限制。\n * 组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。\n\n从物理存储上划分：\n\n * 聚集索引(Clustered)：表中各行的物理顺序与键值的逻辑（索引）顺序相同，每个表只能有一个。\n * 非聚集索引(Non-clustered)：非聚集索引指定表的逻辑顺序，也可以视为二级索引。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。\n\n\n# 索引的数据结构\n\n主流数据库的索引一般使用的数据结构为：B 树、B+ 树。\n\n# B 树\n\n一棵 M 阶的 B-Tree 满足以下条件：\n\n * 每个结点至多有 M 个孩子；\n * 除根结点和叶结点外，其它每个结点至少有 M/2 个孩子；\n * 根结点至少有两个孩子（除非该树仅包含一个结点）；\n * 所有叶结点在同一层，叶结点不包含任何关键字信息；\n * 有 K 个关键字的非叶结点恰好包含 K+1 个孩子；\n\n对于任意结点，其内部的关键字 Key 是升序排列的。每个节点中都包含了 data。\n\n\n\n对于每个结点，主要包含一个关键字数组 Key[]，一个指针数组（指向儿子）Son[]。\n\n在 B-Tree 内，查找的流程是：\n\n 1. 使用顺序查找（数组长度较短时）或折半查找方法查找 Key[] 数组，若找到关键字 K，则返回该结点的地址及 K 在 Key[] 中的位置；\n 2. 否则，可确定 K 在某个 Key[i] 和 Key[i+1] 之间，则从 Son[i] 所指的子结点继续查找，直到在某结点中查找成功；\n 3. 或直至找到叶结点且叶结点中的查找仍不成功时，查找过程失败。\n\n# B+ 树\n\nB+Tree 是 B-Tree 的变种：\n\n * 每个节点的指针上限为 2d 而不是 2d+1（d 为节点的出度）。\n * 非叶子节点不存储 data，只存储 key；叶子节点不存储指针。\n\n\n\n由于并不是所有节点都具有相同的域，因此 B+Tree 中叶节点和内节点一般大小不同。这点与 B-Tree 不同，虽然 B-Tree 中不同节点存放的 key 和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中 B-Tree 往往对每个节点申请同等大小的空间。\n\n带有顺序访问指针的 B+Tree\n\n一般在数据库系统或文件系统中使用的 B+Tree 结构都在经典 B+Tree 的基础上进行了优化，增加了顺序访问指针。\n\n\n\n在 B+Tree 的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的 B+Tree。\n\n这个优化的目的是为了提高区间访问的性能，例如上图中如果要查询 key 为从 18 到 49 的所有数据记录，当找到 18 后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。\n\n# B 树 vs. B+ 树\n\n * B+ 树更适合外部存储(一般指磁盘存储)，由于内节点(非叶子节点)不存储 data，所以一个节点可以存储更多的内节点，每个节点能索引的范围更大更精确。也就是说使用 B+ 树单次磁盘 IO 的信息量相比较 B 树更大，IO 效率更高。\n * Mysql 是关系型数据库，经常会按照区间来访问某个索引列，B+ 树的叶子节点间按顺序建立了链指针，加强了区间访问性，所以 B+ 树对索引列上的区间范围查询很友好。而 B 树每个节点的 key 和 data 在一起，无法进行区间查找。\n\n# Hash\n\n> Hash 索引只有精确匹配索引所有列的查询才有效。\n\n对于每一行数据，对所有的索引列计算一个 hashcode。哈希索引将所有的 hashcode 存储在索引中，同时在 Hash 表中保存指向每个数据行的指针。\n\n哈希结构索引的优点：\n\n * 因为索引数据结构紧凑，所以查询速度非常快。\n\n哈希结构索引的缺点：\n\n * 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。\n * 哈希索引不支持部分索引匹配查找。如，在数据列 (A,B) 上建立哈希索引，如果查询只有数据列 A，无法使用该索引。\n * 哈希索引只支持等值比较查询，不支持任何范围查询，如 WHERE price > 100。\n * 哈希索引有可能出现哈希冲突，出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。\n\n\n# 索引策略\n\n# 索引基本原则\n\n * 索引不是越多越好，不要为所有列都创建索引。\n * 要尽量避免冗余和重复索引；\n * 要考虑删除未使用的索引；\n * 尽量的扩展索引，不要新建索引；\n * 频繁作为 WHERE 过滤条件的列应该考虑添加索引\n\n# 独立的列\n\n如果查询中的列不是独立的列，则数据库不会使用索引。\n\n“独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。\n\n❌ 错误示例：\n\nSELECT actor_id FROM actor WHERE actor_id + 1 = 5;\nSELECT ... WHERE TO_DAYS(current_date) - TO_DAYS(date_col) <= 10;\n\n\n# 前缀索引和索引选择性\n\n有时候需要索引很长的字符列，这会让索引变得大且慢。\n\n解决方法是：可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。\n\n索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。\n\n对于 BLOB/TEXT/VARCHAR 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。\n\n要选择足够长的前缀以保证较高的选择性，同时又不能太长（节约空间）。\n\n❌ 低效示例：\n\nSELECT COUNT(*) AS cnt, city FROM sakila.city_demo\nGROUP BY city ORDER BY cnt DESC LIMIT 10;\n\n\n✔ 高效示例：\n\nSELECT COUNT(*) AS cnt, LEFT(city, 3) AS pref FROM sakila.city_demo\nGROUP BY city ORDER BY cnt DESC LIMIT 10;\n\n\n# 多列索引\n\n不要为每个列都创建独立索引。\n\n将选择性高的列或基数大的列优先排在多列索引最前列。但有时，也需要考虑 WHERE 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。\n\n举例来说，有一张 user 表，其中含 name, sex, age 三个列，如果将这三者组合为多列索引，应该用什么样的顺序呢？从选择性高的角度来看：name > age > sex。\n\n# 聚簇索引\n\n聚簇索引不是一种单独的索引类型，而是一种数据存储方式。具体细节依赖于实现方式。如 InnoDB 的聚簇索引实际是在同一个结构中保存了 B 树的索引和数据行。\n\n聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n若没有定义主键，InnoDB 会隐式定义一个主键来作为聚簇索引。\n\n# 覆盖索引\n\n索引包含所有需要查询的字段的值。\n\n具有以下优点：\n\n * 因为索引条目通常远小于数据行的大小，所以若只读取索引，能大大减少数据访问量。\n * 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。\n * 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。\n\n# 使用索引扫描来做排序\n\nMysql 有两种方式可以生成排序结果：通过排序操作；或者按索引顺序扫描。\n\n索引最好既满足排序，又用于查找行。这样，就可以使用索引来对结果排序。\n\n# 最左前缀匹配原则\n\nMySQL 会一直向右匹配直到遇到范围查询 (>,<,BETWEEN,LIKE) 就停止匹配。\n\n * 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。\n * 如果是联合索引，那么 key 也由多个列组成，同时，索引只能用于查找 key 是否存在（相等），遇到范围查询(>、<、between、like 左匹配)等就不能进一步匹配了，后续退化为线性查找。\n * 因此，列的排列顺序决定了可命中索引的列数。\n\n例子：\n\n * 如有索引(a, b, c, d)，查询条件 a = 1 and b = 2 and c > 3 and d = 4，则会在每个节点依次命中 a、b、c，无法命中 d。(很简单：索引命中只能是相等的情况，不能是范围匹配)\n\n# = 和 in 可以乱序\n\n不需要考虑=、in 等的顺序，Mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。\n\n例子：如有索引(a, b, c, d)，查询条件 c > 3 and b = 2 and a = 1 and d < 4 与 a = 1 and c > 3 and b = 2 and d < 4 等顺序都是可以的，MySQL 会自动优化为 a = 1 and b = 2 and c > 3 and d < 4，依次命中 a、b、c。\n\n\n# 约束\n\n数据库约束（CONSTRAINT）有哪些：\n\n * NOT NULL - 用于控制字段的内容一定不能为空（NULL）。\n * UNIQUE - 字段内容不能重复，一个表允许有多个 Unique 约束。\n * PRIMARY KEY - 数据表中对储存数据对象予以唯一和完整标识的数据列或属性的组合，它在一个表中只允许有一个。主键的取值不能为空值（Null）。\n * FOREIGN KEY - 在一个表中存在的另一个表的主键称此表的外键。用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。\n * CHECK - 用于控制字段的值范围。\n\n\n# 二、并发控制\n\n\n# 乐观锁和悲观锁\n\n>  * 数据库的乐观锁和悲观锁是什么？\n>  * 数据库的乐观锁和悲观锁如何实现？\n\n确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性，乐观锁和悲观锁是并发控制主要采用的技术手段。\n\n * 悲观锁 - 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作\n   * 在查询完数据的时候就把事务锁起来，直到提交事务（COMMIT）\n   * 实现方式：使用数据库中的锁机制\n * 乐观锁 - 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。\n   * 在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定\n   * 实现方式：使用 version 版本或者时间戳\n\n\n# 行级锁和表级锁\n\n>  * 什么是行级锁和表级锁？\n>  * 什么时候用行级锁？什么时候用表级锁？\n\n从数据库的锁粒度来看，MySQL 中提供了两种封锁粒度：行级锁和表级锁。\n\n * 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。\n * 行级锁（row lock） - 仅对指定的行记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。\n\n二者需要权衡：\n\n * 锁定的数据量越少，锁竞争的发生频率就越小，系统的并发程度就越高。\n * 锁粒度越小，系统开销就越大。\n\n在 InnoDB 中，行锁是通过给索引上的索引项加锁来实现的。如果没有索引，InnoDB 将会通过隐藏的聚簇索引来对记录加锁。\n\n\n# 读写锁\n\n>  * 什么是读写锁？\n\n * 独享锁（Exclusive），简写为 X 锁，又称写锁。使用方式：SELECT ... FOR UPDATE;\n * 共享锁（Shared），简写为 S 锁，又称读锁。使用方式：SELECT ... LOCK IN SHARE MODE;\n\n写锁和读锁的关系，简言之：独享锁存在，其他事务就不能做任何操作。\n\nInnoDB 下的行锁、间隙锁、next-key 锁统统属于独享锁。\n\n\n# 意向锁\n\n>  * 什么是意向锁？\n>  * 意向锁有什么用？\n\n意向锁的作用是：当存在表级锁和行级锁的情况下，必须先申请意向锁（表级锁，但不是真的加锁），再获取行级锁。使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。\n\n意向锁是 InnoDB 自动加的，不需要用户干预。\n\n\n# MVCC\n\n> 什么是 MVCC？\n> \n> MVCC 有什么用？解决了什么问题？\n> \n> MVCC 的原理是什么？\n\n多版本并发控制（Multi-Version Concurrency Control, MVCC）是 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。\n\nMVCC 的思想是：\n\n * 保存数据在某个时间点的快照。写操作（DELETE、INSERT、UPDATE）更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 CopyOnWrite 类似。\n * 脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。\n\n\n# Next-key 锁\n\nNext-Key 锁是 MySQL 的 InnoDB 存储引擎的一种锁实现。\n\nMVCC 不能解决幻读问题，Next-Key 锁就是为了解决幻读问题。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key 锁 可以解决幻读问题。\n\n另外，根据针对 SQL 语句检索条件的不同，加锁又有以下三种情形需要我们掌握。\n\n * Record Lock - 行锁对索引项加锁，若没有索引则使用表锁。\n * Gap Lock - 对索引项之间的间隙加锁。锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;\n * Next-key lock -它是 Record Lock 和 Gap Lock 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间。\n\n索引分为主键索引和非主键索引两种，如果一条 SQL 语句操作了主键索引，MySQL 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL 会先锁定该非主键索引，再锁定相关的主键索引。在 UPDATE、DELETE 操作时，MySQL 不仅锁定 WHERE 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key lock。\n\n当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，InnoDB 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。\n\n\n# 三、事务\n\n> 事务简单来说：一个 Session 中所进行所有的操作，要么同时成功，要么同时失败。具体来说，事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。\n\n\n\n\n# ACID\n\nACID — 数据库事务正确执行的四个基本要素：\n\n * 原子性（Atomicity）\n * 一致性（Consistency）\n * 隔离性（Isolation）\n * 持久性（Durability）\n\n一个支持事务（Transaction）中的数据库系统，必需要具有这四种特性，否则在事务过程（Transaction processing）当中无法保证数据的正确性，交易过程极可能达不到交易。\n\n\n\n\n# 并发一致性问题\n\n在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。\n\n * 丢失修改\n\nT1 和 T2 两个事务都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。\n\n\n\n * 脏读\n\nT1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。\n\n\n\n * 不可重复读\n\nT2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n\n\n\n * 幻读\n\nT1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n\n\n\n并发一致性解决方案：\n\n产生并发不一致性问题主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。\n\n并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。\n\n\n# 事务隔离\n\n数据库隔离级别：\n\n * 未提交读（READ UNCOMMITTED） - 事务中的修改，即使没有提交，对其它事务也是可见的。\n * 提交读（READ COMMITTED） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。\n * 重复读（REPEATABLE READ） - 保证在同一个事务中多次读取同样数据的结果是一样的。\n * 串行化（SERIALIXABLE） - 强制事务串行执行。\n\n数据库隔离级别解决的问题：\n\n隔离级别   脏读   不可重复读   幻读\n未提交读   ❌    ❌       ❌\n提交读    ✔️   ❌       ❌\n可重复读   ✔️   ✔️      ❌\n可串行化   ✔️   ✔️      ✔️\n\n\n# 分布式事务\n\n在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为 本地事务。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。\n\n分布式事务 是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。\n\n# 两阶段提交\n\n两阶段提交（XA）对业务侵入很小。 它最大的优势就是对使用方透明，用户可以像使用本地事务一样使用基于 XA 协议的分布式事务。 XA 协议能够严格保障事务 ACID 特性。\n\n严格保障事务 ACID 特性是一把双刃剑。 事务执行在过程中需要将所需资源全部锁定，它更加适用于执行时间确定的短事务。 对于长事务来说，整个事务进行期间对数据的独占，将导致对热点数据依赖的业务系统并发性能衰退明显。 因此，在高并发的性能至上场景中，基于 XA 协议的分布式事务并不是最佳选择。\n\n# 柔性事务\n\n如果将实现了ACID 的事务要素的事务称为刚性事务的话，那么基于BASE事务要素的事务则称为柔性事务。 BASE是基本可用、柔性状态和最终一致性这三个要素的缩写。\n\n * 基本可用（Basically Available）保证分布式事务参与方不一定同时在线。\n * 柔性状态（Soft state）则允许系统状态更新有一定的延时，这个延时对客户来说不一定能够察觉。\n * 而最终一致性（Eventually consistent）通常是通过消息传递的方式保证系统的最终一致性。\n\n在ACID事务中对隔离性的要求很高，在事务执行过程中，必须将所有的资源锁定。 柔性事务的理念则是通过业务逻辑将互斥锁操作从资源层面上移至业务层面。通过放宽对强一致性要求，来换取系统吞吐量的提升。\n\n基于ACID的强一致性事务和基于BASE的最终一致性事务都不是银弹，只有在最适合的场景中才能发挥它们的最大长处。 可通过下表详细对比它们之间的区别，以帮助开发者进行技术选型。\n\n# 事务方案对比\n\n       本地事务       两（三）阶段事务    柔性事务\n业务改造   无          无           实现相关接口\n一致性    不支持        支持          最终一致\n隔离性    不支持        支持          业务方保证\n并发性能   无影响        严重衰退        略微衰退\n适合场景   业务方处理不一致   短事务 & 低并发   长事务 & 高并发\n\n\n# 四、分库分表\n\n\n# 什么是分库分表\n\n> 什么是分库分表？什么是垂直拆分？什么是水平拆分？什么是 Sharding？\n> \n> 分库分表是为了解决什么问题？\n> \n> 分库分表有什么优点？\n> \n> 分库分表有什么策略？\n\n分库分表的基本思想就是：把原本完整的数据切分成多个部分，放到不同的数据库或表上。\n\n分库分表一定是为了支撑 高并发、数据量大两个问题的。\n\n# 垂直切分\n\n> 垂直切分，是 把一个有很多字段的表给拆分成多个表，或者是多个库上去。一般来说，会 将较少的、访问频率较高的字段放到一个表里去，然后 将较多的、访问频率较低的字段放到另外一个表里去。因为数据库是有缓存的，访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。\n\n\n\n一般来说，满足下面的条件就可以考虑扩容了：\n\n * Mysql 单库超过 5000 万条记录，Oracle 单库超过 1 亿条记录，DB 压力就很大。\n * 单库超过每秒 2000 个并发时，而一个健康的单库最好保持在每秒 1000 个并发左右，不要太大。\n\n在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。\n\n# 水平拆分\n\n> 水平拆分 又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。当 单表数据量太大 时，会极大影响 SQL 执行的性能 。分表是将原来一张表的数据分布到数据库集群的不同节点上，从而缓解单点的压力。\n\n\n\n一般来说，单表有 200 万条数据 的时候，性能就会相对差一些了，需要考虑分表了。但是，这也要视具体情况而定，可能是 100 万条，也可能是 500 万条，SQL 越复杂，就最好让单表行数越少。\n\n# 分库分表的优点\n\n#          分库分表前              分库分表后\n并发支撑情况     单机部署，扛不住高并发        从单机到多机，能承受的并发增加了多倍\n磁盘使用情况     单机磁盘容量几乎撑满         拆分为多个库，数据库服务器磁盘使用率大大降低\nSQL 执行性能   单表数据量太大，SQL 越跑越慢   单表数据量减少，SQL 执行效率明显提升\n\n# 分库分表策略\n\n * 哈希取模：hash(key) % N 或 id % N\n   * 优点：可以平均分配每个库的数据量和请求压力（负载均衡）。\n   * 缺点：扩容麻烦，需要数据迁移。\n * 范围：可以按照 ID 或时间划分范围。\n   * 优点：扩容简单。\n   * 缺点：这种策略容易产生热点问题。\n * 映射表：使用单独的一个数据库来存储映射关系。\n   * 缺点：存储映射关系的数据库也可能成为性能瓶颈，且一旦宕机，分库分表的数据库就无法工作。所以不建议使用这种策略。\n   * 优点：扩容简单，可以解决分布式 ID 问题。\n\n\n# 分库分表中间件\n\n> ❓ 常见问题：\n> \n>  * 你用过哪些分库分表中间件，简单介绍一下？\n> \n>  * 不同的分库分表中间件各自有什么特性，有什么优缺点？\n> \n>  * 分库分表中间件技术如何选型？\n\n# 常见的分库分表中间件\n\n * Cobar - 阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 JDBC 驱动访问 cobar 集群，cobar 根据 SQL 和分库规则对 SQL 做分解，然后分发到 MySQL 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。\n * TDDL - 淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。\n * Atlas - 360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。\n * sharding-jdbc - 当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 SQL 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、TCC 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。\n * Mycat - 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。\n\n# 分库分表中间件技术选型\n\n建议使用的是 sharding-jdbc 和 mycat。\n\n * sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 sharding-jdbc 的依赖。其本质上通过配置多数据源，然后根据设定的分库分表策略，计算路由，将请求发送到计算得到的节点上。\n\n * Mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。\n\n通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。\n\n\n# 分库分表的问题\n\n>  * 分库分表的常见问题有哪些？\n> \n>  * 你是如何解决分库分表的问题的？\n> \n> 下文一一讲解常见分库分表的问题及解决方案。\n\n# 分布式事务\n\n方案一：使用数据库事务\n\n * 优点：交由数据库管理，简单有效\n * 缺点：性能代价高，特别是 shard 越来越多时\n\n方案二：由应用程序和数据库共同控制\n\n * 原理：将一个跨多个数据库的分布式事务分拆成多个仅处于单个数据库上面的小事务，并通过应用程序来总控各个小事务。\n * 优点：性能上有优势\n * 缺点：需要应用程序在事务控制上做灵活设计。如果使用了 spring 的事务管理，改动起来会面临一定的困难。\n\n# 跨节点 Join\n\n只要是进行切分，跨节点 Join 的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的 id，根据这些 id 发起第二次请求得到关联数据。\n\n# 跨节点的 count,order by,group by 以及聚合函数\n\n这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。\n\n解决方案：与解决跨节点 join 问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和 join 不同的是每个节点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。\n\n业务角度上的解决方案：\n\n * 如果是在前台应用提供分页，则限定用户只能看前面 n 页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。\n * 如果是后台批处理任务要求分批获取数据，则可以加大 page size，比如每次获取 5000 条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。\n * 分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。\n\n# 分布式 ID\n\n一旦数据库被切分到多个物理节点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的 ID 无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得 ID，以便进行 SQL 路由。\n\n一些常见的主键生成策略：\n\n * 使用全局唯一 ID：GUID。\n * 为每个分片指定一个 ID 范围。\n * 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)。\n\n# 数据迁移，容量规划，扩容等问题\n\n来自淘宝综合业务平台团队，它利用对 2 的倍数取余具有向前兼容的特性（如对 4 取余得 1 的数对 2 取余也是 1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了 Sharding 扩容的难度。\n\n\n# 五、集群\n\n> 这个专题需要根据熟悉哪个数据库而定，但是主流、成熟的数据库都会实现一些基本功能，只是实现方式、策略上有所差异。由于本人较为熟悉 Mysql，所以下面主要介绍 Mysql 系统架构问题。\n\n\n# 复制机制\n\nMysql 支持两种复制：基于行的复制和基于语句的复制。\n\n这两种方式都是在主库上记录二进制日志（binlog），然后在从库上以异步方式更新主库上的日志记录。这意味着：复制过程存在时延，这段时间内，主从数据可能不一致（即最终一致性）。\n\n主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。\n\n * binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。\n * I/O 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的日志中。\n * SQL 线程 ：负责读取日志并执行 SQL 语句以更新数据。\n\n\n\n\n# 读写分离\n\n主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。\n\n读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。\n\nMySQL 读写分离能提高性能的原因在于：\n\n * 主从服务器负责各自的读和写，极大程度缓解了锁的争用；\n * 从服务器可以配置 MyISAM 引擎，提升查询性能以及节约系统开销；\n * 增加冗余，提高可用性。\n\n\n\n\n# 六、数据库优化\n\n数据库优化的路线一般为：SQL 优化、结构优化、配置优化、硬件优化。前两个方向一般是普通开发的考量点，而后两个方向一般是 DBA 的考量点。\n\n\n# SQL 优化\n\n> SQL 优化是数据库优化的最常见、最初级手段。\n> \n> 在执行 SQL 语句，语句中字段的顺序、查询策略等都可能会影响到 SQL 的执行性能。\n\n# 执行计划\n\n如何检验修改后的 SQL 确实有优化效果？这就需要用到执行计划（EXPLAIN）。\n\n使用执行计划 EXPLAIN 用来分析 SELECT 查询效率，开发人员可以通过分析 EXPLAIN 结果来优化查询语句。\n\n比较重要的字段有：\n\n * select_type - 查询类型，有简单查询、联合查询、子查询等\n * key - 使用的索引\n * rows - 扫描的行数\n\n> 更多内容请参考：MySQL 性能优化神器 Explain 使用分析\n\n# 访问数据优化\n\n减少请求的数据量：\n\n * 只返回必要的列 - 不要查询不需要的列，尽量避免使用 SELECT * 语句。\n * 只返回必要的行 - 使用 WHERE 语句进行查询过滤，有时候也需要使用 LIMIT 语句来限制返回的数据。\n * 缓存重复查询的数据 - 使用缓存可以避免在数据库中进行查询，特别要查询的数据经常被重复查询，缓存可以带来的查询性能提升将会是非常明显的。\n\n减少服务器端扫描的行数：\n\n * 最有效的方式是使用索引来覆盖查询（即 WHERE 后的过滤查询字段最好是索引字段）。\n\n# 重构查询方式\n\n# 切分查询\n\n一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。\n\nDELEFT FROM messages WHERE create < DATE_SUB(NOW(), INTERVAL 3 MONTH);\n\n\nrows_affected = 0\ndo {\n    rows_affected = do_query(\n    \"DELETE FROM messages WHERE create  < DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000\")\n} while rows_affected > 0\n\n\n# 分解关联查询\n\n将一个大连接查询（JOIN）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有：\n\n * 缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。\n * 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。\n * 减少锁竞争；\n * 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。\n * 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。\n\nSELECT * FROM tag\nJOIN tag_post ON tag_post.tag_id=tag.id\nJOIN post ON tag_post.post_id=post.id\nWHERE tag.tag='mysql';\nSELECT * FROM tag WHERE tag='mysql';\nSELECT * FROM tag_post WHERE tag_id=1234;\nSELECT * FROM post WHERE post.id IN (123,456,567,9098,8904);\n\n\n# SQL 语句细节\n\n# 选择最有效率的表名顺序\n\n数据库按照从右到左的顺序处理 FROM 子句中的表名，FROM 子句中写在最后的表将被最先处理。\n\n在 FROM 子句中包含多个表的情况下：\n\n * 如果多个表是完全无关系的话，将记录和列名最少的表，写在最后，然后依次类推。也就是说：选择记录条数最少的表放在最后。\n\n如果有 3 个以上的表连接查询：\n\n * 如果多个表是有关系的话，将引用最多的表，放在最后，然后依次类推。也就是说：被其他表所引用的表放在最后。\n\n例如：查询员工的编号，姓名，工资，工资等级，部门名\n\nemp 表被引用得最多，记录数也是最多，因此放在 form 字句的最后面\n\nselect emp.empno,emp.ename,emp.sal,salgrade.grade,dept.dname\nfrom salgrade,dept,emp\nwhere (emp.deptno = dept.deptno) and (emp.sal between salgrade.losal and salgrade.hisal)\n\n\n# WHERE 子句中的连接顺序\n\n数据库按照从右到左的顺序解析 WHERE 子句。\n\n因此，表之间的连接必须写在其他 WHERE 条件的左边，那些可以过滤掉最大数量记录的条件必须写在 WHERE 子句的之右。\n\nemp.sal 可以过滤多条记录，写在 WHERE 字句的最右边\n\nselect emp.empno,emp.ename,emp.sal,dept.dname\nfrom dept,emp\nwhere (emp.deptno = dept.deptno) and (emp.sal > 1500)\n\n\n# SELECT 子句中避免使用 * 号\n\n我们当时学习的时候，“*” 号是可以获取表中全部的字段数据的。\n\n * 但是它要通过查询数据字典完成的，这意味着将耗费更多的时间\n * 使用*号写出来的 SQL 语句也不够直观。\n\n----------------------------------------\n\n# 用 TRUNCATE 替代 DELETE\n\n如果需要清空所有表记录，使用 TRUNCATE 比 DELETE 执行效率高：\n\nDELETE 是一条一条记录的删除，而 Truncate 是将整个表删除，仅保留表结构\n\n# 使用内部函数提高 SQL 效率\n\n例如使用 mysql 的 concat() 函数会比使用 || 拼接速度快，因为 concat() 函数已经被 mysql 优化过了。\n\n# 使用表或列的别名\n\n如果表或列的名称太长了，使用一些简短的别名也能稍微提高一些 SQL 的性能。毕竟要扫描的字符长度就变少了。\n\n# SQL 关键字大写\n\n我们在编写 SQL 的时候，官方推荐的是使用大写来写关键字，因为 Oracle 服务器总是先将小写字母转成大写后，才执行\n\n# 用 >= 替代 >\n\n❌ 低效方式：\n\n-- 首先定位到DEPTNO=3的记录并且扫描到第一个DEPT大于3的记录\nSELECT * FROM EMP WHERE DEPTNO > 3\n\n\n✔ 高效方式：\n\n-- 直接跳到第一个DEPT等于4的记录\nSELECT * FROM EMP WHERE DEPTNO >= 4\n\n\n# 用 IN 替代 OR\n\n❌ 低效方式：\n\nselect * from emp where sal = 1500 or sal = 3000 or sal = 800;\n\n\n✔ 高效方式：\n\nselect * from emp where sal in (1500,3000,800);\n\n\n# 总是使用索引的第一个列\n\n如果索引是建立在多个列上，只有在它的第一个列被 WHERE 子句引用时，优化器才会选择使用该索引。 当只引用索引的第二个列时，不引用索引的第一个列时，优化器使用了全表扫描而忽略了索引\n\ncreate index emp_sal_job_idex\non emp(sal,job);\n----------------------------------\nselect *\nfrom emp\nwhere job != 'SALES';\n\n\n# SQL 关键字尽量大写\n\nSQL 关键字尽量大写，如：Oracle 默认会将 SQL 语句中的关键字转为大写后在执行。\n\n\n# 结构优化\n\n数据库结构优化可以从以下方向着手：\n\n * 数据类型优化\n * 范式和反范式优化\n * 索引优化 - 细节请看索引和约束章节\n * 分库分表 - 细节请看分库分表章节\n\n# 数据类型优化原则\n\n * 更小的通常更好\n * 简单就好，如整型比字符型操作代价低\n * 尽量避免 NULL\n\n# 范式和反范式\n\n范式和反范式各有利弊，需要根据实际情况权衡。\n\n范式化的目标是尽力减少冗余列，节省空间。\n\n * 范式化的优点是：\n   \n   * 减少冗余列，要写的数据就少，写操作的性能提高；\n   * 检索列数据时，DISTINCT 或 GROUP BY 操作减少。\n\n * 范式化的缺点是：增加关联查询。\n\n反范式化的目标是适当增加冗余列，以避免关联查询。\n\n反范式化的缺点是：\n\n * 冗余列增多，空间变大，写操作性能下降；\n * 检索列数据时，DISTINCT 或 GROUP BY 操作变多；\n\n\n# 配置优化\n\n> 配置优化主要是针对 Mysql 服务器，例如：max_connections、max_heap_table_size、open_files_limit、max_allowed_packet 等等。\n> \n> 在不同环境，不同场景下，应该酌情使用合理的配置。这种优化比较考验 Mysql 运维经验，一般是 DBA 的考量，普通开发接触的较少。\n> \n> Mysql 配置说明请参考：Mysql 服务器配置说明\n\n\n# 硬件优化\n\n数据库扩容、使用高配设备等等。核心就是一个字：钱。\n\n\n# 七、数据库理论\n\n\n# 函数依赖\n\n记 A->B 表示 A 函数决定 B，也可以说 B 函数依赖于 A。\n\n如果 {A1，A2，... ，An} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。\n\n对于 A->B，如果能找到 A 的真子集 A'，使得 A'-> B，那么 A->B 就是部分函数依赖，否则就是完全函数依赖；\n\n对于 A->B，B->C，则 A->C 是一个传递依赖。\n\n\n# 异常\n\n以下的学生课程关系的函数依赖为 Sno, Cname -> Sname, Sdept, Mname, Grade，键码为 {Sno, Cname}。也就是说，确定学生和课程之后，就能确定其它信息。\n\nSNO   SNAME   SDEPT   MNAME   CNAME   GRADE\n1     学生-1    学院-1    院长-1    课程-1    90\n2     学生-2    学院-2    院长-2    课程-2    80\n2     学生-2    学院-2    院长-2    课程-1    100\n3     学生-3    学院-2    院长-2    课程-2    95\n\n不符合范式的关系，会产生很多异常，主要有以下四种异常：\n\n * 冗余数据：例如 学生-2 出现了两次。\n * 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。\n * 删除异常：删除一个信息，那么也会丢失其它信息。例如如果删除了 课程-1，需要删除第一行和第三行，那么 学生-1 的信息就会丢失。\n * 插入异常，例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。\n\n\n# 范式\n\n范式理论是为了解决以上提到四种异常。\n\n高级别范式的依赖于低级别的范式，1NF 是最低级别的范式。\n\n\n\n# 第一范式 (1NF)\n\n属性不可分。\n\n# 第二范式 (2NF)\n\n * 每个非主属性完全函数依赖于键码。\n\n * 可以通过分解来满足。\n\n分解前\n\nSNO   SNAME   SDEPT   MNAME   CNAME   GRADE\n1     学生-1    学院-1    院长-1    课程-1    90\n2     学生-2    学院-2    院长-2    课程-2    80\n2     学生-2    学院-2    院长-2    课程-1    100\n3     学生-3    学院-2    院长-2    课程-2    95\n\n以上学生课程关系中，{Sno, Cname} 为键码，有如下函数依赖：\n\n * Sno -> Sname, Sdept\n * Sdept -> Mname\n * Sno, Cname-> Grade\n\nGrade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。\n\nSname, Sdept 和 Mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。\n\n分解后\n\n关系-1\n\nSNO   SNAME   SDEPT   MNAME\n1     学生-1    学院-1    院长-1\n2     学生-2    学院-2    院长-2\n3     学生-3    学院-2    院长-2\n\n有以下函数依赖：\n\n * Sno -> Sname, Sdept, Mname\n * Sdept -> Mname\n\n关系-2\n\nSNO   CNAME   GRADE\n1     课程-1    90\n2     课程-2    80\n2     课程-1    100\n3     课程-2    95\n\n有以下函数依赖：\n\n * Sno, Cname -> Grade\n\n# 第三范式 (3NF)\n\n * 非主属性不传递依赖于键码。\n\n上面的 关系-1 中存在以下传递依赖：Sno -> Sdept -> Mname，可以进行以下分解：\n\n关系-11\n\nSNO   SNAME   SDEPT\n1     学生-1    学院-1\n2     学生-2    学院-2\n3     学生-3    学院-2\n\n关系-12\n\nSDEPT   MNAME\n学院-1    院长-1\n学院-2    院长-2\n\n\n# 八、Mysql 存储引擎\n\nMysql 有多种存储引擎，不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在 Mysql 服务层统一处理的。\n\n简单列举几个存储引擎：\n\n * InnoDB - Mysql 的默认事务型存储引擎，并提供了行级锁和外键的约束。性能不错且支持自动故障恢复。\n * MyISAM - Mysql 5.1 版本前的默认存储引擎。特性丰富但不支持事务，也不支持行级锁和外键，也没有故障恢复功能。\n * CSV - 可以将 CSV 文件作为 Mysql 的表来处理，但这种表不支持索引。\n * MEMORY 。所有的数据都在内存中，数据的处理速度快，但是安全性不高。\n\n\n# InnoDB vs. MyISAM\n\nInnoDB 和 MyISAM 是目前使用的最多的两种 Mysql 存储引擎。\n\n * 数据结构比较：\n   * InnoDB 和 MyISAM 的索引数据结构都是 B+ 树。\n   * MyIASM 的 B+ 树中存储的内容实际上是实际数据的地址值。也就是说它的索引和实际数据是分开的，只不过使用索引指向了实际数据。这种索引的模式被称为非聚集索引。\n   * InnoDB 的 B+ 树中存储的内容是实际的数据，这种索引有被称为聚集索引。\n * 事务支持比较：\n   * InnoDB 支持事务，并提供了行级锁和外键的约束。\n   * MyIASM 不支持事务，也不支持行级锁和外键。\n * 故障恢复比较：\n   * InnoDB 支持故障恢复。\n   * MyISAM 不支持故障恢复。\n\n\n# 九、数据库比较\n\n\n# 常见数据库比较\n\n * Oracle - 久负盛名的商业数据库。功能强大、稳定。最大的缺点就是费钱。\n * Mysql - 曾经是互联网公司的最爱，但自动 Mysql 被 Oracle 公司收购后，好日子可能一去不复返。很多公司或开源项目已经逐渐寻找其他的开源产品来替代 Mysql。\n * MariaDB - 开源关系型数据库。 MySQL 的真正开源的发行版本，由 Mysql 部分核心人员创建。可作为 Mysql 的替代产品。\n * PostgreSQL - 开源关系型数据库。和 MySQL 的工作方式非常相似，社区支持做得很好。可作为 Mysql 的替代产品。\n * SQLite - 开源的轻量级数据库，移动端常常使用。\n * H2 - 内存数据库，一般用作开发、测试环境数据库。\n * SQL Server - 微软 Windows 生态系统的数据库。我想，Java 程序员应该没人用吧。\n\n\n# Oracle vs. Mysql\n\n目前为止，Java 领域用的最多的关系型数据库，应该还是 Oracle 和 Mysql，所以这里做一下比较。\n\n# 数据库对象差异\n\n在 Mysql 中，一个用户可以创建多个库。\n\n而在 Oracle 中，Oracle 服务器是由两部分组成\n\n * 数据库实例【理解为对象，看不见的】\n * 数据库【理解为类，看得见的】\n\n一个数据库实例可拥有多个用户，一个用户默认拥有一个表空间。\n\n表空间是存储我们数据库表的地方，表空间内可以有多个文件。\n\n# SQL 差异\n\n（1）主键递增\n\nMysql 可以设置 AUTO_INCREMENT 约束来指定主键为自增序列。\n\nOracle 需要通过 CREATE SEQUENCE 创建序列。\n\n（2）分页查询\n\nMysql 分页基于 SELECT ... FROM ... LIMIT ... 完成，较简单。\n\nselect * from help_category order by parent_category_id limit 10,5;\n\n\nOracle 分页基于 SELECT ... FROM (SELECT ROWNUM ...) WHERE ... 完成，较复杂。\n\nselect * from\n(select rownum rr,a.* from (select * from emp order by sal) a )\nwhere rr>5 and rr<=10;\n\n\n# 事务差异\n\n * auto commit\n   * Mysql 事务是 autocommit 模式，即自动提交事务；\n   * Oracle 事务需要手动 COMMIT。\n * 事务隔离级别\n   * Mysql 默认的事务隔离级别是可重复读（REPEATABLE READ）\n   * Oracle 支持读已提交（READ COMMITTED）和串行化（SERIALIZABLE） 两种事务隔离级别，默认事务隔离级别是读已提交（READ COMMITTED）\n\n\n# 数据类型比较\n\n> 不同数据库中，对数据类型的支持是不一样的。\n> \n> 即使存在同一种数据类型，也可能存在名称不同、或大小不同等问题。\n> \n> 因此，对于数据类型的支持详情必须参考各数据库的官方文档。\n\n下面列举一些常见数据类型对比：\n\n数据类型                ORACLE             MYSQL         POSTGRESQL\nboolean             Byte               N/A           Boolean\ninteger             Number             Int Integer   Int Integer\nfloat               Number             Float         Numeric\ncurrency            N/A                N/A           Money\nstring (fixed)      Char               Char          Char\nstring (variable)   Varchar Varchar2   Varchar       Varchar\nbinary object       Long Raw           Blob Text     Binary Varbinary\n\n> 数据类型对比表摘自 SQL 通用数据类型、SQL 用于各种数据库的数据类型\n\n\n# 参考资料\n\n * 数据库面试题(开发者必看)\n * 数据库系统原理\n * 数据库两大神器【索引和锁】\n * 分库分表需要考虑的问题及方案\n * 数据库分库分表(sharding)系列(二) 全局主键生成策略\n * 一种支持自由规划无须数据迁移和修改路由代码的 Sharding 扩容方案\n * ShardingSphere 分布式事务\n * mysql 和 oracle 的区别\n * RUNOOB SQL 教程\n * 如果有人问你数据库的原理，叫他看这篇文章",normalizedContent:"# 关系型数据库面试\n\n\n# 一、索引和约束\n\n\n# 什么是索引\n\n索引是对数据库表中一或多个列的值进行排序的结构，是帮助数据库高效查询数据的数据结构。\n\n\n# 索引的优缺点\n\n✔ 索引的优点：\n\n * 索引大大减少了服务器需要扫描的数据量，从而加快检索速度。\n * 支持行级锁的数据库，如 innodb 会在访问行的时候加锁。使用索引可以减少访问的行数，从而减少锁的竞争，提高并发。\n * 索引可以帮助服务器避免排序和临时表。\n * 索引可以将随机 i/o 变为顺序 i/o。\n * 唯一索引可以确保每一行数据的唯一性，通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能。\n\n❌ 索引的缺点：\n\n * 创建和维护索引要耗费时间，这会随着数据量的增加而增加。\n * 索引需要占用额外的物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。\n * 写操作（insert/update/delete）时很可能需要更新索引，导致数据库的写操作性能降低。\n\n\n# 何时使用索引\n\n索引能够轻易将查询性能提升几个数量级。\n\n✔ 什么情况适用索引：\n\n * 表经常进行 select 操作；\n * 表的数据量比较大；\n * 列名经常出现在 where 或连接（join）条件中\n\n❌ 什么情况不适用索引：\n\n * 频繁写操作（ insert/update/delete ）- 需要更新索引空间；\n * 非常小的表 - 对于非常小的表，大部分情况下简单的全表扫描更高效。\n * 列名不经常出现在 where 或连接（join）条件中 - 索引就会经常不命中，没有意义，还增加空间开销。\n * 对于特大型表，建立和使用索引的代价将随之增长。可以考虑使用分区技术或 nosql。\n\n\n# 索引的类型\n\n主流的关系型数据库一般都支持以下索引类型：\n\n从逻辑类型上划分（即一般创建表时设置的索引类型）：\n\n * 唯一索引（unique）：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。\n * 主键索引（primary）：一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引。\n * 普通索引（index）：最基本的索引，没有任何限制。\n * 组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。\n\n从物理存储上划分：\n\n * 聚集索引(clustered)：表中各行的物理顺序与键值的逻辑（索引）顺序相同，每个表只能有一个。\n * 非聚集索引(non-clustered)：非聚集索引指定表的逻辑顺序，也可以视为二级索引。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。\n\n\n# 索引的数据结构\n\n主流数据库的索引一般使用的数据结构为：b 树、b+ 树。\n\n# b 树\n\n一棵 m 阶的 b-tree 满足以下条件：\n\n * 每个结点至多有 m 个孩子；\n * 除根结点和叶结点外，其它每个结点至少有 m/2 个孩子；\n * 根结点至少有两个孩子（除非该树仅包含一个结点）；\n * 所有叶结点在同一层，叶结点不包含任何关键字信息；\n * 有 k 个关键字的非叶结点恰好包含 k+1 个孩子；\n\n对于任意结点，其内部的关键字 key 是升序排列的。每个节点中都包含了 data。\n\n\n\n对于每个结点，主要包含一个关键字数组 key[]，一个指针数组（指向儿子）son[]。\n\n在 b-tree 内，查找的流程是：\n\n 1. 使用顺序查找（数组长度较短时）或折半查找方法查找 key[] 数组，若找到关键字 k，则返回该结点的地址及 k 在 key[] 中的位置；\n 2. 否则，可确定 k 在某个 key[i] 和 key[i+1] 之间，则从 son[i] 所指的子结点继续查找，直到在某结点中查找成功；\n 3. 或直至找到叶结点且叶结点中的查找仍不成功时，查找过程失败。\n\n# b+ 树\n\nb+tree 是 b-tree 的变种：\n\n * 每个节点的指针上限为 2d 而不是 2d+1（d 为节点的出度）。\n * 非叶子节点不存储 data，只存储 key；叶子节点不存储指针。\n\n\n\n由于并不是所有节点都具有相同的域，因此 b+tree 中叶节点和内节点一般大小不同。这点与 b-tree 不同，虽然 b-tree 中不同节点存放的 key 和指针可能数量不一致，但是每个节点的域和上限是一致的，所以在实现中 b-tree 往往对每个节点申请同等大小的空间。\n\n带有顺序访问指针的 b+tree\n\n一般在数据库系统或文件系统中使用的 b+tree 结构都在经典 b+tree 的基础上进行了优化，增加了顺序访问指针。\n\n\n\n在 b+tree 的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的 b+tree。\n\n这个优化的目的是为了提高区间访问的性能，例如上图中如果要查询 key 为从 18 到 49 的所有数据记录，当找到 18 后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。\n\n# b 树 vs. b+ 树\n\n * b+ 树更适合外部存储(一般指磁盘存储)，由于内节点(非叶子节点)不存储 data，所以一个节点可以存储更多的内节点，每个节点能索引的范围更大更精确。也就是说使用 b+ 树单次磁盘 io 的信息量相比较 b 树更大，io 效率更高。\n * mysql 是关系型数据库，经常会按照区间来访问某个索引列，b+ 树的叶子节点间按顺序建立了链指针，加强了区间访问性，所以 b+ 树对索引列上的区间范围查询很友好。而 b 树每个节点的 key 和 data 在一起，无法进行区间查找。\n\n# hash\n\n> hash 索引只有精确匹配索引所有列的查询才有效。\n\n对于每一行数据，对所有的索引列计算一个 hashcode。哈希索引将所有的 hashcode 存储在索引中，同时在 hash 表中保存指向每个数据行的指针。\n\n哈希结构索引的优点：\n\n * 因为索引数据结构紧凑，所以查询速度非常快。\n\n哈希结构索引的缺点：\n\n * 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。\n * 哈希索引不支持部分索引匹配查找。如，在数据列 (a,b) 上建立哈希索引，如果查询只有数据列 a，无法使用该索引。\n * 哈希索引只支持等值比较查询，不支持任何范围查询，如 where price > 100。\n * 哈希索引有可能出现哈希冲突，出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。\n\n\n# 索引策略\n\n# 索引基本原则\n\n * 索引不是越多越好，不要为所有列都创建索引。\n * 要尽量避免冗余和重复索引；\n * 要考虑删除未使用的索引；\n * 尽量的扩展索引，不要新建索引；\n * 频繁作为 where 过滤条件的列应该考虑添加索引\n\n# 独立的列\n\n如果查询中的列不是独立的列，则数据库不会使用索引。\n\n“独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。\n\n❌ 错误示例：\n\nselect actor_id from actor where actor_id + 1 = 5;\nselect ... where to_days(current_date) - to_days(date_col) <= 10;\n\n\n# 前缀索引和索引选择性\n\n有时候需要索引很长的字符列，这会让索引变得大且慢。\n\n解决方法是：可以索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。\n\n索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。\n\n对于 blob/text/varchar 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。\n\n要选择足够长的前缀以保证较高的选择性，同时又不能太长（节约空间）。\n\n❌ 低效示例：\n\nselect count(*) as cnt, city from sakila.city_demo\ngroup by city order by cnt desc limit 10;\n\n\n✔ 高效示例：\n\nselect count(*) as cnt, left(city, 3) as pref from sakila.city_demo\ngroup by city order by cnt desc limit 10;\n\n\n# 多列索引\n\n不要为每个列都创建独立索引。\n\n将选择性高的列或基数大的列优先排在多列索引最前列。但有时，也需要考虑 where 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。\n\n举例来说，有一张 user 表，其中含 name, sex, age 三个列，如果将这三者组合为多列索引，应该用什么样的顺序呢？从选择性高的角度来看：name > age > sex。\n\n# 聚簇索引\n\n聚簇索引不是一种单独的索引类型，而是一种数据存储方式。具体细节依赖于实现方式。如 innodb 的聚簇索引实际是在同一个结构中保存了 b 树的索引和数据行。\n\n聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n若没有定义主键，innodb 会隐式定义一个主键来作为聚簇索引。\n\n# 覆盖索引\n\n索引包含所有需要查询的字段的值。\n\n具有以下优点：\n\n * 因为索引条目通常远小于数据行的大小，所以若只读取索引，能大大减少数据访问量。\n * 一些存储引擎（例如 myisam）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。\n * 对于 innodb 引擎，若辅助索引能够覆盖查询，则无需访问主索引。\n\n# 使用索引扫描来做排序\n\nmysql 有两种方式可以生成排序结果：通过排序操作；或者按索引顺序扫描。\n\n索引最好既满足排序，又用于查找行。这样，就可以使用索引来对结果排序。\n\n# 最左前缀匹配原则\n\nmysql 会一直向右匹配直到遇到范围查询 (>,<,between,like) 就停止匹配。\n\n * 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。\n * 如果是联合索引，那么 key 也由多个列组成，同时，索引只能用于查找 key 是否存在（相等），遇到范围查询(>、<、between、like 左匹配)等就不能进一步匹配了，后续退化为线性查找。\n * 因此，列的排列顺序决定了可命中索引的列数。\n\n例子：\n\n * 如有索引(a, b, c, d)，查询条件 a = 1 and b = 2 and c > 3 and d = 4，则会在每个节点依次命中 a、b、c，无法命中 d。(很简单：索引命中只能是相等的情况，不能是范围匹配)\n\n# = 和 in 可以乱序\n\n不需要考虑=、in 等的顺序，mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。\n\n例子：如有索引(a, b, c, d)，查询条件 c > 3 and b = 2 and a = 1 and d < 4 与 a = 1 and c > 3 and b = 2 and d < 4 等顺序都是可以的，mysql 会自动优化为 a = 1 and b = 2 and c > 3 and d < 4，依次命中 a、b、c。\n\n\n# 约束\n\n数据库约束（constraint）有哪些：\n\n * not null - 用于控制字段的内容一定不能为空（null）。\n * unique - 字段内容不能重复，一个表允许有多个 unique 约束。\n * primary key - 数据表中对储存数据对象予以唯一和完整标识的数据列或属性的组合，它在一个表中只允许有一个。主键的取值不能为空值（null）。\n * foreign key - 在一个表中存在的另一个表的主键称此表的外键。用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。\n * check - 用于控制字段的值范围。\n\n\n# 二、并发控制\n\n\n# 乐观锁和悲观锁\n\n>  * 数据库的乐观锁和悲观锁是什么？\n>  * 数据库的乐观锁和悲观锁如何实现？\n\n确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性，乐观锁和悲观锁是并发控制主要采用的技术手段。\n\n * 悲观锁 - 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作\n   * 在查询完数据的时候就把事务锁起来，直到提交事务（commit）\n   * 实现方式：使用数据库中的锁机制\n * 乐观锁 - 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。\n   * 在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定\n   * 实现方式：使用 version 版本或者时间戳\n\n\n# 行级锁和表级锁\n\n>  * 什么是行级锁和表级锁？\n>  * 什么时候用行级锁？什么时候用表级锁？\n\n从数据库的锁粒度来看，mysql 中提供了两种封锁粒度：行级锁和表级锁。\n\n * 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。\n * 行级锁（row lock） - 仅对指定的行记录进行加锁，这样其它进程还是可以对同一个表中的其它记录进行操作。\n\n二者需要权衡：\n\n * 锁定的数据量越少，锁竞争的发生频率就越小，系统的并发程度就越高。\n * 锁粒度越小，系统开销就越大。\n\n在 innodb 中，行锁是通过给索引上的索引项加锁来实现的。如果没有索引，innodb 将会通过隐藏的聚簇索引来对记录加锁。\n\n\n# 读写锁\n\n>  * 什么是读写锁？\n\n * 独享锁（exclusive），简写为 x 锁，又称写锁。使用方式：select ... for update;\n * 共享锁（shared），简写为 s 锁，又称读锁。使用方式：select ... lock in share mode;\n\n写锁和读锁的关系，简言之：独享锁存在，其他事务就不能做任何操作。\n\ninnodb 下的行锁、间隙锁、next-key 锁统统属于独享锁。\n\n\n# 意向锁\n\n>  * 什么是意向锁？\n>  * 意向锁有什么用？\n\n意向锁的作用是：当存在表级锁和行级锁的情况下，必须先申请意向锁（表级锁，但不是真的加锁），再获取行级锁。使用意向锁（intention locks）可以更容易地支持多粒度封锁。\n\n意向锁是 innodb 自动加的，不需要用户干预。\n\n\n# mvcc\n\n> 什么是 mvcc？\n> \n> mvcc 有什么用？解决了什么问题？\n> \n> mvcc 的原理是什么？\n\n多版本并发控制（multi-version concurrency control, mvcc）是 innodb 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 mvcc。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 mvcc 无法实现。\n\nmvcc 的思想是：\n\n * 保存数据在某个时间点的快照。写操作（delete、insert、update）更新最新的版本快照，而读操作去读旧版本快照，没有互斥关系，这一点和 copyonwrite 类似。\n * 脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，mvcc 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。\n\n\n# next-key 锁\n\nnext-key 锁是 mysql 的 innodb 存储引擎的一种锁实现。\n\nmvcc 不能解决幻读问题，next-key 锁就是为了解决幻读问题。在可重复读（repeatable read）隔离级别下，使用 mvcc + next-key 锁 可以解决幻读问题。\n\n另外，根据针对 sql 语句检索条件的不同，加锁又有以下三种情形需要我们掌握。\n\n * record lock - 行锁对索引项加锁，若没有索引则使用表锁。\n * gap lock - 对索引项之间的间隙加锁。锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15。select c from t where c between 10 and 20 for update;\n * next-key lock -它是 record lock 和 gap lock 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间。\n\n索引分为主键索引和非主键索引两种，如果一条 sql 语句操作了主键索引，mysql 就会锁定这条主键索引；如果一条语句操作了非主键索引，mysql 会先锁定该非主键索引，再锁定相关的主键索引。在 update、delete 操作时，mysql 不仅锁定 where 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key lock。\n\n当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，innodb 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。\n\n\n# 三、事务\n\n> 事务简单来说：一个 session 中所进行所有的操作，要么同时成功，要么同时失败。具体来说，事务指的是满足 acid 特性的一组操作，可以通过 commit 提交一个事务，也可以使用 rollback 进行回滚。\n\n\n\n\n# acid\n\nacid — 数据库事务正确执行的四个基本要素：\n\n * 原子性（atomicity）\n * 一致性（consistency）\n * 隔离性（isolation）\n * 持久性（durability）\n\n一个支持事务（transaction）中的数据库系统，必需要具有这四种特性，否则在事务过程（transaction processing）当中无法保证数据的正确性，交易过程极可能达不到交易。\n\n\n\n\n# 并发一致性问题\n\n在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题。\n\n * 丢失修改\n\nt1 和 t2 两个事务都对一个数据进行修改，t1 先修改，t2 随后修改，t2 的修改覆盖了 t1 的修改。\n\n\n\n * 脏读\n\nt1 修改一个数据，t2 随后读取这个数据。如果 t1 撤销了这次修改，那么 t2 读取的数据是脏数据。\n\n\n\n * 不可重复读\n\nt2 读取一个数据，t1 对该数据做了修改。如果 t2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n\n\n\n * 幻读\n\nt1 读取某个范围的数据，t2 在这个范围内插入新的数据，t1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n\n\n\n并发一致性解决方案：\n\n产生并发不一致性问题主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。\n\n并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。\n\n\n# 事务隔离\n\n数据库隔离级别：\n\n * 未提交读（read uncommitted） - 事务中的修改，即使没有提交，对其它事务也是可见的。\n * 提交读（read committed） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。\n * 重复读（repeatable read） - 保证在同一个事务中多次读取同样数据的结果是一样的。\n * 串行化（serialixable） - 强制事务串行执行。\n\n数据库隔离级别解决的问题：\n\n隔离级别   脏读   不可重复读   幻读\n未提交读   ❌    ❌       ❌\n提交读    ✔️   ❌       ❌\n可重复读   ✔️   ✔️      ❌\n可串行化   ✔️   ✔️      ✔️\n\n\n# 分布式事务\n\n在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为 本地事务。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。\n\n分布式事务 是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。\n\n# 两阶段提交\n\n两阶段提交（xa）对业务侵入很小。 它最大的优势就是对使用方透明，用户可以像使用本地事务一样使用基于 xa 协议的分布式事务。 xa 协议能够严格保障事务 acid 特性。\n\n严格保障事务 acid 特性是一把双刃剑。 事务执行在过程中需要将所需资源全部锁定，它更加适用于执行时间确定的短事务。 对于长事务来说，整个事务进行期间对数据的独占，将导致对热点数据依赖的业务系统并发性能衰退明显。 因此，在高并发的性能至上场景中，基于 xa 协议的分布式事务并不是最佳选择。\n\n# 柔性事务\n\n如果将实现了acid 的事务要素的事务称为刚性事务的话，那么基于base事务要素的事务则称为柔性事务。 base是基本可用、柔性状态和最终一致性这三个要素的缩写。\n\n * 基本可用（basically available）保证分布式事务参与方不一定同时在线。\n * 柔性状态（soft state）则允许系统状态更新有一定的延时，这个延时对客户来说不一定能够察觉。\n * 而最终一致性（eventually consistent）通常是通过消息传递的方式保证系统的最终一致性。\n\n在acid事务中对隔离性的要求很高，在事务执行过程中，必须将所有的资源锁定。 柔性事务的理念则是通过业务逻辑将互斥锁操作从资源层面上移至业务层面。通过放宽对强一致性要求，来换取系统吞吐量的提升。\n\n基于acid的强一致性事务和基于base的最终一致性事务都不是银弹，只有在最适合的场景中才能发挥它们的最大长处。 可通过下表详细对比它们之间的区别，以帮助开发者进行技术选型。\n\n# 事务方案对比\n\n       本地事务       两（三）阶段事务    柔性事务\n业务改造   无          无           实现相关接口\n一致性    不支持        支持          最终一致\n隔离性    不支持        支持          业务方保证\n并发性能   无影响        严重衰退        略微衰退\n适合场景   业务方处理不一致   短事务 & 低并发   长事务 & 高并发\n\n\n# 四、分库分表\n\n\n# 什么是分库分表\n\n> 什么是分库分表？什么是垂直拆分？什么是水平拆分？什么是 sharding？\n> \n> 分库分表是为了解决什么问题？\n> \n> 分库分表有什么优点？\n> \n> 分库分表有什么策略？\n\n分库分表的基本思想就是：把原本完整的数据切分成多个部分，放到不同的数据库或表上。\n\n分库分表一定是为了支撑 高并发、数据量大两个问题的。\n\n# 垂直切分\n\n> 垂直切分，是 把一个有很多字段的表给拆分成多个表，或者是多个库上去。一般来说，会 将较少的、访问频率较高的字段放到一个表里去，然后 将较多的、访问频率较低的字段放到另外一个表里去。因为数据库是有缓存的，访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。\n\n\n\n一般来说，满足下面的条件就可以考虑扩容了：\n\n * mysql 单库超过 5000 万条记录，oracle 单库超过 1 亿条记录，db 压力就很大。\n * 单库超过每秒 2000 个并发时，而一个健康的单库最好保持在每秒 1000 个并发左右，不要太大。\n\n在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。\n\n# 水平拆分\n\n> 水平拆分 又称为 sharding，它是将同一个表中的记录拆分到多个结构相同的表中。当 单表数据量太大 时，会极大影响 sql 执行的性能 。分表是将原来一张表的数据分布到数据库集群的不同节点上，从而缓解单点的压力。\n\n\n\n一般来说，单表有 200 万条数据 的时候，性能就会相对差一些了，需要考虑分表了。但是，这也要视具体情况而定，可能是 100 万条，也可能是 500 万条，sql 越复杂，就最好让单表行数越少。\n\n# 分库分表的优点\n\n#          分库分表前              分库分表后\n并发支撑情况     单机部署，扛不住高并发        从单机到多机，能承受的并发增加了多倍\n磁盘使用情况     单机磁盘容量几乎撑满         拆分为多个库，数据库服务器磁盘使用率大大降低\nsql 执行性能   单表数据量太大，sql 越跑越慢   单表数据量减少，sql 执行效率明显提升\n\n# 分库分表策略\n\n * 哈希取模：hash(key) % n 或 id % n\n   * 优点：可以平均分配每个库的数据量和请求压力（负载均衡）。\n   * 缺点：扩容麻烦，需要数据迁移。\n * 范围：可以按照 id 或时间划分范围。\n   * 优点：扩容简单。\n   * 缺点：这种策略容易产生热点问题。\n * 映射表：使用单独的一个数据库来存储映射关系。\n   * 缺点：存储映射关系的数据库也可能成为性能瓶颈，且一旦宕机，分库分表的数据库就无法工作。所以不建议使用这种策略。\n   * 优点：扩容简单，可以解决分布式 id 问题。\n\n\n# 分库分表中间件\n\n> ❓ 常见问题：\n> \n>  * 你用过哪些分库分表中间件，简单介绍一下？\n> \n>  * 不同的分库分表中间件各自有什么特性，有什么优缺点？\n> \n>  * 分库分表中间件技术如何选型？\n\n# 常见的分库分表中间件\n\n * cobar - 阿里 b2b 团队开发和开源的，属于 proxy 层方案，就是介于应用服务器和数据库服务器之间。应用程序通过 jdbc 驱动访问 cobar 集群，cobar 根据 sql 和分库规则对 sql 做分解，然后分发到 mysql 集群不同的数据库实例上执行。早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。\n * tddl - 淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。\n * atlas - 360 开源的，属于 proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。\n * sharding-jdbc - 当当开源的，属于 client 层方案。确实之前用的还比较多一些，因为 sql 语法支持也比较多，没有太多限制，而且目前推出到了 2.0 版本，支持分库分表、读写分离、分布式 id 生成、柔性事务（最大努力送达型事务、tcc 事务）。而且确实之前使用的公司会比较多一些（这个在官网有登记使用的公司，可以看到从 2017 年一直到现在，是有不少公司在用的），目前社区也还一直在开发和维护，还算是比较活跃，个人认为算是一个现在也可以选择的方案。\n * mycat - 基于 cobar 改造的，属于 proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 sharding jdbc 来说，年轻一些，经历的锤炼少一些。\n\n# 分库分表中间件技术选型\n\n建议使用的是 sharding-jdbc 和 mycat。\n\n * sharding-jdbc 这种 client 层方案的优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要耦合 sharding-jdbc 的依赖。其本质上通过配置多数据源，然后根据设定的分库分表策略，计算路由，将请求发送到计算得到的节点上。\n\n * mycat 这种 proxy 层方案的缺点在于需要部署，自己运维一套中间件，运维成本高，但是好处在于对于各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就行了。\n\n通常来说，这两个方案其实都可以选用，但是我个人建议中小型公司选用 sharding-jdbc，client 层方案轻便，而且维护成本低，不需要额外增派人手，而且中小型公司系统复杂度会低一些，项目也没那么多；但是中大型公司最好还是选用 mycat 这类 proxy 层方案，因为可能大公司系统和项目非常多，团队很大，人员充足，那么最好是专门弄个人来研究和维护 mycat，然后大量项目直接透明使用即可。\n\n\n# 分库分表的问题\n\n>  * 分库分表的常见问题有哪些？\n> \n>  * 你是如何解决分库分表的问题的？\n> \n> 下文一一讲解常见分库分表的问题及解决方案。\n\n# 分布式事务\n\n方案一：使用数据库事务\n\n * 优点：交由数据库管理，简单有效\n * 缺点：性能代价高，特别是 shard 越来越多时\n\n方案二：由应用程序和数据库共同控制\n\n * 原理：将一个跨多个数据库的分布式事务分拆成多个仅处于单个数据库上面的小事务，并通过应用程序来总控各个小事务。\n * 优点：性能上有优势\n * 缺点：需要应用程序在事务控制上做灵活设计。如果使用了 spring 的事务管理，改动起来会面临一定的困难。\n\n# 跨节点 join\n\n只要是进行切分，跨节点 join 的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的 id，根据这些 id 发起第二次请求得到关联数据。\n\n# 跨节点的 count,order by,group by 以及聚合函数\n\n这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。\n\n解决方案：与解决跨节点 join 问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和 join 不同的是每个节点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。\n\n业务角度上的解决方案：\n\n * 如果是在前台应用提供分页，则限定用户只能看前面 n 页，这个限制在业务上也是合理的，一般看后面的分页意义不大（如果一定要看，可以要求用户缩小范围重新查询）。\n * 如果是后台批处理任务要求分批获取数据，则可以加大 page size，比如每次获取 5000 条记录，有效减少分页数（当然离线访问一般走备库，避免冲击主库）。\n * 分库设计时，一般还有配套大数据平台汇总所有分库的记录，有些分页查询可以考虑走大数据平台。\n\n# 分布式 id\n\n一旦数据库被切分到多个物理节点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的 id 无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得 id，以便进行 sql 路由。\n\n一些常见的主键生成策略：\n\n * 使用全局唯一 id：guid。\n * 为每个分片指定一个 id 范围。\n * 分布式 id 生成器 (如 twitter 的 snowflake 算法)。\n\n# 数据迁移，容量规划，扩容等问题\n\n来自淘宝综合业务平台团队，它利用对 2 的倍数取余具有向前兼容的特性（如对 4 取余得 1 的数对 2 取余也是 1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了 sharding 扩容的难度。\n\n\n# 五、集群\n\n> 这个专题需要根据熟悉哪个数据库而定，但是主流、成熟的数据库都会实现一些基本功能，只是实现方式、策略上有所差异。由于本人较为熟悉 mysql，所以下面主要介绍 mysql 系统架构问题。\n\n\n# 复制机制\n\nmysql 支持两种复制：基于行的复制和基于语句的复制。\n\n这两种方式都是在主库上记录二进制日志（binlog），然后在从库上以异步方式更新主库上的日志记录。这意味着：复制过程存在时延，这段时间内，主从数据可能不一致（即最终一致性）。\n\n主要涉及三个线程：binlog 线程、i/o 线程和 sql 线程。\n\n * binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。\n * i/o 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的日志中。\n * sql 线程 ：负责读取日志并执行 sql 语句以更新数据。\n\n\n\n\n# 读写分离\n\n主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。\n\n读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。\n\nmysql 读写分离能提高性能的原因在于：\n\n * 主从服务器负责各自的读和写，极大程度缓解了锁的争用；\n * 从服务器可以配置 myisam 引擎，提升查询性能以及节约系统开销；\n * 增加冗余，提高可用性。\n\n\n\n\n# 六、数据库优化\n\n数据库优化的路线一般为：sql 优化、结构优化、配置优化、硬件优化。前两个方向一般是普通开发的考量点，而后两个方向一般是 dba 的考量点。\n\n\n# sql 优化\n\n> sql 优化是数据库优化的最常见、最初级手段。\n> \n> 在执行 sql 语句，语句中字段的顺序、查询策略等都可能会影响到 sql 的执行性能。\n\n# 执行计划\n\n如何检验修改后的 sql 确实有优化效果？这就需要用到执行计划（explain）。\n\n使用执行计划 explain 用来分析 select 查询效率，开发人员可以通过分析 explain 结果来优化查询语句。\n\n比较重要的字段有：\n\n * select_type - 查询类型，有简单查询、联合查询、子查询等\n * key - 使用的索引\n * rows - 扫描的行数\n\n> 更多内容请参考：mysql 性能优化神器 explain 使用分析\n\n# 访问数据优化\n\n减少请求的数据量：\n\n * 只返回必要的列 - 不要查询不需要的列，尽量避免使用 select * 语句。\n * 只返回必要的行 - 使用 where 语句进行查询过滤，有时候也需要使用 limit 语句来限制返回的数据。\n * 缓存重复查询的数据 - 使用缓存可以避免在数据库中进行查询，特别要查询的数据经常被重复查询，缓存可以带来的查询性能提升将会是非常明显的。\n\n减少服务器端扫描的行数：\n\n * 最有效的方式是使用索引来覆盖查询（即 where 后的过滤查询字段最好是索引字段）。\n\n# 重构查询方式\n\n# 切分查询\n\n一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。\n\ndeleft from messages where create < date_sub(now(), interval 3 month);\n\n\nrows_affected = 0\ndo {\n    rows_affected = do_query(\n    \"delete from messages where create  < date_sub(now(), interval 3 month) limit 10000\")\n} while rows_affected > 0\n\n\n# 分解关联查询\n\n将一个大连接查询（join）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有：\n\n * 缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。\n * 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。\n * 减少锁竞争；\n * 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。\n * 查询本身效率也可能会有所提升。例如下面的例子中，使用 in() 代替连接查询，可以让 mysql 按照 id 顺序进行查询，这可能比随机的连接要更高效。\n\nselect * from tag\njoin tag_post on tag_post.tag_id=tag.id\njoin post on tag_post.post_id=post.id\nwhere tag.tag='mysql';\nselect * from tag where tag='mysql';\nselect * from tag_post where tag_id=1234;\nselect * from post where post.id in (123,456,567,9098,8904);\n\n\n# sql 语句细节\n\n# 选择最有效率的表名顺序\n\n数据库按照从右到左的顺序处理 from 子句中的表名，from 子句中写在最后的表将被最先处理。\n\n在 from 子句中包含多个表的情况下：\n\n * 如果多个表是完全无关系的话，将记录和列名最少的表，写在最后，然后依次类推。也就是说：选择记录条数最少的表放在最后。\n\n如果有 3 个以上的表连接查询：\n\n * 如果多个表是有关系的话，将引用最多的表，放在最后，然后依次类推。也就是说：被其他表所引用的表放在最后。\n\n例如：查询员工的编号，姓名，工资，工资等级，部门名\n\nemp 表被引用得最多，记录数也是最多，因此放在 form 字句的最后面\n\nselect emp.empno,emp.ename,emp.sal,salgrade.grade,dept.dname\nfrom salgrade,dept,emp\nwhere (emp.deptno = dept.deptno) and (emp.sal between salgrade.losal and salgrade.hisal)\n\n\n# where 子句中的连接顺序\n\n数据库按照从右到左的顺序解析 where 子句。\n\n因此，表之间的连接必须写在其他 where 条件的左边，那些可以过滤掉最大数量记录的条件必须写在 where 子句的之右。\n\nemp.sal 可以过滤多条记录，写在 where 字句的最右边\n\nselect emp.empno,emp.ename,emp.sal,dept.dname\nfrom dept,emp\nwhere (emp.deptno = dept.deptno) and (emp.sal > 1500)\n\n\n# select 子句中避免使用 * 号\n\n我们当时学习的时候，“*” 号是可以获取表中全部的字段数据的。\n\n * 但是它要通过查询数据字典完成的，这意味着将耗费更多的时间\n * 使用*号写出来的 sql 语句也不够直观。\n\n----------------------------------------\n\n# 用 truncate 替代 delete\n\n如果需要清空所有表记录，使用 truncate 比 delete 执行效率高：\n\ndelete 是一条一条记录的删除，而 truncate 是将整个表删除，仅保留表结构\n\n# 使用内部函数提高 sql 效率\n\n例如使用 mysql 的 concat() 函数会比使用 || 拼接速度快，因为 concat() 函数已经被 mysql 优化过了。\n\n# 使用表或列的别名\n\n如果表或列的名称太长了，使用一些简短的别名也能稍微提高一些 sql 的性能。毕竟要扫描的字符长度就变少了。\n\n# sql 关键字大写\n\n我们在编写 sql 的时候，官方推荐的是使用大写来写关键字，因为 oracle 服务器总是先将小写字母转成大写后，才执行\n\n# 用 >= 替代 >\n\n❌ 低效方式：\n\n-- 首先定位到deptno=3的记录并且扫描到第一个dept大于3的记录\nselect * from emp where deptno > 3\n\n\n✔ 高效方式：\n\n-- 直接跳到第一个dept等于4的记录\nselect * from emp where deptno >= 4\n\n\n# 用 in 替代 or\n\n❌ 低效方式：\n\nselect * from emp where sal = 1500 or sal = 3000 or sal = 800;\n\n\n✔ 高效方式：\n\nselect * from emp where sal in (1500,3000,800);\n\n\n# 总是使用索引的第一个列\n\n如果索引是建立在多个列上，只有在它的第一个列被 where 子句引用时，优化器才会选择使用该索引。 当只引用索引的第二个列时，不引用索引的第一个列时，优化器使用了全表扫描而忽略了索引\n\ncreate index emp_sal_job_idex\non emp(sal,job);\n----------------------------------\nselect *\nfrom emp\nwhere job != 'sales';\n\n\n# sql 关键字尽量大写\n\nsql 关键字尽量大写，如：oracle 默认会将 sql 语句中的关键字转为大写后在执行。\n\n\n# 结构优化\n\n数据库结构优化可以从以下方向着手：\n\n * 数据类型优化\n * 范式和反范式优化\n * 索引优化 - 细节请看索引和约束章节\n * 分库分表 - 细节请看分库分表章节\n\n# 数据类型优化原则\n\n * 更小的通常更好\n * 简单就好，如整型比字符型操作代价低\n * 尽量避免 null\n\n# 范式和反范式\n\n范式和反范式各有利弊，需要根据实际情况权衡。\n\n范式化的目标是尽力减少冗余列，节省空间。\n\n * 范式化的优点是：\n   \n   * 减少冗余列，要写的数据就少，写操作的性能提高；\n   * 检索列数据时，distinct 或 group by 操作减少。\n\n * 范式化的缺点是：增加关联查询。\n\n反范式化的目标是适当增加冗余列，以避免关联查询。\n\n反范式化的缺点是：\n\n * 冗余列增多，空间变大，写操作性能下降；\n * 检索列数据时，distinct 或 group by 操作变多；\n\n\n# 配置优化\n\n> 配置优化主要是针对 mysql 服务器，例如：max_connections、max_heap_table_size、open_files_limit、max_allowed_packet 等等。\n> \n> 在不同环境，不同场景下，应该酌情使用合理的配置。这种优化比较考验 mysql 运维经验，一般是 dba 的考量，普通开发接触的较少。\n> \n> mysql 配置说明请参考：mysql 服务器配置说明\n\n\n# 硬件优化\n\n数据库扩容、使用高配设备等等。核心就是一个字：钱。\n\n\n# 七、数据库理论\n\n\n# 函数依赖\n\n记 a->b 表示 a 函数决定 b，也可以说 b 函数依赖于 a。\n\n如果 {a1，a2，... ，an} 是关系的一个或多个属性的集合，该集合函数决定了关系的其它所有属性并且是最小的，那么该集合就称为键码。\n\n对于 a->b，如果能找到 a 的真子集 a'，使得 a'-> b，那么 a->b 就是部分函数依赖，否则就是完全函数依赖；\n\n对于 a->b，b->c，则 a->c 是一个传递依赖。\n\n\n# 异常\n\n以下的学生课程关系的函数依赖为 sno, cname -> sname, sdept, mname, grade，键码为 {sno, cname}。也就是说，确定学生和课程之后，就能确定其它信息。\n\nsno   sname   sdept   mname   cname   grade\n1     学生-1    学院-1    院长-1    课程-1    90\n2     学生-2    学院-2    院长-2    课程-2    80\n2     学生-2    学院-2    院长-2    课程-1    100\n3     学生-3    学院-2    院长-2    课程-2    95\n\n不符合范式的关系，会产生很多异常，主要有以下四种异常：\n\n * 冗余数据：例如 学生-2 出现了两次。\n * 修改异常：修改了一个记录中的信息，但是另一个记录中相同的信息却没有被修改。\n * 删除异常：删除一个信息，那么也会丢失其它信息。例如如果删除了 课程-1，需要删除第一行和第三行，那么 学生-1 的信息就会丢失。\n * 插入异常，例如想要插入一个学生的信息，如果这个学生还没选课，那么就无法插入。\n\n\n# 范式\n\n范式理论是为了解决以上提到四种异常。\n\n高级别范式的依赖于低级别的范式，1nf 是最低级别的范式。\n\n\n\n# 第一范式 (1nf)\n\n属性不可分。\n\n# 第二范式 (2nf)\n\n * 每个非主属性完全函数依赖于键码。\n\n * 可以通过分解来满足。\n\n分解前\n\nsno   sname   sdept   mname   cname   grade\n1     学生-1    学院-1    院长-1    课程-1    90\n2     学生-2    学院-2    院长-2    课程-2    80\n2     学生-2    学院-2    院长-2    课程-1    100\n3     学生-3    学院-2    院长-2    课程-2    95\n\n以上学生课程关系中，{sno, cname} 为键码，有如下函数依赖：\n\n * sno -> sname, sdept\n * sdept -> mname\n * sno, cname-> grade\n\ngrade 完全函数依赖于键码，它没有任何冗余数据，每个学生的每门课都有特定的成绩。\n\nsname, sdept 和 mname 都部分依赖于键码，当一个学生选修了多门课时，这些数据就会出现多次，造成大量冗余数据。\n\n分解后\n\n关系-1\n\nsno   sname   sdept   mname\n1     学生-1    学院-1    院长-1\n2     学生-2    学院-2    院长-2\n3     学生-3    学院-2    院长-2\n\n有以下函数依赖：\n\n * sno -> sname, sdept, mname\n * sdept -> mname\n\n关系-2\n\nsno   cname   grade\n1     课程-1    90\n2     课程-2    80\n2     课程-1    100\n3     课程-2    95\n\n有以下函数依赖：\n\n * sno, cname -> grade\n\n# 第三范式 (3nf)\n\n * 非主属性不传递依赖于键码。\n\n上面的 关系-1 中存在以下传递依赖：sno -> sdept -> mname，可以进行以下分解：\n\n关系-11\n\nsno   sname   sdept\n1     学生-1    学院-1\n2     学生-2    学院-2\n3     学生-3    学院-2\n\n关系-12\n\nsdept   mname\n学院-1    院长-1\n学院-2    院长-2\n\n\n# 八、mysql 存储引擎\n\nmysql 有多种存储引擎，不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在 mysql 服务层统一处理的。\n\n简单列举几个存储引擎：\n\n * innodb - mysql 的默认事务型存储引擎，并提供了行级锁和外键的约束。性能不错且支持自动故障恢复。\n * myisam - mysql 5.1 版本前的默认存储引擎。特性丰富但不支持事务，也不支持行级锁和外键，也没有故障恢复功能。\n * csv - 可以将 csv 文件作为 mysql 的表来处理，但这种表不支持索引。\n * memory 。所有的数据都在内存中，数据的处理速度快，但是安全性不高。\n\n\n# innodb vs. myisam\n\ninnodb 和 myisam 是目前使用的最多的两种 mysql 存储引擎。\n\n * 数据结构比较：\n   * innodb 和 myisam 的索引数据结构都是 b+ 树。\n   * myiasm 的 b+ 树中存储的内容实际上是实际数据的地址值。也就是说它的索引和实际数据是分开的，只不过使用索引指向了实际数据。这种索引的模式被称为非聚集索引。\n   * innodb 的 b+ 树中存储的内容是实际的数据，这种索引有被称为聚集索引。\n * 事务支持比较：\n   * innodb 支持事务，并提供了行级锁和外键的约束。\n   * myiasm 不支持事务，也不支持行级锁和外键。\n * 故障恢复比较：\n   * innodb 支持故障恢复。\n   * myisam 不支持故障恢复。\n\n\n# 九、数据库比较\n\n\n# 常见数据库比较\n\n * oracle - 久负盛名的商业数据库。功能强大、稳定。最大的缺点就是费钱。\n * mysql - 曾经是互联网公司的最爱，但自动 mysql 被 oracle 公司收购后，好日子可能一去不复返。很多公司或开源项目已经逐渐寻找其他的开源产品来替代 mysql。\n * mariadb - 开源关系型数据库。 mysql 的真正开源的发行版本，由 mysql 部分核心人员创建。可作为 mysql 的替代产品。\n * postgresql - 开源关系型数据库。和 mysql 的工作方式非常相似，社区支持做得很好。可作为 mysql 的替代产品。\n * sqlite - 开源的轻量级数据库，移动端常常使用。\n * h2 - 内存数据库，一般用作开发、测试环境数据库。\n * sql server - 微软 windows 生态系统的数据库。我想，java 程序员应该没人用吧。\n\n\n# oracle vs. mysql\n\n目前为止，java 领域用的最多的关系型数据库，应该还是 oracle 和 mysql，所以这里做一下比较。\n\n# 数据库对象差异\n\n在 mysql 中，一个用户可以创建多个库。\n\n而在 oracle 中，oracle 服务器是由两部分组成\n\n * 数据库实例【理解为对象，看不见的】\n * 数据库【理解为类，看得见的】\n\n一个数据库实例可拥有多个用户，一个用户默认拥有一个表空间。\n\n表空间是存储我们数据库表的地方，表空间内可以有多个文件。\n\n# sql 差异\n\n（1）主键递增\n\nmysql 可以设置 auto_increment 约束来指定主键为自增序列。\n\noracle 需要通过 create sequence 创建序列。\n\n（2）分页查询\n\nmysql 分页基于 select ... from ... limit ... 完成，较简单。\n\nselect * from help_category order by parent_category_id limit 10,5;\n\n\noracle 分页基于 select ... from (select rownum ...) where ... 完成，较复杂。\n\nselect * from\n(select rownum rr,a.* from (select * from emp order by sal) a )\nwhere rr>5 and rr<=10;\n\n\n# 事务差异\n\n * auto commit\n   * mysql 事务是 autocommit 模式，即自动提交事务；\n   * oracle 事务需要手动 commit。\n * 事务隔离级别\n   * mysql 默认的事务隔离级别是可重复读（repeatable read）\n   * oracle 支持读已提交（read committed）和串行化（serializable） 两种事务隔离级别，默认事务隔离级别是读已提交（read committed）\n\n\n# 数据类型比较\n\n> 不同数据库中，对数据类型的支持是不一样的。\n> \n> 即使存在同一种数据类型，也可能存在名称不同、或大小不同等问题。\n> \n> 因此，对于数据类型的支持详情必须参考各数据库的官方文档。\n\n下面列举一些常见数据类型对比：\n\n数据类型                oracle             mysql         postgresql\nboolean             byte               n/a           boolean\ninteger             number             int integer   int integer\nfloat               number             float         numeric\ncurrency            n/a                n/a           money\nstring (fixed)      char               char          char\nstring (variable)   varchar varchar2   varchar       varchar\nbinary object       long raw           blob text     binary varbinary\n\n> 数据类型对比表摘自 sql 通用数据类型、sql 用于各种数据库的数据类型\n\n\n# 参考资料\n\n * 数据库面试题(开发者必看)\n * 数据库系统原理\n * 数据库两大神器【索引和锁】\n * 分库分表需要考虑的问题及方案\n * 数据库分库分表(sharding)系列(二) 全局主键生成策略\n * 一种支持自由规划无须数据迁移和修改路由代码的 sharding 扩容方案\n * shardingsphere 分布式事务\n * mysql 和 oracle 的区别\n * runoob sql 教程\n * 如果有人问你数据库的原理，叫他看这篇文章",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"sql-cheat-sheet",frontmatter:{title:"sql-cheat-sheet",date:"2018-06-15T16:07:17.000Z",categories:["数据库","关系型数据库","综合"],tags:["数据库","关系型数据库","SQL"],permalink:"/pages/b71c9e/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E7%BB%BC%E5%90%88/02.SqlCheatSheet.html",relativePath:"12.数据库/03.关系型数据库/01.综合/02.SqlCheatSheet.md",key:"v-4a7a8afa",path:"/pages/b71c9e/",headers:[{level:2,title:"一、基本概念",slug:"一、基本概念",normalizedTitle:"一、基本概念",charIndex:111},{level:3,title:"数据库术语",slug:"数据库术语",normalizedTitle:"数据库术语",charIndex:122},{level:3,title:"SQL 语法",slug:"sql-语法",normalizedTitle:"sql 语法",charIndex:407},{level:4,title:"SQL 语法结构",slug:"sql-语法结构",normalizedTitle:"sql 语法结构",charIndex:528},{level:4,title:"SQL 语法要点",slug:"sql-语法要点",normalizedTitle:"sql 语法要点",charIndex:787},{level:4,title:"SQL 分类",slug:"sql-分类",normalizedTitle:"sql 分类",charIndex:1179},{level:4,title:"数据定义语言（DDL）",slug:"数据定义语言-ddl",normalizedTitle:"数据定义语言（ddl）",charIndex:1189},{level:4,title:"数据操纵语言（DML）",slug:"数据操纵语言-dml",normalizedTitle:"数据操纵语言（dml）",charIndex:1323},{level:4,title:"事务控制语言（TCL）",slug:"事务控制语言-tcl",normalizedTitle:"事务控制语言（tcl）",charIndex:1536},{level:4,title:"数据控制语言（DCL）",slug:"数据控制语言-dcl",normalizedTitle:"数据控制语言（dcl）",charIndex:1672},{level:2,title:"二、增删改查",slug:"二、增删改查",normalizedTitle:"二、增删改查",charIndex:2016},{level:3,title:"插入数据",slug:"插入数据",normalizedTitle:"插入数据",charIndex:2059},{level:3,title:"更新数据",slug:"更新数据",normalizedTitle:"更新数据",charIndex:2336},{level:3,title:"删除数据",slug:"删除数据",normalizedTitle:"删除数据",charIndex:2449},{level:3,title:"查询数据",slug:"查询数据",normalizedTitle:"查询数据",charIndex:2608},{level:2,title:"三、子查询",slug:"三、子查询",normalizedTitle:"三、子查询",charIndex:3106},{level:3,title:"WHERE",slug:"where",normalizedTitle:"where",charIndex:1019},{level:3,title:"IN 和 BETWEEN",slug:"in-和-between",normalizedTitle:"in 和 between",charIndex:4286},{level:3,title:"AND、OR、NOT",slug:"and、or、not",normalizedTitle:"and、or、not",charIndex:4532},{level:3,title:"LIKE",slug:"like",normalizedTitle:"like",charIndex:3981},{level:2,title:"四、连接和组合",slug:"四、连接和组合",normalizedTitle:"四、连接和组合",charIndex:5335},{level:3,title:"连接（JOIN）",slug:"连接-join",normalizedTitle:"连接（join）",charIndex:5347},{level:4,title:"内连接（INNER JOIN）",slug:"内连接-inner-join",normalizedTitle:"内连接（inner join）",charIndex:5531},{level:5,title:"自连接（=）",slug:"自连接",normalizedTitle:"自连接（=）",charIndex:5712},{level:5,title:"自然连接（NATURAL JOIN）",slug:"自然连接-natural-join",normalizedTitle:"自然连接（natural join）",charIndex:5931},{level:4,title:"外连接（OUTER JOIN）",slug:"外连接-outer-join",normalizedTitle:"外连接（outer join）",charIndex:6053},{level:5,title:"左连接（LEFT JOIN）",slug:"左连接-left-join",normalizedTitle:"左连接（left join）",charIndex:6152},{level:5,title:"右连接（RIGHT JOIN）",slug:"右连接-right-join",normalizedTitle:"右连接（right join）",charIndex:6305},{level:3,title:"组合（UNION）",slug:"组合-union",normalizedTitle:"组合（union）",charIndex:6461},{level:3,title:"JOIN vs UNION",slug:"join-vs-union",normalizedTitle:"join vs union",charIndex:6939},{level:2,title:"五、函数",slug:"五、函数",normalizedTitle:"五、函数",charIndex:7069},{level:3,title:"文本处理",slug:"文本处理",normalizedTitle:"文本处理",charIndex:7128},{level:3,title:"日期和时间处理",slug:"日期和时间处理",normalizedTitle:"日期和时间处理",charIndex:7396},{level:3,title:"数值处理",slug:"数值处理",normalizedTitle:"数值处理",charIndex:7981},{level:3,title:"汇总",slug:"汇总",normalizedTitle:"汇总",charIndex:8115},{level:2,title:"六、排序和分组",slug:"六、排序和分组",normalizedTitle:"六、排序和分组",charIndex:8330},{level:3,title:"ORDER BY",slug:"order-by",normalizedTitle:"order by",charIndex:6654},{level:3,title:"GROUP BY",slug:"group-by",normalizedTitle:"group by",charIndex:8534},{level:3,title:"HAVING",slug:"having",normalizedTitle:"having",charIndex:8917},{level:2,title:"七、数据定义",slug:"七、数据定义",normalizedTitle:"七、数据定义",charIndex:9303},{level:3,title:"数据库（DATABASE）",slug:"数据库-database",normalizedTitle:"数据库（database）",charIndex:9354},{level:4,title:"创建数据库",slug:"创建数据库",normalizedTitle:"创建数据库",charIndex:9371},{level:4,title:"删除数据库",slug:"删除数据库",normalizedTitle:"删除数据库",charIndex:9404},{level:4,title:"选择数据库",slug:"选择数据库",normalizedTitle:"选择数据库",charIndex:9435},{level:3,title:"数据表（TABLE）",slug:"数据表-table",normalizedTitle:"数据表（table）",charIndex:9457},{level:4,title:"创建数据表",slug:"创建数据表",normalizedTitle:"创建数据表",charIndex:9471},{level:4,title:"删除数据表",slug:"删除数据表",normalizedTitle:"删除数据表",charIndex:9818},{level:4,title:"修改数据表",slug:"修改数据表",normalizedTitle:"修改数据表",charIndex:9846},{level:3,title:"视图（VIEW）",slug:"视图-view",normalizedTitle:"视图（view）",charIndex:10080},{level:4,title:"创建视图",slug:"创建视图",normalizedTitle:"创建视图",charIndex:10257},{level:4,title:"删除视图",slug:"删除视图",normalizedTitle:"删除视图",charIndex:10344},{level:3,title:"索引（INDEX）",slug:"索引-index",normalizedTitle:"索引（index）",charIndex:10383},{level:4,title:"创建索引",slug:"创建索引",normalizedTitle:"创建索引",charIndex:10508},{level:4,title:"创建唯一索引",slug:"创建唯一索引",normalizedTitle:"创建唯一索引",charIndex:10599},{level:4,title:"删除索引",slug:"删除索引",normalizedTitle:"删除索引",charIndex:10656},{level:3,title:"约束",slug:"约束",normalizedTitle:"约束",charIndex:10707},{level:2,title:"八、事务处理",slug:"八、事务处理",normalizedTitle:"八、事务处理",charIndex:11587},{level:2,title:"九、权限控制",slug:"九、权限控制",normalizedTitle:"九、权限控制",charIndex:12387},{level:3,title:"创建账户",slug:"创建账户",normalizedTitle:"创建账户",charIndex:12672},{level:3,title:"修改账户名",slug:"修改账户名",normalizedTitle:"修改账户名",charIndex:12730},{level:3,title:"删除账户",slug:"删除账户",normalizedTitle:"删除账户",charIndex:12812},{level:3,title:"查看权限",slug:"查看权限",normalizedTitle:"查看权限",charIndex:12841},{level:3,title:"授予权限",slug:"授予权限",normalizedTitle:"授予权限",charIndex:12876},{level:3,title:"删除权限",slug:"删除权限",normalizedTitle:"删除权限",charIndex:12926},{level:3,title:"更改密码",slug:"更改密码",normalizedTitle:"更改密码",charIndex:12979},{level:2,title:"十、存储过程和游标",slug:"十、存储过程和游标",normalizedTitle:"十、存储过程和游标",charIndex:13026},{level:3,title:"存储过程优缺点",slug:"存储过程优缺点",normalizedTitle:"存储过程优缺点",charIndex:13115},{level:3,title:"使用存储过程",slug:"使用存储过程",normalizedTitle:"使用存储过程",charIndex:13310},{level:3,title:"游标",slug:"游标",normalizedTitle:"游标",charIndex:13033},{level:3,title:"触发器",slug:"触发器",normalizedTitle:"触发器",charIndex:14749},{level:4,title:"触发器特性",slug:"触发器特性",normalizedTitle:"触发器特性",charIndex:14844},{level:4,title:"触发器指令",slug:"触发器指令",normalizedTitle:"触发器指令",charIndex:15600},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:16389}],headersStr:"一、基本概念 数据库术语 SQL 语法 SQL 语法结构 SQL 语法要点 SQL 分类 数据定义语言（DDL） 数据操纵语言（DML） 事务控制语言（TCL） 数据控制语言（DCL） 二、增删改查 插入数据 更新数据 删除数据 查询数据 三、子查询 WHERE IN 和 BETWEEN AND、OR、NOT LIKE 四、连接和组合 连接（JOIN） 内连接（INNER JOIN） 自连接（=） 自然连接（NATURAL JOIN） 外连接（OUTER JOIN） 左连接（LEFT JOIN） 右连接（RIGHT JOIN） 组合（UNION） JOIN vs UNION 五、函数 文本处理 日期和时间处理 数值处理 汇总 六、排序和分组 ORDER BY GROUP BY HAVING 七、数据定义 数据库（DATABASE） 创建数据库 删除数据库 选择数据库 数据表（TABLE） 创建数据表 删除数据表 修改数据表 视图（VIEW） 创建视图 删除视图 索引（INDEX） 创建索引 创建唯一索引 删除索引 约束 八、事务处理 九、权限控制 创建账户 修改账户名 删除账户 查看权限 授予权限 删除权限 更改密码 十、存储过程和游标 存储过程优缺点 使用存储过程 游标 触发器 触发器特性 触发器指令 参考资料",content:"# SQL Cheat Sheet\n\n> 本文针对关系型数据库的基本语法。限于篇幅，本文侧重说明用法，不会展开讲解特性、原理。\n> \n> 本文语法主要针对 Mysql，但大部分的语法对其他关系型数据库也适用。\n\n\n\n\n# 一、基本概念\n\n\n# 数据库术语\n\n * 数据库（database） - 保存有组织的数据的容器（通常是一个文件或一组文件）。\n * 数据表（table） - 某种特定类型数据的结构化清单。\n * 模式（schema） - 关于数据库和表的布局及特性的信息。模式定义了数据在表中如何存储，包含存储什么样的数据，数据如何分解，各部分信息如何命名等信息。数据库和表都有模式。\n * 列（column） - 表中的一个字段。所有表都是由一个或多个列组成的。\n * 行（row） - 表中的一个记录。\n * 主键（primary key） - 一列（或一组列），其值能够唯一标识表中每一行。\n\n\n# SQL 语法\n\n> SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。\n\n# SQL 语法结构\n\n\n\nSQL 语法结构包括：\n\n * 子句 - 是语句和查询的组成成分。（在某些情况下，这些都是可选的。）\n * 表达式 - 可以产生任何标量值，或由列和行的数据库表\n * 谓词 - 给需要评估的 SQL 三值逻辑（3VL）（true/false/unknown）或布尔真值指定条件，并限制语句和查询的效果，或改变程序流程。\n * 查询 - 基于特定条件检索数据。这是 SQL 的一个重要组成部分。\n * 语句 - 可以持久地影响纲要和数据，也可以控制数据库事务、程序流程、连接、会话或诊断。\n\n# SQL 语法要点\n\n * SQL 语句不区分大小写，但是数据库表名、列名和值是否区分，依赖于具体的 DBMS 以及配置。\n\n例如：SELECT 与 select 、Select 是相同的。\n\n * 多条 SQL 语句必须以分号（;）分隔。\n\n * 处理 SQL 语句时，所有空格都被忽略。SQL 语句可以写成一行，也可以分写为多行。\n\n-- 一行 SQL 语句\nUPDATE user SET username='robot', password='robot' WHERE username = 'root';\n\n-- 多行 SQL 语句\nUPDATE user\nSET username='robot', password='robot'\nWHERE username = 'root';\n\n\n * SQL 支持三种注释\n\n## 注释1\n-- 注释2\n/* 注释3 */\n\n\n# SQL 分类\n\n# 数据定义语言（DDL）\n\n数据定义语言（Data Definition Language，DDL）是 SQL 语言集中负责数据结构定义与数据库对象定义的语言。\n\nDDL 的主要功能是定义数据库对象。\n\nDDL 的核心指令是 CREATE、ALTER、DROP。\n\n# 数据操纵语言（DML）\n\n数据操纵语言（Data Manipulation Language, DML）是用于数据库操作，对数据库其中的对象和数据运行访问工作的编程语句。\n\nDML 的主要功能是 访问数据，因此其语法都是以读写数据库为主。\n\nDML 的核心指令是 INSERT、UPDATE、DELETE、SELECT。这四个指令合称 CRUD(Create, Read, Update, Delete)，即增删改查。\n\n# 事务控制语言（TCL）\n\n事务控制语言 (Transaction Control Language, TCL) 用于管理数据库中的事务。这些用于管理由 DML 语句所做的更改。它还允许将语句分组为逻辑事务。\n\nTCL 的核心指令是 COMMIT、ROLLBACK。\n\n# 数据控制语言（DCL）\n\n数据控制语言 (Data Control Language, DCL) 是一种可对数据访问权进行控制的指令，它可以控制特定用户账户对数据表、查看表、预存程序、用户自定义函数等数据库对象的控制权。\n\nDCL 的核心指令是 GRANT、REVOKE。\n\nDCL 以控制用户的访问权限为主，因此其指令作法并不复杂，可利用 DCL 控制的权限有：CONNECT、SELECT、INSERT、UPDATE、DELETE、EXECUTE、USAGE、REFERENCES。\n\n根据不同的 DBMS 以及不同的安全性实体，其支持的权限控制也有所不同。\n\n----------------------------------------\n\n（以下为 DML 语句用法）\n\n\n# 二、增删改查\n\n> 增删改查，又称为 CRUD，数据库基本操作中的基本操作。\n\n\n# 插入数据\n\n>  * INSERT INTO 语句用于向表中插入新记录。\n\n插入完整的行\n\nINSERT INTO user\nVALUES (10, 'root', 'root', 'xxxx@163.com');\n\n\n插入行的一部分\n\nINSERT INTO user(username, password, email)\nVALUES ('admin', 'admin', 'xxxx@163.com');\n\n\n插入查询出来的数据\n\nINSERT INTO user(username)\nSELECT name\nFROM account;\n\n\n\n# 更新数据\n\n>  * UPDATE 语句用于更新表中的记录。\n\nUPDATE user\nSET username='robot', password='robot'\nWHERE username = 'root';\n\n\n\n# 删除数据\n\n>  * DELETE 语句用于删除表中的记录。\n>  * TRUNCATE TABLE 可以清空表，也就是删除所有行。\n\n删除表中的指定数据\n\nDELETE FROM user\nWHERE username = 'robot';\n\n\n清空表中的数据\n\nTRUNCATE TABLE user;\n\n\n\n# 查询数据\n\n>  * SELECT 语句用于从数据库中查询数据。\n>  * DISTINCT 用于返回唯一不同的值。它作用于所有列，也就是说所有列的值都相同才算相同。\n>  * LIMIT 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。\n>    * ASC ：升序（默认）\n>    * DESC ：降序\n\n查询单列\n\nSELECT prod_name\nFROM products;\n\n\n查询多列\n\nSELECT prod_id, prod_name, prod_price\nFROM products;\n\n\n查询所有列\n\nELECT *\nFROM products;\n\n\n查询不同的值\n\nSELECT DISTINCT\nvend_id FROM products;\n\n\n限制查询结果\n\n-- 返回前 5 行\nSELECT * FROM mytable LIMIT 5;\nSELECT * FROM mytable LIMIT 0, 5;\n-- 返回第 3 ~ 5 行\nSELECT * FROM mytable LIMIT 2, 3;\n\n\n\n# 三、子查询\n\n> 子查询是嵌套在较大查询中的 SQL 查询。子查询也称为内部查询或内部选择，而包含子查询的语句也称为外部查询或外部选择。\n\n * 子查询可以嵌套在 SELECT，INSERT，UPDATE 或 DELETE 语句内或另一个子查询中。\n\n * 子查询通常会在另一个 SELECT 语句的 WHERE 子句中添加。\n\n * 您可以使用比较运算符，如 >，<，或 =。比较运算符也可以是多行运算符，如 IN，ANY 或 ALL。\n\n * 子查询必须被圆括号 () 括起来。\n\n * 内部查询首先在其父查询之前执行，以便可以将内部查询的结果传递给外部查询。执行过程可以参考下图：\n   \n   \n\n子查询的子查询\n\nSELECT cust_name, cust_contact\nFROM customers\nWHERE cust_id IN (SELECT cust_id\n                  FROM orders\n                  WHERE order_num IN (SELECT order_num\n                                      FROM orderitems\n                                      WHERE prod_id = 'RGAN01'));\n\n\n\n# WHERE\n\n> WHERE 子句用于过滤记录，即缩小访问数据的范围。WHERE 后跟一个返回 true 或 false 的条件。\n> \n> WHERE 可以与 SELECT，UPDATE 和 DELETE 一起使用。\n\n可以在 WHERE 子句中使用的操作符：\n\n运算符       描述\n=         等于\n<>        不等于。注释：在 SQL 的一些版本中，该操作符可被写成 !=\n>         大于\n<         小于\n>=        大于等于\n<=        小于等于\nBETWEEN   在某个范围内\nLIKE      搜索某种模式\nIN        指定针对某个列的多个可能值\n\nSELECT 语句中的 WHERE 子句\n\nSELECT * FROM Customers\nWHERE cust_name = 'Kids Place';\n\n\nUPDATE 语句中的 WHERE 子句\n\nUPDATE Customers\nSET cust_name = 'Jack Jones'\nWHERE cust_name = 'Kids Place';\n\n\nDELETE 语句中的 WHERE 子句\n\nDELETE FROM Customers\nWHERE cust_name = 'Kids Place';\n\n\n\n# IN 和 BETWEEN\n\n> IN 操作符在 WHERE 子句中使用，作用是在指定的几个特定值中任选一个值。\n> \n> BETWEEN 操作符在 WHERE 子句中使用，作用是选取介于某个范围内的值。\n\nIN 示例\n\nSELECT *\nFROM products\nWHERE vend_id IN ('DLL01', 'BRS01');\n\n\nBETWEEN 示例\n\nSELECT *\nFROM products\nWHERE prod_price BETWEEN 3 AND 5;\n\n\n\n# AND、OR、NOT\n\n> AND、OR、NOT 是用于对过滤条件的逻辑处理指令。\n> \n> AND 优先级高于 OR，为了明确处理顺序，可以使用 ()。AND 操作符表示左右条件都要满足。\n> \n> OR 操作符表示左右条件满足任意一个即可。\n> \n> NOT 操作符用于否定一个条件。\n\nAND 示例\n\nSELECT prod_id, prod_name, prod_price\nFROM products\nWHERE vend_id = 'DLL01' AND prod_price <= 4;\n\n\nOR 示例\n\nSELECT prod_id, prod_name, prod_price\nFROM products\nWHERE vend_id = 'DLL01' OR vend_id = 'BRS01';\n\n\nNOT 示例\n\nSELECT *\nFROM products\nWHERE prod_price NOT BETWEEN 3 AND 5;\n\n\n\n# LIKE\n\n> LIKE 操作符在 WHERE 子句中使用，作用是确定字符串是否匹配模式。只有字段是文本值时才使用 LIKE。\n> \n> LIKE 支持两个通配符匹配选项：% 和 _。\n> \n>  * % 表示任何字符出现任意次数。\n>  * _ 表示任何字符出现一次。\n> \n> 不要滥用通配符，通配符位于开头处匹配会非常慢。\n\n% 示例：\n\nSELECT prod_id, prod_name, prod_price\nFROM products\nWHERE prod_name LIKE '%bean bag%';\n\n\n_ 示例：\n\nSELECT prod_id, prod_name, prod_price\nFROM products\nWHERE prod_name LIKE '__ inch teddy bear';\n\n\n\n# 四、连接和组合\n\n\n# 连接（JOIN）\n\n> 连接用于连接多个表，使用 JOIN 关键字，并且条件语句使用 ON 而不是 WHERE。\n\n如果一个 JOIN 至少有一个公共字段并且它们之间存在关系，则该 JOIN 可以在两个或多个表上工作。\n\nJOIN 保持基表（结构和数据）不变。连接可以替换子查询，并且比子查询的效率一般会更快。\n\nJOIN 有两种连接类型：内连接和外连接。\n\n\n\n# 内连接（INNER JOIN）\n\n内连接又称等值连接，使用 INNER JOIN 关键字。在没有条件语句的情况下返回笛卡尔积。\n\nSELECT vend_name, prod_name, prod_price\nFROM vendors INNER JOIN products\nON vendors.vend_id = products.vend_id;\n\n\n# 自连接（=）\n\n自连接可以看成内连接的一种，只是连接的表是自身而已。自然连接是把同名列通过 = 连接起来的，同名列可以有多个。\n\nSELECT c1.cust_id, c1.cust_name, c1.cust_contact\nFROM customers c1, customers c2\nWHERE c1.cust_name = c2.cust_name\nAND c2.cust_contact = 'Jim Jones';\n\n\n# 自然连接（NATURAL JOIN）\n\n内连接提供连接的列，而自然连接自动连接所有同名列。自然连接使用 NATURAL JOIN 关键字。\n\nSELECT *\nFROM Products\nNATURAL JOIN Customers;\n\n\n# 外连接（OUTER JOIN）\n\n外连接返回一个表中的所有行，并且仅返回来自此表中满足连接条件的那些行，即两个表中的列是相等的。外连接分为左外连接、右外连接、全外连接（Mysql 不支持）。\n\n# 左连接（LEFT JOIN）\n\n左外连接就是保留左表没有关联的行。\n\nSELECT customers.cust_id, orders.order_num\nFROM customers LEFT JOIN orders\nON customers.cust_id = orders.cust_id;\n\n\n# 右连接（RIGHT JOIN）\n\n右外连接就是保留右表没有关联的行。\n\nSELECT customers.cust_id, orders.order_num\nFROM customers RIGHT JOIN orders\nON customers.cust_id = orders.cust_id;\n\n\n\n# 组合（UNION）\n\n> UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。\n\nUNION 基本规则：\n\n * 所有查询的列数和列顺序必须相同。\n * 每个查询中涉及表的列的数据类型必须相同或兼容。\n * 通常返回的列名取自第一个查询。\n\n默认会去除相同行，如果需要保留相同行，使用 UNION ALL。\n\n只能包含一个 ORDER BY 子句，并且必须位于语句的最后。\n\n应用场景：\n\n * 在一个查询中从不同的表返回结构数据。\n * 对一个表执行多个查询，按一个查询返回数据。\n\n组合查询示例：\n\nSELECT cust_name, cust_contact, cust_email\nFROM customers\nWHERE cust_state IN ('IL', 'IN', 'MI')\nUNION\nSELECT cust_name, cust_contact, cust_email\nFROM customers\nWHERE cust_name = 'Fun4All';\n\n\n\n# JOIN vs UNION\n\n * JOIN 中连接表的列可能不同，但在 UNION 中，所有查询的列数和列顺序必须相同。\n * UNION 将查询之后的行放在一起（垂直放置），但 JOIN 将查询之后的列放在一起（水平放置），即它构成一个笛卡尔积。\n\n\n# 五、函数\n\n> 🔔 注意：不同数据库的函数往往各不相同，因此不可移植。本节主要以 Mysql 的函数为例。\n\n\n# 文本处理\n\n函数                说明\nLEFT()、RIGHT()    左边或者右边的字符\nLOWER()、UPPER()   转换为小写或者大写\nLTRIM()、RTIM()    去除左边或者右边的空格\nLENGTH()          长度\nSOUNDEX()         转换为语音值\n\n其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。\n\nSELECT *\nFROM mytable\nWHERE SOUNDEX(col1) = SOUNDEX('apple')\n\n\n\n# 日期和时间处理\n\n * 日期格式：YYYY-MM-DD\n * 时间格式：HH:MM:SS\n\n函 数             说 明\nAddDate()       增加一个日期（天、周等）\nAddTime()       增加一个时间（时、分等）\nCurDate()       返回当前日期\nCurTime()       返回当前时间\nDate()          返回日期时间的日期部分\nDateDiff()      计算两个日期之差\nDate_Add()      高度灵活的日期运算函数\nDate_Format()   返回一个格式化的日期或时间串\nDay()           返回一个日期的天数部分\nDayOfWeek()     对于一个日期，返回对应的星期几\nHour()          返回一个时间的小时部分\nMinute()        返回一个时间的分钟部分\nMonth()         返回一个日期的月份部分\nNow()           返回当前日期和时间\nSecond()        返回一个时间的秒部分\nTime()          返回一个日期时间的时间部分\nYear()          返回一个日期的年份部分\n\nmysql> SELECT NOW();\n\n\n2018-4-14 20:25:11\n\n\n\n# 数值处理\n\n函数       说明\nSIN()    正弦\nCOS()    余弦\nTAN()    正切\nABS()    绝对值\nSQRT()   平方根\nMOD()    余数\nEXP()    指数\nPI()     圆周率\nRAND()   随机数\n\n\n# 汇总\n\n函 数       说 明\nAVG()     返回某列的平均值\nCOUNT()   返回某列的行数\nMAX()     返回某列的最大值\nMIN()     返回某列的最小值\nSUM()     返回某列值之和\n\nAVG() 会忽略 NULL 行。\n\n使用 DISTINCT 可以让汇总函数值汇总不同的值。\n\nSELECT AVG(DISTINCT col1) AS avg_col\nFROM mytable\n\n\n\n# 六、排序和分组\n\n\n# ORDER BY\n\n> ORDER BY 用于对结果集进行排序。\n\nORDER BY 有两种排序模式：\n\n * ASC ：升序（默认）\n * DESC ：降序\n\n可以按多个列进行排序，并且为每个列指定不同的排序方式。\n\n指定多个列的排序示例：\n\nSELECT * FROM products\nORDER BY prod_price DESC, prod_name ASC;\n\n\n\n# GROUP BY\n\n> GROUP BY 子句将记录分组到汇总行中，GROUP BY 为每个组返回一个记录。\n\nGROUP BY 可以按一列或多列进行分组。\n\nGROUP BY 通常还涉及聚合函数：COUNT，MAX，SUM，AVG 等。\n\nGROUP BY 按分组字段进行排序后，ORDER BY 可以以汇总字段来进行排序。\n\n分组示例：\n\nSELECT cust_name, COUNT(cust_address) AS addr_num\nFROM Customers GROUP BY cust_name;\n\n\n分组后排序示例：\n\nSELECT cust_name, COUNT(cust_address) AS addr_num\nFROM Customers GROUP BY cust_name\nORDER BY cust_name DESC;\n\n\n\n# HAVING\n\n> HAVING 用于对汇总的 GROUP BY 结果进行过滤。HAVING 要求存在一个 GROUP BY 子句。\n\nWHERE 和 HAVING 可以在相同的查询中。\n\nHAVING vs WHERE：\n\n * WHERE 和 HAVING 都是用于过滤。\n * HAVING 适用于汇总的组记录；而 WHERE 适用于单个记录。\n\n使用 WHERE 和 HAVING 过滤数据示例：\n\nSELECT cust_name, COUNT(*) AS num\nFROM Customers\nWHERE cust_email IS NOT NULL\nGROUP BY cust_name\nHAVING COUNT(*) >= 1;\n\n\n----------------------------------------\n\n（以下为 DDL 语句用法）\n\n\n# 七、数据定义\n\n> DDL 的主要功能是定义数据库对象（如：数据库、数据表、视图、索引等）。\n\n\n# 数据库（DATABASE）\n\n# 创建数据库\n\nCREATE DATABASE test;\n\n\n# 删除数据库\n\nDROP DATABASE test;\n\n\n# 选择数据库\n\nUSE test;\n\n\n\n# 数据表（TABLE）\n\n# 创建数据表\n\n普通创建\n\nCREATE TABLE user (\n  id int(10) unsigned NOT NULL COMMENT 'Id',\n  username varchar(64) NOT NULL DEFAULT 'default' COMMENT '用户名',\n  password varchar(64) NOT NULL DEFAULT 'default' COMMENT '密码',\n  email varchar(64) NOT NULL DEFAULT 'default' COMMENT '邮箱'\n) COMMENT='用户表';\n\n\n根据已有的表创建新表\n\nCREATE TABLE vip_user AS\nSELECT * FROM user;\n\n\n# 删除数据表\n\nDROP TABLE user;\n\n\n# 修改数据表\n\n添加列\n\nALTER TABLE user\nADD age int(3);\n\n\n删除列\n\nALTER TABLE user\nDROP COLUMN age;\n\n\n修改列\n\nALTER TABLE `user`\nMODIFY COLUMN age tinyint;\n\n\n添加主键\n\nALTER TABLE user\nADD PRIMARY KEY (id);\n\n\n删除主键\n\nALTER TABLE user\nDROP PRIMARY KEY;\n\n\n\n# 视图（VIEW）\n\n> 视图是基于 SQL 语句的结果集的可视化的表。视图是虚拟的表，本身不存储数据，也就不能对其进行索引操作。对视图的操作和对普通表的操作一样。\n\n视图的作用：\n\n * 简化复杂的 SQL 操作，比如复杂的连接。\n * 只使用实际表的一部分数据。\n * 通过只给用户访问视图的权限，保证数据的安全性。\n * 更改数据格式和表示。\n\n# 创建视图\n\nCREATE VIEW top_10_user_view AS\nSELECT id, username\nFROM user\nWHERE id < 10;\n\n\n# 删除视图\n\nDROP VIEW top_10_user_view;\n\n\n\n# 索引（INDEX）\n\n> 通过索引可以更加快速高效地查询数据。用户无法看到索引，它们只能被用来加速查询。\n\n更新一个包含索引的表需要比更新一个没有索引的表花费更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。\n\n唯一索引：唯一索引表明此索引的每一个索引值只对应唯一的数据记录。\n\n# 创建索引\n\nCREATE INDEX user_index\nON user (id);\n\n\n# 创建唯一索引\n\nCREATE UNIQUE INDEX user_index\nON user (id);\n\n\n# 删除索引\n\nALTER TABLE user\nDROP INDEX user_index;\n\n\n\n# 约束\n\n> SQL 约束用于规定表中的数据规则。\n\n * 如果存在违反约束的数据行为，行为会被约束终止。\n * 约束可以在创建表时规定（通过 CREATE TABLE 语句），或者在表创建之后规定（通过 ALTER TABLE 语句）。\n * 约束类型\n   * NOT NULL - 指示某列不能存储 NULL 值。\n   * UNIQUE - 保证某列的每行必须有唯一的值。\n   * PRIMARY KEY - NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。\n   * FOREIGN KEY - 保证一个表中的数据匹配另一个表中的值的参照完整性。\n   * CHECK - 保证列中的值符合指定的条件。\n   * DEFAULT - 规定没有给列赋值时的默认值。\n\n创建表时使用约束条件：\n\nCREATE TABLE Users (\n  Id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '自增Id',\n  Username VARCHAR(64) NOT NULL UNIQUE DEFAULT 'default' COMMENT '用户名',\n  Password VARCHAR(64) NOT NULL DEFAULT 'default' COMMENT '密码',\n  Email VARCHAR(64) NOT NULL DEFAULT 'default' COMMENT '邮箱地址',\n  Enabled TINYINT(4) DEFAULT NULL COMMENT '是否有效',\n  PRIMARY KEY (Id)\n) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT='用户表';\n\n\n----------------------------------------\n\n（以下为 TCL 语句用法）\n\n\n# 八、事务处理\n\n不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。\n\nMySQL 默认采用隐式提交策略（autocommit），每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。\n\n通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。\n\n事务处理指令：\n\n * START TRANSACTION - 指令用于标记事务的起始点。\n * SAVEPOINT - 指令用于创建保留点。\n * ROLLBACK TO - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 START TRANSACTION 语句处。\n * COMMIT - 提交事务。\n\n事务处理示例：\n\n-- 开始事务\nSTART TRANSACTION;\n\n-- 插入操作 A\nINSERT INTO `user`\nVALUES (1, 'root1', 'root1', 'xxxx@163.com');\n\n-- 创建保留点 updateA\nSAVEPOINT updateA;\n\n-- 插入操作 B\nINSERT INTO `user`\nVALUES (2, 'root2', 'root2', 'xxxx@163.com');\n\n-- 回滚到保留点 updateA\nROLLBACK TO updateA;\n\n-- 提交事务，只有操作 A 生效\nCOMMIT;\n\n\n----------------------------------------\n\n（以下为 DCL 语句用法）\n\n\n# 九、权限控制\n\nGRANT 和 REVOKE 可在几个层次上控制访问权限：\n\n * 整个服务器，使用 GRANT ALL 和 REVOKE ALL；\n * 整个数据库，使用 ON database.*；\n * 特定的表，使用 ON database.table；\n * 特定的列；\n * 特定的存储过程。\n\n新创建的账户没有任何权限。\n\n账户用 username@host 的形式定义，username@% 使用的是默认主机名。\n\nMySQL 的账户信息保存在 mysql 这个数据库中。\n\nUSE mysql;\nSELECT user FROM user;\n\n\n\n# 创建账户\n\nCREATE USER myuser IDENTIFIED BY 'mypassword';\n\n\n\n# 修改账户名\n\nUPDATE user SET user='newuser' WHERE user='myuser';\nFLUSH PRIVILEGES;\n\n\n\n# 删除账户\n\nDROP USER myuser;\n\n\n\n# 查看权限\n\nSHOW GRANTS FOR myuser;\n\n\n\n# 授予权限\n\nGRANT SELECT, INSERT ON *.* TO myuser;\n\n\n\n# 删除权限\n\nREVOKE SELECT, INSERT ON *.* FROM myuser;\n\n\n\n# 更改密码\n\nSET PASSWORD FOR myuser = 'mypass';\n\n\n\n# 十、存储过程和游标\n\n> 存储过程可以看成是对一系列 SQL 操作的批处理，保存在数据库中。它就像我们编程语言中的函数一样，封装了我们的代码(PLSQL、T-SQL)。\n\n\n# 存储过程优缺点\n\n存储过程的优点：\n\n * 代码封装，保证了一定的安全性。\n * 让编程语言进行调用，提高代码复用。\n * 由于是预先编译，因此具有很高的性能。\n * 一个存储过程替代大量 T_SQL 语句 ，可以降低网络通信量，提高通信速率。\n\n存储过程的缺点：\n\n * 由于不同数据库的存储过程语法几乎都不一样，十分难以维护（不通用）。\n * 业务逻辑放在数据库上，难以迭代。\n\n\n# 使用存储过程\n\n创建存储过程的要点：\n\n * 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。\n * 包含 in、out 和 inout 三种参数。\n * 给变量赋值都需要用 select into 语句。\n * 每次只能给一个变量赋值，不支持集合的操作。\n\n创建存储过程示例：\n\nDROP PROCEDURE IF EXISTS `proc_adder`;\nDELIMITER ;;\nCREATE DEFINER=`root`@`localhost` PROCEDURE `proc_adder`(IN a int, IN b int, OUT sum int)\nBEGIN\n    DECLARE c int;\n    if a is null then set a = 0;\n    end if;\n\n    if b is null then set b = 0;\n    end if;\n\n    set sum  = a + b;\nEND\n;;\nDELIMITER ;\n\n\n使用存储过程示例：\n\nset @b=5;\ncall proc_adder(2,@b,@s);\nselect @s as sum;\n\n\n\n# 游标\n\n> 游标（CURSOR）是一个存储在 DBMS 服务器上的数据库查询，它不是一条 SELECT 语句，而是被该语句检索出来的结果集。在存储过程中使用游标可以对一个结果集进行移动遍历。\n\n游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。\n\n使用游标的四个步骤：\n\n 1. 声明游标，这个过程没有实际检索出数据；\n 2. 打开游标；\n 3. 取出数据；\n 4. 关闭游标；\n\n游标使用示例：\n\nDELIMITER $\nCREATE PROCEDURE getTotal()\nBEGIN\n    DECLARE total INT;\n    -- 创建接收游标数据的变量\n    DECLARE sid INT;\n    DECLARE sname VARCHAR(10);\n    -- 创建总数变量\n    DECLARE sage INT;\n    -- 创建结束标志变量\n    DECLARE done INT DEFAULT false;\n    -- 创建游标\n    DECLARE cur CURSOR FOR SELECT id,name,age from cursor_table where age>30;\n    -- 指定游标循环结束时的返回值\n    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true;\n    SET total = 0;\n    OPEN cur;\n    FETCH cur INTO sid, sname, sage;\n    WHILE(NOT done)\n    DO\n        SET total = total + 1;\n        FETCH cur INTO sid, sname, sage;\n    END WHILE;\n\n    CLOSE cur;\n    SELECT total;\nEND $\nDELIMITER ;\n\n-- 调用存储过程\ncall getTotal();\n\n\n\n# 触发器\n\n> 触发器可以视为一种特殊的存储过程。\n> \n> 触发器是一种与表操作有关的数据库对象，当触发器所在表上出现指定事件时，将调用该对象，即表的操作事件触发表上的触发器的执行。\n\n# 触发器特性\n\n可以使用触发器来进行审计跟踪，把修改记录到另外一张表中。\n\nMySQL 不允许在触发器中使用 CALL 语句 ，也就是不能调用存储过程。\n\nBEGIN 和 END\n\n当触发器的触发条件满足时，将会执行 BEGIN 和 END 之间的触发器执行动作。\n\n> 🔔 注意：在 MySQL 中，分号 ; 是语句结束的标识符，遇到分号表示该段语句已经结束，MySQL 可以开始执行了。因此，解释器遇到触发器执行动作中的分号后就开始执行，然后会报错，因为没有找到和 BEGIN 匹配的 END。\n> \n> 这时就会用到 DELIMITER 命令（DELIMITER 是定界符，分隔符的意思）。它是一条命令，不需要语句结束标识，语法为：DELIMITER new_delemiter。new_delemiter 可以设为 1 个或多个长度的符号，默认的是分号 ;，我们可以把它修改为其他符号，如 $ - DELIMITER $ 。在这之后的语句，以分号结束，解释器不会有什么反应，只有遇到了 $，才认为是语句结束。注意，使用完之后，我们还应该记得把它给修改回来。\n\nNEW 和 OLD\n\n * MySQL 中定义了 NEW 和 OLD 关键字，用来表示触发器的所在表中，触发了触发器的那一行数据。\n * 在 INSERT 型触发器中，NEW 用来表示将要（BEFORE）或已经（AFTER）插入的新数据；\n * 在 UPDATE 型触发器中，OLD 用来表示将要或已经被修改的原数据，NEW 用来表示将要或已经修改为的新数据；\n * 在 DELETE 型触发器中，OLD 用来表示将要或已经被删除的原数据；\n * 使用方法： NEW.columnName （columnName 为相应数据表某一列名）\n\n# 触发器指令\n\n> 提示：为了理解触发器的要点，有必要先了解一下创建触发器的指令。\n\nCREATE TRIGGER 指令用于创建触发器。\n\n语法：\n\nCREATE TRIGGER trigger_name\ntrigger_time\ntrigger_event\nON table_name\nFOR EACH ROW\nBEGIN\n  trigger_statements\nEND;\n\n\n说明：\n\n * trigger_name：触发器名\n * trigger_time: 触发器的触发时机。取值为 BEFORE 或 AFTER。\n * trigger_event: 触发器的监听事件。取值为 INSERT、UPDATE 或 DELETE。\n * table_name: 触发器的监听目标。指定在哪张表上建立触发器。\n * FOR EACH ROW: 行级监视，Mysql 固定写法，其他 DBMS 不同。\n * trigger_statements: 触发器执行动作。是一条或多条 SQL 语句的列表，列表内的每条语句都必须用分号 ; 来结尾。\n\n创建触发器示例：\n\nDELIMITER $\nCREATE TRIGGER `trigger_insert_user`\nAFTER INSERT ON `user`\nFOR EACH ROW\nBEGIN\n    INSERT INTO `user_history`(user_id, operate_type, operate_time)\n    VALUES (NEW.id, 'add a user',  now());\nEND $\nDELIMITER ;\n\n\n查看触发器示例：\n\nSHOW TRIGGERS;\n\n\n删除触发器示例：\n\nDROP TRIGGER IF EXISTS trigger_insert_user;\n\n\n\n# 参考资料\n\n * 《SQL 必知必会》\n * 『浅入深出』MySQL 中事务的实现\n * MySQL 的学习--触发器\n * 维基百科词条 - SQL\n * https://www.sitesbay.com/sql/index\n * SQL Subqueries\n * Quick breakdown of the types of joins\n * SQL UNION\n * SQL database security\n * Mysql 中的存储过程",normalizedContent:"# sql cheat sheet\n\n> 本文针对关系型数据库的基本语法。限于篇幅，本文侧重说明用法，不会展开讲解特性、原理。\n> \n> 本文语法主要针对 mysql，但大部分的语法对其他关系型数据库也适用。\n\n\n\n\n# 一、基本概念\n\n\n# 数据库术语\n\n * 数据库（database） - 保存有组织的数据的容器（通常是一个文件或一组文件）。\n * 数据表（table） - 某种特定类型数据的结构化清单。\n * 模式（schema） - 关于数据库和表的布局及特性的信息。模式定义了数据在表中如何存储，包含存储什么样的数据，数据如何分解，各部分信息如何命名等信息。数据库和表都有模式。\n * 列（column） - 表中的一个字段。所有表都是由一个或多个列组成的。\n * 行（row） - 表中的一个记录。\n * 主键（primary key） - 一列（或一组列），其值能够唯一标识表中每一行。\n\n\n# sql 语法\n\n> sql（structured query language)，标准 sql 由 ansi 标准委员会管理，从而称为 ansi sql。各个 dbms 都有自己的实现，如 pl/sql、transact-sql 等。\n\n# sql 语法结构\n\n\n\nsql 语法结构包括：\n\n * 子句 - 是语句和查询的组成成分。（在某些情况下，这些都是可选的。）\n * 表达式 - 可以产生任何标量值，或由列和行的数据库表\n * 谓词 - 给需要评估的 sql 三值逻辑（3vl）（true/false/unknown）或布尔真值指定条件，并限制语句和查询的效果，或改变程序流程。\n * 查询 - 基于特定条件检索数据。这是 sql 的一个重要组成部分。\n * 语句 - 可以持久地影响纲要和数据，也可以控制数据库事务、程序流程、连接、会话或诊断。\n\n# sql 语法要点\n\n * sql 语句不区分大小写，但是数据库表名、列名和值是否区分，依赖于具体的 dbms 以及配置。\n\n例如：select 与 select 、select 是相同的。\n\n * 多条 sql 语句必须以分号（;）分隔。\n\n * 处理 sql 语句时，所有空格都被忽略。sql 语句可以写成一行，也可以分写为多行。\n\n-- 一行 sql 语句\nupdate user set username='robot', password='robot' where username = 'root';\n\n-- 多行 sql 语句\nupdate user\nset username='robot', password='robot'\nwhere username = 'root';\n\n\n * sql 支持三种注释\n\n## 注释1\n-- 注释2\n/* 注释3 */\n\n\n# sql 分类\n\n# 数据定义语言（ddl）\n\n数据定义语言（data definition language，ddl）是 sql 语言集中负责数据结构定义与数据库对象定义的语言。\n\nddl 的主要功能是定义数据库对象。\n\nddl 的核心指令是 create、alter、drop。\n\n# 数据操纵语言（dml）\n\n数据操纵语言（data manipulation language, dml）是用于数据库操作，对数据库其中的对象和数据运行访问工作的编程语句。\n\ndml 的主要功能是 访问数据，因此其语法都是以读写数据库为主。\n\ndml 的核心指令是 insert、update、delete、select。这四个指令合称 crud(create, read, update, delete)，即增删改查。\n\n# 事务控制语言（tcl）\n\n事务控制语言 (transaction control language, tcl) 用于管理数据库中的事务。这些用于管理由 dml 语句所做的更改。它还允许将语句分组为逻辑事务。\n\ntcl 的核心指令是 commit、rollback。\n\n# 数据控制语言（dcl）\n\n数据控制语言 (data control language, dcl) 是一种可对数据访问权进行控制的指令，它可以控制特定用户账户对数据表、查看表、预存程序、用户自定义函数等数据库对象的控制权。\n\ndcl 的核心指令是 grant、revoke。\n\ndcl 以控制用户的访问权限为主，因此其指令作法并不复杂，可利用 dcl 控制的权限有：connect、select、insert、update、delete、execute、usage、references。\n\n根据不同的 dbms 以及不同的安全性实体，其支持的权限控制也有所不同。\n\n----------------------------------------\n\n（以下为 dml 语句用法）\n\n\n# 二、增删改查\n\n> 增删改查，又称为 crud，数据库基本操作中的基本操作。\n\n\n# 插入数据\n\n>  * insert into 语句用于向表中插入新记录。\n\n插入完整的行\n\ninsert into user\nvalues (10, 'root', 'root', 'xxxx@163.com');\n\n\n插入行的一部分\n\ninsert into user(username, password, email)\nvalues ('admin', 'admin', 'xxxx@163.com');\n\n\n插入查询出来的数据\n\ninsert into user(username)\nselect name\nfrom account;\n\n\n\n# 更新数据\n\n>  * update 语句用于更新表中的记录。\n\nupdate user\nset username='robot', password='robot'\nwhere username = 'root';\n\n\n\n# 删除数据\n\n>  * delete 语句用于删除表中的记录。\n>  * truncate table 可以清空表，也就是删除所有行。\n\n删除表中的指定数据\n\ndelete from user\nwhere username = 'robot';\n\n\n清空表中的数据\n\ntruncate table user;\n\n\n\n# 查询数据\n\n>  * select 语句用于从数据库中查询数据。\n>  * distinct 用于返回唯一不同的值。它作用于所有列，也就是说所有列的值都相同才算相同。\n>  * limit 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。\n>    * asc ：升序（默认）\n>    * desc ：降序\n\n查询单列\n\nselect prod_name\nfrom products;\n\n\n查询多列\n\nselect prod_id, prod_name, prod_price\nfrom products;\n\n\n查询所有列\n\nelect *\nfrom products;\n\n\n查询不同的值\n\nselect distinct\nvend_id from products;\n\n\n限制查询结果\n\n-- 返回前 5 行\nselect * from mytable limit 5;\nselect * from mytable limit 0, 5;\n-- 返回第 3 ~ 5 行\nselect * from mytable limit 2, 3;\n\n\n\n# 三、子查询\n\n> 子查询是嵌套在较大查询中的 sql 查询。子查询也称为内部查询或内部选择，而包含子查询的语句也称为外部查询或外部选择。\n\n * 子查询可以嵌套在 select，insert，update 或 delete 语句内或另一个子查询中。\n\n * 子查询通常会在另一个 select 语句的 where 子句中添加。\n\n * 您可以使用比较运算符，如 >，<，或 =。比较运算符也可以是多行运算符，如 in，any 或 all。\n\n * 子查询必须被圆括号 () 括起来。\n\n * 内部查询首先在其父查询之前执行，以便可以将内部查询的结果传递给外部查询。执行过程可以参考下图：\n   \n   \n\n子查询的子查询\n\nselect cust_name, cust_contact\nfrom customers\nwhere cust_id in (select cust_id\n                  from orders\n                  where order_num in (select order_num\n                                      from orderitems\n                                      where prod_id = 'rgan01'));\n\n\n\n# where\n\n> where 子句用于过滤记录，即缩小访问数据的范围。where 后跟一个返回 true 或 false 的条件。\n> \n> where 可以与 select，update 和 delete 一起使用。\n\n可以在 where 子句中使用的操作符：\n\n运算符       描述\n=         等于\n<>        不等于。注释：在 sql 的一些版本中，该操作符可被写成 !=\n>         大于\n<         小于\n>=        大于等于\n<=        小于等于\nbetween   在某个范围内\nlike      搜索某种模式\nin        指定针对某个列的多个可能值\n\nselect 语句中的 where 子句\n\nselect * from customers\nwhere cust_name = 'kids place';\n\n\nupdate 语句中的 where 子句\n\nupdate customers\nset cust_name = 'jack jones'\nwhere cust_name = 'kids place';\n\n\ndelete 语句中的 where 子句\n\ndelete from customers\nwhere cust_name = 'kids place';\n\n\n\n# in 和 between\n\n> in 操作符在 where 子句中使用，作用是在指定的几个特定值中任选一个值。\n> \n> between 操作符在 where 子句中使用，作用是选取介于某个范围内的值。\n\nin 示例\n\nselect *\nfrom products\nwhere vend_id in ('dll01', 'brs01');\n\n\nbetween 示例\n\nselect *\nfrom products\nwhere prod_price between 3 and 5;\n\n\n\n# and、or、not\n\n> and、or、not 是用于对过滤条件的逻辑处理指令。\n> \n> and 优先级高于 or，为了明确处理顺序，可以使用 ()。and 操作符表示左右条件都要满足。\n> \n> or 操作符表示左右条件满足任意一个即可。\n> \n> not 操作符用于否定一个条件。\n\nand 示例\n\nselect prod_id, prod_name, prod_price\nfrom products\nwhere vend_id = 'dll01' and prod_price <= 4;\n\n\nor 示例\n\nselect prod_id, prod_name, prod_price\nfrom products\nwhere vend_id = 'dll01' or vend_id = 'brs01';\n\n\nnot 示例\n\nselect *\nfrom products\nwhere prod_price not between 3 and 5;\n\n\n\n# like\n\n> like 操作符在 where 子句中使用，作用是确定字符串是否匹配模式。只有字段是文本值时才使用 like。\n> \n> like 支持两个通配符匹配选项：% 和 _。\n> \n>  * % 表示任何字符出现任意次数。\n>  * _ 表示任何字符出现一次。\n> \n> 不要滥用通配符，通配符位于开头处匹配会非常慢。\n\n% 示例：\n\nselect prod_id, prod_name, prod_price\nfrom products\nwhere prod_name like '%bean bag%';\n\n\n_ 示例：\n\nselect prod_id, prod_name, prod_price\nfrom products\nwhere prod_name like '__ inch teddy bear';\n\n\n\n# 四、连接和组合\n\n\n# 连接（join）\n\n> 连接用于连接多个表，使用 join 关键字，并且条件语句使用 on 而不是 where。\n\n如果一个 join 至少有一个公共字段并且它们之间存在关系，则该 join 可以在两个或多个表上工作。\n\njoin 保持基表（结构和数据）不变。连接可以替换子查询，并且比子查询的效率一般会更快。\n\njoin 有两种连接类型：内连接和外连接。\n\n\n\n# 内连接（inner join）\n\n内连接又称等值连接，使用 inner join 关键字。在没有条件语句的情况下返回笛卡尔积。\n\nselect vend_name, prod_name, prod_price\nfrom vendors inner join products\non vendors.vend_id = products.vend_id;\n\n\n# 自连接（=）\n\n自连接可以看成内连接的一种，只是连接的表是自身而已。自然连接是把同名列通过 = 连接起来的，同名列可以有多个。\n\nselect c1.cust_id, c1.cust_name, c1.cust_contact\nfrom customers c1, customers c2\nwhere c1.cust_name = c2.cust_name\nand c2.cust_contact = 'jim jones';\n\n\n# 自然连接（natural join）\n\n内连接提供连接的列，而自然连接自动连接所有同名列。自然连接使用 natural join 关键字。\n\nselect *\nfrom products\nnatural join customers;\n\n\n# 外连接（outer join）\n\n外连接返回一个表中的所有行，并且仅返回来自此表中满足连接条件的那些行，即两个表中的列是相等的。外连接分为左外连接、右外连接、全外连接（mysql 不支持）。\n\n# 左连接（left join）\n\n左外连接就是保留左表没有关联的行。\n\nselect customers.cust_id, orders.order_num\nfrom customers left join orders\non customers.cust_id = orders.cust_id;\n\n\n# 右连接（right join）\n\n右外连接就是保留右表没有关联的行。\n\nselect customers.cust_id, orders.order_num\nfrom customers right join orders\non customers.cust_id = orders.cust_id;\n\n\n\n# 组合（union）\n\n> union 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 union 中参与查询的提取行。\n\nunion 基本规则：\n\n * 所有查询的列数和列顺序必须相同。\n * 每个查询中涉及表的列的数据类型必须相同或兼容。\n * 通常返回的列名取自第一个查询。\n\n默认会去除相同行，如果需要保留相同行，使用 union all。\n\n只能包含一个 order by 子句，并且必须位于语句的最后。\n\n应用场景：\n\n * 在一个查询中从不同的表返回结构数据。\n * 对一个表执行多个查询，按一个查询返回数据。\n\n组合查询示例：\n\nselect cust_name, cust_contact, cust_email\nfrom customers\nwhere cust_state in ('il', 'in', 'mi')\nunion\nselect cust_name, cust_contact, cust_email\nfrom customers\nwhere cust_name = 'fun4all';\n\n\n\n# join vs union\n\n * join 中连接表的列可能不同，但在 union 中，所有查询的列数和列顺序必须相同。\n * union 将查询之后的行放在一起（垂直放置），但 join 将查询之后的列放在一起（水平放置），即它构成一个笛卡尔积。\n\n\n# 五、函数\n\n> 🔔 注意：不同数据库的函数往往各不相同，因此不可移植。本节主要以 mysql 的函数为例。\n\n\n# 文本处理\n\n函数                说明\nleft()、right()    左边或者右边的字符\nlower()、upper()   转换为小写或者大写\nltrim()、rtim()    去除左边或者右边的空格\nlength()          长度\nsoundex()         转换为语音值\n\n其中， soundex() 可以将一个字符串转换为描述其语音表示的字母数字模式。\n\nselect *\nfrom mytable\nwhere soundex(col1) = soundex('apple')\n\n\n\n# 日期和时间处理\n\n * 日期格式：yyyy-mm-dd\n * 时间格式：hh:mm:ss\n\n函 数             说 明\nadddate()       增加一个日期（天、周等）\naddtime()       增加一个时间（时、分等）\ncurdate()       返回当前日期\ncurtime()       返回当前时间\ndate()          返回日期时间的日期部分\ndatediff()      计算两个日期之差\ndate_add()      高度灵活的日期运算函数\ndate_format()   返回一个格式化的日期或时间串\nday()           返回一个日期的天数部分\ndayofweek()     对于一个日期，返回对应的星期几\nhour()          返回一个时间的小时部分\nminute()        返回一个时间的分钟部分\nmonth()         返回一个日期的月份部分\nnow()           返回当前日期和时间\nsecond()        返回一个时间的秒部分\ntime()          返回一个日期时间的时间部分\nyear()          返回一个日期的年份部分\n\nmysql> select now();\n\n\n2018-4-14 20:25:11\n\n\n\n# 数值处理\n\n函数       说明\nsin()    正弦\ncos()    余弦\ntan()    正切\nabs()    绝对值\nsqrt()   平方根\nmod()    余数\nexp()    指数\npi()     圆周率\nrand()   随机数\n\n\n# 汇总\n\n函 数       说 明\navg()     返回某列的平均值\ncount()   返回某列的行数\nmax()     返回某列的最大值\nmin()     返回某列的最小值\nsum()     返回某列值之和\n\navg() 会忽略 null 行。\n\n使用 distinct 可以让汇总函数值汇总不同的值。\n\nselect avg(distinct col1) as avg_col\nfrom mytable\n\n\n\n# 六、排序和分组\n\n\n# order by\n\n> order by 用于对结果集进行排序。\n\norder by 有两种排序模式：\n\n * asc ：升序（默认）\n * desc ：降序\n\n可以按多个列进行排序，并且为每个列指定不同的排序方式。\n\n指定多个列的排序示例：\n\nselect * from products\norder by prod_price desc, prod_name asc;\n\n\n\n# group by\n\n> group by 子句将记录分组到汇总行中，group by 为每个组返回一个记录。\n\ngroup by 可以按一列或多列进行分组。\n\ngroup by 通常还涉及聚合函数：count，max，sum，avg 等。\n\ngroup by 按分组字段进行排序后，order by 可以以汇总字段来进行排序。\n\n分组示例：\n\nselect cust_name, count(cust_address) as addr_num\nfrom customers group by cust_name;\n\n\n分组后排序示例：\n\nselect cust_name, count(cust_address) as addr_num\nfrom customers group by cust_name\norder by cust_name desc;\n\n\n\n# having\n\n> having 用于对汇总的 group by 结果进行过滤。having 要求存在一个 group by 子句。\n\nwhere 和 having 可以在相同的查询中。\n\nhaving vs where：\n\n * where 和 having 都是用于过滤。\n * having 适用于汇总的组记录；而 where 适用于单个记录。\n\n使用 where 和 having 过滤数据示例：\n\nselect cust_name, count(*) as num\nfrom customers\nwhere cust_email is not null\ngroup by cust_name\nhaving count(*) >= 1;\n\n\n----------------------------------------\n\n（以下为 ddl 语句用法）\n\n\n# 七、数据定义\n\n> ddl 的主要功能是定义数据库对象（如：数据库、数据表、视图、索引等）。\n\n\n# 数据库（database）\n\n# 创建数据库\n\ncreate database test;\n\n\n# 删除数据库\n\ndrop database test;\n\n\n# 选择数据库\n\nuse test;\n\n\n\n# 数据表（table）\n\n# 创建数据表\n\n普通创建\n\ncreate table user (\n  id int(10) unsigned not null comment 'id',\n  username varchar(64) not null default 'default' comment '用户名',\n  password varchar(64) not null default 'default' comment '密码',\n  email varchar(64) not null default 'default' comment '邮箱'\n) comment='用户表';\n\n\n根据已有的表创建新表\n\ncreate table vip_user as\nselect * from user;\n\n\n# 删除数据表\n\ndrop table user;\n\n\n# 修改数据表\n\n添加列\n\nalter table user\nadd age int(3);\n\n\n删除列\n\nalter table user\ndrop column age;\n\n\n修改列\n\nalter table `user`\nmodify column age tinyint;\n\n\n添加主键\n\nalter table user\nadd primary key (id);\n\n\n删除主键\n\nalter table user\ndrop primary key;\n\n\n\n# 视图（view）\n\n> 视图是基于 sql 语句的结果集的可视化的表。视图是虚拟的表，本身不存储数据，也就不能对其进行索引操作。对视图的操作和对普通表的操作一样。\n\n视图的作用：\n\n * 简化复杂的 sql 操作，比如复杂的连接。\n * 只使用实际表的一部分数据。\n * 通过只给用户访问视图的权限，保证数据的安全性。\n * 更改数据格式和表示。\n\n# 创建视图\n\ncreate view top_10_user_view as\nselect id, username\nfrom user\nwhere id < 10;\n\n\n# 删除视图\n\ndrop view top_10_user_view;\n\n\n\n# 索引（index）\n\n> 通过索引可以更加快速高效地查询数据。用户无法看到索引，它们只能被用来加速查询。\n\n更新一个包含索引的表需要比更新一个没有索引的表花费更多的时间，这是由于索引本身也需要更新。因此，理想的做法是仅仅在常常被搜索的列（以及表）上面创建索引。\n\n唯一索引：唯一索引表明此索引的每一个索引值只对应唯一的数据记录。\n\n# 创建索引\n\ncreate index user_index\non user (id);\n\n\n# 创建唯一索引\n\ncreate unique index user_index\non user (id);\n\n\n# 删除索引\n\nalter table user\ndrop index user_index;\n\n\n\n# 约束\n\n> sql 约束用于规定表中的数据规则。\n\n * 如果存在违反约束的数据行为，行为会被约束终止。\n * 约束可以在创建表时规定（通过 create table 语句），或者在表创建之后规定（通过 alter table 语句）。\n * 约束类型\n   * not null - 指示某列不能存储 null 值。\n   * unique - 保证某列的每行必须有唯一的值。\n   * primary key - not null 和 unique 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。\n   * foreign key - 保证一个表中的数据匹配另一个表中的值的参照完整性。\n   * check - 保证列中的值符合指定的条件。\n   * default - 规定没有给列赋值时的默认值。\n\n创建表时使用约束条件：\n\ncreate table users (\n  id int(10) unsigned not null auto_increment comment '自增id',\n  username varchar(64) not null unique default 'default' comment '用户名',\n  password varchar(64) not null default 'default' comment '密码',\n  email varchar(64) not null default 'default' comment '邮箱地址',\n  enabled tinyint(4) default null comment '是否有效',\n  primary key (id)\n) engine=innodb auto_increment=2 default charset=utf8mb4 comment='用户表';\n\n\n----------------------------------------\n\n（以下为 tcl 语句用法）\n\n\n# 八、事务处理\n\n不能回退 select 语句，回退 select 语句也没意义；也不能回退 create 和 drop 语句。\n\nmysql 默认采用隐式提交策略（autocommit），每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 start transaction 语句时，会关闭隐式提交；当 commit 或 rollback 语句执行后，事务会自动关闭，重新恢复隐式提交。\n\n通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。\n\n事务处理指令：\n\n * start transaction - 指令用于标记事务的起始点。\n * savepoint - 指令用于创建保留点。\n * rollback to - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 start transaction 语句处。\n * commit - 提交事务。\n\n事务处理示例：\n\n-- 开始事务\nstart transaction;\n\n-- 插入操作 a\ninsert into `user`\nvalues (1, 'root1', 'root1', 'xxxx@163.com');\n\n-- 创建保留点 updatea\nsavepoint updatea;\n\n-- 插入操作 b\ninsert into `user`\nvalues (2, 'root2', 'root2', 'xxxx@163.com');\n\n-- 回滚到保留点 updatea\nrollback to updatea;\n\n-- 提交事务，只有操作 a 生效\ncommit;\n\n\n----------------------------------------\n\n（以下为 dcl 语句用法）\n\n\n# 九、权限控制\n\ngrant 和 revoke 可在几个层次上控制访问权限：\n\n * 整个服务器，使用 grant all 和 revoke all；\n * 整个数据库，使用 on database.*；\n * 特定的表，使用 on database.table；\n * 特定的列；\n * 特定的存储过程。\n\n新创建的账户没有任何权限。\n\n账户用 username@host 的形式定义，username@% 使用的是默认主机名。\n\nmysql 的账户信息保存在 mysql 这个数据库中。\n\nuse mysql;\nselect user from user;\n\n\n\n# 创建账户\n\ncreate user myuser identified by 'mypassword';\n\n\n\n# 修改账户名\n\nupdate user set user='newuser' where user='myuser';\nflush privileges;\n\n\n\n# 删除账户\n\ndrop user myuser;\n\n\n\n# 查看权限\n\nshow grants for myuser;\n\n\n\n# 授予权限\n\ngrant select, insert on *.* to myuser;\n\n\n\n# 删除权限\n\nrevoke select, insert on *.* from myuser;\n\n\n\n# 更改密码\n\nset password for myuser = 'mypass';\n\n\n\n# 十、存储过程和游标\n\n> 存储过程可以看成是对一系列 sql 操作的批处理，保存在数据库中。它就像我们编程语言中的函数一样，封装了我们的代码(plsql、t-sql)。\n\n\n# 存储过程优缺点\n\n存储过程的优点：\n\n * 代码封装，保证了一定的安全性。\n * 让编程语言进行调用，提高代码复用。\n * 由于是预先编译，因此具有很高的性能。\n * 一个存储过程替代大量 t_sql 语句 ，可以降低网络通信量，提高通信速率。\n\n存储过程的缺点：\n\n * 由于不同数据库的存储过程语法几乎都不一样，十分难以维护（不通用）。\n * 业务逻辑放在数据库上，难以迭代。\n\n\n# 使用存储过程\n\n创建存储过程的要点：\n\n * 命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。\n * 包含 in、out 和 inout 三种参数。\n * 给变量赋值都需要用 select into 语句。\n * 每次只能给一个变量赋值，不支持集合的操作。\n\n创建存储过程示例：\n\ndrop procedure if exists `proc_adder`;\ndelimiter ;;\ncreate definer=`root`@`localhost` procedure `proc_adder`(in a int, in b int, out sum int)\nbegin\n    declare c int;\n    if a is null then set a = 0;\n    end if;\n\n    if b is null then set b = 0;\n    end if;\n\n    set sum  = a + b;\nend\n;;\ndelimiter ;\n\n\n使用存储过程示例：\n\nset @b=5;\ncall proc_adder(2,@b,@s);\nselect @s as sum;\n\n\n\n# 游标\n\n> 游标（cursor）是一个存储在 dbms 服务器上的数据库查询，它不是一条 select 语句，而是被该语句检索出来的结果集。在存储过程中使用游标可以对一个结果集进行移动遍历。\n\n游标主要用于交互式应用，其中用户需要对数据集中的任意行进行浏览和修改。\n\n使用游标的四个步骤：\n\n 1. 声明游标，这个过程没有实际检索出数据；\n 2. 打开游标；\n 3. 取出数据；\n 4. 关闭游标；\n\n游标使用示例：\n\ndelimiter $\ncreate procedure gettotal()\nbegin\n    declare total int;\n    -- 创建接收游标数据的变量\n    declare sid int;\n    declare sname varchar(10);\n    -- 创建总数变量\n    declare sage int;\n    -- 创建结束标志变量\n    declare done int default false;\n    -- 创建游标\n    declare cur cursor for select id,name,age from cursor_table where age>30;\n    -- 指定游标循环结束时的返回值\n    declare continue handler for not found set done = true;\n    set total = 0;\n    open cur;\n    fetch cur into sid, sname, sage;\n    while(not done)\n    do\n        set total = total + 1;\n        fetch cur into sid, sname, sage;\n    end while;\n\n    close cur;\n    select total;\nend $\ndelimiter ;\n\n-- 调用存储过程\ncall gettotal();\n\n\n\n# 触发器\n\n> 触发器可以视为一种特殊的存储过程。\n> \n> 触发器是一种与表操作有关的数据库对象，当触发器所在表上出现指定事件时，将调用该对象，即表的操作事件触发表上的触发器的执行。\n\n# 触发器特性\n\n可以使用触发器来进行审计跟踪，把修改记录到另外一张表中。\n\nmysql 不允许在触发器中使用 call 语句 ，也就是不能调用存储过程。\n\nbegin 和 end\n\n当触发器的触发条件满足时，将会执行 begin 和 end 之间的触发器执行动作。\n\n> 🔔 注意：在 mysql 中，分号 ; 是语句结束的标识符，遇到分号表示该段语句已经结束，mysql 可以开始执行了。因此，解释器遇到触发器执行动作中的分号后就开始执行，然后会报错，因为没有找到和 begin 匹配的 end。\n> \n> 这时就会用到 delimiter 命令（delimiter 是定界符，分隔符的意思）。它是一条命令，不需要语句结束标识，语法为：delimiter new_delemiter。new_delemiter 可以设为 1 个或多个长度的符号，默认的是分号 ;，我们可以把它修改为其他符号，如 $ - delimiter $ 。在这之后的语句，以分号结束，解释器不会有什么反应，只有遇到了 $，才认为是语句结束。注意，使用完之后，我们还应该记得把它给修改回来。\n\nnew 和 old\n\n * mysql 中定义了 new 和 old 关键字，用来表示触发器的所在表中，触发了触发器的那一行数据。\n * 在 insert 型触发器中，new 用来表示将要（before）或已经（after）插入的新数据；\n * 在 update 型触发器中，old 用来表示将要或已经被修改的原数据，new 用来表示将要或已经修改为的新数据；\n * 在 delete 型触发器中，old 用来表示将要或已经被删除的原数据；\n * 使用方法： new.columnname （columnname 为相应数据表某一列名）\n\n# 触发器指令\n\n> 提示：为了理解触发器的要点，有必要先了解一下创建触发器的指令。\n\ncreate trigger 指令用于创建触发器。\n\n语法：\n\ncreate trigger trigger_name\ntrigger_time\ntrigger_event\non table_name\nfor each row\nbegin\n  trigger_statements\nend;\n\n\n说明：\n\n * trigger_name：触发器名\n * trigger_time: 触发器的触发时机。取值为 before 或 after。\n * trigger_event: 触发器的监听事件。取值为 insert、update 或 delete。\n * table_name: 触发器的监听目标。指定在哪张表上建立触发器。\n * for each row: 行级监视，mysql 固定写法，其他 dbms 不同。\n * trigger_statements: 触发器执行动作。是一条或多条 sql 语句的列表，列表内的每条语句都必须用分号 ; 来结尾。\n\n创建触发器示例：\n\ndelimiter $\ncreate trigger `trigger_insert_user`\nafter insert on `user`\nfor each row\nbegin\n    insert into `user_history`(user_id, operate_type, operate_time)\n    values (new.id, 'add a user',  now());\nend $\ndelimiter ;\n\n\n查看触发器示例：\n\nshow triggers;\n\n\n删除触发器示例：\n\ndrop trigger if exists trigger_insert_user;\n\n\n\n# 参考资料\n\n * 《sql 必知必会》\n * 『浅入深出』mysql 中事务的实现\n * mysql 的学习--触发器\n * 维基百科词条 - sql\n * https://www.sitesbay.com/sql/index\n * sql subqueries\n * quick breakdown of the types of joins\n * sql union\n * sql database security\n * mysql 中的存储过程",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"扩展 SQL",frontmatter:{title:"扩展 SQL",date:"2020-10-10T19:03:05.000Z",categories:["数据库","关系型数据库","综合"],tags:["数据库","关系型数据库","SQL"],permalink:"/pages/55e9a7/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E7%BB%BC%E5%90%88/03.%E6%89%A9%E5%B1%95SQL.html",relativePath:"12.数据库/03.关系型数据库/01.综合/03.扩展SQL.md",key:"v-49d5bdfa",path:"/pages/55e9a7/",headers:[{level:2,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:66},{level:2,title:"表",slug:"表",normalizedTitle:"表",charIndex:74},{level:3,title:"查看表的基本信息",slug:"查看表的基本信息",normalizedTitle:"查看表的基本信息",charIndex:80},{level:3,title:"查看表的列信息",slug:"查看表的列信息",normalizedTitle:"查看表的列信息",charIndex:188},{level:3,title:"如何批量删除大量数据",slug:"如何批量删除大量数据",normalizedTitle:"如何批量删除大量数据",charIndex:296},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:688}],headersStr:"数据库 表 查看表的基本信息 查看表的列信息 如何批量删除大量数据 参考资料",content:"# 扩展 SQL\n\n> 不同于 SQL Cheat Sheet 中的一般语法，本文主要整理收集一些高级但是很有用的 SQL\n\n\n# 数据库\n\n\n# 表\n\n\n# 查看表的基本信息\n\nSELECT * FROM information_schema.tables\nWHERE table_schema = 'test' AND table_name = 'user';\n\n\n\n# 查看表的列信息\n\nSELECT * FROM information_schema.columns\nWHERE table_schema = 'test' AND table_name = 'user';\n\n\n\n# 如何批量删除大量数据\n\n如果要根据时间范围批量删除大量数据，最简单的语句如下：\n\ndelete from orders\nwhere timestamp < SUBDATE(CURDATE(),INTERVAL 3 month);\n\n\n上面的语句，大概率执行会报错，提示删除失败，因为需要删除的数据量太大了，所以需要分批删除。\n\n可以先通过一次查询，找到符合条件的历史订单中最大的那个订单 ID，然后在删除语句中把删除的条件转换成按主键删除。\n\nselect max(id) from orders\nwhere timestamp < SUBDATE(CURDATE(),INTERVAL 3 month);\n\n-- 分批删除，? 填上一条语句查到的最大 ID\ndelete from orders\nwhere id <= ?\norder by id limit 1000;\n\n\n\n# 参考资料\n\n * 《SQL 必知必会》",normalizedContent:"# 扩展 sql\n\n> 不同于 sql cheat sheet 中的一般语法，本文主要整理收集一些高级但是很有用的 sql\n\n\n# 数据库\n\n\n# 表\n\n\n# 查看表的基本信息\n\nselect * from information_schema.tables\nwhere table_schema = 'test' and table_name = 'user';\n\n\n\n# 查看表的列信息\n\nselect * from information_schema.columns\nwhere table_schema = 'test' and table_name = 'user';\n\n\n\n# 如何批量删除大量数据\n\n如果要根据时间范围批量删除大量数据，最简单的语句如下：\n\ndelete from orders\nwhere timestamp < subdate(curdate(),interval 3 month);\n\n\n上面的语句，大概率执行会报错，提示删除失败，因为需要删除的数据量太大了，所以需要分批删除。\n\n可以先通过一次查询，找到符合条件的历史订单中最大的那个订单 id，然后在删除语句中把删除的条件转换成按主键删除。\n\nselect max(id) from orders\nwhere timestamp < subdate(curdate(),interval 3 month);\n\n-- 分批删除，? 填上一条语句查到的最大 id\ndelete from orders\nwhere id <= ?\norder by id limit 1000;\n\n\n\n# 参考资料\n\n * 《sql 必知必会》",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"关系型数据库综合知识",frontmatter:{title:"关系型数据库综合知识",date:"2020-07-16T11:14:07.000Z",categories:["数据库","关系型数据库","综合"],tags:["数据库","关系型数据库"],permalink:"/pages/22f2e3/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/01.%E7%BB%BC%E5%90%88/",relativePath:"12.数据库/03.关系型数据库/01.综合/README.md",key:"v-0752b95e",path:"/pages/22f2e3/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:17},{level:3,title:"关系型数据库面试题 💯",slug:"关系型数据库面试题-💯",normalizedTitle:"关系型数据库面试题 💯",charIndex:27},{level:3,title:"SQL Cheat Sheet",slug:"sql-cheat-sheet",normalizedTitle:"sql cheat sheet",charIndex:44},{level:3,title:"分库分表基本原理",slug:"分库分表基本原理",normalizedTitle:"分库分表基本原理",charIndex:66},{level:3,title:"分布式事务基本原理",slug:"分布式事务基本原理",normalizedTitle:"分布式事务基本原理",charIndex:81},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:95},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:315}],headersStr:"📖 内容 关系型数据库面试题 💯 SQL Cheat Sheet 分库分表基本原理 分布式事务基本原理 📚 资料 🚪 传送",content:"# 关系型数据库综合知识\n\n\n# 📖 内容\n\n\n# 关系型数据库面试题 💯\n\n\n# SQL Cheat Sheet\n\n\n\n\n# 分库分表基本原理\n\n\n\n\n# 分布式事务基本原理\n\n\n# 📚 资料\n\n * 官方\n   * Mysql 官网\n   * Mysql 官方文档\n   * Mysql 官方文档之命令行客户端\n * 书籍\n   * 《高性能 MySQL》 - Mysql 经典\n   * 《SQL 必知必会》 - SQL 入门\n * 教程\n   * runoob.com MySQL 教程 - 入门级 SQL 教程\n   * mysql-tutorial\n * 更多资源\n   * awesome-mysql\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# 关系型数据库综合知识\n\n\n# 📖 内容\n\n\n# 关系型数据库面试题 💯\n\n\n# sql cheat sheet\n\n\n\n\n# 分库分表基本原理\n\n\n\n\n# 分布式事务基本原理\n\n\n# 📚 资料\n\n * 官方\n   * mysql 官网\n   * mysql 官方文档\n   * mysql 官方文档之命令行客户端\n * 书籍\n   * 《高性能 mysql》 - mysql 经典\n   * 《sql 必知必会》 - sql 入门\n * 教程\n   * runoob.com mysql 教程 - 入门级 sql 教程\n   * mysql-tutorial\n * 更多资源\n   * awesome-mysql\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 应用指南",frontmatter:{title:"Mysql 应用指南",date:"2020-07-13T10:08:37.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql"],permalink:"/pages/5fe0f3/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/01.Mysql%E5%BA%94%E7%94%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/01.Mysql应用指南.md",key:"v-b19bf61e",path:"/pages/5fe0f3/",headers:[{level:2,title:"1. SQL 执行过程",slug:"_1-sql-执行过程",normalizedTitle:"1. sql 执行过程",charIndex:17},{level:2,title:"2. 存储引擎",slug:"_2-存储引擎",normalizedTitle:"2. 存储引擎",charIndex:82},{level:3,title:"2.1. 选择存储引擎",slug:"_2-1-选择存储引擎",normalizedTitle:"2.1. 选择存储引擎",charIndex:313},{level:4,title:"Mysql 内置的存储引擎",slug:"mysql-内置的存储引擎",normalizedTitle:"mysql 内置的存储引擎",charIndex:328},{level:4,title:"如何选择合适的存储引擎",slug:"如何选择合适的存储引擎",normalizedTitle:"如何选择合适的存储引擎",charIndex:2358},{level:4,title:"转换表的存储引擎",slug:"转换表的存储引擎",normalizedTitle:"转换表的存储引擎",charIndex:2787},{level:3,title:"2.2. MyISAM",slug:"_2-2-myisam",normalizedTitle:"2.2. myisam",charIndex:2871},{level:3,title:"2.3. InnoDB",slug:"_2-3-innodb",normalizedTitle:"2.3. innodb",charIndex:3078},{level:2,title:"3. 数据类型",slug:"_3-数据类型",normalizedTitle:"3. 数据类型",charIndex:3653},{level:3,title:"3.1. 整型",slug:"_3-1-整型",normalizedTitle:"3.1. 整型",charIndex:3665},{level:3,title:"3.2. 浮点型",slug:"_3-2-浮点型",normalizedTitle:"3.2. 浮点型",charIndex:3843},{level:3,title:"3.3. 字符串",slug:"_3-3-字符串",normalizedTitle:"3.3. 字符串",charIndex:4063},{level:3,title:"3.4. 时间和日期",slug:"_3-4-时间和日期",normalizedTitle:"3.4. 时间和日期",charIndex:4279},{level:4,title:"DATATIME",slug:"datatime",normalizedTitle:"datatime",charIndex:4312},{level:4,title:"TIMESTAMP",slug:"timestamp",normalizedTitle:"timestamp",charIndex:4323},{level:3,title:"3.5. BLOB 和 TEXT",slug:"_3-5-blob-和-text",normalizedTitle:"3.5. blob 和 text",charIndex:4796},{level:3,title:"3.6. 枚举类型",slug:"_3-6-枚举类型",normalizedTitle:"3.6. 枚举类型",charIndex:4898},{level:3,title:"3.7. 类型的选择",slug:"_3-7-类型的选择",normalizedTitle:"3.7. 类型的选择",charIndex:5006},{level:2,title:"4. 索引",slug:"_4-索引",normalizedTitle:"4. 索引",charIndex:5346},{level:2,title:"5. 锁",slug:"_5-锁",normalizedTitle:"5. 锁",charIndex:5371},{level:2,title:"6. 事务",slug:"_6-事务",normalizedTitle:"6. 事务",charIndex:5394},{level:2,title:"7. 性能优化",slug:"_7-性能优化",normalizedTitle:"7. 性能优化",charIndex:5419},{level:2,title:"8. 复制",slug:"_8-复制",normalizedTitle:"8. 复制",charIndex:5448},{level:3,title:"8.1. 主从复制",slug:"_8-1-主从复制",normalizedTitle:"8.1. 主从复制",charIndex:5458},{level:3,title:"8.2. 读写分离",slug:"_8-2-读写分离",normalizedTitle:"8.2. 读写分离",charIndex:5738},{level:2,title:'<img src="https://raw.githubusercontent.com/dunwu/images/dev/cs/database/mysql/master-slave-proxy.png" />',slug:"div-align-center-img-src-https-raw-githubusercontent-com-dunwu-images-dev-cs-database-mysql-master-slave-proxy-png-div",normalizedTitle:'<img src="https://raw.githubusercontent.com/dunwu/images/dev/cs/database/mysql/master-slave-proxy.png" />',charIndex:null},{level:2,title:"9. 分布式事务",slug:"_9-分布式事务",normalizedTitle:"9. 分布式事务",charIndex:5979},{level:2,title:"10. 分库分表",slug:"_10-分库分表",normalizedTitle:"10. 分库分表",charIndex:6008},{level:2,title:"11. 参考资料",slug:"_11-参考资料",normalizedTitle:"11. 参考资料",charIndex:6036},{level:2,title:"12. 传送门",slug:"_12-传送门",normalizedTitle:"12. 传送门",charIndex:6182}],headersStr:'1. SQL 执行过程 2. 存储引擎 2.1. 选择存储引擎 Mysql 内置的存储引擎 如何选择合适的存储引擎 转换表的存储引擎 2.2. MyISAM 2.3. InnoDB 3. 数据类型 3.1. 整型 3.2. 浮点型 3.3. 字符串 3.4. 时间和日期 DATATIME TIMESTAMP 3.5. BLOB 和 TEXT 3.6. 枚举类型 3.7. 类型的选择 4. 索引 5. 锁 6. 事务 7. 性能优化 8. 复制 8.1. 主从复制 8.2. 读写分离 <img src="https://raw.githubusercontent.com/dunwu/images/dev/cs/database/mysql/master-slave-proxy.png" /> 9. 分布式事务 10. 分库分表 11. 参考资料 12. 传送门',content:"# Mysql 应用指南\n\n\n# 1. SQL 执行过程\n\n学习 Mysql，最好是先从宏观上了解 Mysql 工作原理。\n\n> 参考：Mysql 工作流\n\n\n# 2. 存储引擎\n\n在文件系统中，Mysql 将每个数据库（也可以成为 schema）保存为数据目录下的一个子目录。创建表示，Mysql 会在数据库子目录下创建一个和表同名的 .frm 文件保存表的定义。因为 Mysql 使用文件系统的目录和文件来保存数据库和表的定义，大小写敏感性和具体平台密切相关。Windows 中大小写不敏感；类 Unix 中大小写敏感。不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在 Mysql 服务层统一处理的。\n\n\n# 2.1. 选择存储引擎\n\n# Mysql 内置的存储引擎\n\nmysql> SHOW ENGINES;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine             | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| FEDERATED          | NO      | Federated MySQL storage engine                                 | NULL         | NULL | NULL       |\n| MEMORY             | YES     | Hash based, stored in memory, useful for temporary tables      | NO           | NO   | NO         |\n| InnoDB             | DEFAULT | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n| PERFORMANCE_SCHEMA | YES     | Performance Schema                                             | NO           | NO   | NO         |\n| MyISAM             | YES     | MyISAM storage engine                                          | NO           | NO   | NO         |\n| MRG_MYISAM         | YES     | Collection of identical MyISAM tables                          | NO           | NO   | NO         |\n| BLACKHOLE          | YES     | /dev/null storage engine (anything you write to it disappears) | NO           | NO   | NO         |\n| CSV                | YES     | CSV storage engine                                             | NO           | NO   | NO         |\n| ARCHIVE            | YES     | Archive storage engine                                         | NO           | NO   | NO         |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n9 rows in set (0.00 sec)\n\n\n * InnoDB - Mysql 的默认事务型存储引擎，并且提供了行级锁和外键的约束。性能不错且支持自动崩溃恢复。\n * MyISAM - Mysql 5.1 版本前的默认存储引擎。特性丰富但不支持事务，也不支持行级锁和外键，也没有崩溃恢复功能。\n * CSV - 可以将 CSV 文件作为 Mysql 的表来处理，但这种表不支持索引。\n * Memory - 适合快速访问数据，且数据不会被修改，重启丢失也没有关系。\n * NDB - 用于 Mysql 集群场景。\n\n# 如何选择合适的存储引擎\n\n大多数情况下，InnoDB 都是正确的选择，除非需要用到 InnoDB 不具备的特性。\n\n如果应用需要选择 InnoDB 以外的存储引擎，可以考虑以下因素：\n\n * 事务：如果需要支持事务，InnoDB 是首选。如果不需要支持事务，且主要是 SELECT 和 INSERT 操作，MyISAM 是不错的选择。所以，如果 Mysql 部署方式为主备模式，并进行读写分离。那么可以这么做：主节点只支持写操作，默认引擎为 InnoDB；备节点只支持读操作，默认引擎为 MyISAM。\n * 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。所以，InnoDB 并发性能更高。\n * 外键：InnoDB 支持外键。\n * 备份：InnoDB 支持在线热备份。\n * 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。\n * 其它特性：MyISAM 支持压缩表和空间数据索引。\n\n# 转换表的存储引擎\n\n下面的语句可以将 mytable 表的引擎修改为 InnoDB\n\nALTER TABLE mytable ENGINE = InnoDB\n\n\n\n# 2.2. MyISAM\n\nMyISAM 设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用 MyISAM。\n\nMyISAM 引擎使用 B+Tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址。\n\nMyISAM 提供了大量的特性，包括：全文索引、压缩表、空间函数等。但是，MyISAM 不支持事务和行级锁。并且 MyISAM 不支持崩溃后的安全恢复。\n\n\n# 2.3. InnoDB\n\nInnoDB 是 MySQL 默认的事务型存储引擎，只有在需要 InnoDB 不支持的特性时，才考虑使用其它存储引擎。\n\n然 InnoDB 也使用 B+Tree 作为索引结构，但具体实现方式却与 MyISAM 截然不同。MyISAM 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在 InnoDB 中，表数据文件本身就是按 B+Tree 组织的一个索引结构，这棵树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此InnoDB 表数据文件本身就是主索引。\n\nInnoDB 采用 MVCC 来支持高并发，并且实现了四个标准的隔离级别。其默认级别是可重复读（REPEATABLE READ），并且通过间隙锁（next-key locking）防止幻读。\n\nInnoDB 是基于聚簇索引建立的，与其他存储引擎有很大不同。在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。\n\n内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。\n\n支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。\n\n\n# 3. 数据类型\n\n\n# 3.1. 整型\n\nTINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。\n\nUNSIGNED 表示不允许负值，大致可以使正数的上限提高一倍。\n\nINT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。\n\n\n# 3.2. 浮点型\n\nFLOAT 和 DOUBLE 为浮点类型。\n\nDECIMAL 类型主要用于精确计算，代价较高，应该尽量只在对小数进行精确计算时才使用 DECIMAL ——例如存储财务数据。数据量比较大的时候，可以使用 BIGINT 代替 DECIMAL。\n\nFLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。\n\n\n# 3.3. 字符串\n\n主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。\n\nVARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长。当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。\n\nVARCHAR 会保留字符串末尾的空格，而 CHAR 会删除。\n\n\n# 3.4. 时间和日期\n\nMySQL 提供了两种相似的日期时间类型：DATATIME 和 TIMESTAMP。\n\n# DATATIME\n\n能够保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。\n\n它与时区无关。\n\n默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATATIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。\n\n# TIMESTAMP\n\n和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年 到 2038 年。\n\n它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。\n\nMySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。\n\n默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。\n\n应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。\n\n\n# 3.5. BLOB 和 TEXT\n\nBLOB 和 TEXT 都是为了存储大的数据而设计，前者存储二进制数据，后者存储字符串数据。\n\n不能对 BLOB 和 TEXT 类型的全部内容进行排序、索引。\n\n\n# 3.6. 枚举类型\n\n大多数情况下没有使用枚举类型的必要，其中一个缺点是：枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用ALTER TABLE（如果只只是在列表末尾追加元素，不需要重建表）。\n\n\n# 3.7. 类型的选择\n\n * 整数类型通常是标识列最好的选择，因为它们很快并且可以使用 AUTO_INCREMENT。\n\n * ENUM 和 SET 类型通常是一个糟糕的选择，应尽量避免。\n\n * 应该尽量避免用字符串类型作为标识列，因为它们很消耗空间，并且通常比数字类型慢。对于 MD5、SHA、UUID 这类随机字符串，由于比较随机，所以可能分布在很大的空间内，导致 INSERT 以及一些 SELECT 语句变得很慢。\n   \n   * 如果存储 UUID ，应该移除 - 符号；更好的做法是，用 UNHEX() 函数转换 UUID 值为 16 字节的数字，并存储在一个 BINARY(16) 的列中，检索时，可以通过 HEX() 函数来格式化为 16 进制格式。\n\n\n# 4. 索引\n\n> 详见：Mysql 索引\n\n\n# 5. 锁\n\n> 详见：Mysql 锁\n\n\n# 6. 事务\n\n> 详见：Mysql 事务\n\n\n# 7. 性能优化\n\n> 详见：Mysql 性能优化\n\n\n# 8. 复制\n\n\n# 8.1. 主从复制\n\nMysql 支持两种复制：基于行的复制和基于语句的复制。\n\n这两种方式都是在主库上记录二进制日志，然后在从库重放日志的方式来实现异步的数据复制。这意味着：复制过程存在时延，这段时间内，主从数据可能不一致。\n\n主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。\n\n * binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。\n * I/O 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的中继日志中。\n * SQL 线程 ：负责读取中继日志并重放其中的 SQL 语句。\n\n\n\n\n# 8.2. 读写分离\n\n主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。\n\n读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。\n\nMySQL 读写分离能提高性能的原因在于：\n\n * 主从服务器负责各自的读和写，极大程度缓解了锁的争用；\n * 从服务器可以配置 MyISAM 引擎，提升查询性能以及节约系统开销；\n * 增加冗余，提高可用性。\n\n\n#\n\n\n（分割线）以下为高级特性，也是关系型数据库通用方案\n\n\n# 9. 分布式事务\n\n> 参考：分布式事务基本原理\n\n\n# 10. 分库分表\n\n> 参考：分库分表基本原理\n\n\n# 11. 参考资料\n\n * 《高性能 MySQL》\n * 20+ 条 MySQL 性能优化的最佳经验\n * How to create unique row ID in sharded databases?\n * SQL Azure Federation – Introduction\n\n\n# 12. 传送门\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# mysql 应用指南\n\n\n# 1. sql 执行过程\n\n学习 mysql，最好是先从宏观上了解 mysql 工作原理。\n\n> 参考：mysql 工作流\n\n\n# 2. 存储引擎\n\n在文件系统中，mysql 将每个数据库（也可以成为 schema）保存为数据目录下的一个子目录。创建表示，mysql 会在数据库子目录下创建一个和表同名的 .frm 文件保存表的定义。因为 mysql 使用文件系统的目录和文件来保存数据库和表的定义，大小写敏感性和具体平台密切相关。windows 中大小写不敏感；类 unix 中大小写敏感。不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在 mysql 服务层统一处理的。\n\n\n# 2.1. 选择存储引擎\n\n# mysql 内置的存储引擎\n\nmysql> show engines;\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| engine             | support | comment                                                        | transactions | xa   | savepoints |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| federated          | no      | federated mysql storage engine                                 | null         | null | null       |\n| memory             | yes     | hash based, stored in memory, useful for temporary tables      | no           | no   | no         |\n| innodb             | default | supports transactions, row-level locking, and foreign keys     | yes          | yes  | yes        |\n| performance_schema | yes     | performance schema                                             | no           | no   | no         |\n| myisam             | yes     | myisam storage engine                                          | no           | no   | no         |\n| mrg_myisam         | yes     | collection of identical myisam tables                          | no           | no   | no         |\n| blackhole          | yes     | /dev/null storage engine (anything you write to it disappears) | no           | no   | no         |\n| csv                | yes     | csv storage engine                                             | no           | no   | no         |\n| archive            | yes     | archive storage engine                                         | no           | no   | no         |\n+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+\n9 rows in set (0.00 sec)\n\n\n * innodb - mysql 的默认事务型存储引擎，并且提供了行级锁和外键的约束。性能不错且支持自动崩溃恢复。\n * myisam - mysql 5.1 版本前的默认存储引擎。特性丰富但不支持事务，也不支持行级锁和外键，也没有崩溃恢复功能。\n * csv - 可以将 csv 文件作为 mysql 的表来处理，但这种表不支持索引。\n * memory - 适合快速访问数据，且数据不会被修改，重启丢失也没有关系。\n * ndb - 用于 mysql 集群场景。\n\n# 如何选择合适的存储引擎\n\n大多数情况下，innodb 都是正确的选择，除非需要用到 innodb 不具备的特性。\n\n如果应用需要选择 innodb 以外的存储引擎，可以考虑以下因素：\n\n * 事务：如果需要支持事务，innodb 是首选。如果不需要支持事务，且主要是 select 和 insert 操作，myisam 是不错的选择。所以，如果 mysql 部署方式为主备模式，并进行读写分离。那么可以这么做：主节点只支持写操作，默认引擎为 innodb；备节点只支持读操作，默认引擎为 myisam。\n * 并发：myisam 只支持表级锁，而 innodb 还支持行级锁。所以，innodb 并发性能更高。\n * 外键：innodb 支持外键。\n * 备份：innodb 支持在线热备份。\n * 崩溃恢复：myisam 崩溃后发生损坏的概率比 innodb 高很多，而且恢复的速度也更慢。\n * 其它特性：myisam 支持压缩表和空间数据索引。\n\n# 转换表的存储引擎\n\n下面的语句可以将 mytable 表的引擎修改为 innodb\n\nalter table mytable engine = innodb\n\n\n\n# 2.2. myisam\n\nmyisam 设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用 myisam。\n\nmyisam 引擎使用 b+tree 作为索引结构，叶节点的 data 域存放的是数据记录的地址。\n\nmyisam 提供了大量的特性，包括：全文索引、压缩表、空间函数等。但是，myisam 不支持事务和行级锁。并且 myisam 不支持崩溃后的安全恢复。\n\n\n# 2.3. innodb\n\ninnodb 是 mysql 默认的事务型存储引擎，只有在需要 innodb 不支持的特性时，才考虑使用其它存储引擎。\n\n然 innodb 也使用 b+tree 作为索引结构，但具体实现方式却与 myisam 截然不同。myisam 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在 innodb 中，表数据文件本身就是按 b+tree 组织的一个索引结构，这棵树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键，因此innodb 表数据文件本身就是主索引。\n\ninnodb 采用 mvcc 来支持高并发，并且实现了四个标准的隔离级别。其默认级别是可重复读（repeatable read），并且通过间隙锁（next-key locking）防止幻读。\n\ninnodb 是基于聚簇索引建立的，与其他存储引擎有很大不同。在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。\n\n内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。\n\n支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。\n\n\n# 3. 数据类型\n\n\n# 3.1. 整型\n\ntinyint, smallint, mediumint, int, bigint 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。\n\nunsigned 表示不允许负值，大致可以使正数的上限提高一倍。\n\nint(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。\n\n\n# 3.2. 浮点型\n\nfloat 和 double 为浮点类型。\n\ndecimal 类型主要用于精确计算，代价较高，应该尽量只在对小数进行精确计算时才使用 decimal ——例如存储财务数据。数据量比较大的时候，可以使用 bigint 代替 decimal。\n\nfloat、double 和 decimal 都可以指定列宽，例如 decimal(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。\n\n\n# 3.3. 字符串\n\n主要有 char 和 varchar 两种类型，一种是定长的，一种是变长的。\n\nvarchar 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 update 时可能会使行变得比原来长。当超出一个页所能容纳的大小时，就要执行额外的操作。myisam 会将行拆成不同的片段存储，而 innodb 则需要分裂页来使行放进页内。\n\nvarchar 会保留字符串末尾的空格，而 char 会删除。\n\n\n# 3.4. 时间和日期\n\nmysql 提供了两种相似的日期时间类型：datatime 和 timestamp。\n\n# datatime\n\n能够保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。\n\n它与时区无关。\n\n默认情况下，mysql 以一种可排序的、无歧义的格式显示 datatime 值，例如“2008-01-16 22:37:08”，这是 ansi 标准定义的日期和时间表示方法。\n\n# timestamp\n\n和 unix 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年 到 2038 年。\n\n它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。\n\nmysql 提供了 from_unixtime() 函数把 unix 时间戳转换为日期，并提供了 unix_timestamp() 函数把日期转换为 unix 时间戳。\n\n默认情况下，如果插入时没有指定 timestamp 列的值，会将这个值设置为当前时间。\n\n应该尽量使用 timestamp，因为它比 datetime 空间效率更高。\n\n\n# 3.5. blob 和 text\n\nblob 和 text 都是为了存储大的数据而设计，前者存储二进制数据，后者存储字符串数据。\n\n不能对 blob 和 text 类型的全部内容进行排序、索引。\n\n\n# 3.6. 枚举类型\n\n大多数情况下没有使用枚举类型的必要，其中一个缺点是：枚举的字符串列表是固定的，添加和删除字符串（枚举选项）必须使用alter table（如果只只是在列表末尾追加元素，不需要重建表）。\n\n\n# 3.7. 类型的选择\n\n * 整数类型通常是标识列最好的选择，因为它们很快并且可以使用 auto_increment。\n\n * enum 和 set 类型通常是一个糟糕的选择，应尽量避免。\n\n * 应该尽量避免用字符串类型作为标识列，因为它们很消耗空间，并且通常比数字类型慢。对于 md5、sha、uuid 这类随机字符串，由于比较随机，所以可能分布在很大的空间内，导致 insert 以及一些 select 语句变得很慢。\n   \n   * 如果存储 uuid ，应该移除 - 符号；更好的做法是，用 unhex() 函数转换 uuid 值为 16 字节的数字，并存储在一个 binary(16) 的列中，检索时，可以通过 hex() 函数来格式化为 16 进制格式。\n\n\n# 4. 索引\n\n> 详见：mysql 索引\n\n\n# 5. 锁\n\n> 详见：mysql 锁\n\n\n# 6. 事务\n\n> 详见：mysql 事务\n\n\n# 7. 性能优化\n\n> 详见：mysql 性能优化\n\n\n# 8. 复制\n\n\n# 8.1. 主从复制\n\nmysql 支持两种复制：基于行的复制和基于语句的复制。\n\n这两种方式都是在主库上记录二进制日志，然后在从库重放日志的方式来实现异步的数据复制。这意味着：复制过程存在时延，这段时间内，主从数据可能不一致。\n\n主要涉及三个线程：binlog 线程、i/o 线程和 sql 线程。\n\n * binlog 线程 ：负责将主服务器上的数据更改写入二进制文件（binlog）中。\n * i/o 线程 ：负责从主服务器上读取二进制日志文件，并写入从服务器的中继日志中。\n * sql 线程 ：负责读取中继日志并重放其中的 sql 语句。\n\n\n\n\n# 8.2. 读写分离\n\n主服务器用来处理写操作以及实时性要求比较高的读操作，而从服务器用来处理读操作。\n\n读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。\n\nmysql 读写分离能提高性能的原因在于：\n\n * 主从服务器负责各自的读和写，极大程度缓解了锁的争用；\n * 从服务器可以配置 myisam 引擎，提升查询性能以及节约系统开销；\n * 增加冗余，提高可用性。\n\n\n#\n\n\n（分割线）以下为高级特性，也是关系型数据库通用方案\n\n\n# 9. 分布式事务\n\n> 参考：分布式事务基本原理\n\n\n# 10. 分库分表\n\n> 参考：分库分表基本原理\n\n\n# 11. 参考资料\n\n * 《高性能 mysql》\n * 20+ 条 mysql 性能优化的最佳经验\n * how to create unique row id in sharded databases?\n * sql azure federation – introduction\n\n\n# 12. 传送门\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MySQL 工作流",frontmatter:{title:"MySQL 工作流",date:"2020-07-16T11:14:07.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql"],permalink:"/pages/8262aa/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/02.MySQL%E5%B7%A5%E4%BD%9C%E6%B5%81.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/02.MySQL工作流.md",key:"v-e2f7f754",path:"/pages/8262aa/",headers:[{level:2,title:"1. 基础架构",slug:"_1-基础架构",normalizedTitle:"1. 基础架构",charIndex:16},{level:2,title:"2. 查询过程",slug:"_2-查询过程",normalizedTitle:"2. 查询过程",charIndex:289},{level:3,title:"2.1. （一）连接",slug:"_2-1-一-连接",normalizedTitle:"2.1. （一）连接",charIndex:584},{level:3,title:"2.2. （二）查询缓存",slug:"_2-2-二-查询缓存",normalizedTitle:"2.2. （二）查询缓存",charIndex:1520},{level:3,title:"2.3. （三）语法分析",slug:"_2-3-三-语法分析",normalizedTitle:"2.3. （三）语法分析",charIndex:2391},{level:3,title:"2.4. （四）查询优化",slug:"_2-4-四-查询优化",normalizedTitle:"2.4. （四）查询优化",charIndex:2919},{level:3,title:"2.5. （五）查询执行引擎",slug:"_2-5-五-查询执行引擎",normalizedTitle:"2.5. （五）查询执行引擎",charIndex:4131},{level:3,title:"2.6. （六）返回结果",slug:"_2-6-六-返回结果",normalizedTitle:"2.6. （六）返回结果",charIndex:4415},{level:2,title:"3. 更新过程",slug:"_3-更新过程",normalizedTitle:"3. 更新过程",charIndex:4735},{level:3,title:"3.1. redo log",slug:"_3-1-redo-log",normalizedTitle:"3.1. redo log",charIndex:4836},{level:3,title:"3.2. bin log",slug:"_3-2-bin-log",normalizedTitle:"3.2. bin log",charIndex:5291},{level:3,title:"3.3. redo log vs. bin log",slug:"_3-3-redo-log-vs-bin-log",normalizedTitle:"3.3. redo log vs. bin log",charIndex:5534},{level:3,title:"3.4. 两阶段提交",slug:"_3-4-两阶段提交",normalizedTitle:"3.4. 两阶段提交",charIndex:6308},{level:2,title:"4. 参考资料",slug:"_4-参考资料",normalizedTitle:"4. 参考资料",charIndex:7026}],headersStr:"1. 基础架构 2. 查询过程 2.1. （一）连接 2.2. （二）查询缓存 2.3. （三）语法分析 2.4. （四）查询优化 2.5. （五）查询执行引擎 2.6. （六）返回结果 3. 更新过程 3.1. redo log 3.2. bin log 3.3. redo log vs. bin log 3.4. 两阶段提交 4. 参考资料",content:'# MySQL 工作流\n\n\n# 1. 基础架构\n\n大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。\n\nServer 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n\n存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。\n\n\n\n\n# 2. 查询过程\n\nSQL 语句在 MySQL 中是如何执行的？\n\nMySQL 整个查询执行过程，总的来说分为 6 个步骤：\n\n 1. 客户端和 MySQL 服务器建立连接；客户端向 MySQL 服务器发送一条查询请求。\n 2. MySQL 服务器首先检查查询缓存，如果命中缓存，则立刻返回结果。否则进入下一阶段。\n 3. MySQL 服务器进行 SQL 分析：语法分析、词法分析。\n 4. MySQL 服务器用优化器生成对应的执行计划。\n 5. MySQL 服务器根据执行计划，调用存储引擎的 API 来执行查询。\n 6. MySQL 服务器将结果返回给客户端，同时缓存查询结果。\n\n\n# 2.1. （一）连接\n\n使用 MySQL 第一步自然是要连接数据库。\n\nMySQL 客户端/服务端通信是半双工模式：即任一时刻，要么是服务端向客户端发送数据，要么是客户端向服务器发送数据。客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。\n\nMySQL 客户端连接命令：mysql -h<主机> -P<端口> -u<用户名> -p<密码>。如果没有显式指定密码，会要求输入密码才能访问。\n\n连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。客户端如果太长时间没动静，连接器就会自动将它断开。客户端连接维持时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。\n\n建立连接的过程通常是比较复杂的，建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。为了在程序中提高数据库连接的服用了，一般会使用数据库连接池来维护管理。\n\n但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。\n\n怎么解决这个问题呢？你可以考虑以下两种方案。\n\n * 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。\n * 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n\n# 2.2. （二）查询缓存\n\n> 不建议使用数据库缓存，因为往往弊大于利。\n\n解析一个查询语句前，如果查询缓存是打开的，那么 MySQL 会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。\n\nMySQL 将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。\n\n如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql 库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。\n\n不建议使用数据库缓存，因为往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。\n\n好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样：\n\nselect SQL_CACHE * from T where ID=10;\n\n\n> 注意：MySQL 8.0 版本直接将查询缓存的整块功能删掉了。\n\n\n# 2.3. （三）语法分析\n\n如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。MySQL 通过关键字对 SQL 语句进行解析，并生成一颗对应的语法解析树。这个过程中，分析器主要通过语法规则来验证和解析。比如 SQL 中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据 MySQL 规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。\n\n * 分析器先会先做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。MySQL 从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。\n * 接下来，要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。\n\n\n# 2.4. （四）查询优化\n\n经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。\n\n经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成执行计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。\n\nMySQL 使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在 MySQL 可以通过查询当前会话的 last_query_cost 的值来得到其计算当前查询的成本。\n\nmysql> select * from t_message limit 10;\n...省略结果集\n\nmysql> show status like \'last_query_cost\';\n+-----------------+-------------+\n| Variable_name   | Value       |\n+-----------------+-------------+\n| Last_query_cost | 6391.799000 |\n+-----------------+-------------+\n\n\n示例中的结果表示优化器认为大概需要做 6391 个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。\n\n有非常多的原因会导致 MySQL 选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL 认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但 MySQL 值选择它认为成本小的，但成本小并不意味着执行时间短）等等。\n\nMySQL 的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划：\n\n * 重新定义表的关联顺序（多张表关联查询时，并不一定按照 SQL 中指定的顺序进行，但有一些技巧可以指定关联顺序）\n * 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找 B+Tree 索引最左端，反之则可以找到最大值，具体原理见下文）\n * 提前终止查询（比如：使用 Limit 时，查找到满足数量的结果集后会立即终止查询）\n * 优化排序（在老版本 MySQL 会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于 I/O 密集型应用，效率会高很多）\n\n随着 MySQL 的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。\n\n\n# 2.5. （五）查询执行引擎\n\n在完成解析和优化阶段以后，MySQL 会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL 在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。\n\n\n# 2.6. （六）返回结果\n\n查询过程的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL 仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。\n\n如果查询缓存被打开且这个查询可以被缓存，MySQL 也会将结果存放到缓存中。\n\n结果集返回客户端是一个增量且逐步返回的过程。有可能 MySQL 在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足 ① 中所描述的通信协议的数据包发送，再通过 TCP 协议进行传输，在传输过程中，可能对 MySQL 的数据包进行缓存然后批量发送。\n\n\n# 3. 更新过程\n\nMySQL 更新过程和 MySQL 查询过程类似，也会将流程走一遍。不一样的是：更新流程还涉及两个重要的日志模块，：redo log（重做日志）和 binlog（归档日志）。\n\n\n# 3.1. redo log\n\nredo log 是 InnoDB 引擎特有的日志。redo log 即重做日志。redo log 是物理日志，记录的是“在某个数据页上做了什么修改”。\n\nredo log 是基于 WAL 技术。WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log 里，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。\n\nInnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。\n\n\n\n有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。\n\n\n# 3.2. bin log\n\nbin log 即归档日志。binlog 是逻辑日志，记录的是这个语句的原始逻辑。\n\nbinlog 是可以追加写入的，即写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\nbinlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\n\nsync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。\n\n\n# 3.3. redo log vs. bin log\n\n这两种日志有以下三点不同。\n\n * redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\n * redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。\n * redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\n有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。\n\n 1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n 2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n 3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。\n 4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。\n 5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。\n\n这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。\n\n\n\n\n# 3.4. 两阶段提交\n\nredo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。为什么日志需要“两阶段提交”。\n\n由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。\n\n * 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。\n * 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。\n\n可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。\n\n\n# 4. 参考资料\n\n * 《高性能 MySQL》\n * MySQL 实战 45 讲',normalizedContent:'# mysql 工作流\n\n\n# 1. 基础架构\n\n大体来说，mysql 可以分为 server 层和存储引擎层两部分。\n\nserver 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 mysql 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。\n\n存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 innodb、myisam、memory 等多个存储引擎。现在最常用的存储引擎是 innodb，它从 mysql 5.5.5 版本开始成为了默认存储引擎。\n\n\n\n\n# 2. 查询过程\n\nsql 语句在 mysql 中是如何执行的？\n\nmysql 整个查询执行过程，总的来说分为 6 个步骤：\n\n 1. 客户端和 mysql 服务器建立连接；客户端向 mysql 服务器发送一条查询请求。\n 2. mysql 服务器首先检查查询缓存，如果命中缓存，则立刻返回结果。否则进入下一阶段。\n 3. mysql 服务器进行 sql 分析：语法分析、词法分析。\n 4. mysql 服务器用优化器生成对应的执行计划。\n 5. mysql 服务器根据执行计划，调用存储引擎的 api 来执行查询。\n 6. mysql 服务器将结果返回给客户端，同时缓存查询结果。\n\n\n# 2.1. （一）连接\n\n使用 mysql 第一步自然是要连接数据库。\n\nmysql 客户端/服务端通信是半双工模式：即任一时刻，要么是服务端向客户端发送数据，要么是客户端向服务器发送数据。客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。\n\nmysql 客户端连接命令：mysql -h<主机> -p<端口> -u<用户名> -p<密码>。如果没有显式指定密码，会要求输入密码才能访问。\n\n连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。客户端如果太长时间没动静，连接器就会自动将它断开。客户端连接维持时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： lost connection to mysql server during query。这时候如果你要继续，就需要重连，然后再执行请求了。\n\n建立连接的过程通常是比较复杂的，建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。为了在程序中提高数据库连接的服用了，一般会使用数据库连接池来维护管理。\n\n但是全部使用长连接后，你可能会发现，有些时候 mysql 占用内存涨得特别快，这是因为 mysql 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（oom），从现象看就是 mysql 异常重启了。\n\n怎么解决这个问题呢？你可以考虑以下两种方案。\n\n * 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。\n * 如果你用的是 mysql 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n\n# 2.2. （二）查询缓存\n\n> 不建议使用数据库缓存，因为往往弊大于利。\n\n解析一个查询语句前，如果查询缓存是打开的，那么 mysql 会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。\n\nmysql 将缓存存放在一个引用表（不要理解成table，可以认为是类似于hashmap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。\n\n如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql 库中的系统表，其查询结果都不会被缓存。比如函数now()或者current_date()会因为不同的查询时间，返回不同的查询结果，再比如包含current_user或者connecion_id()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。\n\n不建议使用数据库缓存，因为往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。\n\n好在 mysql 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 demand，这样对于默认的 sql 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 sql_cache 显式指定，像下面这个语句一样：\n\nselect sql_cache * from t where id=10;\n\n\n> 注意：mysql 8.0 版本直接将查询缓存的整块功能删掉了。\n\n\n# 2.3. （三）语法分析\n\n如果没有命中查询缓存，就要开始真正执行语句了。首先，mysql 需要知道你要做什么，因此需要对 sql 语句做解析。mysql 通过关键字对 sql 语句进行解析，并生成一颗对应的语法解析树。这个过程中，分析器主要通过语法规则来验证和解析。比如 sql 中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据 mysql 规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。\n\n * 分析器先会先做“词法分析”。你输入的是由多个字符串和空格组成的一条 sql 语句，mysql 需要识别出里面的字符串分别是什么，代表什么。mysql 从你输入的"select"这个关键字识别出来，这是一个查询语句。它也要把字符串“t”识别成“表名 t”，把字符串“id”识别成“列 id”。\n * 接下来，要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 sql 语句是否满足 mysql 语法。如果你的语句不对，就会收到“you have an error in your sql syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。\n\n\n# 2.4. （四）查询优化\n\n经过了分析器，mysql 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。\n\n经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成执行计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。\n\nmysql 使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在 mysql 可以通过查询当前会话的 last_query_cost 的值来得到其计算当前查询的成本。\n\nmysql> select * from t_message limit 10;\n...省略结果集\n\nmysql> show status like \'last_query_cost\';\n+-----------------+-------------+\n| variable_name   | value       |\n+-----------------+-------------+\n| last_query_cost | 6391.799000 |\n+-----------------+-------------+\n\n\n示例中的结果表示优化器认为大概需要做 6391 个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。\n\n有非常多的原因会导致 mysql 选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、mysql 认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但 mysql 值选择它认为成本小的，但成本小并不意味着执行时间短）等等。\n\nmysql 的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划：\n\n * 重新定义表的关联顺序（多张表关联查询时，并不一定按照 sql 中指定的顺序进行，但有一些技巧可以指定关联顺序）\n * 优化min()和max()函数（找某列的最小值，如果该列有索引，只需要查找 b+tree 索引最左端，反之则可以找到最大值，具体原理见下文）\n * 提前终止查询（比如：使用 limit 时，查找到满足数量的结果集后会立即终止查询）\n * 优化排序（在老版本 mysql 会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于 i/o 密集型应用，效率会高很多）\n\n随着 mysql 的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。\n\n\n# 2.5. （五）查询执行引擎\n\n在完成解析和优化阶段以后，mysql 会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler api。查询过程中的每一张表由一个handler实例表示。实际上，mysql 在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。\n\n\n# 2.6. （六）返回结果\n\n查询过程的最后一个阶段就是将结果返回给客户端。即使查询不到数据，mysql 仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。\n\n如果查询缓存被打开且这个查询可以被缓存，mysql 也会将结果存放到缓存中。\n\n结果集返回客户端是一个增量且逐步返回的过程。有可能 mysql 在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足 ① 中所描述的通信协议的数据包发送，再通过 tcp 协议进行传输，在传输过程中，可能对 mysql 的数据包进行缓存然后批量发送。\n\n\n# 3. 更新过程\n\nmysql 更新过程和 mysql 查询过程类似，也会将流程走一遍。不一样的是：更新流程还涉及两个重要的日志模块，：redo log（重做日志）和 binlog（归档日志）。\n\n\n# 3.1. redo log\n\nredo log 是 innodb 引擎特有的日志。redo log 即重做日志。redo log 是物理日志，记录的是“在某个数据页上做了什么修改”。\n\nredo log 是基于 wal 技术。wal 的全称是 write-ahead logging，它的关键点就是先写日志，再写磁盘。具体来说，当有一条记录需要更新的时候，innodb 引擎就会先把记录写到 redo log 里，并更新内存，这个时候更新就算完成了。同时，innodb 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。\n\ninnodb 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1gb，那么这块“粉板”总共就可以记录 4gb 的操作。从头开始写，写到末尾就又回到开头循环写。\n\n\n\n有了 redo log，innodb 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。\n\n\n# 3.2. bin log\n\nbin log 即归档日志。binlog 是逻辑日志，记录的是这个语句的原始逻辑。\n\nbinlog 是可以追加写入的，即写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\nbinlog 是 mysql 的 server 层实现的，所有引擎都可以使用。\n\nsync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 mysql 异常重启之后 binlog 不丢失。\n\n\n# 3.3. redo log vs. bin log\n\n这两种日志有以下三点不同。\n\n * redo log 是 innodb 引擎特有的；binlog 是 mysql 的 server 层实现的，所有引擎都可以使用。\n * redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 id=2 这一行的 c 字段加 1 ”。\n * redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\n有了对这两个日志的概念性理解，我们再来看执行器和 innodb 引擎在执行这个简单的 update 语句时的内部流程。\n\n 1. 执行器先找引擎取 id=2 这一行。id 是主键，引擎直接用树搜索找到这一行。如果 id=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n 2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 n，现在就是 n+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n 3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。\n 4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。\n 5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。\n\n这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 innodb 内部执行的，深色框表示是在执行器中执行的。\n\n\n\n\n# 3.4. 两阶段提交\n\nredo log 的写入拆成了两个步骤：prepare 和 commit，这就是"两阶段提交"。为什么日志需要“两阶段提交”。\n\n由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。\n\n * 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，mysql 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。\n * 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。\n\n可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。\n\n\n# 4. 参考资料\n\n * 《高性能 mysql》\n * mysql 实战 45 讲',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 事务",frontmatter:{title:"Mysql 事务",date:"2020-06-03T19:32:09.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","事务"],permalink:"/pages/00b04d/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/03.Mysql%E4%BA%8B%E5%8A%A1.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/03.Mysql事务.md",key:"v-413ab35d",path:"/pages/00b04d/",headers:[{level:2,title:"1. 事务简介",slug:"_1-事务简介",normalizedTitle:"1. 事务简介",charIndex:156},{level:2,title:"2. 事务用法",slug:"_2-事务用法",normalizedTitle:"2. 事务用法",charIndex:474},{level:3,title:"2.1. 事务处理指令",slug:"_2-1-事务处理指令",normalizedTitle:"2.1. 事务处理指令",charIndex:486},{level:3,title:"2.2. AUTOCOMMIT",slug:"_2-2-autocommit",normalizedTitle:"2.2. autocommit",charIndex:1497},{level:2,title:"3. ACID",slug:"_3-acid",normalizedTitle:"3. acid",charIndex:1866},{level:2,title:"4. 事务隔离级别",slug:"_4-事务隔离级别",normalizedTitle:"4. 事务隔离级别",charIndex:2617},{level:3,title:"4.1. 事务隔离简介",slug:"_4-1-事务隔离简介",normalizedTitle:"4.1. 事务隔离简介",charIndex:2631},{level:3,title:"4.2. 未提交读",slug:"_4-2-未提交读",normalizedTitle:"4.2. 未提交读",charIndex:3197},{level:3,title:"4.3. 提交读",slug:"_4-3-提交读",normalizedTitle:"4.3. 提交读",charIndex:3363},{level:3,title:"4.4. 可重复读",slug:"_4-4-可重复读",normalizedTitle:"4.4. 可重复读",charIndex:3586},{level:3,title:"4.5. 串行化",slug:"_4-5-串行化",normalizedTitle:"4.5. 串行化",charIndex:3846},{level:3,title:"4.6. 隔离级别小结",slug:"_4-6-隔离级别小结",normalizedTitle:"4.6. 隔离级别小结",charIndex:3980},{level:2,title:"5. 死锁",slug:"_5-死锁",normalizedTitle:"5. 死锁",charIndex:4376},{level:3,title:"5.1. 死锁的原因",slug:"_5-1-死锁的原因",normalizedTitle:"5.1. 死锁的原因",charIndex:4501},{level:3,title:"5.2. 避免死锁",slug:"_5-2-避免死锁",normalizedTitle:"5.2. 避免死锁",charIndex:5685},{level:3,title:"5.3. 解决死锁",slug:"_5-3-解决死锁",normalizedTitle:"5.3. 解决死锁",charIndex:6165},{level:2,title:"6. 分布式事务",slug:"_6-分布式事务",normalizedTitle:"6. 分布式事务",charIndex:6825},{level:2,title:"7. 事务最佳实践",slug:"_7-事务最佳实践",normalizedTitle:"7. 事务最佳实践",charIndex:8008},{level:3,title:"7.1. 尽量使用低级别事务隔离",slug:"_7-1-尽量使用低级别事务隔离",normalizedTitle:"7.1. 尽量使用低级别事务隔离",charIndex:8041},{level:3,title:"7.2. 避免行锁升级表锁",slug:"_7-2-避免行锁升级表锁",normalizedTitle:"7.2. 避免行锁升级表锁",charIndex:8082},{level:3,title:"7.3. 缩小事务范围",slug:"_7-3-缩小事务范围",normalizedTitle:"7.3. 缩小事务范围",charIndex:8181},{level:2,title:"8. 参考资料",slug:"_8-参考资料",normalizedTitle:"8. 参考资料",charIndex:8759}],headersStr:"1. 事务简介 2. 事务用法 2.1. 事务处理指令 2.2. AUTOCOMMIT 3. ACID 4. 事务隔离级别 4.1. 事务隔离简介 4.2. 未提交读 4.3. 提交读 4.4. 可重复读 4.5. 串行化 4.6. 隔离级别小结 5. 死锁 5.1. 死锁的原因 5.2. 避免死锁 5.3. 解决死锁 6. 分布式事务 7. 事务最佳实践 7.1. 尽量使用低级别事务隔离 7.2. 避免行锁升级表锁 7.3. 缩小事务范围 8. 参考资料",content:"# Mysql 事务\n\n> 不是所有的 Mysql 存储引擎都实现了事务处理。支持事务的存储引擎有：InnoDB 和 NDB Cluster。不支持事务的存储引擎，代表有：MyISAM。\n> \n> 用户可以根据业务是否需要事务处理（事务处理可以保证数据安全，但会增加系统开销），选择合适的存储引擎。\n\n\n\n\n# 1. 事务简介\n\n> 事务简单来说：一个 Session 中所进行所有的操作，要么同时成功，要么同时失败。进一步说，事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。\n\n\n\n事务就是一组原子性的 SQL 语句。具体来说，事务指的是满足 ACID 特性的一组操作。\n\n事务内的 SQL 语句，要么全执行成功，要么全执行失败。\n\n通过加锁的方式，可以实现不同的事务隔离机制。\n\n想象一下，如果没有事务，在并发环境下，就可能出现丢失修改的问题。\n\nT1 和 T2 两个线程都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。\n\n\n\n\n# 2. 事务用法\n\n\n# 2.1. 事务处理指令\n\nMysql 中，使用 START TRANSACTION 语句开始一个事务；使用 COMMIT 语句提交所有的修改；使用 ROLLBACK 语句撤销所有的修改。不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。\n\n * START TRANSACTION - 指令用于标记事务的起始点。\n * SAVEPOINT - 指令用于创建保留点。\n * ROLLBACK TO - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 START TRANSACTION 语句处。\n * COMMIT - 提交事务。\n\n事务处理示例：\n\n（1）创建一张示例表\n\n-- 撤销表 user\nDROP TABLE IF EXISTS user;\n\n-- 创建表 user\nCREATE TABLE user (\n  id int(10) unsigned NOT NULL COMMENT 'Id',\n  username varchar(64) NOT NULL DEFAULT 'default' COMMENT '用户名',\n  password varchar(64) NOT NULL DEFAULT 'default' COMMENT '密码',\n  email varchar(64) NOT NULL DEFAULT 'default' COMMENT '邮箱'\n) COMMENT='用户表';\n\n\n（2）执行事务操作\n\n-- 开始事务\nSTART TRANSACTION;\n\n-- 插入操作 A\nINSERT INTO `user`\nVALUES (1, 'root1', 'root1', 'xxxx@163.com');\n\n-- 创建保留点 updateA\nSAVEPOINT updateA;\n\n-- 插入操作 B\nINSERT INTO `user`\nVALUES (2, 'root2', 'root2', 'xxxx@163.com');\n\n-- 回滚到保留点 updateA\nROLLBACK TO updateA;\n\n-- 提交事务，只有操作 A 生效\nCOMMIT;\n\n\n（3）执行结果\n\nSELECT * FROM user;\n\n\n结果：\n\n1\troot1\troot1\txxxx@163.com\n\n\n\n# 2.2. AUTOCOMMIT\n\nMySQL 默认采用隐式提交策略（autocommit）。每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。\n\n通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。\n\n-- 查看 AUTOCOMMIT\nSHOW VARIABLES LIKE 'AUTOCOMMIT';\n\n-- 关闭 AUTOCOMMIT\nSET autocommit = 0;\n\n-- 开启 AUTOCOMMIT\nSET autocommit = 1;\n\n\n\n# 3. ACID\n\nACID 是数据库事务正确执行的四个基本要素。\n\n * 原子性（Atomicity）\n   * 事务被视为不可分割的最小单元，事务中的所有操作要么全部提交成功，要么全部失败回滚。\n   * 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。\n * 一致性（Consistency）\n   * 数据库在事务执行前后都保持一致性状态。\n   * 在一致性状态下，所有事务对一个数据的读取结果都是相同的。\n * 隔离性（Isolation）\n   * 一个事务所做的修改在最终提交以前，对其它事务是不可见的。\n * 持久性（Durability）\n   * 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。\n   * 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。\n\n一个支持事务（Transaction）中的数据库系统，必需要具有这四种特性，否则在事务过程（Transaction processing）当中无法保证数据的正确性，交易过程极可能达不到交易。\n\n * 只有满足一致性，事务的执行结果才是正确的。\n * 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。\n * 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。\n * 事务满足持久化是为了能应对系统崩溃的情况。\n\n\n\n> MySQL 默认采用自动提交模式（AUTO COMMIT）。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。\n\n\n# 4. 事务隔离级别\n\n\n# 4.1. 事务隔离简介\n\n在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题：\n\n * 丢失修改\n * 脏读\n * 不可重复读\n * 幻读\n\n在 SQL 标准中，定义了四种事务隔离级别（级别由低到高）：\n\n * 未提交读\n * 提交读\n * 可重复读\n * 串行化\n\nMysql 中查看和设置事务隔离级别：\n\n-- 查看事务隔离级别\nSHOW VARIABLES LIKE 'transaction_isolation';\n\n-- 设置事务隔离级别为 READ UNCOMMITTED\nSET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\n\n-- 设置事务隔离级别为 READ COMMITTED\nSET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;\n\n-- 设置事务隔离级别为 REPEATABLE READ\nSET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n\n-- 设置事务隔离级别为 SERIALIZABLE\nSET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE;\n\n\n\n# 4.2. 未提交读\n\n未提交读（READ UNCOMMITTED） 是指：事务中的修改，即使没有提交，对其它事务也是可见的。\n\n未提交读的问题：事务可以读取未提交的数据，也被称为 脏读（Dirty Read）。\n\nT1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。\n\n\n\n\n# 4.3. 提交读\n\n提交读（READ COMMITTED） 是指：事务提交后，其他事务才能看到它的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。提交读解决了脏读的问题。\n\n提交读是大多数数据库的默认事务隔离级别。\n\n提交读有时也叫不可重复读，它的问题是：执行两次相同的查询，得到的结果可能不一致。\n\nT2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n\n\n\n\n# 4.4. 可重复读\n\n可重复读（REPEATABLE READ） 是指：保证在同一个事务中多次读取同样数据的结果是一样的。可重复读解决了不可重复读问题。\n\n可重复读是 Mysql 的默认事务隔离级别。\n\n可重复读的问题：当某个事务读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务又再次读取该范围的记录时，会产生 幻读（Phantom Read）。\n\nT1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n\n\n\n\n# 4.5. 串行化\n\n串行化（SERIALIXABLE） 是指：强制事务串行执行。\n\n强制事务串行执行，则避免了所有的并发问题。串行化策略会在读取的每一行数据上都加锁，这可能导致大量的超时和锁竞争。这对于高并发应用基本上是不可接受的，所以一般不会采用这个级别。\n\n\n# 4.6. 隔离级别小结\n\n * 未提交读（READ UNCOMMITTED） - 事务中的修改，即使没有提交，对其它事务也是可见的。\n * 提交读（READ COMMITTED） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。\n * 重复读（REPEATABLE READ） - 保证在同一个事务中多次读取同样数据的结果是一样的。\n * 串行化（SERIALIXABLE） - 强制事务串行执行。\n\n数据库隔离级别解决的问题：\n\n隔离级别   丢失修改   脏读   不可重复读   幻读\n未提交读   ✔️     ❌    ❌       ❌\n提交读    ✔️     ✔️   ❌       ❌\n可重复读   ✔️     ✔️   ✔️      ❌\n可串行化   ✔️     ✔️   ✔️      ✔️\n\n\n# 5. 死锁\n\n死锁是指两个或多个事务竞争同一资源，并请求锁定对方占用的资源，从而导致恶性循环的现象。\n\n产生死锁的场景：\n\n * 当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。\n\n * 多个事务同时锁定同一个资源时，也会产生死锁。\n\n\n# 5.1. 死锁的原因\n\n行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。record lock 是专门对索引项加锁；gap lock 是对索引项之间的间隙加锁；next-key lock 则是前面两种的组合，对索引项以其之间的间隙加锁。\n\n只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock，在 Select、Update 和 Delete 时，除了基于唯一索引的查询之外，其它索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。主键索引也属于唯一索引，所以主键索引是不会使用 gap lock 或 next-key lock。\n\n在 MySQL 中，gap lock 默认是开启的，即 innodb_locks_unsafe_for_binlog 参数值是 disable 的，且 MySQL 中默认的是 RR 事务隔离级别。\n\n当我们执行以下查询 SQL 时，由于 order_no 列为非唯一索引，此时又是 RR 事务隔离级别，所以 SELECT 的加锁类型为 gap lock，这里的 gap 范围是 (4,+∞）。\n\n> SELECT id FROM demo.order_record where order_no = 4 for update;\n\n执行查询 SQL 语句获取的 gap lock 并不会导致阻塞，而当我们执行以下插入 SQL 时，会在插入间隙上再次获取插入意向锁。插入意向锁其实也是一种 gap 锁，它与 gap lock 是冲突的，所以当其它事务持有该间隙的 gap lock 时，需要等待其它事务释放 gap lock 之后，才能获取到插入意向锁。\n\n以上事务 A 和事务 B 都持有间隙 (4,+∞）的 gap 锁，而接下来的插入操作为了获取到插入意向锁，都在等待对方事务的 gap 锁释放，于是就造成了循环等待，导致死锁。\n\n> INSERT INTO demo.order_record(order_no, status, create_date) VALUES (5, 1, ‘2019-07-13 10:57:03’);\n\n\n\n另一个死锁场景\n\nInnoDB 存储引擎的主键索引为聚簇索引，其它索引为辅助索引。如果使用辅助索引来更新数据库，就需要使用聚簇索引来更新数据库字段。如果两个更新事务使用了不同的辅助索引，或一个使用了辅助索引，一个使用了聚簇索引，就都有可能导致锁资源的循环等待。由于本身两个事务是互斥，也就构成了以上死锁的四个必要条件了。\n\n\n\n出现死锁的步骤：\n\n\n\n综上可知，在更新操作时，我们应该尽量使用主键来更新表字段，这样可以有效避免一些不必要的死锁发生。\n\n\n# 5.2. 避免死锁\n\n预防死锁的注意事项：\n\n * 在编程中尽量按照固定的顺序来处理数据库记录，假设有两个更新操作，分别更新两条相同的记录，但更新顺序不一样，有可能导致死锁；\n * 在允许幻读和不可重复读的情况下，尽量使用 RC 事务隔离级别，可以避免 gap lock 导致的死锁问题；\n * 更新表时，尽量使用主键更新；\n * 避免长事务，尽量将长事务拆解，可以降低与其它事务发生冲突的概率；\n * 设置合理的锁等待超时参数，我们可以通过 innodb_lock_wait_timeout 设置合理的等待超时阈值，特别是在一些高并发的业务中，我们可以尽量将该值设置得小一些，避免大量事务等待，占用系统资源，造成严重的性能开销。\n\n另外，我们还可以将 order_no 列设置为唯一索引列。虽然不能防止幻读，但我们可以利用它的唯一性来保证订单记录不重复创建，这种方式唯一的缺点就是当遇到重复创建订单时会抛出异常。\n\n我们还可以使用其它的方式来代替数据库实现幂等性校验。例如，使用 Redis 以及 ZooKeeper 来实现，运行效率比数据库更佳。\n\n\n# 5.3. 解决死锁\n\n当出现死锁以后，有两种策略：\n\n * 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。\n * 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。\n\n在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。\n\n但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。\n\n所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。为了解决死锁问题，不同数据库实现了各自的死锁检测和超时机制。InnoDB 的处理策略是：将持有最少行级排它锁的事务进行回滚。\n\n主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。\n\n\n# 6. 分布式事务\n\n在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为 本地事务。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。\n\n分布式事务指的是事务操作跨越多个节点，并且要求满足事务的 ACID 特性。\n\n分布式事务的常见方案如下：\n\n * 两阶段提交（2PC） - 将事务的提交过程分为两个阶段来进行处理：准备阶段和提交阶段。参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。\n * 三阶段提交（3PC） - 与二阶段提交不同的是，引入超时机制。同时在协调者和参与者中都引入超时机制。将二阶段的准备阶段拆分为 2 个阶段，插入了一个 preCommit 阶段，使得原先在二阶段提交中，参与者在准备之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。\n * 补偿事务（TCC）\n   * Try - 操作作为一阶段，负责资源的检查和预留。\n   * Confirm - 操作作为二阶段提交操作，执行真正的业务。\n   * Cancel - 是预留资源的取消。\n * 本地消息表 - 在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。\n * MQ 事务 - 基于 MQ 的分布式事务方案其实是对本地消息表的封装。\n * SAGA - Saga 事务核心思想是将长事务拆分为多个本地短事务，由 Saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。\n\n分布式事务方案分析：\n\n * 2PC/3PC 依赖于数据库，能够很好的提供强一致性和强事务性，但相对来说延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。\n * TCC 适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。\n * 本地消息表/MQ 事务 都适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。\n * Saga 事务 由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。 Saga 相比缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。Saga 事务较适用于补偿动作容易处理的场景。\n\n> 分布式事务详细说明、分析请参考：分布式事务基本原理\n\n\n# 7. 事务最佳实践\n\n高并发场景下的事务到底该如何调优？\n\n\n# 7.1. 尽量使用低级别事务隔离\n\n结合业务场景，尽量使用低级别事务隔离\n\n\n# 7.2. 避免行锁升级表锁\n\n在 InnoDB 中，行锁是通过索引实现的，如果不通过索引条件检索数据，行锁将会升级到表锁。我们知道，表锁是会严重影响到整张表的操作性能的，所以应该尽力避免。\n\n\n# 7.3. 缩小事务范围\n\n有时候，数据库并发访问量太大，会出现以下异常：\n\nMySQLQueryInterruptedException: Query execution was interrupted\n\n\n高并发时对一条记录进行更新的情况下，由于更新记录所在的事务还可能存在其他操作，导致一个事务比较长，当有大量请求进入时，就可能导致一些请求同时进入到事务中。\n\n又因为锁的竞争是不公平的，当多个事务同时对一条记录进行更新时，极端情况下，一个更新操作进去排队系统后，可能会一直拿不到锁，最后因超时被系统打断踢出。\n\n\n\n如上图中的操作，虽然都是在一个事务中，但锁的申请在不同时间，只有当其他操作都执行完，才会释放所有锁。因为扣除库存是更新操作，属于行锁，这将会影响到其他操作该数据的事务，所以我们应该尽量避免长时间地持有该锁，尽快释放该锁。又因为先新建订单和先扣除库存都不会影响业务，所以我们可以将扣除库存操作放到最后，也就是使用执行顺序 1，以此尽量减小锁的持有时间。\n\n在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。\n\n知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。\n\n\n# 8. 参考资料\n\n * 《高性能 MySQL》\n * 《Java 性能调优实战》\n * ShardingSphere 分布式事务",normalizedContent:"# mysql 事务\n\n> 不是所有的 mysql 存储引擎都实现了事务处理。支持事务的存储引擎有：innodb 和 ndb cluster。不支持事务的存储引擎，代表有：myisam。\n> \n> 用户可以根据业务是否需要事务处理（事务处理可以保证数据安全，但会增加系统开销），选择合适的存储引擎。\n\n\n\n\n# 1. 事务简介\n\n> 事务简单来说：一个 session 中所进行所有的操作，要么同时成功，要么同时失败。进一步说，事务指的是满足 acid 特性的一组操作，可以通过 commit 提交一个事务，也可以使用 rollback 进行回滚。\n\n\n\n事务就是一组原子性的 sql 语句。具体来说，事务指的是满足 acid 特性的一组操作。\n\n事务内的 sql 语句，要么全执行成功，要么全执行失败。\n\n通过加锁的方式，可以实现不同的事务隔离机制。\n\n想象一下，如果没有事务，在并发环境下，就可能出现丢失修改的问题。\n\nt1 和 t2 两个线程都对一个数据进行修改，t1 先修改，t2 随后修改，t2 的修改覆盖了 t1 的修改。\n\n\n\n\n# 2. 事务用法\n\n\n# 2.1. 事务处理指令\n\nmysql 中，使用 start transaction 语句开始一个事务；使用 commit 语句提交所有的修改；使用 rollback 语句撤销所有的修改。不能回退 select 语句，回退 select 语句也没意义；也不能回退 create 和 drop 语句。\n\n * start transaction - 指令用于标记事务的起始点。\n * savepoint - 指令用于创建保留点。\n * rollback to - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 start transaction 语句处。\n * commit - 提交事务。\n\n事务处理示例：\n\n（1）创建一张示例表\n\n-- 撤销表 user\ndrop table if exists user;\n\n-- 创建表 user\ncreate table user (\n  id int(10) unsigned not null comment 'id',\n  username varchar(64) not null default 'default' comment '用户名',\n  password varchar(64) not null default 'default' comment '密码',\n  email varchar(64) not null default 'default' comment '邮箱'\n) comment='用户表';\n\n\n（2）执行事务操作\n\n-- 开始事务\nstart transaction;\n\n-- 插入操作 a\ninsert into `user`\nvalues (1, 'root1', 'root1', 'xxxx@163.com');\n\n-- 创建保留点 updatea\nsavepoint updatea;\n\n-- 插入操作 b\ninsert into `user`\nvalues (2, 'root2', 'root2', 'xxxx@163.com');\n\n-- 回滚到保留点 updatea\nrollback to updatea;\n\n-- 提交事务，只有操作 a 生效\ncommit;\n\n\n（3）执行结果\n\nselect * from user;\n\n\n结果：\n\n1\troot1\troot1\txxxx@163.com\n\n\n\n# 2.2. autocommit\n\nmysql 默认采用隐式提交策略（autocommit）。每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 start transaction 语句时，会关闭隐式提交；当 commit 或 rollback 语句执行后，事务会自动关闭，重新恢复隐式提交。\n\n通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。\n\n-- 查看 autocommit\nshow variables like 'autocommit';\n\n-- 关闭 autocommit\nset autocommit = 0;\n\n-- 开启 autocommit\nset autocommit = 1;\n\n\n\n# 3. acid\n\nacid 是数据库事务正确执行的四个基本要素。\n\n * 原子性（atomicity）\n   * 事务被视为不可分割的最小单元，事务中的所有操作要么全部提交成功，要么全部失败回滚。\n   * 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。\n * 一致性（consistency）\n   * 数据库在事务执行前后都保持一致性状态。\n   * 在一致性状态下，所有事务对一个数据的读取结果都是相同的。\n * 隔离性（isolation）\n   * 一个事务所做的修改在最终提交以前，对其它事务是不可见的。\n * 持久性（durability）\n   * 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。\n   * 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。\n\n一个支持事务（transaction）中的数据库系统，必需要具有这四种特性，否则在事务过程（transaction processing）当中无法保证数据的正确性，交易过程极可能达不到交易。\n\n * 只有满足一致性，事务的执行结果才是正确的。\n * 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。\n * 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。\n * 事务满足持久化是为了能应对系统崩溃的情况。\n\n\n\n> mysql 默认采用自动提交模式（auto commit）。也就是说，如果不显式使用 start transaction 语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。\n\n\n# 4. 事务隔离级别\n\n\n# 4.1. 事务隔离简介\n\n在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题：\n\n * 丢失修改\n * 脏读\n * 不可重复读\n * 幻读\n\n在 sql 标准中，定义了四种事务隔离级别（级别由低到高）：\n\n * 未提交读\n * 提交读\n * 可重复读\n * 串行化\n\nmysql 中查看和设置事务隔离级别：\n\n-- 查看事务隔离级别\nshow variables like 'transaction_isolation';\n\n-- 设置事务隔离级别为 read uncommitted\nset session transaction isolation level read uncommitted;\n\n-- 设置事务隔离级别为 read committed\nset session transaction isolation level read committed;\n\n-- 设置事务隔离级别为 repeatable read\nset session transaction isolation level repeatable read;\n\n-- 设置事务隔离级别为 serializable\nset session transaction isolation level serializable;\n\n\n\n# 4.2. 未提交读\n\n未提交读（read uncommitted） 是指：事务中的修改，即使没有提交，对其它事务也是可见的。\n\n未提交读的问题：事务可以读取未提交的数据，也被称为 脏读（dirty read）。\n\nt1 修改一个数据，t2 随后读取这个数据。如果 t1 撤销了这次修改，那么 t2 读取的数据是脏数据。\n\n\n\n\n# 4.3. 提交读\n\n提交读（read committed） 是指：事务提交后，其他事务才能看到它的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。提交读解决了脏读的问题。\n\n提交读是大多数数据库的默认事务隔离级别。\n\n提交读有时也叫不可重复读，它的问题是：执行两次相同的查询，得到的结果可能不一致。\n\nt2 读取一个数据，t1 对该数据做了修改。如果 t2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。\n\n\n\n\n# 4.4. 可重复读\n\n可重复读（repeatable read） 是指：保证在同一个事务中多次读取同样数据的结果是一样的。可重复读解决了不可重复读问题。\n\n可重复读是 mysql 的默认事务隔离级别。\n\n可重复读的问题：当某个事务读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务又再次读取该范围的记录时，会产生 幻读（phantom read）。\n\nt1 读取某个范围的数据，t2 在这个范围内插入新的数据，t1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。\n\n\n\n\n# 4.5. 串行化\n\n串行化（serialixable） 是指：强制事务串行执行。\n\n强制事务串行执行，则避免了所有的并发问题。串行化策略会在读取的每一行数据上都加锁，这可能导致大量的超时和锁竞争。这对于高并发应用基本上是不可接受的，所以一般不会采用这个级别。\n\n\n# 4.6. 隔离级别小结\n\n * 未提交读（read uncommitted） - 事务中的修改，即使没有提交，对其它事务也是可见的。\n * 提交读（read committed） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。\n * 重复读（repeatable read） - 保证在同一个事务中多次读取同样数据的结果是一样的。\n * 串行化（serialixable） - 强制事务串行执行。\n\n数据库隔离级别解决的问题：\n\n隔离级别   丢失修改   脏读   不可重复读   幻读\n未提交读   ✔️     ❌    ❌       ❌\n提交读    ✔️     ✔️   ❌       ❌\n可重复读   ✔️     ✔️   ✔️      ❌\n可串行化   ✔️     ✔️   ✔️      ✔️\n\n\n# 5. 死锁\n\n死锁是指两个或多个事务竞争同一资源，并请求锁定对方占用的资源，从而导致恶性循环的现象。\n\n产生死锁的场景：\n\n * 当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。\n\n * 多个事务同时锁定同一个资源时，也会产生死锁。\n\n\n# 5.1. 死锁的原因\n\n行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。record lock 是专门对索引项加锁；gap lock 是对索引项之间的间隙加锁；next-key lock 则是前面两种的组合，对索引项以其之间的间隙加锁。\n\n只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock，在 select、update 和 delete 时，除了基于唯一索引的查询之外，其它索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。主键索引也属于唯一索引，所以主键索引是不会使用 gap lock 或 next-key lock。\n\n在 mysql 中，gap lock 默认是开启的，即 innodb_locks_unsafe_for_binlog 参数值是 disable 的，且 mysql 中默认的是 rr 事务隔离级别。\n\n当我们执行以下查询 sql 时，由于 order_no 列为非唯一索引，此时又是 rr 事务隔离级别，所以 select 的加锁类型为 gap lock，这里的 gap 范围是 (4,+∞）。\n\n> select id from demo.order_record where order_no = 4 for update;\n\n执行查询 sql 语句获取的 gap lock 并不会导致阻塞，而当我们执行以下插入 sql 时，会在插入间隙上再次获取插入意向锁。插入意向锁其实也是一种 gap 锁，它与 gap lock 是冲突的，所以当其它事务持有该间隙的 gap lock 时，需要等待其它事务释放 gap lock 之后，才能获取到插入意向锁。\n\n以上事务 a 和事务 b 都持有间隙 (4,+∞）的 gap 锁，而接下来的插入操作为了获取到插入意向锁，都在等待对方事务的 gap 锁释放，于是就造成了循环等待，导致死锁。\n\n> insert into demo.order_record(order_no, status, create_date) values (5, 1, ‘2019-07-13 10:57:03’);\n\n\n\n另一个死锁场景\n\ninnodb 存储引擎的主键索引为聚簇索引，其它索引为辅助索引。如果使用辅助索引来更新数据库，就需要使用聚簇索引来更新数据库字段。如果两个更新事务使用了不同的辅助索引，或一个使用了辅助索引，一个使用了聚簇索引，就都有可能导致锁资源的循环等待。由于本身两个事务是互斥，也就构成了以上死锁的四个必要条件了。\n\n\n\n出现死锁的步骤：\n\n\n\n综上可知，在更新操作时，我们应该尽量使用主键来更新表字段，这样可以有效避免一些不必要的死锁发生。\n\n\n# 5.2. 避免死锁\n\n预防死锁的注意事项：\n\n * 在编程中尽量按照固定的顺序来处理数据库记录，假设有两个更新操作，分别更新两条相同的记录，但更新顺序不一样，有可能导致死锁；\n * 在允许幻读和不可重复读的情况下，尽量使用 rc 事务隔离级别，可以避免 gap lock 导致的死锁问题；\n * 更新表时，尽量使用主键更新；\n * 避免长事务，尽量将长事务拆解，可以降低与其它事务发生冲突的概率；\n * 设置合理的锁等待超时参数，我们可以通过 innodb_lock_wait_timeout 设置合理的等待超时阈值，特别是在一些高并发的业务中，我们可以尽量将该值设置得小一些，避免大量事务等待，占用系统资源，造成严重的性能开销。\n\n另外，我们还可以将 order_no 列设置为唯一索引列。虽然不能防止幻读，但我们可以利用它的唯一性来保证订单记录不重复创建，这种方式唯一的缺点就是当遇到重复创建订单时会抛出异常。\n\n我们还可以使用其它的方式来代替数据库实现幂等性校验。例如，使用 redis 以及 zookeeper 来实现，运行效率比数据库更佳。\n\n\n# 5.3. 解决死锁\n\n当出现死锁以后，有两种策略：\n\n * 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。\n * 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。\n\n在 innodb 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。\n\n但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。\n\n所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。为了解决死锁问题，不同数据库实现了各自的死锁检测和超时机制。innodb 的处理策略是：将持有最少行级排它锁的事务进行回滚。\n\n主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。\n\n\n# 6. 分布式事务\n\n在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为 本地事务。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。\n\n分布式事务指的是事务操作跨越多个节点，并且要求满足事务的 acid 特性。\n\n分布式事务的常见方案如下：\n\n * 两阶段提交（2pc） - 将事务的提交过程分为两个阶段来进行处理：准备阶段和提交阶段。参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。\n * 三阶段提交（3pc） - 与二阶段提交不同的是，引入超时机制。同时在协调者和参与者中都引入超时机制。将二阶段的准备阶段拆分为 2 个阶段，插入了一个 precommit 阶段，使得原先在二阶段提交中，参与者在准备之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。\n * 补偿事务（tcc）\n   * try - 操作作为一阶段，负责资源的检查和预留。\n   * confirm - 操作作为二阶段提交操作，执行真正的业务。\n   * cancel - 是预留资源的取消。\n * 本地消息表 - 在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。\n * mq 事务 - 基于 mq 的分布式事务方案其实是对本地消息表的封装。\n * saga - saga 事务核心思想是将长事务拆分为多个本地短事务，由 saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。\n\n分布式事务方案分析：\n\n * 2pc/3pc 依赖于数据库，能够很好的提供强一致性和强事务性，但相对来说延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。\n * tcc 适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。\n * 本地消息表/mq 事务 都适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。\n * saga 事务 由于 saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。 saga 相比缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。saga 事务较适用于补偿动作容易处理的场景。\n\n> 分布式事务详细说明、分析请参考：分布式事务基本原理\n\n\n# 7. 事务最佳实践\n\n高并发场景下的事务到底该如何调优？\n\n\n# 7.1. 尽量使用低级别事务隔离\n\n结合业务场景，尽量使用低级别事务隔离\n\n\n# 7.2. 避免行锁升级表锁\n\n在 innodb 中，行锁是通过索引实现的，如果不通过索引条件检索数据，行锁将会升级到表锁。我们知道，表锁是会严重影响到整张表的操作性能的，所以应该尽力避免。\n\n\n# 7.3. 缩小事务范围\n\n有时候，数据库并发访问量太大，会出现以下异常：\n\nmysqlqueryinterruptedexception: query execution was interrupted\n\n\n高并发时对一条记录进行更新的情况下，由于更新记录所在的事务还可能存在其他操作，导致一个事务比较长，当有大量请求进入时，就可能导致一些请求同时进入到事务中。\n\n又因为锁的竞争是不公平的，当多个事务同时对一条记录进行更新时，极端情况下，一个更新操作进去排队系统后，可能会一直拿不到锁，最后因超时被系统打断踢出。\n\n\n\n如上图中的操作，虽然都是在一个事务中，但锁的申请在不同时间，只有当其他操作都执行完，才会释放所有锁。因为扣除库存是更新操作，属于行锁，这将会影响到其他操作该数据的事务，所以我们应该尽量避免长时间地持有该锁，尽快释放该锁。又因为先新建订单和先扣除库存都不会影响业务，所以我们可以将扣除库存操作放到最后，也就是使用执行顺序 1，以此尽量减小锁的持有时间。\n\n在 innodb 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。\n\n知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。\n\n\n# 8. 参考资料\n\n * 《高性能 mysql》\n * 《java 性能调优实战》\n * shardingsphere 分布式事务",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 锁",frontmatter:{title:"Mysql 锁",date:"2020-09-07T07:54:19.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","锁"],permalink:"/pages/f1f151/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/04.Mysql%E9%94%81.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/04.Mysql锁.md",key:"v-41343aac",path:"/pages/f1f151/",headers:[{level:2,title:"1. 悲观锁和乐观锁",slug:"_1-悲观锁和乐观锁",normalizedTitle:"1. 悲观锁和乐观锁",charIndex:16},{level:2,title:"2. 表级锁和行级锁",slug:"_2-表级锁和行级锁",normalizedTitle:"2. 表级锁和行级锁",charIndex:608},{level:2,title:"3. 读写锁",slug:"_3-读写锁",normalizedTitle:"3. 读写锁",charIndex:1029},{level:2,title:"4. 意向锁",slug:"_4-意向锁",normalizedTitle:"4. 意向锁",charIndex:1234},{level:2,title:"5. MVCC",slug:"_5-mvcc",normalizedTitle:"5. mvcc",charIndex:2032},{level:3,title:"5.1. MVCC 思想",slug:"_5-1-mvcc-思想",normalizedTitle:"5.1. mvcc 思想",charIndex:2316},{level:3,title:"5.2. 版本号",slug:"_5-2-版本号",normalizedTitle:"5.2. 版本号",charIndex:2637},{level:3,title:"5.3. Undo 日志",slug:"_5-3-undo-日志",normalizedTitle:"5.3. undo 日志",charIndex:2865},{level:3,title:"5.4. ReadView",slug:"_5-4-readview",normalizedTitle:"5.4. readview",charIndex:3370},{level:3,title:"5.5. 快照读与当前读",slug:"_5-5-快照读与当前读",normalizedTitle:"5.5. 快照读与当前读",charIndex:4241},{level:2,title:"6. 行锁",slug:"_6-行锁",normalizedTitle:"6. 行锁",charIndex:4605},{level:2,title:"7. 参考资料",slug:"_7-参考资料",normalizedTitle:"7. 参考资料",charIndex:5592}],headersStr:"1. 悲观锁和乐观锁 2. 表级锁和行级锁 3. 读写锁 4. 意向锁 5. MVCC 5.1. MVCC 思想 5.2. 版本号 5.3. Undo 日志 5.4. ReadView 5.5. 快照读与当前读 6. 行锁 7. 参考资料",content:'# Mysql 锁\n\n\n\n\n# 1. 悲观锁和乐观锁\n\n确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性，乐观锁和悲观锁是并发控制主要采用的技术手段。\n\n * 悲观锁 - 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作\n   * 在查询完数据的时候就把事务锁起来，直到提交事务（COMMIT）\n   * 实现方式：使用数据库中的锁机制。\n * 乐观锁 - 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。\n   * 在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定\n   * 实现方式：使用 version 版本或者时间戳。\n\n【示例】乐观锁示例\n\n商品 goods 表中有一个字段 status，status 为 1 代表商品未被下单，status 为 2 代表商品已经被下单，那么我们对某个商品下单时必须确保该商品 status 为 1。假设商品的 id 为 1。\n\nselect (status,status,version) from t_goods where id=#{id}\n\nupdate t_goods\nset status=2,version=version+1\nwhere id=#{id} and version=#{version};\n\n\n> 更详细的乐观锁说可以参考：使用 mysql 乐观锁解决并发问题\n\n\n# 2. 表级锁和行级锁\n\n从数据库的锁粒度来看，MySQL 中提供了两种封锁粒度：行级锁和表级锁。\n\n * 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。\n * 行级锁（row lock） - 锁定指定的行记录。这样其它进程还是可以对同一个表中的其它记录进行操作。\n\n应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，锁竞争的发生频率就越小，系统的并发程度就越高。但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此锁粒度越小，系统开销就越大。\n\n在选择锁粒度时，需要在锁开销和并发程度之间做一个权衡。\n\n在 InnoDB 中，行锁是通过给索引上的索引项加锁来实现的。如果没有索引，InnoDB 将会通过隐藏的聚簇索引来对记录加锁。\n\n\n# 3. 读写锁\n\n * 独享锁（Exclusive），简写为 X 锁，又称写锁。使用方式：SELECT ... FOR UPDATE;\n * 共享锁（Shared），简写为 S 锁，又称读锁。使用方式：SELECT ... LOCK IN SHARE MODE;\n\n写锁和读锁的关系，简言之：独享锁存在，其他事务就不能做任何操作。\n\nInnoDB 下的行锁、间隙锁、next-key 锁统统属于独享锁。\n\n\n# 4. 意向锁\n\n当存在表级锁和行级锁的情况下，必须先申请意向锁（表级锁，但不是真的加锁），再获取行级锁。使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。\n\n意向锁是 InnoDB 自动加的，不需要用户干预。\n\n在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。\n\n意向锁规定：\n\n * IX/IS 是表锁；\n * X/S 是行锁。\n * 一个事务在获得某个数据行的 S 锁之前，必须先获得表的 IS 锁或者更强的锁；\n * 一个事务在获得某个数据行的 X 锁之前，必须先获得表的 IX 锁。\n\n通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X/IX/S/IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。\n\n各种锁的兼容关系如下：\n\n-    X   IX   S    IS\nX    ❌   ❌    ❌    ❌\nIX   ❌   ✔️   ❌    ✔️\nS    ❌   ❌    ✔️   ✔️\nIS   ❌   ✔️   ✔️   ✔️\n\n解释如下：\n\n * 任意 IS/IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁；\n * 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。）\n\n\n# 5. MVCC\n\n多版本并发控制（Multi-Version Concurrency Control, MVCC）可以视为行级锁的一个变种。它在很多情况下都避免了加锁操作，因此开销更低。不仅是 Mysql，包括 Oracle、PostgreSQL 等其他数据库都实现了各自的 MVCC，实现机制没有统一标准。\n\nMVCC 是 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。\n\n\n# 5.1. MVCC 思想\n\n加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的。\n\nMVCC 的思想是：\n\n * 保存数据在某个时间点的快照，写操作（DELETE、INSERT、UPDATE）更新最新的版本快照；而读操作去读旧版本快照，没有互斥关系。这一点和 CopyOnWrite 类似。\n * 脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。\n\n\n# 5.2. 版本号\n\nInnoDB 的 MVCC 实现是：在每行记录后面保存两个隐藏列，一个列保存行的创建时间，另一个列保存行的过期时间（这里的时间是指系统版本号）。每开始一个新事务，系统版本号会自动递增，事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。\n\n * 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。\n * 事务版本号 TRX_ID ：事务开始时的系统版本号。\n\n\n# 5.3. Undo 日志\n\nMVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。\n\n例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。\n\nINSERT INTO t(id, x) VALUES(1, "a");\nUPDATE t SET x="b" WHERE id=1;\nUPDATE t SET x="c" WHERE id=1;\n\n\n因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。\n\nINSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。\n\n\n# 5.4. ReadView\n\nMVCC 维护了一个一致性读视图 consistent read view ，主要包含了当前系统未提交的事务列表 TRX_IDs {TRX_ID_1, TRX_ID_2, ...}，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。\n\n\n\n这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：\n\n 1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；\n 2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；\n 3. 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。\n\n在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用：\n\n * TRX_ID < TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。\n * TRX_ID > TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。\n * TRX_ID_MIN <= TRX_ID <= TRX_ID_MAX，需要根据隔离级别再进行判断：\n   * 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。\n   * 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。\n\n在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。\n\n\n# 5.5. 快照读与当前读\n\n快照读\n\nMVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。\n\nSELECT * FROM table ...;\n\n\n当前读\n\nMVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。\n\nINSERT;\nUPDATE;\nDELETE;\n\n\n在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。\n\nSELECT * FROM table WHERE ? lock in share mode;\nSELECT * FROM table WHERE ? for update;\n\n\n\n# 6. 行锁\n\n行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。\n\n * Record Lock - 行锁对索引项加锁，若没有索引则使用表锁。\n * Gap Lock - 对索引项之间的间隙加锁。锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15：SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;。在 MySQL 中，gap lock 默认是开启的，即 innodb_locks_unsafe_for_binlog 参数值是 disable 的，且 MySQL 中默认的是 RR 事务隔离级别。\n * Next-key lock -它是 Record Lock 和 Gap Lock 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间。\n\n只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock。在 Select、Update 和 Delete 时，除了基于唯一索引的查询之外，其它索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。主键索引也属于唯一索引，所以主键索引是不会使用 gap lock 或 next-key lock。\n\nMVCC 不能解决幻读问题，Next-Key 锁就是为了解决幻读问题。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key 锁 可以解决幻读问题。\n\n索引分为主键索引和非主键索引两种，如果一条 SQL 语句操作了主键索引，MySQL 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL 会先锁定该非主键索引，再锁定相关的主键索引。在 UPDATE、DELETE 操作时，MySQL 不仅锁定 WHERE 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key lock。\n\n当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，InnoDB 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。\n\n\n# 7. 参考资料\n\n * 《高性能 MySQL》\n * 《Java 性能调优实战》\n * 数据库系统原理\n * 数据库两大神器【索引和锁】\n * 使用 mysql 乐观锁解决并发问题',normalizedContent:'# mysql 锁\n\n\n\n\n# 1. 悲观锁和乐观锁\n\n确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性，乐观锁和悲观锁是并发控制主要采用的技术手段。\n\n * 悲观锁 - 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作\n   * 在查询完数据的时候就把事务锁起来，直到提交事务（commit）\n   * 实现方式：使用数据库中的锁机制。\n * 乐观锁 - 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。\n   * 在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定\n   * 实现方式：使用 version 版本或者时间戳。\n\n【示例】乐观锁示例\n\n商品 goods 表中有一个字段 status，status 为 1 代表商品未被下单，status 为 2 代表商品已经被下单，那么我们对某个商品下单时必须确保该商品 status 为 1。假设商品的 id 为 1。\n\nselect (status,status,version) from t_goods where id=#{id}\n\nupdate t_goods\nset status=2,version=version+1\nwhere id=#{id} and version=#{version};\n\n\n> 更详细的乐观锁说可以参考：使用 mysql 乐观锁解决并发问题\n\n\n# 2. 表级锁和行级锁\n\n从数据库的锁粒度来看，mysql 中提供了两种封锁粒度：行级锁和表级锁。\n\n * 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。\n * 行级锁（row lock） - 锁定指定的行记录。这样其它进程还是可以对同一个表中的其它记录进行操作。\n\n应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，锁竞争的发生频率就越小，系统的并发程度就越高。但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此锁粒度越小，系统开销就越大。\n\n在选择锁粒度时，需要在锁开销和并发程度之间做一个权衡。\n\n在 innodb 中，行锁是通过给索引上的索引项加锁来实现的。如果没有索引，innodb 将会通过隐藏的聚簇索引来对记录加锁。\n\n\n# 3. 读写锁\n\n * 独享锁（exclusive），简写为 x 锁，又称写锁。使用方式：select ... for update;\n * 共享锁（shared），简写为 s 锁，又称读锁。使用方式：select ... lock in share mode;\n\n写锁和读锁的关系，简言之：独享锁存在，其他事务就不能做任何操作。\n\ninnodb 下的行锁、间隙锁、next-key 锁统统属于独享锁。\n\n\n# 4. 意向锁\n\n当存在表级锁和行级锁的情况下，必须先申请意向锁（表级锁，但不是真的加锁），再获取行级锁。使用意向锁（intention locks）可以更容易地支持多粒度封锁。\n\n意向锁是 innodb 自动加的，不需要用户干预。\n\n在存在行级锁和表级锁的情况下，事务 t 想要对表 a 加 x 锁，就需要先检测是否有其它事务对表 a 或者表 a 中的任意一行加了锁，那么就需要对表 a 的每一行都检测一次，这是非常耗时的。\n\n意向锁规定：\n\n * ix/is 是表锁；\n * x/s 是行锁。\n * 一个事务在获得某个数据行的 s 锁之前，必须先获得表的 is 锁或者更强的锁；\n * 一个事务在获得某个数据行的 x 锁之前，必须先获得表的 ix 锁。\n\n通过引入意向锁，事务 t 想要对表 a 加 x 锁，只需要先检测是否有其它事务对表 a 加了 x/ix/s/is 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 t 加 x 锁失败。\n\n各种锁的兼容关系如下：\n\n-    x   ix   s    is\nx    ❌   ❌    ❌    ❌\nix   ❌   ✔️   ❌    ✔️\ns    ❌   ❌    ✔️   ✔️\nis   ❌   ✔️   ✔️   ✔️\n\n解释如下：\n\n * 任意 is/ix 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁；\n * 这里兼容关系针对的是表级锁，而表级的 ix 锁和行级的 x 锁兼容，两个事务可以对两个数据行加 x 锁。（事务 t1 想要对数据行 r1 加 x 锁，事务 t2 想要对同一个表的数据行 r2 加 x 锁，两个事务都需要对该表加 ix 锁，但是 ix 锁是兼容的，并且 ix 锁与行级的 x 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。）\n\n\n# 5. mvcc\n\n多版本并发控制（multi-version concurrency control, mvcc）可以视为行级锁的一个变种。它在很多情况下都避免了加锁操作，因此开销更低。不仅是 mysql，包括 oracle、postgresql 等其他数据库都实现了各自的 mvcc，实现机制没有统一标准。\n\nmvcc 是 innodb 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 mvcc。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 mvcc 无法实现。\n\n\n# 5.1. mvcc 思想\n\n加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的。\n\nmvcc 的思想是：\n\n * 保存数据在某个时间点的快照，写操作（delete、insert、update）更新最新的版本快照；而读操作去读旧版本快照，没有互斥关系。这一点和 copyonwrite 类似。\n * 脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，mvcc 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。\n\n\n# 5.2. 版本号\n\ninnodb 的 mvcc 实现是：在每行记录后面保存两个隐藏列，一个列保存行的创建时间，另一个列保存行的过期时间（这里的时间是指系统版本号）。每开始一个新事务，系统版本号会自动递增，事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。\n\n * 系统版本号 sys_id：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。\n * 事务版本号 trx_id ：事务开始时的系统版本号。\n\n\n# 5.3. undo 日志\n\nmvcc 的多版本指的是多个版本的快照，快照存储在 undo 日志中，该日志通过回滚指针 roll_ptr 把一个数据行的所有快照连接起来。\n\n例如在 mysql 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。\n\ninsert into t(id, x) values(1, "a");\nupdate t set x="b" where id=1;\nupdate t set x="c" where id=1;\n\n\n因为没有使用 start transaction 将上面的操作当成一个事务来执行，根据 mysql 的 autocommit 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 trx_id 和操作之外，还记录了一个 bit 的 del 字段，用于标记是否被删除。\n\ninsert、update、delete 操作会创建一个日志，并将事务版本号 trx_id 写入。delete 可以看成是一个特殊的 update，还会额外将 del 字段设置为 1。\n\n\n# 5.4. readview\n\nmvcc 维护了一个一致性读视图 consistent read view ，主要包含了当前系统未提交的事务列表 trx_ids {trx_id_1, trx_id_2, ...}，还有该列表的最小值 trx_id_min 和 trx_id_max。\n\n\n\n这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：\n\n 1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；\n 2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；\n 3. 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。\n\n在进行 select 操作时，根据数据行快照的 trx_id 与 trx_id_min 和 trx_id_max 之间的关系，从而判断数据行快照是否可以使用：\n\n * trx_id < trx_id_min，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。\n * trx_id > trx_id_max，表示该数据行快照是在事务启动之后被更改的，因此不可使用。\n * trx_id_min <= trx_id <= trx_id_max，需要根据隔离级别再进行判断：\n   * 提交读：如果 trx_id 在 trx_ids 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。\n   * 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。\n\n在数据行快照不可使用的情况下，需要沿着 undo log 的回滚指针 roll_ptr 找到下一个快照，再进行上面的判断。\n\n\n# 5.5. 快照读与当前读\n\n快照读\n\nmvcc 的 select 操作是快照中的数据，不需要进行加锁操作。\n\nselect * from table ...;\n\n\n当前读\n\nmvcc 其它会对数据库进行修改的操作（insert、update、delete）需要进行加锁操作，从而读取最新的数据。可以看到 mvcc 并不是完全不用加锁，而只是避免了 select 的加锁操作。\n\ninsert;\nupdate;\ndelete;\n\n\n在进行 select 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 s 锁，第二个需要加 x 锁。\n\nselect * from table where ? lock in share mode;\nselect * from table where ? for update;\n\n\n\n# 6. 行锁\n\n行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。\n\n * record lock - 行锁对索引项加锁，若没有索引则使用表锁。\n * gap lock - 对索引项之间的间隙加锁。锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15：select c from t where c between 10 and 20 for update;。在 mysql 中，gap lock 默认是开启的，即 innodb_locks_unsafe_for_binlog 参数值是 disable 的，且 mysql 中默认的是 rr 事务隔离级别。\n * next-key lock -它是 record lock 和 gap lock 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间。\n\n只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock。在 select、update 和 delete 时，除了基于唯一索引的查询之外，其它索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。主键索引也属于唯一索引，所以主键索引是不会使用 gap lock 或 next-key lock。\n\nmvcc 不能解决幻读问题，next-key 锁就是为了解决幻读问题。在可重复读（repeatable read）隔离级别下，使用 mvcc + next-key 锁 可以解决幻读问题。\n\n索引分为主键索引和非主键索引两种，如果一条 sql 语句操作了主键索引，mysql 就会锁定这条主键索引；如果一条语句操作了非主键索引，mysql 会先锁定该非主键索引，再锁定相关的主键索引。在 update、delete 操作时，mysql 不仅锁定 where 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key lock。\n\n当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，innodb 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。\n\n\n# 7. 参考资料\n\n * 《高性能 mysql》\n * 《java 性能调优实战》\n * 数据库系统原理\n * 数据库两大神器【索引和锁】\n * 使用 mysql 乐观锁解决并发问题',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 索引",frontmatter:{title:"Mysql 索引",date:"2020-07-16T11:14:07.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","索引"],permalink:"/pages/fcb19c/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/05.Mysql%E7%B4%A2%E5%BC%95.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/05.Mysql索引.md",key:"v-54c62a9f",path:"/pages/fcb19c/",headers:[{level:2,title:"1. 索引简介",slug:"_1-索引简介",normalizedTitle:"1. 索引简介",charIndex:261},{level:3,title:"1.1. 索引的优缺点",slug:"_1-1-索引的优缺点",normalizedTitle:"1.1. 索引的优缺点",charIndex:388},{level:3,title:"1.2. 何时使用索引",slug:"_1-2-何时使用索引",normalizedTitle:"1.2. 何时使用索引",charIndex:903},{level:2,title:"2. 索引的数据结构",slug:"_2-索引的数据结构",normalizedTitle:"2. 索引的数据结构",charIndex:1219},{level:3,title:"数组",slug:"数组",normalizedTitle:"数组",charIndex:1299},{level:3,title:"2.1. 哈希索引",slug:"_2-1-哈希索引",normalizedTitle:"2.1. 哈希索引",charIndex:1491},{level:3,title:"2.2. B 树索引",slug:"_2-2-b-树索引",normalizedTitle:"2.2. b 树索引",charIndex:2376},{level:4,title:"二叉搜索树",slug:"二叉搜索树",normalizedTitle:"二叉搜索树",charIndex:2657},{level:4,title:"B+ 树",slug:"b-树",normalizedTitle:"b+ 树",charIndex:3057},{level:3,title:"2.3. 全文索引",slug:"_2-3-全文索引",normalizedTitle:"2.3. 全文索引",charIndex:4137},{level:3,title:"2.4. 空间数据索引",slug:"_2-4-空间数据索引",normalizedTitle:"2.4. 空间数据索引",charIndex:4303},{level:2,title:"3. 索引的类型",slug:"_3-索引的类型",normalizedTitle:"3. 索引的类型",charIndex:4419},{level:3,title:"3.1. 主键索引（PRIMARY）",slug:"_3-1-主键索引-primary",normalizedTitle:"3.1. 主键索引（primary）",charIndex:4455},{level:3,title:"3.2. 唯一索引（UNIQUE）",slug:"_3-2-唯一索引-unique",normalizedTitle:"3.2. 唯一索引（unique）",charIndex:4651},{level:3,title:"3.3. 普通索引（INDEX）",slug:"_3-3-普通索引-index",normalizedTitle:"3.3. 普通索引（index）",charIndex:4788},{level:3,title:"3.4. 全文索引（FULLTEXT）",slug:"_3-4-全文索引-fulltext",normalizedTitle:"3.4. 全文索引（fulltext）",charIndex:4902},{level:3,title:"3.5. 联合索引",slug:"_3-5-联合索引",normalizedTitle:"3.5. 联合索引",charIndex:5321},{level:2,title:"4. 索引的策略",slug:"_4-索引的策略",normalizedTitle:"4. 索引的策略",charIndex:5492},{level:3,title:"4.1. 索引基本原则",slug:"_4-1-索引基本原则",normalizedTitle:"4.1. 索引基本原则",charIndex:5745},{level:3,title:"4.2. 独立的列",slug:"_4-2-独立的列",normalizedTitle:"4.2. 独立的列",charIndex:5962},{level:3,title:"4.3. 覆盖索引",slug:"_4-3-覆盖索引",normalizedTitle:"4.3. 覆盖索引",charIndex:6209},{level:3,title:"4.4. 使用索引来排序",slug:"_4-4-使用索引来排序",normalizedTitle:"4.4. 使用索引来排序",charIndex:6982},{level:3,title:"4.5. 前缀索引",slug:"_4-5-前缀索引",normalizedTitle:"4.5. 前缀索引",charIndex:7314},{level:3,title:"4.6. 最左前缀匹配原则",slug:"_4-6-最左前缀匹配原则",normalizedTitle:"4.6. 最左前缀匹配原则",charIndex:8090},{level:3,title:"4.7. = 和 in 可以乱序",slug:"_4-7-和-in-可以乱序",normalizedTitle:"4.7. = 和 in 可以乱序",charIndex:9017},{level:2,title:"5. 索引最佳实践",slug:"_5-索引最佳实践",normalizedTitle:"5. 索引最佳实践",charIndex:9261},{level:2,title:"6. 参考资料",slug:"_6-参考资料",normalizedTitle:"6. 参考资料",charIndex:9553}],headersStr:"1. 索引简介 1.1. 索引的优缺点 1.2. 何时使用索引 2. 索引的数据结构 数组 2.1. 哈希索引 2.2. B 树索引 二叉搜索树 B+ 树 2.3. 全文索引 2.4. 空间数据索引 3. 索引的类型 3.1. 主键索引（PRIMARY） 3.2. 唯一索引（UNIQUE） 3.3. 普通索引（INDEX） 3.4. 全文索引（FULLTEXT） 3.5. 联合索引 4. 索引的策略 4.1. 索引基本原则 4.2. 独立的列 4.3. 覆盖索引 4.4. 使用索引来排序 4.5. 前缀索引 4.6. 最左前缀匹配原则 4.7. = 和 in 可以乱序 5. 索引最佳实践 6. 参考资料",content:"# Mysql 索引\n\n> 索引是提高 MySQL 查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的 SQL 才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。\n> \n> 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。\n\n\n\n\n# 1. 索引简介\n\n索引是数据库为了提高查找效率的一种数据结构。\n\n索引对于良好的性能非常关键，在数据量小且负载较低时，不恰当的索引对于性能的影响可能还不明显；但随着数据量逐渐增大，性能则会急剧下降。因此，索引优化应该是查询性能优化的最有效手段。\n\n\n# 1.1. 索引的优缺点\n\nB 树是最常见的索引，按照顺序存储数据，所以 Mysql 可以用来做 ORDER BY 和 GROUP BY 操作。因为数据是有序的，所以 B 树也就会将相关的列值都存储在一起。最后，因为索引中存储了实际的列值，所以某些查询只使用索引就能够完成全部查询。\n\n✔ 索引的优点：\n\n * 索引大大减少了服务器需要扫描的数据量，从而加快检索速度。\n * 索引可以帮助服务器避免排序和临时表。\n * 索引可以将随机 I/O 变为顺序 I/O。\n * 支持行级锁的数据库，如 InnoDB 会在访问行的时候加锁。使用索引可以减少访问的行数，从而减少锁的竞争，提高并发。\n * 唯一索引可以确保每一行数据的唯一性，通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能。\n\n❌ 索引的缺点：\n\n * 创建和维护索引要耗费时间，这会随着数据量的增加而增加。\n * 索引需要占用额外的物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。\n * 写操作（INSERT/UPDATE/DELETE）时很可能需要更新索引，导致数据库的写操作性能降低。\n\n\n# 1.2. 何时使用索引\n\n> 索引能够轻易将查询性能提升几个数量级。\n\n✔ 什么情况适用索引：\n\n * 频繁读操作（ SELECT ）\n * 表的数据量比较大。\n * 列名经常出现在 WHERE 或连接（JOIN）条件中。\n\n❌ 什么情况不适用索引：\n\n * 频繁写操作（ INSERT/UPDATE/DELETE ），也就意味着需要更新索引。\n * 列名不经常出现在 WHERE 或连接（JOIN）条件中，也就意味着索引会经常无法命中，没有意义，还增加空间开销。\n * 非常小的表，对于非常小的表，大部分情况下简单的全表扫描更高效。\n * 特大型的表，建立和使用索引的代价将随之增长。可以考虑使用分区技术或 Nosql。\n\n\n# 2. 索引的数据结构\n\n在 Mysql 中，索引是在存储引擎层而不是服务器层实现的。所以，并没有统一的索引标准；不同存储引擎的索引的数据结构也不相同。\n\n\n# 数组\n\n数组是用连续的内存空间来存储数据，并且支持随机访问。\n\n有序数组可以使用二分查找法，其时间复杂度为 O(log n)，无论是等值查询还是范围查询，都非常高效。\n\n但数组有两个重要限制：\n\n * 数组的空间大小固定，如果要扩容只能采用复制数组的方式。\n * 插入、删除时间复杂度为 O(n)。\n\n这意味着，如果使用数组作为索引，如果要保证数组有序，其更新操作代价高昂。\n\n\n# 2.1. 哈希索引\n\n哈希表是一种以键 - 值（key-value）对形式存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 Value。\n\n哈希表 使用 哈希函数 组织数据，以支持快速插入和搜索的数据结构。哈希表的本质是一个数组，其思路是：使用 Hash 函数将 Key 转换为数组下标，利用数组的随机访问特性，使得我们能在 O(1) 的时间代价内完成检索。\n\n\n\n有两种不同类型的哈希表：哈希集合 和 哈希映射。\n\n * 哈希集合 是集合数据结构的实现之一，用于存储非重复值。\n * 哈希映射 是映射 数据结构的实现之一，用于存储键值对。\n\n哈希索引基于哈希表实现，只适用于等值查询。对于每一行数据，哈希索引都会将所有的索引列计算一个哈希码（hashcode），哈希码是一个较小的值。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。\n\n在 Mysql 中，只有 Memory 存储引擎显示支持哈希索引。\n\n✔ 哈希索引的优点：\n\n * 因为索引数据结构紧凑，所以查询速度非常快。\n\n❌ 哈希索引的缺点：\n\n * 哈希索引值包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能影响不大。\n * 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。\n * 哈希索引不支持部分索引匹配查找，因为哈希索引时使用索引列的全部内容来进行哈希计算的。如，在数据列 (A,B) 上建立哈希索引，如果查询只有数据列 A，无法使用该索引。\n * 哈希索引只支持等值比较查询，包括 =、IN()、<=>；不支持任何范围查询，如 WHERE price > 100。\n * 哈希索引有可能出现哈希冲突\n   * 出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。\n   * 如果哈希冲突多的话，维护索引的代价会很高。\n\n> 因为种种限制，所以哈希索引只适用于特定的场合。而一旦使用哈希索引，则它带来的性能提升会非常显著。\n\n\n# 2.2. B 树索引\n\n通常我们所说的索引是指B-Tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用B-Tree这个术语，是因为 MySQL 在CREATE TABLE或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 InnoDB 就是使用的B+Tree。\n\nB+Tree中的 B 是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。\n\n# 二叉搜索树\n\n二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。其查询时间复杂度是 $$O(log(N))$$。\n\n当然为了维持 $$O(log(N))$$ 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 $$O(log(N))$$。\n\n随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗，相对于内存存取，I/O 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 I/O 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 I/O 存取次数？\n\n一种行之有效的解决方法是减少树的深度，将二叉树变为 N 叉树（多路搜索树），而 B+ 树就是一种多路搜索树。\n\n# B+ 树\n\nB+ 树索引适用于全键值查找、键值范围查找和键前缀查找，其中键前缀查找只适用于最左前缀查找。\n\n理解B+Tree时，只需要理解其最重要的两个特征即可：\n\n * 第一，所有的关键字（可以理解为数据）都存储在叶子节点，非叶子节点并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。\n * 其次，所有的叶子节点由指针连接。如下图为简化了的B+Tree。\n\n\n\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n * 聚簇索引（clustered）：又称为主键索引，其叶子节点存的是整行数据。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。InnoDB 的聚簇索引实际是在同一个结构中保存了 B 树的索引和数据行。\n * 非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary）。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。\n\n聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n聚簇索引和非聚簇索引的查询有什么区别\n\n * 如果语句是 select * from T where ID=500，即聚簇索引查询方式，则只需要搜索 ID 这棵 B+ 树；\n * 如果语句是 select * from T where k=5，即非聚簇索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非聚簇索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n\n显然，主键长度越小，非聚簇索引的叶子节点就越小，非聚簇索引占用的空间也就越小。\n\n自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。从性能和存储空间方面考量，自增主键往往是更合理的选择。有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：\n\n * 只有一个索引；\n * 该索引必须是唯一索引。\n\n由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。\n\n这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。\n\n\n# 2.3. 全文索引\n\nMyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 MATCH AGAINST，而不是普通的 WHERE。\n\n全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。\n\nInnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。\n\n\n# 2.4. 空间数据索引\n\nMyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。\n\n必须使用 GIS 相关的函数来维护数据。\n\n\n# 3. 索引的类型\n\n主流的关系型数据库一般都支持以下索引类型：\n\n\n# 3.1. 主键索引（PRIMARY）\n\n主键索引：一种特殊的唯一索引，不允许有空值。一个表只能有一个主键（在 InnoDB 中本质上即聚簇索引），一般是在建表的时候同时创建主键索引。\n\nCREATE TABLE `table` (\n    `id` int(11) NOT NULL AUTO_INCREMENT,\n    ...\n    PRIMARY KEY (`id`)\n)\n\n\n\n# 3.2. 唯一索引（UNIQUE）\n\n唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。\n\nCREATE TABLE `table` (\n    ...\n    UNIQUE indexName (title(length))\n)\n\n\n\n# 3.3. 普通索引（INDEX）\n\n普通索引：最基本的索引，没有任何限制。\n\nCREATE TABLE `table` (\n    ...\n    INDEX index_name (title(length))\n)\n\n\n\n# 3.4. 全文索引（FULLTEXT）\n\n全文索引：主要用来查找文本中的关键字，而不是直接与索引中的值相比较。\n\n全文索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的 WHERE 语句的参数匹配。全文索引配合 match against 操作使用，而不是一般的 WHERE 语句加 LIKE。它可以在 CREATE TABLE，ALTER TABLE ，CREATE INDEX 使用，不过目前只有 char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用 CREATE INDEX 创建全文索引，要比先为一张表建立全文索引然后再将数据写入的速度快很多。\n\nCREATE TABLE `table` (\n    `content` text CHARACTER NULL,\n    ...\n    FULLTEXT (content)\n)\n\n\n\n# 3.5. 联合索引\n\n组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。\n\nCREATE TABLE `table` (\n    ...\n    INDEX index_name (title(length), title(length), ...)\n)\n\n\n\n# 4. 索引的策略\n\n假设有以下表：\n\nCREATE TABLE `t` (\n  `id` int(11) NOT NULL,\n  `city` varchar(16) NOT NULL,\n  `name` varchar(16) NOT NULL,\n  `age` int(11) NOT NULL,\n  `addr` varchar(128) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  KEY `city` (`city`)\n) ENGINE=InnoDB;\n\n\n\n# 4.1. 索引基本原则\n\n * 索引不是越多越好，不要为所有列都创建索引。要考虑到索引的维护代价、空间占用和查询时回表的代价。索引一定是按需创建的，并且要尽可能确保足够轻量。一旦创建了多字段的联合索引，我们要考虑尽可能利用索引本身完成数据查询，减少回表的成本。\n * 要尽量避免冗余和重复索引。\n * 要考虑删除未使用的索引。\n * 尽量的扩展索引，不要新建索引。\n * 频繁作为 WHERE 过滤条件的列应该考虑添加索引。\n\n\n# 4.2. 独立的列\n\n“独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。\n\n对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。\n\n如果查询中的列不是独立的列，则数据库不会使用索引。\n\n❌ 错误示例：\n\nSELECT actor_id FROM actor WHERE actor_id + 1 = 5;\nSELECT ... WHERE TO_DAYS(current_date) - TO_DAYS(date_col) <= 10;\n\n\n\n# 4.3. 覆盖索引\n\n覆盖索引是指，索引上的信息足够满足查询请求，不需要回表查询数据。\n\n【示例】范围查询\n\ncreate table T (\nID int primary key,\nk int NOT NULL DEFAULT 0,\ns varchar(16) NOT NULL DEFAULT '',\nindex k(k))\nengine=InnoDB;\n\ninsert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');\n\nselect * from T where k between 3 and 5\n\n\n需要执行几次树的搜索操作，会扫描多少行？\n\n 1. 在 k 索引树上找到 k=3 的记录，取得 ID = 300；\n 2. 再到 ID 索引树查到 ID=300 对应的 R3；\n 3. 在 k 索引树取下一个值 k=5，取得 ID=500；\n 4. 再回到 ID 索引树查到 ID=500 对应的 R4；\n 5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。\n\n在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。\n\n如果执行的语句是 select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。索引包含所有需要查询的字段的值，称为覆盖索引。\n\n由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。\n\n\n# 4.4. 使用索引来排序\n\nMysql 有两种方式可以生成排序结果：通过排序操作；或者按索引顺序扫描。\n\n索引最好既满足排序，又用于查找行。这样，就可以通过命中覆盖索引直接将结果查出来，也就不再需要排序了。\n\n这样整个查询语句的执行流程就变成了：\n\n 1. 从索引 (city,name,age) 找到第一个满足 city='杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；\n 2. 从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；\n 3. 重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。\n\n\n# 4.5. 前缀索引\n\n有时候需要索引很长的字符列，这会让索引变得大且慢。\n\n这时，可以使用前缀索引，即只索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。对于 BLOB/TEXT/VARCHAR 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。\n\n索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。如果存在多条命中前缀索引的情况，就需要依次扫描，直到最终找到正确记录。\n\n使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。\n\n那么，如何确定前缀索引合适的长度呢？\n\n可以使用下面这个语句，算出这个列上有多少个不同的值：\n\nselect count(distinct email) as L from SUser;\n\n\n然后，依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引，可以用这个语句：\n\nselect\n  count(distinct left(email,4)）as L4,\n  count(distinct left(email,5)）as L5,\n  count(distinct left(email,6)）as L6,\n  count(distinct left(email,7)）as L7,\nfrom SUser;\n\n\n当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，你就可以选择前缀长度为 6。\n\n此外，order by 无法使用前缀索引，无法把前缀索引用作覆盖索引。\n\n\n# 4.6. 最左前缀匹配原则\n\n不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。\n\nMySQL 会一直向右匹配直到遇到范围查询 (>,<,BETWEEN,LIKE) 就停止匹配。\n\n * 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。\n * 如果是联合索引，那么 key 也由多个列组成，同时，索引只能用于查找 key 是否存在（相等），遇到范围查询(>、<、between、like 左匹配)等就不能进一步匹配了，后续退化为线性查找。\n * 因此，列的排列顺序决定了可命中索引的列数。\n\n不要为每个列都创建独立索引。\n\n将选择性高的列或基数大的列优先排在多列索引最前列。但有时，也需要考虑 WHERE 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。\n\n例如：a = 1 and b = 2 and c > 3 and d = 4，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d 的顺序可以任意调整。\n\n让选择性最强的索引列放在前面，索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。\n\n例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。\n\nSELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,\nCOUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,\nCOUNT(*)\nFROM payment;\n\n\n   staff_id_selectivity: 0.0001\ncustomer_id_selectivity: 0.0373\n               COUNT(*): 16049\n\n\n\n# 4.7. = 和 in 可以乱序\n\n不需要考虑 =、IN 等的顺序，Mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。\n\n【示例】如有索引 (a, b, c, d)，查询条件 c > 3 and b = 2 and a = 1 and d < 4 与 a = 1 and c > 3 and b = 2 and d < 4 等顺序都是可以的，MySQL 会自动优化为 a = 1 and b = 2 and c > 3 and d < 4，依次命中 a、b、c、d。\n\n\n# 5. 索引最佳实践\n\n创建了索引，并非一定有效。比如不满足前缀索引、最左前缀匹配原则、查询条件涉及函数计算等情况都无法使用索引。此外，即使 SQL 本身符合索引的使用条件，MySQL 也会通过评估各种查询方式的代价，来决定是否走索引，以及走哪个索引。\n\n因此，在尝试通过索引进行 SQL 性能优化的时候，务必通过执行计划（EXPLAIN）或实际的效果来确认索引是否能有效改善性能问题，否则增加了索引不但没解决性能问题，还增加了数据库增删改的负担。如果对 EXPLAIN 给出的执行计划有疑问的话，你还可以利用 optimizer_trace 查看详细的执行计划做进一步分析。\n\n\n# 6. 参考资料\n\n * 《高性能 MySQL》\n * 数据库两大神器【索引和锁】\n * MySQL 索引背后的数据结构及算法原理\n * MySQL 实战 45 讲",normalizedContent:"# mysql 索引\n\n> 索引是提高 mysql 查询性能的一个重要途径，但过多的索引可能会导致过高的磁盘使用率以及过高的内存占用，从而影响应用程序的整体性能。应当尽量避免事后才想起添加索引，因为事后可能需要监控大量的 sql 才能定位到问题所在，而且添加索引的时间肯定是远大于初始添加索引所需要的时间，可见索引的添加也是非常有技术含量的。\n> \n> 接下来将向你展示一系列创建高性能索引的策略，以及每条策略其背后的工作原理。但在此之前，先了解与索引相关的一些算法和数据结构，将有助于更好的理解后文的内容。\n\n\n\n\n# 1. 索引简介\n\n索引是数据库为了提高查找效率的一种数据结构。\n\n索引对于良好的性能非常关键，在数据量小且负载较低时，不恰当的索引对于性能的影响可能还不明显；但随着数据量逐渐增大，性能则会急剧下降。因此，索引优化应该是查询性能优化的最有效手段。\n\n\n# 1.1. 索引的优缺点\n\nb 树是最常见的索引，按照顺序存储数据，所以 mysql 可以用来做 order by 和 group by 操作。因为数据是有序的，所以 b 树也就会将相关的列值都存储在一起。最后，因为索引中存储了实际的列值，所以某些查询只使用索引就能够完成全部查询。\n\n✔ 索引的优点：\n\n * 索引大大减少了服务器需要扫描的数据量，从而加快检索速度。\n * 索引可以帮助服务器避免排序和临时表。\n * 索引可以将随机 i/o 变为顺序 i/o。\n * 支持行级锁的数据库，如 innodb 会在访问行的时候加锁。使用索引可以减少访问的行数，从而减少锁的竞争，提高并发。\n * 唯一索引可以确保每一行数据的唯一性，通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能。\n\n❌ 索引的缺点：\n\n * 创建和维护索引要耗费时间，这会随着数据量的增加而增加。\n * 索引需要占用额外的物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立组合索引那么需要的空间就会更大。\n * 写操作（insert/update/delete）时很可能需要更新索引，导致数据库的写操作性能降低。\n\n\n# 1.2. 何时使用索引\n\n> 索引能够轻易将查询性能提升几个数量级。\n\n✔ 什么情况适用索引：\n\n * 频繁读操作（ select ）\n * 表的数据量比较大。\n * 列名经常出现在 where 或连接（join）条件中。\n\n❌ 什么情况不适用索引：\n\n * 频繁写操作（ insert/update/delete ），也就意味着需要更新索引。\n * 列名不经常出现在 where 或连接（join）条件中，也就意味着索引会经常无法命中，没有意义，还增加空间开销。\n * 非常小的表，对于非常小的表，大部分情况下简单的全表扫描更高效。\n * 特大型的表，建立和使用索引的代价将随之增长。可以考虑使用分区技术或 nosql。\n\n\n# 2. 索引的数据结构\n\n在 mysql 中，索引是在存储引擎层而不是服务器层实现的。所以，并没有统一的索引标准；不同存储引擎的索引的数据结构也不相同。\n\n\n# 数组\n\n数组是用连续的内存空间来存储数据，并且支持随机访问。\n\n有序数组可以使用二分查找法，其时间复杂度为 o(log n)，无论是等值查询还是范围查询，都非常高效。\n\n但数组有两个重要限制：\n\n * 数组的空间大小固定，如果要扩容只能采用复制数组的方式。\n * 插入、删除时间复杂度为 o(n)。\n\n这意味着，如果使用数组作为索引，如果要保证数组有序，其更新操作代价高昂。\n\n\n# 2.1. 哈希索引\n\n哈希表是一种以键 - 值（key-value）对形式存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 value。\n\n哈希表 使用 哈希函数 组织数据，以支持快速插入和搜索的数据结构。哈希表的本质是一个数组，其思路是：使用 hash 函数将 key 转换为数组下标，利用数组的随机访问特性，使得我们能在 o(1) 的时间代价内完成检索。\n\n\n\n有两种不同类型的哈希表：哈希集合 和 哈希映射。\n\n * 哈希集合 是集合数据结构的实现之一，用于存储非重复值。\n * 哈希映射 是映射 数据结构的实现之一，用于存储键值对。\n\n哈希索引基于哈希表实现，只适用于等值查询。对于每一行数据，哈希索引都会将所有的索引列计算一个哈希码（hashcode），哈希码是一个较小的值。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。\n\n在 mysql 中，只有 memory 存储引擎显示支持哈希索引。\n\n✔ 哈希索引的优点：\n\n * 因为索引数据结构紧凑，所以查询速度非常快。\n\n❌ 哈希索引的缺点：\n\n * 哈希索引值包含哈希值和行指针，而不存储字段值，所以不能使用索引中的值来避免读取行。不过，访问内存中的行的速度很快，所以大部分情况下这一点对性能影响不大。\n * 哈希索引数据不是按照索引值顺序存储的，所以无法用于排序。\n * 哈希索引不支持部分索引匹配查找，因为哈希索引时使用索引列的全部内容来进行哈希计算的。如，在数据列 (a,b) 上建立哈希索引，如果查询只有数据列 a，无法使用该索引。\n * 哈希索引只支持等值比较查询，包括 =、in()、<=>；不支持任何范围查询，如 where price > 100。\n * 哈希索引有可能出现哈希冲突\n   * 出现哈希冲突时，必须遍历链表中所有的行指针，逐行比较，直到找到符合条件的行。\n   * 如果哈希冲突多的话，维护索引的代价会很高。\n\n> 因为种种限制，所以哈希索引只适用于特定的场合。而一旦使用哈希索引，则它带来的性能提升会非常显著。\n\n\n# 2.2. b 树索引\n\n通常我们所说的索引是指b-tree索引，它是目前关系型数据库中查找数据最为常用和有效的索引，大多数存储引擎都支持这种索引。使用b-tree这个术语，是因为 mysql 在create table或其它语句中使用了这个关键字，但实际上不同的存储引擎可能使用不同的数据结构，比如 innodb 就是使用的b+tree。\n\nb+tree中的 b 是指balance，意为平衡。需要注意的是，b+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。\n\n# 二叉搜索树\n\n二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。其查询时间复杂度是 $$o(log(n))$$。\n\n当然为了维持 $$o(log(n))$$ 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 $$o(log(n))$$。\n\n随着数据库中数据的增加，索引本身大小随之增加，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 i/o 消耗，相对于内存存取，i/o 存取的消耗要高几个数量级。可以想象一下一棵几百万节点的二叉树的深度是多少？如果将这么大深度的一颗二叉树放磁盘上，每读取一个节点，需要一次磁盘的 i/o 读取，整个查找的耗时显然是不能够接受的。那么如何减少查找过程中的 i/o 存取次数？\n\n一种行之有效的解决方法是减少树的深度，将二叉树变为 n 叉树（多路搜索树），而 b+ 树就是一种多路搜索树。\n\n# b+ 树\n\nb+ 树索引适用于全键值查找、键值范围查找和键前缀查找，其中键前缀查找只适用于最左前缀查找。\n\n理解b+tree时，只需要理解其最重要的两个特征即可：\n\n * 第一，所有的关键字（可以理解为数据）都存储在叶子节点，非叶子节点并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。\n * 其次，所有的叶子节点由指针连接。如下图为简化了的b+tree。\n\n\n\n根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n * 聚簇索引（clustered）：又称为主键索引，其叶子节点存的是整行数据。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。innodb 的聚簇索引实际是在同一个结构中保存了 b 树的索引和数据行。\n * 非主键索引的叶子节点内容是主键的值。在 innodb 里，非主键索引也被称为二级索引（secondary）。数据存储在一个位置，索引存储在另一个位置，索引中包含指向数据存储位置的指针。可以有多个，小于 249 个。\n\n聚簇表示数据行和相邻的键值紧凑地存储在一起，因为数据紧凑，所以访问快。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个聚簇索引。\n\n聚簇索引和非聚簇索引的查询有什么区别\n\n * 如果语句是 select * from t where id=500，即聚簇索引查询方式，则只需要搜索 id 这棵 b+ 树；\n * 如果语句是 select * from t where k=5，即非聚簇索引查询方式，则需要先搜索 k 索引树，得到 id 的值为 500，再到 id 索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非聚簇索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n\n显然，主键长度越小，非聚簇索引的叶子节点就越小，非聚簇索引占用的空间也就越小。\n\n自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： not null primary key auto_increment。从性能和存储空间方面考量，自增主键往往是更合理的选择。有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：\n\n * 只有一个索引；\n * 该索引必须是唯一索引。\n\n由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。\n\n这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。\n\n\n# 2.3. 全文索引\n\nmyisam 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 match against，而不是普通的 where。\n\n全文索引一般使用倒排索引实现，它记录着关键词到其所在文档的映射。\n\ninnodb 存储引擎在 mysql 5.6.4 版本中也开始支持全文索引。\n\n\n# 2.4. 空间数据索引\n\nmyisam 存储引擎支持空间数据索引（r-tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。\n\n必须使用 gis 相关的函数来维护数据。\n\n\n# 3. 索引的类型\n\n主流的关系型数据库一般都支持以下索引类型：\n\n\n# 3.1. 主键索引（primary）\n\n主键索引：一种特殊的唯一索引，不允许有空值。一个表只能有一个主键（在 innodb 中本质上即聚簇索引），一般是在建表的时候同时创建主键索引。\n\ncreate table `table` (\n    `id` int(11) not null auto_increment,\n    ...\n    primary key (`id`)\n)\n\n\n\n# 3.2. 唯一索引（unique）\n\n唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。\n\ncreate table `table` (\n    ...\n    unique indexname (title(length))\n)\n\n\n\n# 3.3. 普通索引（index）\n\n普通索引：最基本的索引，没有任何限制。\n\ncreate table `table` (\n    ...\n    index index_name (title(length))\n)\n\n\n\n# 3.4. 全文索引（fulltext）\n\n全文索引：主要用来查找文本中的关键字，而不是直接与索引中的值相比较。\n\n全文索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的 where 语句的参数匹配。全文索引配合 match against 操作使用，而不是一般的 where 语句加 like。它可以在 create table，alter table ，create index 使用，不过目前只有 char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用 create index 创建全文索引，要比先为一张表建立全文索引然后再将数据写入的速度快很多。\n\ncreate table `table` (\n    `content` text character null,\n    ...\n    fulltext (content)\n)\n\n\n\n# 3.5. 联合索引\n\n组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。\n\ncreate table `table` (\n    ...\n    index index_name (title(length), title(length), ...)\n)\n\n\n\n# 4. 索引的策略\n\n假设有以下表：\n\ncreate table `t` (\n  `id` int(11) not null,\n  `city` varchar(16) not null,\n  `name` varchar(16) not null,\n  `age` int(11) not null,\n  `addr` varchar(128) default null,\n  primary key (`id`),\n  key `city` (`city`)\n) engine=innodb;\n\n\n\n# 4.1. 索引基本原则\n\n * 索引不是越多越好，不要为所有列都创建索引。要考虑到索引的维护代价、空间占用和查询时回表的代价。索引一定是按需创建的，并且要尽可能确保足够轻量。一旦创建了多字段的联合索引，我们要考虑尽可能利用索引本身完成数据查询，减少回表的成本。\n * 要尽量避免冗余和重复索引。\n * 要考虑删除未使用的索引。\n * 尽量的扩展索引，不要新建索引。\n * 频繁作为 where 过滤条件的列应该考虑添加索引。\n\n\n# 4.2. 独立的列\n\n“独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。\n\n对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。\n\n如果查询中的列不是独立的列，则数据库不会使用索引。\n\n❌ 错误示例：\n\nselect actor_id from actor where actor_id + 1 = 5;\nselect ... where to_days(current_date) - to_days(date_col) <= 10;\n\n\n\n# 4.3. 覆盖索引\n\n覆盖索引是指，索引上的信息足够满足查询请求，不需要回表查询数据。\n\n【示例】范围查询\n\ncreate table t (\nid int primary key,\nk int not null default 0,\ns varchar(16) not null default '',\nindex k(k))\nengine=innodb;\n\ninsert into t values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');\n\nselect * from t where k between 3 and 5\n\n\n需要执行几次树的搜索操作，会扫描多少行？\n\n 1. 在 k 索引树上找到 k=3 的记录，取得 id = 300；\n 2. 再到 id 索引树查到 id=300 对应的 r3；\n 3. 在 k 索引树取下一个值 k=5，取得 id=500；\n 4. 再回到 id 索引树查到 id=500 对应的 r4；\n 5. 在 k 索引树取下一个值 k=6，不满足条件，循环结束。\n\n在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。\n\n如果执行的语句是 select id from t where k between 3 and 5，这时只需要查 id 的值，而 id 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。索引包含所有需要查询的字段的值，称为覆盖索引。\n\n由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。\n\n\n# 4.4. 使用索引来排序\n\nmysql 有两种方式可以生成排序结果：通过排序操作；或者按索引顺序扫描。\n\n索引最好既满足排序，又用于查找行。这样，就可以通过命中覆盖索引直接将结果查出来，也就不再需要排序了。\n\n这样整个查询语句的执行流程就变成了：\n\n 1. 从索引 (city,name,age) 找到第一个满足 city='杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；\n 2. 从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；\n 3. 重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city='杭州’条件时循环结束。\n\n\n# 4.5. 前缀索引\n\n有时候需要索引很长的字符列，这会让索引变得大且慢。\n\n这时，可以使用前缀索引，即只索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。对于 blob/text/varchar 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。\n\n索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。如果存在多条命中前缀索引的情况，就需要依次扫描，直到最终找到正确记录。\n\n使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。\n\n那么，如何确定前缀索引合适的长度呢？\n\n可以使用下面这个语句，算出这个列上有多少个不同的值：\n\nselect count(distinct email) as l from suser;\n\n\n然后，依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引，可以用这个语句：\n\nselect\n  count(distinct left(email,4)）as l4,\n  count(distinct left(email,5)）as l5,\n  count(distinct left(email,6)）as l6,\n  count(distinct left(email,7)）as l7,\nfrom suser;\n\n\n当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 l4~l7 中，找出不小于 l * 95% 的值，假设这里 l6、l7 都满足，你就可以选择前缀长度为 6。\n\n此外，order by 无法使用前缀索引，无法把前缀索引用作覆盖索引。\n\n\n# 4.6. 最左前缀匹配原则\n\n不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 n 个字段，也可以是字符串索引的最左 m 个字符。\n\nmysql 会一直向右匹配直到遇到范围查询 (>,<,between,like) 就停止匹配。\n\n * 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。\n * 如果是联合索引，那么 key 也由多个列组成，同时，索引只能用于查找 key 是否存在（相等），遇到范围查询(>、<、between、like 左匹配)等就不能进一步匹配了，后续退化为线性查找。\n * 因此，列的排列顺序决定了可命中索引的列数。\n\n不要为每个列都创建独立索引。\n\n将选择性高的列或基数大的列优先排在多列索引最前列。但有时，也需要考虑 where 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。\n\n例如：a = 1 and b = 2 and c > 3 and d = 4，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d 的顺序可以任意调整。\n\n让选择性最强的索引列放在前面，索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。\n\n例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。\n\nselect count(distinct staff_id)/count(*) as staff_id_selectivity,\ncount(distinct customer_id)/count(*) as customer_id_selectivity,\ncount(*)\nfrom payment;\n\n\n   staff_id_selectivity: 0.0001\ncustomer_id_selectivity: 0.0373\n               count(*): 16049\n\n\n\n# 4.7. = 和 in 可以乱序\n\n不需要考虑 =、in 等的顺序，mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。\n\n【示例】如有索引 (a, b, c, d)，查询条件 c > 3 and b = 2 and a = 1 and d < 4 与 a = 1 and c > 3 and b = 2 and d < 4 等顺序都是可以的，mysql 会自动优化为 a = 1 and b = 2 and c > 3 and d < 4，依次命中 a、b、c、d。\n\n\n# 5. 索引最佳实践\n\n创建了索引，并非一定有效。比如不满足前缀索引、最左前缀匹配原则、查询条件涉及函数计算等情况都无法使用索引。此外，即使 sql 本身符合索引的使用条件，mysql 也会通过评估各种查询方式的代价，来决定是否走索引，以及走哪个索引。\n\n因此，在尝试通过索引进行 sql 性能优化的时候，务必通过执行计划（explain）或实际的效果来确认索引是否能有效改善性能问题，否则增加了索引不但没解决性能问题，还增加了数据库增删改的负担。如果对 explain 给出的执行计划有疑问的话，你还可以利用 optimizer_trace 查看详细的执行计划做进一步分析。\n\n\n# 6. 参考资料\n\n * 《高性能 mysql》\n * 数据库两大神器【索引和锁】\n * mysql 索引背后的数据结构及算法原理\n * mysql 实战 45 讲",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 性能优化",frontmatter:{title:"Mysql 性能优化",date:"2020-06-03T20:16:48.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","性能"],permalink:"/pages/396816/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/06.Mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/06.Mysql性能优化.md",key:"v-6d5ab7d6",path:"/pages/396816/",headers:[{level:2,title:"1. 数据结构优化",slug:"_1-数据结构优化",normalizedTitle:"1. 数据结构优化",charIndex:17},{level:3,title:"1.1. 数据类型优化",slug:"_1-1-数据类型优化",normalizedTitle:"1.1. 数据类型优化",charIndex:53},{level:4,title:"数据类型优化基本原则",slug:"数据类型优化基本原则",normalizedTitle:"数据类型优化基本原则",charIndex:68},{level:4,title:"类型的选择",slug:"类型的选择",normalizedTitle:"类型的选择",charIndex:341},{level:3,title:"1.2. 表设计",slug:"_1-2-表设计",normalizedTitle:"1.2. 表设计",charIndex:676},{level:3,title:"1.3. 范式和反范式",slug:"_1-3-范式和反范式",normalizedTitle:"1.3. 范式和反范式",charIndex:945},{level:3,title:"1.4. 索引优化",slug:"_1-4-索引优化",normalizedTitle:"1.4. 索引优化",charIndex:1152},{level:4,title:"何时使用索引",slug:"何时使用索引",normalizedTitle:"何时使用索引",charIndex:1218},{level:4,title:"索引优化策略",slug:"索引优化策略",normalizedTitle:"索引优化策略",charIndex:1356},{level:2,title:"2. SQL 优化",slug:"_2-sql-优化",normalizedTitle:"2. sql 优化",charIndex:1720},{level:3,title:"2.1. 优化 COUNT() 查询",slug:"_2-1-优化-count-查询",normalizedTitle:"2.1. 优化 count() 查询",charIndex:1977},{level:3,title:"2.2. 优化关联查询",slug:"_2-2-优化关联查询",normalizedTitle:"2.2. 优化关联查询",charIndex:2640},{level:3,title:"2.3. 优化 GROUP BY 和 DISTINCT",slug:"_2-3-优化-group-by-和-distinct",normalizedTitle:"2.3. 优化 group by 和 distinct",charIndex:3770},{level:3,title:"2.4. 优化 LIMIT",slug:"_2-4-优化-limit",normalizedTitle:"2.4. 优化 limit",charIndex:3857},{level:3,title:"2.5. 优化 UNION",slug:"_2-5-优化-union",normalizedTitle:"2.5. 优化 union",charIndex:4670},{level:3,title:"2.6. 优化查询方式",slug:"_2-6-优化查询方式",normalizedTitle:"2.6. 优化查询方式",charIndex:4993},{level:4,title:"切分大查询",slug:"切分大查询",normalizedTitle:"切分大查询",charIndex:5008},{level:4,title:"分解大连接查询",slug:"分解大连接查询",normalizedTitle:"分解大连接查询",charIndex:5318},{level:2,title:"3. 执行计划（EXPLAIN）",slug:"_3-执行计划-explain",normalizedTitle:"3. 执行计划（explain）",charIndex:5914},{level:2,title:"4. optimizer trace",slug:"_4-optimizer-trace",normalizedTitle:"4. optimizer trace",charIndex:7464},{level:2,title:"5. 数据模型和业务",slug:"_5-数据模型和业务",normalizedTitle:"5. 数据模型和业务",charIndex:7921},{level:2,title:"6. 参考资料",slug:"_6-参考资料",normalizedTitle:"6. 参考资料",charIndex:8177}],headersStr:"1. 数据结构优化 1.1. 数据类型优化 数据类型优化基本原则 类型的选择 1.2. 表设计 1.3. 范式和反范式 1.4. 索引优化 何时使用索引 索引优化策略 2. SQL 优化 2.1. 优化 COUNT() 查询 2.2. 优化关联查询 2.3. 优化 GROUP BY 和 DISTINCT 2.4. 优化 LIMIT 2.5. 优化 UNION 2.6. 优化查询方式 切分大查询 分解大连接查询 3. 执行计划（EXPLAIN） 4. optimizer trace 5. 数据模型和业务 6. 参考资料",content:"# Mysql 性能优化\n\n\n# 1. 数据结构优化\n\n良好的逻辑设计和物理设计是高性能的基石。\n\n\n# 1.1. 数据类型优化\n\n# 数据类型优化基本原则\n\n * 更小的通常更好 - 越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的 CPU 周期也更少。\n   * 例如：整型比字符类型操作代价低，因而会使用整型来存储 IP 地址，使用 DATETIME 来存储时间，而不是使用字符串。\n * 简单就好 - 如整型比字符型操作代价低。\n   * 例如：很多软件会用整型来存储 IP 地址。\n   * 例如：UNSIGNED 表示不允许负值，大致可以使正数的上限提高一倍。\n * 尽量避免 NULL - 可为 NULL 的列会使得索引、索引统计和值比较都更复杂。\n\n# 类型的选择\n\n * 整数类型通常是标识列最好的选择，因为它们很快并且可以使用 AUTO_INCREMENT。\n\n * ENUM 和 SET 类型通常是一个糟糕的选择，应尽量避免。\n\n * 应该尽量避免用字符串类型作为标识列，因为它们很消耗空间，并且通常比数字类型慢。对于 MD5、SHA、UUID 这类随机字符串，由于比较随机，所以可能分布在很大的空间内，导致 INSERT 以及一些 SELECT 语句变得很慢。\n   \n   * 如果存储 UUID ，应该移除 - 符号；更好的做法是，用 UNHEX() 函数转换 UUID 值为 16 字节的数字，并存储在一个 BINARY(16) 的列中，检索时，可以通过 HEX() 函数来格式化为 16 进制格式。\n\n\n# 1.2. 表设计\n\n应该避免的设计问题：\n\n * 太多的列 - 设计者为了图方便，将大量冗余列加入表中，实际查询中，表中很多列是用不到的。这种宽表模式设计，会造成不小的性能代价，尤其是 ALTER TABLE 非常耗时。\n * 太多的关联 - 所谓的实体 - 属性 - 值（EVA）设计模式是一个常见的糟糕设计模式。Mysql 限制了每个关联操作最多只能有 61 张表，但 EVA 模式需要许多自关联。\n * 枚举 - 尽量不要用枚举，因为添加和删除字符串（枚举选项）必须使用 ALTER TABLE。\n * 尽量避免 NULL\n\n\n# 1.3. 范式和反范式\n\n范式化目标是尽量减少冗余，而反范式化则相反。\n\n范式化的优点：\n\n * 比反范式更节省空间\n * 更新操作比反范式快\n * 更少需要 DISTINCT 或 GROUP BY 语句\n\n范式化的缺点：\n\n * 通常需要关联查询。而关联查询代价较高，如果是分表的关联查询，代价更是高昂。\n\n在真实世界中，很少会极端地使用范式化或反范式化。实际上，应该权衡范式和反范式的利弊，混合使用。\n\n\n# 1.4. 索引优化\n\n> 索引优化应该是查询性能优化的最有效手段。\n> \n> 如果想详细了解索引特性请参考：Mysql 索引\n\n# 何时使用索引\n\n * 对于非常小的表，大部分情况下简单的全表扫描更高效。\n * 对于中、大型表，索引非常有效。\n * 对于特大型表，建立和使用索引的代价将随之增长。可以考虑使用分区技术。\n * 如果表的数量特别多，可以建立一个元数据信息表，用来查询需要用到的某些特性。\n\n# 索引优化策略\n\n * 索引基本原则\n   * 索引不是越多越好，不要为所有列都创建索引。\n   * 要尽量避免冗余和重复索引。\n   * 要考虑删除未使用的索引。\n   * 尽量的扩展索引，不要新建索引。\n   * 频繁作为 WHERE 过滤条件的列应该考虑添加索引。\n * 独立的列 - “独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。\n * 前缀索引 - 索引很长的字符列，可以索引开始的部分字符，这样可以大大节约索引空间。\n * 最左匹配原则 - 将选择性高的列或基数大的列优先排在多列索引最前列。\n * 使用索引来排序 - 索引最好既满足排序，又用于查找行。这样，就可以使用索引来对结果排序。\n * =、IN 可以乱序 - 不需要考虑 =、IN 等的顺序\n * 覆盖索引\n * 自增字段作主键\n\n\n# 2. SQL 优化\n\n使用 EXPLAIN 命令查看当前 SQL 是否使用了索引，优化后，再通过执行计划（EXPLAIN）来查看优化效果。\n\nSQL 优化基本思路：\n\n * 只返回必要的列 - 最好不要使用 SELECT * 语句。\n\n * 只返回必要的行 - 使用 WHERE 子查询语句进行过滤查询，有时候也需要使用 LIMIT 语句来限制返回的数据。\n\n * 缓存重复查询的数据 - 应该考虑在客户端使用缓存，尽量不要使用 Mysql 服务器缓存（存在较多问题和限制）。\n\n * 使用索引来覆盖查询\n\n\n# 2.1. 优化 COUNT() 查询\n\nCOUNT() 有两种作用：\n\n * 统计某个列值的数量。统计列值时，要求列值是非 NULL 的，它不会统计 NULL。\n * 统计行数。\n\n统计列值时，要求列值是非空的，它不会统计 NULL。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用 COUNT(*) 时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。\n\n我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用 COUNT(*)，意义清晰，且性能更好。\n\n（1）简单优化\n\nSELECT count(*) FROM world.city WHERE id > 5;\n\nSELECT (SELECT count(*) FROM world.city) - count(*)\nFROM world.city WHERE id <= 5;\n\n\n（2）使用近似值\n\n有时候某些业务场景并不需要完全精确的统计值，可以用近似值来代替，EXPLAIN 出来的行数就是一个不错的近似值，而且执行 EXPLAIN 并不需要真正地去执行查询，所以成本非常低。通常来说，执行 COUNT() 都需要扫描大量的行才能获取到精确的数据，因此很难优化，MySQL 层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用 Redis 这样的外部缓存系统。\n\n\n# 2.2. 优化关联查询\n\n在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用 JOIN 有更好的性能。\n\n如果确实需要使用关联查询的情况下，需要特别注意的是：\n\n * 确保 ON 和 USING 字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表 A 和表 B 用某列 column 关联的时候，如果优化器关联的顺序是 A、B，那么就不需要在 A 表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。\n * 确保任何的 GROUP BY 和 ORDER BY 中的表达式只涉及到一个表中的列，这样 MySQL 才有可能使用索引来优化。\n\n要理解优化关联查询的第一个技巧，就需要理解 MySQL 是如何执行关联查询的。当前 MySQL 关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。\n\n太抽象了？以上面的示例来说明，比如有这样的一个查询：\n\nSELECT A.xx,B.yy\nFROM A INNER JOIN B USING(c)\nWHERE A.xx IN (5,6)\n\n\n假设 MySQL 按照查询中的关联顺序 A、B 来进行关联操作，那么可以用下面的伪代码表示 MySQL 如何完成这个查询：\n\nouter_iterator = SELECT A.xx,A.c FROM A WHERE A.xx IN (5,6);\nouter_row = outer_iterator.next;\nwhile(outer_row) {\n    inner_iterator = SELECT B.yy FROM B WHERE B.c = outer_row.c;\n    inner_row = inner_iterator.next;\n    while(inner_row) {\n        output[inner_row.yy,outer_row.xx];\n        inner_row = inner_iterator.next;\n    }\n    outer_row = outer_iterator.next;\n}\n\n\n可以看到，最外层的查询是根据A.xx列来查询的，A.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显B.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。\n\n\n# 2.3. 优化 GROUP BY 和 DISTINCT\n\nMysql 优化器会在内部处理的时候相互转化这两类查询。它们都可以使用索引来优化，这也是最有效的优化方法。\n\n\n# 2.4. 优化 LIMIT\n\n当需要分页操作时，通常会使用 LIMIT 加上偏移量的办法实现，同时加上合适的 ORDER BY 字句。如果有对应的索引，通常效率会不错，否则，MySQL 需要做大量的文件排序操作。\n\n一个常见的问题是当偏移量非常大的时候，比如：LIMIT 10000 20这样的查询，MySQL 需要查询 10020 条记录然后只返回 20 条记录，前面的 10000 条都将被抛弃，这样的代价非常高。\n\n优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询：\n\nSELECT film_id,description FROM film ORDER BY title LIMIT 50,5;\n\n\n如果这张表非常大，那么这个查询最好改成下面的样子：\n\nSELECT film.film_id,film.description\nFROM film INNER JOIN (\n    SELECT film_id FROM film ORDER BY title LIMIT 50,5\n) AS tmp USING(film_id);\n\n\n这里的延迟关联将大大提升查询效率，让 MySQL 扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。\n\n有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用OFFSET，比如下面的查询：\n\nSELECT id FROM t LIMIT 10000, 10;\n改为：\nSELECT id FROM t WHERE id > 10000 LIMIT 10;\n\n\n其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。\n\n\n# 2.5. 优化 UNION\n\nMySQL 总是通过创建并填充临时表的方式来执行 UNION 查询。因此很多优化策略在UNION查询中都没有办法很好的时候。经常需要手动将WHERE、LIMIT、ORDER BY等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。\n\n除非确实需要服务器去重，否则就一定要使用UNION ALL，如果没有ALL关键字，MySQL 会给临时表加上DISTINCT选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用 ALL 关键字，MySQL 总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。\n\n\n# 2.6. 优化查询方式\n\n# 切分大查询\n\n一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。\n\nDELEFT FROM messages WHERE create < DATE_SUB(NOW(), INTERVAL 3 MONTH);\n\n\nrows_affected = 0\ndo {\n    rows_affected = do_query(\n    \"DELETE FROM messages WHERE create  < DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000\")\n} while rows_affected > 0\n\n\n# 分解大连接查询\n\n将一个大连接查询（JOIN）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有：\n\n * 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。\n * 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。\n * 减少锁竞争；\n * 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。\n * 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。\n\nSELECT * FROM tag\nJOIN tag_post ON tag_post.tag_id=tag.id\nJOIN post ON tag_post.post_id=post.id\nWHERE tag.tag='mysql';\n\n\nSELECT * FROM tag WHERE tag='mysql';\nSELECT * FROM tag_post WHERE tag_id=1234;\nSELECT * FROM post WHERE post.id IN (123,456,567,9098,8904);\n\n\n\n# 3. 执行计划（EXPLAIN）\n\n如何判断当前 SQL 是否使用了索引？如何检验修改后的 SQL 确实有优化效果？\n\n在 SQL 中，可以通过执行计划（EXPLAIN）分析 SELECT 查询效率。\n\nmysql> explain select * from user_info where id = 2\\G\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: user_info\n   partitions: NULL\n         type: const\npossible_keys: PRIMARY\n          key: PRIMARY\n      key_len: 8\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n\n\nEXPLAIN 参数说明：\n\n * id: SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符.\n * select_type ⭐ ：SELECT 查询的类型.\n   * SIMPLE：表示此查询不包含 UNION 查询或子查询\n   * PRIMARY：表示此查询是最外层的查询\n   * UNION：表示此查询是 UNION 的第二或随后的查询\n   * DEPENDENT UNION：UNION 中的第二个或后面的查询语句, 取决于外面的查询\n   * UNION RESULT：UNION 的结果\n   * SUBQUERY：子查询中的第一个 SELECT\n   * DEPENDENT SUBQUERY: 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果.\n * table: 查询的是哪个表，如果给表起别名了，则显示别名。\n * partitions：匹配的分区\n * type ⭐：表示从表中查询到行所执行的方式，查询方式是 SQL 优化中一个很重要的指标，结果值从好到差依次是：system > const > eq_ref > ref > range > index > ALL。\n   * system/const：表中只有一行数据匹配，此时根据索引查询一次就能找到对应的数据。如果是 B + 树索引，我们知道此时索引构造成了多个层级的树，当查询的索引在树的底层时，查询效率就越低。const 表示此时索引在第一层，只需访问一层便能得到数据。\n   * eq_ref：使用唯一索引扫描，常见于多表连接中使用主键和唯一索引作为关联条件。\n   * ref：非唯一索引扫描，还可见于唯一索引最左原则匹配扫描。\n   * range：索引范围扫描，比如，<，>，between 等操作。\n   * index：索引全表扫描，此时遍历整个索引树。\n   * ALL：表示全表扫描，需要遍历全表来找到对应的行。\n * possible_keys：此次查询中可能选用的索引。\n * key ⭐：此次查询中实际使用的索引。\n * ref：哪个字段或常数与 key 一起被使用。\n * rows ⭐：显示此查询一共扫描了多少行，这个是一个估计值。\n * filtered：表示此查询条件所过滤的数据的百分比。\n * extra：额外的信息。\n\n> 更多内容请参考：MySQL 性能优化神器 Explain 使用分析\n\n\n# 4. optimizer trace\n\n在 MySQL 5.6 及之后的版本中，我们可以使用 optimizer trace 功能查看优化器生成执行计划的整个过程。有了这个功能，我们不仅可以了解优化器的选择过程，更可以了解每一个执行环节的成本，然后依靠这些信息进一步优化查询。\n\n如下代码所示，打开 optimizer_trace 后，再执行 SQL 就可以查询 information_schema.OPTIMIZER_TRACE 表查看执行计划了，最后可以关闭 optimizer_trace 功能：\n\nSET optimizer_trace=\"enabled=on\";\nSELECT * FROM person WHERE NAME >'name84059' AND create_time>'2020-01-24 05:00\nSELECT * FROM information_schema.OPTIMIZER_TRACE;\nSET optimizer_trace=\"enabled=off\";\n\n\n\n# 5. 数据模型和业务\n\n * 表字段比较复杂、易变动、结构难以统一的情况下，可以考虑使用 Nosql 来代替关系数据库表存储，如 ElasticSearch、MongoDB。\n * 在高并发情况下的查询操作，可以使用缓存（如 Redis）代替数据库操作，提高并发性能。\n * 数据量增长较快的表，需要考虑水平分表或分库，避免单表操作的性能瓶颈。\n * 除此之外，我们应该通过一些优化，尽量避免比较复杂的 JOIN 查询操作，例如冗余一些字段，减少 JOIN 查询；创建一些中间表，减少 JOIN 查询。\n\n\n# 6. 参考资料\n\n * 《高性能 MySQL》\n * 《Java 性能调优实战》\n * 我必须得告诉大家的 MySQL 优化原理\n * 20+ 条 MySQL 性能优化的最佳经验\n * MySQL 性能优化神器 Explain 使用分析",normalizedContent:"# mysql 性能优化\n\n\n# 1. 数据结构优化\n\n良好的逻辑设计和物理设计是高性能的基石。\n\n\n# 1.1. 数据类型优化\n\n# 数据类型优化基本原则\n\n * 更小的通常更好 - 越小的数据类型通常会更快，占用更少的磁盘、内存，处理时需要的 cpu 周期也更少。\n   * 例如：整型比字符类型操作代价低，因而会使用整型来存储 ip 地址，使用 datetime 来存储时间，而不是使用字符串。\n * 简单就好 - 如整型比字符型操作代价低。\n   * 例如：很多软件会用整型来存储 ip 地址。\n   * 例如：unsigned 表示不允许负值，大致可以使正数的上限提高一倍。\n * 尽量避免 null - 可为 null 的列会使得索引、索引统计和值比较都更复杂。\n\n# 类型的选择\n\n * 整数类型通常是标识列最好的选择，因为它们很快并且可以使用 auto_increment。\n\n * enum 和 set 类型通常是一个糟糕的选择，应尽量避免。\n\n * 应该尽量避免用字符串类型作为标识列，因为它们很消耗空间，并且通常比数字类型慢。对于 md5、sha、uuid 这类随机字符串，由于比较随机，所以可能分布在很大的空间内，导致 insert 以及一些 select 语句变得很慢。\n   \n   * 如果存储 uuid ，应该移除 - 符号；更好的做法是，用 unhex() 函数转换 uuid 值为 16 字节的数字，并存储在一个 binary(16) 的列中，检索时，可以通过 hex() 函数来格式化为 16 进制格式。\n\n\n# 1.2. 表设计\n\n应该避免的设计问题：\n\n * 太多的列 - 设计者为了图方便，将大量冗余列加入表中，实际查询中，表中很多列是用不到的。这种宽表模式设计，会造成不小的性能代价，尤其是 alter table 非常耗时。\n * 太多的关联 - 所谓的实体 - 属性 - 值（eva）设计模式是一个常见的糟糕设计模式。mysql 限制了每个关联操作最多只能有 61 张表，但 eva 模式需要许多自关联。\n * 枚举 - 尽量不要用枚举，因为添加和删除字符串（枚举选项）必须使用 alter table。\n * 尽量避免 null\n\n\n# 1.3. 范式和反范式\n\n范式化目标是尽量减少冗余，而反范式化则相反。\n\n范式化的优点：\n\n * 比反范式更节省空间\n * 更新操作比反范式快\n * 更少需要 distinct 或 group by 语句\n\n范式化的缺点：\n\n * 通常需要关联查询。而关联查询代价较高，如果是分表的关联查询，代价更是高昂。\n\n在真实世界中，很少会极端地使用范式化或反范式化。实际上，应该权衡范式和反范式的利弊，混合使用。\n\n\n# 1.4. 索引优化\n\n> 索引优化应该是查询性能优化的最有效手段。\n> \n> 如果想详细了解索引特性请参考：mysql 索引\n\n# 何时使用索引\n\n * 对于非常小的表，大部分情况下简单的全表扫描更高效。\n * 对于中、大型表，索引非常有效。\n * 对于特大型表，建立和使用索引的代价将随之增长。可以考虑使用分区技术。\n * 如果表的数量特别多，可以建立一个元数据信息表，用来查询需要用到的某些特性。\n\n# 索引优化策略\n\n * 索引基本原则\n   * 索引不是越多越好，不要为所有列都创建索引。\n   * 要尽量避免冗余和重复索引。\n   * 要考虑删除未使用的索引。\n   * 尽量的扩展索引，不要新建索引。\n   * 频繁作为 where 过滤条件的列应该考虑添加索引。\n * 独立的列 - “独立的列” 是指索引列不能是表达式的一部分，也不能是函数的参数。\n * 前缀索引 - 索引很长的字符列，可以索引开始的部分字符，这样可以大大节约索引空间。\n * 最左匹配原则 - 将选择性高的列或基数大的列优先排在多列索引最前列。\n * 使用索引来排序 - 索引最好既满足排序，又用于查找行。这样，就可以使用索引来对结果排序。\n * =、in 可以乱序 - 不需要考虑 =、in 等的顺序\n * 覆盖索引\n * 自增字段作主键\n\n\n# 2. sql 优化\n\n使用 explain 命令查看当前 sql 是否使用了索引，优化后，再通过执行计划（explain）来查看优化效果。\n\nsql 优化基本思路：\n\n * 只返回必要的列 - 最好不要使用 select * 语句。\n\n * 只返回必要的行 - 使用 where 子查询语句进行过滤查询，有时候也需要使用 limit 语句来限制返回的数据。\n\n * 缓存重复查询的数据 - 应该考虑在客户端使用缓存，尽量不要使用 mysql 服务器缓存（存在较多问题和限制）。\n\n * 使用索引来覆盖查询\n\n\n# 2.1. 优化 count() 查询\n\ncount() 有两种作用：\n\n * 统计某个列值的数量。统计列值时，要求列值是非 null 的，它不会统计 null。\n * 统计行数。\n\n统计列值时，要求列值是非空的，它不会统计 null。如果确认括号中的表达式不可能为空时，实际上就是在统计行数。最简单的就是当使用 count(*) 时，并不是我们所想象的那样扩展成所有的列，实际上，它会忽略所有的列而直接统计行数。\n\n我们最常见的误解也就在这儿，在括号内指定了一列却希望统计结果是行数，而且还常常误以为前者的性能会更好。但实际并非这样，如果要统计行数，直接使用 count(*)，意义清晰，且性能更好。\n\n（1）简单优化\n\nselect count(*) from world.city where id > 5;\n\nselect (select count(*) from world.city) - count(*)\nfrom world.city where id <= 5;\n\n\n（2）使用近似值\n\n有时候某些业务场景并不需要完全精确的统计值，可以用近似值来代替，explain 出来的行数就是一个不错的近似值，而且执行 explain 并不需要真正地去执行查询，所以成本非常低。通常来说，执行 count() 都需要扫描大量的行才能获取到精确的数据，因此很难优化，mysql 层面还能做得也就只有覆盖索引了。如果不还能解决问题，只有从架构层面解决了，比如添加汇总表，或者使用 redis 这样的外部缓存系统。\n\n\n# 2.2. 优化关联查询\n\n在大数据场景下，表与表之间通过一个冗余字段来关联，要比直接使用 join 有更好的性能。\n\n如果确实需要使用关联查询的情况下，需要特别注意的是：\n\n * 确保 on 和 using 字句中的列上有索引。在创建索引的时候就要考虑到关联的顺序。当表 a 和表 b 用某列 column 关联的时候，如果优化器关联的顺序是 a、b，那么就不需要在 a 表的对应列上创建索引。没有用到的索引会带来额外的负担，一般来说，除非有其他理由，只需要在关联顺序中的第二张表的相应列上创建索引（具体原因下文分析）。\n * 确保任何的 group by 和 order by 中的表达式只涉及到一个表中的列，这样 mysql 才有可能使用索引来优化。\n\n要理解优化关联查询的第一个技巧，就需要理解 mysql 是如何执行关联查询的。当前 mysql 关联执行的策略非常简单，它对任何的关联都执行嵌套循环关联操作，即先在一个表中循环取出单条数据，然后在嵌套循环到下一个表中寻找匹配的行，依次下去，直到找到所有表中匹配的行为为止。然后根据各个表匹配的行，返回查询中需要的各个列。\n\n太抽象了？以上面的示例来说明，比如有这样的一个查询：\n\nselect a.xx,b.yy\nfrom a inner join b using(c)\nwhere a.xx in (5,6)\n\n\n假设 mysql 按照查询中的关联顺序 a、b 来进行关联操作，那么可以用下面的伪代码表示 mysql 如何完成这个查询：\n\nouter_iterator = select a.xx,a.c from a where a.xx in (5,6);\nouter_row = outer_iterator.next;\nwhile(outer_row) {\n    inner_iterator = select b.yy from b where b.c = outer_row.c;\n    inner_row = inner_iterator.next;\n    while(inner_row) {\n        output[inner_row.yy,outer_row.xx];\n        inner_row = inner_iterator.next;\n    }\n    outer_row = outer_iterator.next;\n}\n\n\n可以看到，最外层的查询是根据a.xx列来查询的，a.c上如果有索引的话，整个关联查询也不会使用。再看内层的查询，很明显b.c上如果有索引的话，能够加速查询，因此只需要在关联顺序中的第二张表的相应列上创建索引即可。\n\n\n# 2.3. 优化 group by 和 distinct\n\nmysql 优化器会在内部处理的时候相互转化这两类查询。它们都可以使用索引来优化，这也是最有效的优化方法。\n\n\n# 2.4. 优化 limit\n\n当需要分页操作时，通常会使用 limit 加上偏移量的办法实现，同时加上合适的 order by 字句。如果有对应的索引，通常效率会不错，否则，mysql 需要做大量的文件排序操作。\n\n一个常见的问题是当偏移量非常大的时候，比如：limit 10000 20这样的查询，mysql 需要查询 10020 条记录然后只返回 20 条记录，前面的 10000 条都将被抛弃，这样的代价非常高。\n\n优化这种查询一个最简单的办法就是尽可能的使用覆盖索引扫描，而不是查询所有的列。然后根据需要做一次关联查询再返回所有的列。对于偏移量很大时，这样做的效率会提升非常大。考虑下面的查询：\n\nselect film_id,description from film order by title limit 50,5;\n\n\n如果这张表非常大，那么这个查询最好改成下面的样子：\n\nselect film.film_id,film.description\nfrom film inner join (\n    select film_id from film order by title limit 50,5\n) as tmp using(film_id);\n\n\n这里的延迟关联将大大提升查询效率，让 mysql 扫描尽可能少的页面，获取需要访问的记录后在根据关联列回原表查询所需要的列。\n\n有时候如果可以使用书签记录上次取数据的位置，那么下次就可以直接从该书签记录的位置开始扫描，这样就可以避免使用offset，比如下面的查询：\n\nselect id from t limit 10000, 10;\n改为：\nselect id from t where id > 10000 limit 10;\n\n\n其他优化的办法还包括使用预先计算的汇总表，或者关联到一个冗余表，冗余表中只包含主键列和需要做排序的列。\n\n\n# 2.5. 优化 union\n\nmysql 总是通过创建并填充临时表的方式来执行 union 查询。因此很多优化策略在union查询中都没有办法很好的时候。经常需要手动将where、limit、order by等字句“下推”到各个子查询中，以便优化器可以充分利用这些条件先优化。\n\n除非确实需要服务器去重，否则就一定要使用union all，如果没有all关键字，mysql 会给临时表加上distinct选项，这会导致整个临时表的数据做唯一性检查，这样做的代价非常高。当然即使使用 all 关键字，mysql 总是将结果放入临时表，然后再读出，再返回给客户端。虽然很多时候没有这个必要，比如有时候可以直接把每个子查询的结果返回给客户端。\n\n\n# 2.6. 优化查询方式\n\n# 切分大查询\n\n一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。\n\ndeleft from messages where create < date_sub(now(), interval 3 month);\n\n\nrows_affected = 0\ndo {\n    rows_affected = do_query(\n    \"delete from messages where create  < date_sub(now(), interval 3 month) limit 10000\")\n} while rows_affected > 0\n\n\n# 分解大连接查询\n\n将一个大连接查询（join）分解成对每一个表进行一次单表查询，然后将结果在应用程序中进行关联，这样做的好处有：\n\n * 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。\n * 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。\n * 减少锁竞争；\n * 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可扩展。\n * 查询本身效率也可能会有所提升。例如下面的例子中，使用 in() 代替连接查询，可以让 mysql 按照 id 顺序进行查询，这可能比随机的连接要更高效。\n\nselect * from tag\njoin tag_post on tag_post.tag_id=tag.id\njoin post on tag_post.post_id=post.id\nwhere tag.tag='mysql';\n\n\nselect * from tag where tag='mysql';\nselect * from tag_post where tag_id=1234;\nselect * from post where post.id in (123,456,567,9098,8904);\n\n\n\n# 3. 执行计划（explain）\n\n如何判断当前 sql 是否使用了索引？如何检验修改后的 sql 确实有优化效果？\n\n在 sql 中，可以通过执行计划（explain）分析 select 查询效率。\n\nmysql> explain select * from user_info where id = 2\\g\n*************************** 1. row ***************************\n           id: 1\n  select_type: simple\n        table: user_info\n   partitions: null\n         type: const\npossible_keys: primary\n          key: primary\n      key_len: 8\n          ref: const\n         rows: 1\n     filtered: 100.00\n        extra: null\n1 row in set, 1 warning (0.00 sec)\n\n\nexplain 参数说明：\n\n * id: select 查询的标识符. 每个 select 都会自动分配一个唯一的标识符.\n * select_type ⭐ ：select 查询的类型.\n   * simple：表示此查询不包含 union 查询或子查询\n   * primary：表示此查询是最外层的查询\n   * union：表示此查询是 union 的第二或随后的查询\n   * dependent union：union 中的第二个或后面的查询语句, 取决于外面的查询\n   * union result：union 的结果\n   * subquery：子查询中的第一个 select\n   * dependent subquery: 子查询中的第一个 select, 取决于外面的查询. 即子查询依赖于外层查询的结果.\n * table: 查询的是哪个表，如果给表起别名了，则显示别名。\n * partitions：匹配的分区\n * type ⭐：表示从表中查询到行所执行的方式，查询方式是 sql 优化中一个很重要的指标，结果值从好到差依次是：system > const > eq_ref > ref > range > index > all。\n   * system/const：表中只有一行数据匹配，此时根据索引查询一次就能找到对应的数据。如果是 b + 树索引，我们知道此时索引构造成了多个层级的树，当查询的索引在树的底层时，查询效率就越低。const 表示此时索引在第一层，只需访问一层便能得到数据。\n   * eq_ref：使用唯一索引扫描，常见于多表连接中使用主键和唯一索引作为关联条件。\n   * ref：非唯一索引扫描，还可见于唯一索引最左原则匹配扫描。\n   * range：索引范围扫描，比如，<，>，between 等操作。\n   * index：索引全表扫描，此时遍历整个索引树。\n   * all：表示全表扫描，需要遍历全表来找到对应的行。\n * possible_keys：此次查询中可能选用的索引。\n * key ⭐：此次查询中实际使用的索引。\n * ref：哪个字段或常数与 key 一起被使用。\n * rows ⭐：显示此查询一共扫描了多少行，这个是一个估计值。\n * filtered：表示此查询条件所过滤的数据的百分比。\n * extra：额外的信息。\n\n> 更多内容请参考：mysql 性能优化神器 explain 使用分析\n\n\n# 4. optimizer trace\n\n在 mysql 5.6 及之后的版本中，我们可以使用 optimizer trace 功能查看优化器生成执行计划的整个过程。有了这个功能，我们不仅可以了解优化器的选择过程，更可以了解每一个执行环节的成本，然后依靠这些信息进一步优化查询。\n\n如下代码所示，打开 optimizer_trace 后，再执行 sql 就可以查询 information_schema.optimizer_trace 表查看执行计划了，最后可以关闭 optimizer_trace 功能：\n\nset optimizer_trace=\"enabled=on\";\nselect * from person where name >'name84059' and create_time>'2020-01-24 05:00\nselect * from information_schema.optimizer_trace;\nset optimizer_trace=\"enabled=off\";\n\n\n\n# 5. 数据模型和业务\n\n * 表字段比较复杂、易变动、结构难以统一的情况下，可以考虑使用 nosql 来代替关系数据库表存储，如 elasticsearch、mongodb。\n * 在高并发情况下的查询操作，可以使用缓存（如 redis）代替数据库操作，提高并发性能。\n * 数据量增长较快的表，需要考虑水平分表或分库，避免单表操作的性能瓶颈。\n * 除此之外，我们应该通过一些优化，尽量避免比较复杂的 join 查询操作，例如冗余一些字段，减少 join 查询；创建一些中间表，减少 join 查询。\n\n\n# 6. 参考资料\n\n * 《高性能 mysql》\n * 《java 性能调优实战》\n * 我必须得告诉大家的 mysql 优化原理\n * 20+ 条 mysql 性能优化的最佳经验\n * mysql 性能优化神器 explain 使用分析",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 运维",frontmatter:{title:"Mysql 运维",date:"2019-11-26T21:37:17.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","运维"],permalink:"/pages/e33b92/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/20.Mysql%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/20.Mysql运维.md",key:"v-4ef63c90",path:"/pages/e33b92/",headers:[{level:2,title:"1. 安装部署",slug:"_1-安装部署",normalizedTitle:"1. 安装部署",charIndex:98},{level:3,title:"1.1. Windows 安装",slug:"_1-1-windows-安装",normalizedTitle:"1.1. windows 安装",charIndex:110},{level:3,title:"1.2. CentOS 安装",slug:"_1-2-centos-安装",normalizedTitle:"1.2. centos 安装",charIndex:782},{level:4,title:"安装 mysql yum 源",slug:"安装-mysql-yum-源",normalizedTitle:"安装 mysql yum 源",charIndex:818},{level:4,title:"mysql 服务管理",slug:"mysql-服务管理",normalizedTitle:"mysql 服务管理",charIndex:1897},{level:3,title:"1.3. 初始化数据库密码",slug:"_1-3-初始化数据库密码",normalizedTitle:"1.3. 初始化数据库密码",charIndex:2168},{level:3,title:"1.4. 配置远程访问",slug:"_1-4-配置远程访问",normalizedTitle:"1.4. 配置远程访问",charIndex:2529},{level:3,title:"1.5. 跳过登录认证",slug:"_1-5-跳过登录认证",normalizedTitle:"1.5. 跳过登录认证",charIndex:2713},{level:2,title:"2. 基本运维",slug:"_2-基本运维",normalizedTitle:"2. 基本运维",charIndex:2859},{level:3,title:"2.1. 客户端连接",slug:"_2-1-客户端连接",normalizedTitle:"2.1. 客户端连接",charIndex:2871},{level:3,title:"2.2. 查看连接",slug:"_2-2-查看连接",normalizedTitle:"2.2. 查看连接",charIndex:3471},{level:3,title:"2.3. 创建用户",slug:"_2-3-创建用户",normalizedTitle:"2.3. 创建用户",charIndex:3608},{level:3,title:"2.4. 查看用户",slug:"_2-4-查看用户",normalizedTitle:"2.4. 查看用户",charIndex:4298},{level:3,title:"2.5. 授权",slug:"_2-5-授权",normalizedTitle:"2.5. 授权",charIndex:4413},{level:3,title:"2.6. 撤销授权",slug:"_2-6-撤销授权",normalizedTitle:"2.6. 撤销授权",charIndex:4996},{level:3,title:"2.7. 查看授权",slug:"_2-7-查看授权",normalizedTitle:"2.7. 查看授权",charIndex:5498},{level:3,title:"2.8. 更改用户密码",slug:"_2-8-更改用户密码",normalizedTitle:"2.8. 更改用户密码",charIndex:5552},{level:3,title:"2.9. 备份与恢复",slug:"_2-9-备份与恢复",normalizedTitle:"2.9. 备份与恢复",charIndex:5743},{level:4,title:"备份一个数据库",slug:"备份一个数据库",normalizedTitle:"备份一个数据库",charIndex:5843},{level:4,title:"备份多个数据库",slug:"备份多个数据库",normalizedTitle:"备份多个数据库",charIndex:6165},{level:4,title:"备份所有数据库",slug:"备份所有数据库",normalizedTitle:"备份所有数据库",charIndex:6258},{level:4,title:"恢复一个数据库",slug:"恢复一个数据库",normalizedTitle:"恢复一个数据库",charIndex:6327},{level:4,title:"恢复所有数据库",slug:"恢复所有数据库",normalizedTitle:"恢复所有数据库",charIndex:6433},{level:3,title:"2.10. 卸载",slug:"_2-10-卸载",normalizedTitle:"2.10. 卸载",charIndex:6498},{level:3,title:"2.11. 主从节点部署",slug:"_2-11-主从节点部署",normalizedTitle:"2.11. 主从节点部署",charIndex:6860},{level:4,title:"主节点上的操作",slug:"主节点上的操作",normalizedTitle:"主节点上的操作",charIndex:6952},{level:4,title:"从节点上的操作",slug:"从节点上的操作",normalizedTitle:"从节点上的操作",charIndex:9196},{level:2,title:"3. 服务器配置",slug:"_3-服务器配置",normalizedTitle:"3. 服务器配置",charIndex:10513},{level:3,title:"3.1. 配置文件路径",slug:"_3-1-配置文件路径",normalizedTitle:"3.1. 配置文件路径",charIndex:10717},{level:3,title:"3.2. 配置项语法",slug:"_3-2-配置项语法",normalizedTitle:"3.2. 配置项语法",charIndex:11080},{level:3,title:"3.3. 常用配置项说明",slug:"_3-3-常用配置项说明",normalizedTitle:"3.3. 常用配置项说明",charIndex:11259},{level:2,title:"4. 常见问题",slug:"_4-常见问题",normalizedTitle:"4. 常见问题",charIndex:15260},{level:3,title:"4.1. Too many connections",slug:"_4-1-too-many-connections",normalizedTitle:"4.1. too many connections",charIndex:15272},{level:3,title:"4.2. 时区（time_zone）偏差",slug:"_4-2-时区-time-zone-偏差",normalizedTitle:"4.2. 时区（time_zone）偏差",charIndex:16488},{level:3,title:"4.3. 数据表损坏如何修复",slug:"_4-3-数据表损坏如何修复",normalizedTitle:"4.3. 数据表损坏如何修复",charIndex:17248},{level:3,title:"4.4. 数据结构",slug:"_4-4-数据结构",normalizedTitle:"4.4. 数据结构",charIndex:17628},{level:2,title:"5. 脚本",slug:"_5-脚本",normalizedTitle:"5. 脚本",charIndex:17789},{level:2,title:"6. 参考资料",slug:"_6-参考资料",normalizedTitle:"6. 参考资料",charIndex:17856}],headersStr:"1. 安装部署 1.1. Windows 安装 1.2. CentOS 安装 安装 mysql yum 源 mysql 服务管理 1.3. 初始化数据库密码 1.4. 配置远程访问 1.5. 跳过登录认证 2. 基本运维 2.1. 客户端连接 2.2. 查看连接 2.3. 创建用户 2.4. 查看用户 2.5. 授权 2.6. 撤销授权 2.7. 查看授权 2.8. 更改用户密码 2.9. 备份与恢复 备份一个数据库 备份多个数据库 备份所有数据库 恢复一个数据库 恢复所有数据库 2.10. 卸载 2.11. 主从节点部署 主节点上的操作 从节点上的操作 3. 服务器配置 3.1. 配置文件路径 3.2. 配置项语法 3.3. 常用配置项说明 4. 常见问题 4.1. Too many connections 4.2. 时区（time_zone）偏差 4.3. 数据表损坏如何修复 4.4. 数据结构 5. 脚本 6. 参考资料",content:"# Mysql 运维\n\n> 如果你的公司有 DBA，那么我恭喜你，你可以无视 Mysql 运维。如果你的公司没有 DBA，那你就好好学两手 Mysql 基本运维操作，行走江湖，防身必备。\n\n\n# 1. 安装部署\n\n\n# 1.1. Windows 安装\n\n（1）下载 Mysql 5.7 免安装版\n\n下载地址：https://dev.mysql.com/downloads/mysql/5.7.html#downloads\n\n（2）解压并创建 my.ini 在根目录\n\nmy.ini 文件示例：\n\n[mysqld]\n#设置3306端口\nport = 3306\n# 设置mysql的安装目录 这块换成自己解压的路径\nbasedir=D:\\\\Tools\\\\DB\\\\mysql\\\\mysql-5.7.31\n# 允许最大连接数\nmax_connections=200\n# 服务端使用的字符集默认为8比特编码的latin1字符集\ncharacter-set-server=utf8\n# 创建新表时将使用的默认存储引擎\ndefault-storage-engine=INNODB\n\n[client]\n# 设置mysql客户端默认字符集\ndefault-character-set=utf8\n\n\n（3）执行安装命令\n\n在控制台 CMD 中依次执行以下安装命令\n\ncd D:\\\\Tools\\\\DB\\\\mysql\\\\mysql-5.7.31\nmysqld --initialize\nmysqld -install\n\n\n说明：\n\n * mysqld --initialize 会自动初始化创建 data 文件夹并初始化 mysql。\n * mysqld -install 会安装 mysql 服务。\n\n（4）启动服务\n\n在控制台执行 net start mysql 启动服务。\n\n\n# 1.2. CentOS 安装\n\n> 本文仅介绍 rpm 安装方式\n\n# 安装 mysql yum 源\n\n官方下载地址：https://dev.mysql.com/downloads/repo/yum/\n\n（1）下载 yum 源\n\nwget https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm\n\n\n（2）安装 yum repo 文件并更新 yum 缓存\n\nrpm -ivh mysql80-community-release-el7-1.noarch.rpm\n\n\n执行结果：\n\n会在 /etc/yum.repos.d/ 目录下生成两个 repo 文件\n\n$ ls | grep mysql\nmysql-community.repo\nmysql-community-source.repo\n\n\n更新 yum：\n\nyum clean all\nyum makecache\n\n\n（3）查看 rpm 安装状态\n\n$ yum search mysql | grep server\nmysql-community-common.i686 : MySQL database common files for server and client\nmysql-community-common.x86_64 : MySQL database common files for server and\nmysql-community-test.x86_64 : Test suite for the MySQL database server\n                       : administering MySQL servers\nmysql-community-server.x86_64 : A very fast and reliable SQL database server\n\n\n通过 yum 安装 mysql 有几个重要目录：\n\n## 配置文件\n/etc/my.cnf\n## 数据库目录\n/var/lib/mysql/\n## 配置文件\n/usr/share/mysql（mysql.server命令及配置文件）\n## 相关命令\n/usr/bin（mysqladmin mysqldump等命令）\n## 启动脚本\n/usr/lib/systemd/system/mysqld.service （注册为 systemd 服务）\n\n\n（4）安装 mysql 服务器\n\nyum install mysql-community-server\n\n\n# mysql 服务管理\n\n通过 yum 方式安装 mysql 后，本地会有一个名为 mysqld 的 systemd 服务。\n\n其服务管理十分简便：\n\n## 查看状态\nsystemctl status mysqld\n## 启用服务\nsystemctl enable mysqld\n## 禁用服务\nsystemctl disable mysqld\n## 启动服务\nsystemctl start mysqld\n## 重启服务\nsystemctl restart mysqld\n## 停止服务\nsystemctl stop mysqld\n\n\n\n# 1.3. 初始化数据库密码\n\n查看一下初始密码\n\n$ grep \"password\" /var/log/mysqld.log\n2018-09-30T03:13:41.727736Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: %:lt+srWu4k1\n\n\n执行命令：\n\nmysql -uroot -p<临时密码>\n\n\n输入临时密码，进入 mysql，如果要修改密码，执行以下指令：\n\nALTER user 'root'@'localhost' IDENTIFIED BY '你的密码';\n\n\n注：密码强度默认为中等，大小写字母、数字、特殊符号，只有修改成功后才能修改配置再设置更简单的密码\n\n\n# 1.4. 配置远程访问\n\nCREATE USER 'root'@'%' IDENTIFIED BY '你的密码';\nGRANT ALL ON *.* TO 'root'@'%';\nALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '你的密码';\nFLUSH PRIVILEGES;\n\n\n\n# 1.5. 跳过登录认证\n\nvim /etc/my.cnf\n\n\n在 [mysqld] 下面加上 skip-grant-tables\n\n作用是登录时跳过登录认证，换句话说就是 root 什么密码都可以登录进去。\n\n执行 systemctl restart mysqld，重启 mysql\n\n\n# 2. 基本运维\n\n\n# 2.1. 客户端连接\n\n语法：mysql -h<主机> -P<端口> -u<用户名> -p<密码>\n\n如果没有显式指定密码，会要求输入密码才能访问。\n\n【示例】连接本地 Mysql\n\n$ mysql -h 127.0.0.1 -P 3306 -u root -p\nEnter password:\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 13501\nServer version: 8.0.19 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql>\n\n\n\n# 2.2. 查看连接\n\n连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。\n\n\n\n\n# 2.3. 创建用户\n\nCREATE USER 'username'@'host' IDENTIFIED BY 'password';\n\n\n说明：\n\n * username：你将创建的用户名\n * host：指定该用户在哪个主机上可以登陆，如果是本地用户可用 localhost，如果想让该用户可以从任意远程主机登陆，可以使用通配符%\n * password：该用户的登陆密码，密码可以为空，如果为空则该用户可以不需要密码登陆服务器\n\n示例：\n\nCREATE USER 'dog'@'localhost' IDENTIFIED BY '123456';\nCREATE USER 'pig'@'192.168.1.101_' IDENDIFIED BY '123456';\nCREATE USER 'pig'@'%' IDENTIFIED BY '123456';\nCREATE USER 'pig'@'%' IDENTIFIED BY '';\nCREATE USER 'pig'@'%';\n\n\n> 注意：在 Mysql 8 中，默认密码验证不再是 password。所以在创建用户时，create user 'username'@'%' identified by 'password'; 客户端是无法连接服务的。\n> \n> 所以，需要加上 IDENTIFIED WITH mysql_native_password，例如：CREATE USER 'slave'@'%' IDENTIFIED WITH mysql_native_password BY '123456';\n\n\n# 2.4. 查看用户\n\n-- 查看所有用户\nSELECT DISTINCT CONCAT('User: ''', user, '''@''', host, ''';') AS query\nFROM mysql.user;\n\n\n\n# 2.5. 授权\n\n命令：\n\nGRANT privileges ON databasename.tablename TO 'username'@'host'\n\n\n说明：\n\n * privileges：用户的操作权限，如SELECT，INSERT，UPDATE等，如果要授予所的权限则使用ALL\n * databasename：数据库名\n * tablename：表名，如果要授予该用户对所有数据库和表的相应操作权限则可用*表示，如*.*\n\n示例：\n\nGRANT SELECT, INSERT ON test.user TO 'pig'@'%';\nGRANT ALL ON *.* TO 'pig'@'%';\nGRANT ALL ON maindataplus.* TO 'pig'@'%';\n\n\n注意：\n\n用以上命令授权的用户不能给其它用户授权，如果想让该用户可以授权，用以下命令:\n\n-- 为指定用户配置指定权限\nGRANT privileges ON databasename.tablename TO 'username'@'host' WITH GRANT OPTION;\n-- 为 root 用户分配所有权限\nGRANT ALL ON *.* TO 'root'@'%' IDENTIFIED BY '密码' WITH GRANT OPTION;\n\n\n\n# 2.6. 撤销授权\n\n命令:\n\nREVOKE privilege ON databasename.tablename FROM 'username'@'host';\n\n\n说明:\n\nprivilege, databasename, tablename：同授权部分\n\n例子:\n\nREVOKE SELECT ON *.* FROM 'pig'@'%';\n\n\n注意:\n\n假如你在给用户'pig'@'%'授权的时候是这样的（或类似的）：GRANT SELECT ON test.user TO 'pig'@'%'，则在使用REVOKE SELECT ON *.* FROM 'pig'@'%';命令并不能撤销该用户对 test 数据库中 user 表的SELECT 操作。相反，如果授权使用的是GRANT SELECT ON *.* TO 'pig'@'%';则REVOKE SELECT ON test.user FROM 'pig'@'%';命令也不能撤销该用户对 test 数据库中 user 表的Select权限。\n\n具体信息可以用命令SHOW GRANTS FOR 'pig'@'%'; 查看。\n\n\n# 2.7. 查看授权\n\n-- 查看用户权限\nSHOW GRANTS FOR 'root'@'%';\n\n\n\n# 2.8. 更改用户密码\n\nSET PASSWORD FOR 'username'@'host' = PASSWORD('newpassword');\n\n\n如果是当前登陆用户用:\n\nSET PASSWORD = PASSWORD(\"newpassword\");\n\n\n示例：\n\nSET PASSWORD FOR 'pig'@'%' = PASSWORD(\"123456\");\n\n\n\n# 2.9. 备份与恢复\n\nMysql 备份数据使用 mysqldump 命令。\n\nmysqldump 将数据库中的数据备份成一个文本文件，表的结构和表中的数据将存储在生成的文本文件中。\n\n备份：\n\n# 备份一个数据库\n\n语法：\n\nmysqldump -h <host> -P<port> -u<username> -p<database> [<table1> <table2> ...] > backup.sql\n\n\n * host - Mysql Server 的 host\n * port - Mysql Server 的端口\n * username - 数据库用户\n * dbname - 数据库名称\n * table1 和 table2 参数表示需要备份的表的名称，为空则整个数据库备份；\n * BackupName.sql 参数表设计备份文件的名称，文件名前面可以加上一个绝对路径。通常将数据库被分成一个后缀名为 sql 的文件\n\n# 备份多个数据库\n\nmysqldump -u <username> -p --databases <database1> <database2> ... > backup.sql\n\n\n# 备份所有数据库\n\nmysqldump -u <username> -p --all-databases > backup.sql\n\n\n# 恢复一个数据库\n\nMysql 恢复数据使用 mysql 命令。\n\n语法：\n\nmysql -h <host> -P<port> -u<username> -p<database> < backup.sql\n\n\n# 恢复所有数据库\n\nmysql -u<username> -p --all-databases < backup.sql\n\n\n\n# 2.10. 卸载\n\n（1）查看已安装的 mysql\n\n$ rpm -qa | grep -i mysql\nperl-DBD-MySQL-4.023-6.el7.x86_64\nmysql80-community-release-el7-1.noarch\nmysql-community-common-8.0.12-1.el7.x86_64\nmysql-community-client-8.0.12-1.el7.x86_64\nmysql-community-libs-compat-8.0.12-1.el7.x86_64\nmysql-community-libs-8.0.12-1.el7.x86_64\n\n\n（2）卸载 mysql\n\nyum remove mysql-community-server.x86_64\n\n\n\n# 2.11. 主从节点部署\n\n假设需要配置一个主从 Mysql 服务器环境\n\n * master 节点：192.168.8.10\n * slave 节点：192.168.8.11\n\n# 主节点上的操作\n\n（1）修改配置并重启\n\n执行 vi /etc/my.cnf ，添加如下配置：\n\n[mysqld]\nserver-id=1\nlog_bin=/var/lib/mysql/binlog\n\n\n * server-id - 服务器 ID 号。在主从架构中，每台机器的 ID 必须唯一。\n * log_bin - 同步的日志路径及文件名，一定注意这个目录要是 mysql 有权限写入的；\n\n修改后，重启 mysql 使配置生效：\n\nsystemctl restart mysql\n\n\n（2）创建用于同步的用户\n\n进入 mysql 命令控制台：\n\n$ mysql -u root -p\nPassword:\n\n\n执行以下 SQL：\n\n-- a. 创建 slave 用户\nCREATE USER 'slave'@'%' IDENTIFIED WITH mysql_native_password BY '密码';\n-- 为 slave 赋予 REPLICATION SLAVE 权限\nGRANT REPLICATION SLAVE ON *.* TO 'slave'@'%';\n\n-- b. 或者，创建 slave 用户，并指定该用户能在任意主机上登录\n-- 如果有多个从节点，又想让所有从节点都使用统一的用户名、密码认证，可以考虑这种方式\nCREATE USER 'slave'@'%' IDENTIFIED WITH mysql_native_password BY '密码';\nGRANT REPLICATION SLAVE ON *.* TO 'slave'@'%';\n\n-- 刷新授权表信息\nFLUSH PRIVILEGES;\n\n\n> 注意：在 Mysql 8 中，默认密码验证不再是 password。所以在创建用户时，create user 'username'@'%' identified by 'password'; 客户端是无法连接服务的。所以，需要加上 IDENTIFIED WITH mysql_native_password BY 'password'\n\n补充用户管理 SQL:\n\n-- 查看所有用户\nSELECT DISTINCT CONCAT('User: ''', user, '''@''', host, ''';') AS query\nFROM mysql.user;\n\n-- 查看用户权限\nSHOW GRANTS FOR 'root'@'%';\n\n-- 创建用户\n-- a. 创建 slave 用户，并指定该用户只能在主机 192.168.8.11 上登录\nCREATE USER 'slave'@'192.168.8.11' IDENTIFIED WITH mysql_native_password BY '密码';\n-- 为 slave 赋予 REPLICATION SLAVE 权限\nGRANT REPLICATION SLAVE ON *.* TO 'slave'@'192.168.8.11';\n\n-- 删除用户\nDROP USER 'slave'@'192.168.8.11';\n\n\n（3）加读锁\n\n为了主库与从库的数据保持一致，我们先为 mysql 加入读锁，使其变为只读。\n\nmysql> FLUSH TABLES WITH READ LOCK;\n\n\n（4）查看主节点状态\n\nmysql> show master status;\n+------------------+----------+--------------+---------------------------------------------+-------------------+\n| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB                            | Executed_Gtid_Set |\n+------------------+----------+--------------+---------------------------------------------+-------------------+\n| mysql-bin.000001 |     4202 |              | mysql,information_schema,performance_schema |                   |\n+------------------+----------+--------------+---------------------------------------------+-------------------+\n1 row in set (0.00 sec)\n\n\n> 注意：需要记录下 File 和 Position，后面会用到。\n\n（5）导出 sql\n\nmysqldump -u root -p --all-databases --master-data > dbdump.sql\n\n\n（6）解除读锁\n\nmysql> UNLOCK TABLES;\n\n\n（7）将 sql 远程传送到从节点上\n\nscp dbdump.sql root@192.168.8.11:/home\n\n\n# 从节点上的操作\n\n（1）修改配置并重启\n\n执行 vi /etc/my.cnf ，添加如下配置：\n\n[mysqld]\nserver-id=2\nlog_bin=/var/lib/mysql/binlog\n\n\n * server-id - 服务器 ID 号。在主从架构中，每台机器的 ID 必须唯一。\n * log_bin - 同步的日志路径及文件名，一定注意这个目录要是 mysql 有权限写入的；\n\n修改后，重启 mysql 使配置生效：\n\nsystemctl restart mysql\n\n\n（2）导入 sql\n\nmysql -u root -p < /home/dbdump.sql\n\n\n（3）在从节点上建立与主节点的连接\n\n进入 mysql 命令控制台：\n\n$ mysql -u root -p\nPassword:\n\n\n执行以下 SQL：\n\n-- 停止从节点服务\nSTOP SLAVE;\n\n-- 注意：MASTER_USER 和\nCHANGE MASTER TO\nMASTER_HOST='192.168.8.10',\nMASTER_USER='slave',\nMASTER_PASSWORD='密码',\nMASTER_LOG_FILE='binlog.000001',\nMASTER_LOG_POS=4202;\n\n\n * MASTER_LOG_FILE 和 MASTER_LOG_POS 参数要分别与 show master status 指令获得的 File 和 Position 属性值对应。\n * MASTER_HOST 是主节点的 HOST。\n * MASTER_USER 和 MASTER_PASSWORD 是在主节点上注册的用户及密码。\n\n（4）启动 slave 进程\n\nmysql> start slave;\n\n\n（5）查看主从同步状态\n\nmysql> show slave status\\G;\n\n\n说明：如果以下两项参数均为 YES，说明配置正确。\n\n * Slave_IO_Running\n * Slave_SQL_Running\n\n（6）将从节点设为只读\n\nmysql> set global read_only=1;\nmysql> set global super_read_only=1;\nmysql> show global variables like \"%read_only%\";\n+-----------------------+-------+\n| Variable_name         | Value |\n+-----------------------+-------+\n| innodb_read_only      | OFF   |\n| read_only             | ON    |\n| super_read_only       | ON    |\n| transaction_read_only | OFF   |\n+-----------------------+-------+\n\n\n> 注：设置 slave 服务器为只读，并不影响主从同步。\n\n\n# 3. 服务器配置\n\n> 大部分情况下，默认的基本配置已经足够应付大多数场景，不要轻易修改 Mysql 服务器配置，除非你明确知道修改项是有益的。\n> \n> 尽量不要使用 Mysql 的缓存功能，因为其要求每次请求参数完全相同，才能命中缓存。这种方式实际上并不高效，还会增加额外开销，实际业务场景中一般使用 Redis 等 key-value 存储来解决缓存问题，性能远高于 Mysql 的查询缓存。\n\n\n# 3.1. 配置文件路径\n\n配置 Mysql 首先要确定配置文件在哪儿。\n\n不同 Linux 操作系统上，Mysql 配置文件路径可能不同。通常的路径为 /etc/my.cnf 或 /etc/mysql/my.cnf 。\n\n如果不知道配置文件路径，可以尝试以下操作：\n\n# which mysqld\n/usr/sbin/mysqld\n# /usr/sbin/mysqld --verbose --help | grep -A 1 'Default options'\nDefault options are read from the following files in the given order:\n/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf\n\n\n\n# 3.2. 配置项语法\n\nMysql 配置项设置都使用小写，单词之间用下划线或横线隔开（二者是等价的）。\n\n建议使用固定的风格，这样检索配置项时较为方便。\n\n# 这两种格式等价\n/usr/sbin/mysqld --auto-increment-offset=5\n/usr/sbin/mysqld --auto_increment_offset=5\n\n\n\n# 3.3. 常用配置项说明\n\n> 这里介绍比较常用的基本配置，更多配置项说明可以参考：Mysql 服务器配置说明\n\n先给出一份常用配置模板，内容如下：\n\n[mysqld]\n# GENERAL\n# -------------------------------------------------------------------------------\ndatadir = /var/lib/mysql\nsocket  = /var/lib/mysql/mysql.sock\npid_file = /var/lib/mysql/mysql.pid\nuser = mysql\nport = 3306\ndefault_storage_engine = InnoDB\ndefault_time_zone = '+8:00'\ncharacter_set_server = utf8mb4\ncollation_server = utf8mb4_0900_ai_ci\n\n# LOG\n# -------------------------------------------------------------------------------\nlog_error = /var/log/mysql/mysql-error.log\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/mysql-slow.log\n\n# InnoDB\n# -------------------------------------------------------------------------------\ninnodb_buffer_pool_size = <value>\ninnodb_log_file_size = <value>\ninnodb_file_per_table = 1\ninnodb_flush_method = O_DIRECT\n\n# MyIsam\n# -------------------------------------------------------------------------------\nkey_buffer_size = <value>\n\n# OTHER\n# -------------------------------------------------------------------------------\ntmp_table_size = 32M\nmax_heap_table_size = 32M\nquery_cache_type = 0\nquery_cache_size = 0\nmax_connections = <value>\nthread_cache = <value>\nopen_files_limit = 65535\n\n[client]\nsocket  = /var/lib/mysql/mysql.sock\nport = 3306\n\n\n * GENERAL\n   \n   * datadir - mysql 数据文件所在目录\n   * socket - scoket 文件\n   * pid_file - PID 文件\n   * user - 启动 mysql 服务进程的用户\n   * port - 服务端口号，默认 3306\n   * default_storage_engine - mysql 5.1 之后，默认引擎是 InnoDB\n   * default_time_zone - 默认时区。中国大部分地区在东八区，即 +8:00\n   * character_set_server - 数据库默认字符集\n   * collation_server - 数据库字符集对应一些排序等规则，注意要和 character_set_server 对应\n\n * LOG\n   \n   * log_error - 错误日志文件地址\n   * slow_query_log - 错误日志文件地址\n\n * InnoDB\n   \n   * innodb_buffer_pool_size - InnoDB 使用一个缓冲池来保存索引和原始数据，不像 MyISAM。这里你设置越大，你在存取表里面数据时所需要的磁盘 I/O 越少。\n     * 在一个独立使用的数据库服务器上,你可以设置这个变量到服务器物理内存大小的 60%-80%\n     * 注意别设置的过大，会导致 system 的 swap 空间被占用，导致操作系统变慢，从而减低 sql 查询的效率\n     * 默认值：128M，建议值：物理内存的 60%-80%\n   * innodb_log_file_size - 日志文件的大小。默认值：48M，建议值：根据你系统的磁盘空间和日志增长情况调整大小\n   * innodb_file_per_table - 说明：mysql5.7 之后默认开启，意思是，每张表一个独立表空间。默认值 1，开启。\n   * innodb_flush_method - 说明：控制着 innodb 数据文件及 redo log 的打开、刷写模式，三种模式：fdatasync(默认)，O_DSYNC，O_DIRECT。默认值为空，建议值：使用 SAN 或者 raid，建议用 O_DIRECT，不懂测试的话，默认生产上使用 O_DIRECT\n     * fdatasync：数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->os buffer->磁盘；\n     * O_DSYNC： 数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->磁盘；\n     * O_DIRECT： 数据文件，buffer pool->磁盘； 日志文件，buffer pool->os buffer->磁盘；\n\n * MyIsam\n   \n   * key_buffer_size - 指定索引缓冲区的大小，为 MYISAM 数据表开启供线程共享的索引缓存，对 INNODB 引擎无效。相当影响 MyISAM 的性能。\n     * 不要将其设置大于你可用内存的 30%，因为一部分内存同样被 OS 用来缓冲行数据\n     * 甚至在你并不使用 MyISAM 表的情况下，你也需要仍旧设置起 8-64M 内存由于它同样会被内部临时磁盘表使用。\n     * 默认值 8M，建议值：对于内存在 4GB 左右的服务器该参数可设置为 256M 或 384M。\n     * 注意：该参数值设置的过大反而会是服务器整体效率降低！\n\n * OTHER\n   \n   * tmp_table_size - 内存临时表的最大值，默认 16M，此处设置成 128M\n   * max_heap_table_size - 用户创建的内存表的大小，默认 16M，往往和 tmp_table_size 一起设置，限制用户临时表大小。超限的话，MySQL 就会自动地把它转化为基于磁盘的 MyISAM 表，存储在指定的 tmpdir 目录下，增大 IO 压力，建议内存大，增大该数值。\n   * query_cache_type - 这个系统变量控制着查询缓存功能的开启和关闭，0 表示关闭，1 表示打开，2 表示只要 select 中明确指定 SQL_CACHE 才缓存。\n   * query_cache_size - 默认值 1M，优点是查询缓存可以极大的提高服务器速度，如果你有大量的相同的查询并且很少修改表。缺点：在你表经常变化的情况下或者如果你的查询原文每次都不同，查询缓存也许引起性能下降而不是性能提升。\n   * max_connections - 最大连接数，可设最大值 16384，一般考虑根据同时在线人数设置一个比较综合的数字，鉴于该数值增大并不太消耗系统资源，建议直接设 10000。如果在访问时经常出现 Too Many Connections 的错误提示，则需要增大该参数值\n   * thread_cache - 当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁。可重用，减小了系统开销。默认值为 9，建议值：两种取值方式，\n     * 方式一，根据物理内存，1G —> 8；2G —> 16； 3G —> 32； >3G —> 64；\n     * 方式二，根据 show status like 'threads%'，查看 Threads_connected 值。\n   * open_files_limit - MySQL 打开的文件描述符限制，默认最小 1024;\n     * 当 open_files_limit 没有被配置的时候，比较 max_connections*5 和 ulimit -n 的值，哪个大用哪个，\n     * 当 open_file_limit 被配置的时候，比较 open_files_limit 和 max_connections*5 的值，哪个大用哪个\n     * 注意：仍然可能出现报错信息 Can't create a new thread；此时观察系统 cat /proc/mysql 进程号/limits，观察进程 ulimit 限制情况\n     * 过小的话，考虑修改系统配置表，/etc/security/limits.conf 和 /etc/security/limits.d/90-nproc.conf\n\n\n# 4. 常见问题\n\n\n# 4.1. Too many connections\n\n现象\n\n尝试连接 Mysql 时，遇到 Too many connections 错误。\n\n原因\n\n数据库连接线程数超过最大值，访问被拒绝。\n\n解决方案\n\n如果实际连接线程数过大，可以考虑增加服务器节点来分流；如果实际线程数并不算过大，那么可以配置 max_connections 来增加允许的最大连接数。需要注意的是，连接数不宜过大，一般来说，单库每秒有 2000 个并发连接时，就可以考虑扩容了，健康的状态应该维持在每秒 1000 个并发连接左右。\n\n（1）查看最大连接数\n\nmysql> show variables like '%max_connections%';\n+------------------------+-------+\n| Variable_name          | Value |\n+------------------------+-------+\n| max_connections        | 151   |\n| mysqlx_max_connections | 100   |\n+------------------------+-------+\n\n\n（2）查看服务器响应的最大连接数\n\nmysql> show global status like 'Max_used_connections';\n+----------------------+-------+\n| Variable_name        | Value |\n+----------------------+-------+\n| Max_used_connections | 142   |\n+----------------------+-------+\n1 row in set (0.00 sec)\n\n\n（3）临时设置最大连接数\n\nset GLOBAL max_connections=256;\n\n\n注意：当服务器重启时，最大连接数会被重置。\n\n（4）永久设置最大连接数\n\n修改 /etc/my.cnf 配置文件，在 [mysqld] 添加以下配置：\n\nmax_connections=256\n\n\n重启 mysql 以生效\n\n（5）修改 Linux 最大文件数限制\n\n设置了最大连接数，如果还是没有生效，考虑检查一下 Linux 最大文件数\n\nMysql 最大连接数会受到最大文件数限制，vim /etc/security/limits.conf，添加 mysql 用户配置\n\nmysql hard nofile 65535\nmysql soft nofile 65535\n\n\n（6）检查 LimitNOFILE\n\n如果是使用 rpm 方式安装 mysql，检查 mysqld.service 文件中的 LimitNOFILE 是否配置的太小。\n\n\n# 4.2. 时区（time_zone）偏差\n\n现象\n\n数据库中存储的 Timestamp 字段值比真实值少了 13 个小时。\n\n原因\n\n * 当 JDBC 与 MySQL 开始建立连接时，会获取服务器参数。\n * 当 MySQL 的 time_zone 值为 SYSTEM 时，会取 system_time_zone 值作为协调时区，若得到的是 CST 那么 Java 会误以为这是 CST -0500 ，因此会给出错误的时区信息（国内一般是CST +0800，即东八区）。\n\n查看时区方法：\n\n通过 show variables like '%time_zone%'; 命令查看 Mysql 时区配置：\n\nmysql> show variables like '%time_zone%';\n+------------------+--------+\n| Variable_name    | Value  |\n+------------------+--------+\n| system_time_zone | CST    |\n| time_zone        | SYSTEM |\n+------------------+--------+\n\n\n解决方案\n\n方案一\n\nmysql> set global time_zone = '+08:00';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> set time_zone = '+08:00';\nQuery OK, 0 rows affected (0.00 sec)\n\n\n方案二\n\n修改 my.cnf 文件，在 [mysqld] 节下增加 default-time-zone='+08:00' ，然后重启。\n\n\n# 4.3. 数据表损坏如何修复\n\n使用 myisamchk 来修复，具体步骤：\n\n 1. 修复前将 mysql 服务停止。\n 2. 打开命令行方式，然后进入到 mysql 的 bin 目录。\n 3. 执行 myisamchk –recover 数据库所在路 /*.MYI\n\n使用 repair table 或者 OPTIMIZE table 命令来修复，REPAIR TABLE table_name 修复表 OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了 OPTIMIZE TABLE 命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库）\n\n\n# 4.4. 数据结构\n\n> 问题现象：ERROR 1071: Specified key was too long; max key length is 767 bytes\n\n问题原因：Mysql 默认情况下单个列的索引不能超过 767 位（不同版本可能存在差异） 。\n\n解决方法：优化索引结构，索引字段不宜过长。\n\n\n# 5. 脚本\n\n这里推荐我写的几个一键运维脚本，非常方便，欢迎使用：\n\n * Mysql 安装脚本\n * Mysql 备份脚本\n\n\n# 6. 参考资料\n\n * 《高性能 MySQL》\n * https://www.cnblogs.com/xiaopotian/p/8196464.html\n * https://www.cnblogs.com/bigbrotherer/p/7241845.html\n * https://blog.csdn.net/managementandjava/article/details/80039650\n * http://www.manongjc.com/article/6996.html\n * https://www.cnblogs.com/xyabk/p/8967990.html\n * MySQL 8.0 主从（Master-Slave）配置\n * Mysql 主从同步实战\n * MySQL 备份和恢复机制",normalizedContent:"# mysql 运维\n\n> 如果你的公司有 dba，那么我恭喜你，你可以无视 mysql 运维。如果你的公司没有 dba，那你就好好学两手 mysql 基本运维操作，行走江湖，防身必备。\n\n\n# 1. 安装部署\n\n\n# 1.1. windows 安装\n\n（1）下载 mysql 5.7 免安装版\n\n下载地址：https://dev.mysql.com/downloads/mysql/5.7.html#downloads\n\n（2）解压并创建 my.ini 在根目录\n\nmy.ini 文件示例：\n\n[mysqld]\n#设置3306端口\nport = 3306\n# 设置mysql的安装目录 这块换成自己解压的路径\nbasedir=d:\\\\tools\\\\db\\\\mysql\\\\mysql-5.7.31\n# 允许最大连接数\nmax_connections=200\n# 服务端使用的字符集默认为8比特编码的latin1字符集\ncharacter-set-server=utf8\n# 创建新表时将使用的默认存储引擎\ndefault-storage-engine=innodb\n\n[client]\n# 设置mysql客户端默认字符集\ndefault-character-set=utf8\n\n\n（3）执行安装命令\n\n在控制台 cmd 中依次执行以下安装命令\n\ncd d:\\\\tools\\\\db\\\\mysql\\\\mysql-5.7.31\nmysqld --initialize\nmysqld -install\n\n\n说明：\n\n * mysqld --initialize 会自动初始化创建 data 文件夹并初始化 mysql。\n * mysqld -install 会安装 mysql 服务。\n\n（4）启动服务\n\n在控制台执行 net start mysql 启动服务。\n\n\n# 1.2. centos 安装\n\n> 本文仅介绍 rpm 安装方式\n\n# 安装 mysql yum 源\n\n官方下载地址：https://dev.mysql.com/downloads/repo/yum/\n\n（1）下载 yum 源\n\nwget https://dev.mysql.com/get/mysql80-community-release-el7-1.noarch.rpm\n\n\n（2）安装 yum repo 文件并更新 yum 缓存\n\nrpm -ivh mysql80-community-release-el7-1.noarch.rpm\n\n\n执行结果：\n\n会在 /etc/yum.repos.d/ 目录下生成两个 repo 文件\n\n$ ls | grep mysql\nmysql-community.repo\nmysql-community-source.repo\n\n\n更新 yum：\n\nyum clean all\nyum makecache\n\n\n（3）查看 rpm 安装状态\n\n$ yum search mysql | grep server\nmysql-community-common.i686 : mysql database common files for server and client\nmysql-community-common.x86_64 : mysql database common files for server and\nmysql-community-test.x86_64 : test suite for the mysql database server\n                       : administering mysql servers\nmysql-community-server.x86_64 : a very fast and reliable sql database server\n\n\n通过 yum 安装 mysql 有几个重要目录：\n\n## 配置文件\n/etc/my.cnf\n## 数据库目录\n/var/lib/mysql/\n## 配置文件\n/usr/share/mysql（mysql.server命令及配置文件）\n## 相关命令\n/usr/bin（mysqladmin mysqldump等命令）\n## 启动脚本\n/usr/lib/systemd/system/mysqld.service （注册为 systemd 服务）\n\n\n（4）安装 mysql 服务器\n\nyum install mysql-community-server\n\n\n# mysql 服务管理\n\n通过 yum 方式安装 mysql 后，本地会有一个名为 mysqld 的 systemd 服务。\n\n其服务管理十分简便：\n\n## 查看状态\nsystemctl status mysqld\n## 启用服务\nsystemctl enable mysqld\n## 禁用服务\nsystemctl disable mysqld\n## 启动服务\nsystemctl start mysqld\n## 重启服务\nsystemctl restart mysqld\n## 停止服务\nsystemctl stop mysqld\n\n\n\n# 1.3. 初始化数据库密码\n\n查看一下初始密码\n\n$ grep \"password\" /var/log/mysqld.log\n2018-09-30t03:13:41.727736z 5 [note] [my-010454] [server] a temporary password is generated for root@localhost: %:lt+srwu4k1\n\n\n执行命令：\n\nmysql -uroot -p<临时密码>\n\n\n输入临时密码，进入 mysql，如果要修改密码，执行以下指令：\n\nalter user 'root'@'localhost' identified by '你的密码';\n\n\n注：密码强度默认为中等，大小写字母、数字、特殊符号，只有修改成功后才能修改配置再设置更简单的密码\n\n\n# 1.4. 配置远程访问\n\ncreate user 'root'@'%' identified by '你的密码';\ngrant all on *.* to 'root'@'%';\nalter user 'root'@'%' identified with mysql_native_password by '你的密码';\nflush privileges;\n\n\n\n# 1.5. 跳过登录认证\n\nvim /etc/my.cnf\n\n\n在 [mysqld] 下面加上 skip-grant-tables\n\n作用是登录时跳过登录认证，换句话说就是 root 什么密码都可以登录进去。\n\n执行 systemctl restart mysqld，重启 mysql\n\n\n# 2. 基本运维\n\n\n# 2.1. 客户端连接\n\n语法：mysql -h<主机> -p<端口> -u<用户名> -p<密码>\n\n如果没有显式指定密码，会要求输入密码才能访问。\n\n【示例】连接本地 mysql\n\n$ mysql -h 127.0.0.1 -p 3306 -u root -p\nenter password:\nwelcome to the mysql monitor.  commands end with ; or \\g.\nyour mysql connection id is 13501\nserver version: 8.0.19 mysql community server - gpl\n\ncopyright (c) 2000, 2020, oracle and/or its affiliates. all rights reserved.\n\noracle is a registered trademark of oracle corporation and/or its\naffiliates. other names may be trademarks of their respective\nowners.\n\ntype 'help;' or '\\h' for help. type '\\c' to clear the current input statement.\n\nmysql>\n\n\n\n# 2.2. 查看连接\n\n连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。\n\n\n\n\n# 2.3. 创建用户\n\ncreate user 'username'@'host' identified by 'password';\n\n\n说明：\n\n * username：你将创建的用户名\n * host：指定该用户在哪个主机上可以登陆，如果是本地用户可用 localhost，如果想让该用户可以从任意远程主机登陆，可以使用通配符%\n * password：该用户的登陆密码，密码可以为空，如果为空则该用户可以不需要密码登陆服务器\n\n示例：\n\ncreate user 'dog'@'localhost' identified by '123456';\ncreate user 'pig'@'192.168.1.101_' idendified by '123456';\ncreate user 'pig'@'%' identified by '123456';\ncreate user 'pig'@'%' identified by '';\ncreate user 'pig'@'%';\n\n\n> 注意：在 mysql 8 中，默认密码验证不再是 password。所以在创建用户时，create user 'username'@'%' identified by 'password'; 客户端是无法连接服务的。\n> \n> 所以，需要加上 identified with mysql_native_password，例如：create user 'slave'@'%' identified with mysql_native_password by '123456';\n\n\n# 2.4. 查看用户\n\n-- 查看所有用户\nselect distinct concat('user: ''', user, '''@''', host, ''';') as query\nfrom mysql.user;\n\n\n\n# 2.5. 授权\n\n命令：\n\ngrant privileges on databasename.tablename to 'username'@'host'\n\n\n说明：\n\n * privileges：用户的操作权限，如select，insert，update等，如果要授予所的权限则使用all\n * databasename：数据库名\n * tablename：表名，如果要授予该用户对所有数据库和表的相应操作权限则可用*表示，如*.*\n\n示例：\n\ngrant select, insert on test.user to 'pig'@'%';\ngrant all on *.* to 'pig'@'%';\ngrant all on maindataplus.* to 'pig'@'%';\n\n\n注意：\n\n用以上命令授权的用户不能给其它用户授权，如果想让该用户可以授权，用以下命令:\n\n-- 为指定用户配置指定权限\ngrant privileges on databasename.tablename to 'username'@'host' with grant option;\n-- 为 root 用户分配所有权限\ngrant all on *.* to 'root'@'%' identified by '密码' with grant option;\n\n\n\n# 2.6. 撤销授权\n\n命令:\n\nrevoke privilege on databasename.tablename from 'username'@'host';\n\n\n说明:\n\nprivilege, databasename, tablename：同授权部分\n\n例子:\n\nrevoke select on *.* from 'pig'@'%';\n\n\n注意:\n\n假如你在给用户'pig'@'%'授权的时候是这样的（或类似的）：grant select on test.user to 'pig'@'%'，则在使用revoke select on *.* from 'pig'@'%';命令并不能撤销该用户对 test 数据库中 user 表的select 操作。相反，如果授权使用的是grant select on *.* to 'pig'@'%';则revoke select on test.user from 'pig'@'%';命令也不能撤销该用户对 test 数据库中 user 表的select权限。\n\n具体信息可以用命令show grants for 'pig'@'%'; 查看。\n\n\n# 2.7. 查看授权\n\n-- 查看用户权限\nshow grants for 'root'@'%';\n\n\n\n# 2.8. 更改用户密码\n\nset password for 'username'@'host' = password('newpassword');\n\n\n如果是当前登陆用户用:\n\nset password = password(\"newpassword\");\n\n\n示例：\n\nset password for 'pig'@'%' = password(\"123456\");\n\n\n\n# 2.9. 备份与恢复\n\nmysql 备份数据使用 mysqldump 命令。\n\nmysqldump 将数据库中的数据备份成一个文本文件，表的结构和表中的数据将存储在生成的文本文件中。\n\n备份：\n\n# 备份一个数据库\n\n语法：\n\nmysqldump -h <host> -p<port> -u<username> -p<database> [<table1> <table2> ...] > backup.sql\n\n\n * host - mysql server 的 host\n * port - mysql server 的端口\n * username - 数据库用户\n * dbname - 数据库名称\n * table1 和 table2 参数表示需要备份的表的名称，为空则整个数据库备份；\n * backupname.sql 参数表设计备份文件的名称，文件名前面可以加上一个绝对路径。通常将数据库被分成一个后缀名为 sql 的文件\n\n# 备份多个数据库\n\nmysqldump -u <username> -p --databases <database1> <database2> ... > backup.sql\n\n\n# 备份所有数据库\n\nmysqldump -u <username> -p --all-databases > backup.sql\n\n\n# 恢复一个数据库\n\nmysql 恢复数据使用 mysql 命令。\n\n语法：\n\nmysql -h <host> -p<port> -u<username> -p<database> < backup.sql\n\n\n# 恢复所有数据库\n\nmysql -u<username> -p --all-databases < backup.sql\n\n\n\n# 2.10. 卸载\n\n（1）查看已安装的 mysql\n\n$ rpm -qa | grep -i mysql\nperl-dbd-mysql-4.023-6.el7.x86_64\nmysql80-community-release-el7-1.noarch\nmysql-community-common-8.0.12-1.el7.x86_64\nmysql-community-client-8.0.12-1.el7.x86_64\nmysql-community-libs-compat-8.0.12-1.el7.x86_64\nmysql-community-libs-8.0.12-1.el7.x86_64\n\n\n（2）卸载 mysql\n\nyum remove mysql-community-server.x86_64\n\n\n\n# 2.11. 主从节点部署\n\n假设需要配置一个主从 mysql 服务器环境\n\n * master 节点：192.168.8.10\n * slave 节点：192.168.8.11\n\n# 主节点上的操作\n\n（1）修改配置并重启\n\n执行 vi /etc/my.cnf ，添加如下配置：\n\n[mysqld]\nserver-id=1\nlog_bin=/var/lib/mysql/binlog\n\n\n * server-id - 服务器 id 号。在主从架构中，每台机器的 id 必须唯一。\n * log_bin - 同步的日志路径及文件名，一定注意这个目录要是 mysql 有权限写入的；\n\n修改后，重启 mysql 使配置生效：\n\nsystemctl restart mysql\n\n\n（2）创建用于同步的用户\n\n进入 mysql 命令控制台：\n\n$ mysql -u root -p\npassword:\n\n\n执行以下 sql：\n\n-- a. 创建 slave 用户\ncreate user 'slave'@'%' identified with mysql_native_password by '密码';\n-- 为 slave 赋予 replication slave 权限\ngrant replication slave on *.* to 'slave'@'%';\n\n-- b. 或者，创建 slave 用户，并指定该用户能在任意主机上登录\n-- 如果有多个从节点，又想让所有从节点都使用统一的用户名、密码认证，可以考虑这种方式\ncreate user 'slave'@'%' identified with mysql_native_password by '密码';\ngrant replication slave on *.* to 'slave'@'%';\n\n-- 刷新授权表信息\nflush privileges;\n\n\n> 注意：在 mysql 8 中，默认密码验证不再是 password。所以在创建用户时，create user 'username'@'%' identified by 'password'; 客户端是无法连接服务的。所以，需要加上 identified with mysql_native_password by 'password'\n\n补充用户管理 sql:\n\n-- 查看所有用户\nselect distinct concat('user: ''', user, '''@''', host, ''';') as query\nfrom mysql.user;\n\n-- 查看用户权限\nshow grants for 'root'@'%';\n\n-- 创建用户\n-- a. 创建 slave 用户，并指定该用户只能在主机 192.168.8.11 上登录\ncreate user 'slave'@'192.168.8.11' identified with mysql_native_password by '密码';\n-- 为 slave 赋予 replication slave 权限\ngrant replication slave on *.* to 'slave'@'192.168.8.11';\n\n-- 删除用户\ndrop user 'slave'@'192.168.8.11';\n\n\n（3）加读锁\n\n为了主库与从库的数据保持一致，我们先为 mysql 加入读锁，使其变为只读。\n\nmysql> flush tables with read lock;\n\n\n（4）查看主节点状态\n\nmysql> show master status;\n+------------------+----------+--------------+---------------------------------------------+-------------------+\n| file             | position | binlog_do_db | binlog_ignore_db                            | executed_gtid_set |\n+------------------+----------+--------------+---------------------------------------------+-------------------+\n| mysql-bin.000001 |     4202 |              | mysql,information_schema,performance_schema |                   |\n+------------------+----------+--------------+---------------------------------------------+-------------------+\n1 row in set (0.00 sec)\n\n\n> 注意：需要记录下 file 和 position，后面会用到。\n\n（5）导出 sql\n\nmysqldump -u root -p --all-databases --master-data > dbdump.sql\n\n\n（6）解除读锁\n\nmysql> unlock tables;\n\n\n（7）将 sql 远程传送到从节点上\n\nscp dbdump.sql root@192.168.8.11:/home\n\n\n# 从节点上的操作\n\n（1）修改配置并重启\n\n执行 vi /etc/my.cnf ，添加如下配置：\n\n[mysqld]\nserver-id=2\nlog_bin=/var/lib/mysql/binlog\n\n\n * server-id - 服务器 id 号。在主从架构中，每台机器的 id 必须唯一。\n * log_bin - 同步的日志路径及文件名，一定注意这个目录要是 mysql 有权限写入的；\n\n修改后，重启 mysql 使配置生效：\n\nsystemctl restart mysql\n\n\n（2）导入 sql\n\nmysql -u root -p < /home/dbdump.sql\n\n\n（3）在从节点上建立与主节点的连接\n\n进入 mysql 命令控制台：\n\n$ mysql -u root -p\npassword:\n\n\n执行以下 sql：\n\n-- 停止从节点服务\nstop slave;\n\n-- 注意：master_user 和\nchange master to\nmaster_host='192.168.8.10',\nmaster_user='slave',\nmaster_password='密码',\nmaster_log_file='binlog.000001',\nmaster_log_pos=4202;\n\n\n * master_log_file 和 master_log_pos 参数要分别与 show master status 指令获得的 file 和 position 属性值对应。\n * master_host 是主节点的 host。\n * master_user 和 master_password 是在主节点上注册的用户及密码。\n\n（4）启动 slave 进程\n\nmysql> start slave;\n\n\n（5）查看主从同步状态\n\nmysql> show slave status\\g;\n\n\n说明：如果以下两项参数均为 yes，说明配置正确。\n\n * slave_io_running\n * slave_sql_running\n\n（6）将从节点设为只读\n\nmysql> set global read_only=1;\nmysql> set global super_read_only=1;\nmysql> show global variables like \"%read_only%\";\n+-----------------------+-------+\n| variable_name         | value |\n+-----------------------+-------+\n| innodb_read_only      | off   |\n| read_only             | on    |\n| super_read_only       | on    |\n| transaction_read_only | off   |\n+-----------------------+-------+\n\n\n> 注：设置 slave 服务器为只读，并不影响主从同步。\n\n\n# 3. 服务器配置\n\n> 大部分情况下，默认的基本配置已经足够应付大多数场景，不要轻易修改 mysql 服务器配置，除非你明确知道修改项是有益的。\n> \n> 尽量不要使用 mysql 的缓存功能，因为其要求每次请求参数完全相同，才能命中缓存。这种方式实际上并不高效，还会增加额外开销，实际业务场景中一般使用 redis 等 key-value 存储来解决缓存问题，性能远高于 mysql 的查询缓存。\n\n\n# 3.1. 配置文件路径\n\n配置 mysql 首先要确定配置文件在哪儿。\n\n不同 linux 操作系统上，mysql 配置文件路径可能不同。通常的路径为 /etc/my.cnf 或 /etc/mysql/my.cnf 。\n\n如果不知道配置文件路径，可以尝试以下操作：\n\n# which mysqld\n/usr/sbin/mysqld\n# /usr/sbin/mysqld --verbose --help | grep -a 1 'default options'\ndefault options are read from the following files in the given order:\n/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf\n\n\n\n# 3.2. 配置项语法\n\nmysql 配置项设置都使用小写，单词之间用下划线或横线隔开（二者是等价的）。\n\n建议使用固定的风格，这样检索配置项时较为方便。\n\n# 这两种格式等价\n/usr/sbin/mysqld --auto-increment-offset=5\n/usr/sbin/mysqld --auto_increment_offset=5\n\n\n\n# 3.3. 常用配置项说明\n\n> 这里介绍比较常用的基本配置，更多配置项说明可以参考：mysql 服务器配置说明\n\n先给出一份常用配置模板，内容如下：\n\n[mysqld]\n# general\n# -------------------------------------------------------------------------------\ndatadir = /var/lib/mysql\nsocket  = /var/lib/mysql/mysql.sock\npid_file = /var/lib/mysql/mysql.pid\nuser = mysql\nport = 3306\ndefault_storage_engine = innodb\ndefault_time_zone = '+8:00'\ncharacter_set_server = utf8mb4\ncollation_server = utf8mb4_0900_ai_ci\n\n# log\n# -------------------------------------------------------------------------------\nlog_error = /var/log/mysql/mysql-error.log\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/mysql-slow.log\n\n# innodb\n# -------------------------------------------------------------------------------\ninnodb_buffer_pool_size = <value>\ninnodb_log_file_size = <value>\ninnodb_file_per_table = 1\ninnodb_flush_method = o_direct\n\n# myisam\n# -------------------------------------------------------------------------------\nkey_buffer_size = <value>\n\n# other\n# -------------------------------------------------------------------------------\ntmp_table_size = 32m\nmax_heap_table_size = 32m\nquery_cache_type = 0\nquery_cache_size = 0\nmax_connections = <value>\nthread_cache = <value>\nopen_files_limit = 65535\n\n[client]\nsocket  = /var/lib/mysql/mysql.sock\nport = 3306\n\n\n * general\n   \n   * datadir - mysql 数据文件所在目录\n   * socket - scoket 文件\n   * pid_file - pid 文件\n   * user - 启动 mysql 服务进程的用户\n   * port - 服务端口号，默认 3306\n   * default_storage_engine - mysql 5.1 之后，默认引擎是 innodb\n   * default_time_zone - 默认时区。中国大部分地区在东八区，即 +8:00\n   * character_set_server - 数据库默认字符集\n   * collation_server - 数据库字符集对应一些排序等规则，注意要和 character_set_server 对应\n\n * log\n   \n   * log_error - 错误日志文件地址\n   * slow_query_log - 错误日志文件地址\n\n * innodb\n   \n   * innodb_buffer_pool_size - innodb 使用一个缓冲池来保存索引和原始数据，不像 myisam。这里你设置越大，你在存取表里面数据时所需要的磁盘 i/o 越少。\n     * 在一个独立使用的数据库服务器上,你可以设置这个变量到服务器物理内存大小的 60%-80%\n     * 注意别设置的过大，会导致 system 的 swap 空间被占用，导致操作系统变慢，从而减低 sql 查询的效率\n     * 默认值：128m，建议值：物理内存的 60%-80%\n   * innodb_log_file_size - 日志文件的大小。默认值：48m，建议值：根据你系统的磁盘空间和日志增长情况调整大小\n   * innodb_file_per_table - 说明：mysql5.7 之后默认开启，意思是，每张表一个独立表空间。默认值 1，开启。\n   * innodb_flush_method - 说明：控制着 innodb 数据文件及 redo log 的打开、刷写模式，三种模式：fdatasync(默认)，o_dsync，o_direct。默认值为空，建议值：使用 san 或者 raid，建议用 o_direct，不懂测试的话，默认生产上使用 o_direct\n     * fdatasync：数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->os buffer->磁盘；\n     * o_dsync： 数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->磁盘；\n     * o_direct： 数据文件，buffer pool->磁盘； 日志文件，buffer pool->os buffer->磁盘；\n\n * myisam\n   \n   * key_buffer_size - 指定索引缓冲区的大小，为 myisam 数据表开启供线程共享的索引缓存，对 innodb 引擎无效。相当影响 myisam 的性能。\n     * 不要将其设置大于你可用内存的 30%，因为一部分内存同样被 os 用来缓冲行数据\n     * 甚至在你并不使用 myisam 表的情况下，你也需要仍旧设置起 8-64m 内存由于它同样会被内部临时磁盘表使用。\n     * 默认值 8m，建议值：对于内存在 4gb 左右的服务器该参数可设置为 256m 或 384m。\n     * 注意：该参数值设置的过大反而会是服务器整体效率降低！\n\n * other\n   \n   * tmp_table_size - 内存临时表的最大值，默认 16m，此处设置成 128m\n   * max_heap_table_size - 用户创建的内存表的大小，默认 16m，往往和 tmp_table_size 一起设置，限制用户临时表大小。超限的话，mysql 就会自动地把它转化为基于磁盘的 myisam 表，存储在指定的 tmpdir 目录下，增大 io 压力，建议内存大，增大该数值。\n   * query_cache_type - 这个系统变量控制着查询缓存功能的开启和关闭，0 表示关闭，1 表示打开，2 表示只要 select 中明确指定 sql_cache 才缓存。\n   * query_cache_size - 默认值 1m，优点是查询缓存可以极大的提高服务器速度，如果你有大量的相同的查询并且很少修改表。缺点：在你表经常变化的情况下或者如果你的查询原文每次都不同，查询缓存也许引起性能下降而不是性能提升。\n   * max_connections - 最大连接数，可设最大值 16384，一般考虑根据同时在线人数设置一个比较综合的数字，鉴于该数值增大并不太消耗系统资源，建议直接设 10000。如果在访问时经常出现 too many connections 的错误提示，则需要增大该参数值\n   * thread_cache - 当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁。可重用，减小了系统开销。默认值为 9，建议值：两种取值方式，\n     * 方式一，根据物理内存，1g —> 8；2g —> 16； 3g —> 32； >3g —> 64；\n     * 方式二，根据 show status like 'threads%'，查看 threads_connected 值。\n   * open_files_limit - mysql 打开的文件描述符限制，默认最小 1024;\n     * 当 open_files_limit 没有被配置的时候，比较 max_connections*5 和 ulimit -n 的值，哪个大用哪个，\n     * 当 open_file_limit 被配置的时候，比较 open_files_limit 和 max_connections*5 的值，哪个大用哪个\n     * 注意：仍然可能出现报错信息 can't create a new thread；此时观察系统 cat /proc/mysql 进程号/limits，观察进程 ulimit 限制情况\n     * 过小的话，考虑修改系统配置表，/etc/security/limits.conf 和 /etc/security/limits.d/90-nproc.conf\n\n\n# 4. 常见问题\n\n\n# 4.1. too many connections\n\n现象\n\n尝试连接 mysql 时，遇到 too many connections 错误。\n\n原因\n\n数据库连接线程数超过最大值，访问被拒绝。\n\n解决方案\n\n如果实际连接线程数过大，可以考虑增加服务器节点来分流；如果实际线程数并不算过大，那么可以配置 max_connections 来增加允许的最大连接数。需要注意的是，连接数不宜过大，一般来说，单库每秒有 2000 个并发连接时，就可以考虑扩容了，健康的状态应该维持在每秒 1000 个并发连接左右。\n\n（1）查看最大连接数\n\nmysql> show variables like '%max_connections%';\n+------------------------+-------+\n| variable_name          | value |\n+------------------------+-------+\n| max_connections        | 151   |\n| mysqlx_max_connections | 100   |\n+------------------------+-------+\n\n\n（2）查看服务器响应的最大连接数\n\nmysql> show global status like 'max_used_connections';\n+----------------------+-------+\n| variable_name        | value |\n+----------------------+-------+\n| max_used_connections | 142   |\n+----------------------+-------+\n1 row in set (0.00 sec)\n\n\n（3）临时设置最大连接数\n\nset global max_connections=256;\n\n\n注意：当服务器重启时，最大连接数会被重置。\n\n（4）永久设置最大连接数\n\n修改 /etc/my.cnf 配置文件，在 [mysqld] 添加以下配置：\n\nmax_connections=256\n\n\n重启 mysql 以生效\n\n（5）修改 linux 最大文件数限制\n\n设置了最大连接数，如果还是没有生效，考虑检查一下 linux 最大文件数\n\nmysql 最大连接数会受到最大文件数限制，vim /etc/security/limits.conf，添加 mysql 用户配置\n\nmysql hard nofile 65535\nmysql soft nofile 65535\n\n\n（6）检查 limitnofile\n\n如果是使用 rpm 方式安装 mysql，检查 mysqld.service 文件中的 limitnofile 是否配置的太小。\n\n\n# 4.2. 时区（time_zone）偏差\n\n现象\n\n数据库中存储的 timestamp 字段值比真实值少了 13 个小时。\n\n原因\n\n * 当 jdbc 与 mysql 开始建立连接时，会获取服务器参数。\n * 当 mysql 的 time_zone 值为 system 时，会取 system_time_zone 值作为协调时区，若得到的是 cst 那么 java 会误以为这是 cst -0500 ，因此会给出错误的时区信息（国内一般是cst +0800，即东八区）。\n\n查看时区方法：\n\n通过 show variables like '%time_zone%'; 命令查看 mysql 时区配置：\n\nmysql> show variables like '%time_zone%';\n+------------------+--------+\n| variable_name    | value  |\n+------------------+--------+\n| system_time_zone | cst    |\n| time_zone        | system |\n+------------------+--------+\n\n\n解决方案\n\n方案一\n\nmysql> set global time_zone = '+08:00';\nquery ok, 0 rows affected (0.00 sec)\n\nmysql> set time_zone = '+08:00';\nquery ok, 0 rows affected (0.00 sec)\n\n\n方案二\n\n修改 my.cnf 文件，在 [mysqld] 节下增加 default-time-zone='+08:00' ，然后重启。\n\n\n# 4.3. 数据表损坏如何修复\n\n使用 myisamchk 来修复，具体步骤：\n\n 1. 修复前将 mysql 服务停止。\n 2. 打开命令行方式，然后进入到 mysql 的 bin 目录。\n 3. 执行 myisamchk –recover 数据库所在路 /*.myi\n\n使用 repair table 或者 optimize table 命令来修复，repair table table_name 修复表 optimize table table_name 优化表 repair table 用于修复被破坏的表。 optimize table 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了 optimize table 命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库）\n\n\n# 4.4. 数据结构\n\n> 问题现象：error 1071: specified key was too long; max key length is 767 bytes\n\n问题原因：mysql 默认情况下单个列的索引不能超过 767 位（不同版本可能存在差异） 。\n\n解决方法：优化索引结构，索引字段不宜过长。\n\n\n# 5. 脚本\n\n这里推荐我写的几个一键运维脚本，非常方便，欢迎使用：\n\n * mysql 安装脚本\n * mysql 备份脚本\n\n\n# 6. 参考资料\n\n * 《高性能 mysql》\n * https://www.cnblogs.com/xiaopotian/p/8196464.html\n * https://www.cnblogs.com/bigbrotherer/p/7241845.html\n * https://blog.csdn.net/managementandjava/article/details/80039650\n * http://www.manongjc.com/article/6996.html\n * https://www.cnblogs.com/xyabk/p/8967990.html\n * mysql 8.0 主从（master-slave）配置\n * mysql 主从同步实战\n * mysql 备份和恢复机制",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 配置",frontmatter:{title:"Mysql 配置",date:"2020-02-29T22:32:57.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","配置"],permalink:"/pages/5da42d/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/21.Mysql%E9%85%8D%E7%BD%AE.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/21.Mysql配置.md",key:"v-076936e6",path:"/pages/5da42d/",headers:[{level:2,title:"1. 基本配置",slug:"_1-基本配置",normalizedTitle:"1. 基本配置",charIndex:22},{level:2,title:"2. 配置项说明",slug:"_2-配置项说明",normalizedTitle:"2. 配置项说明",charIndex:1170},{level:2,title:"3. 参考资料",slug:"_3-参考资料",normalizedTitle:"3. 参考资料",charIndex:13215}],headersStr:"1. 基本配置 2. 配置项说明 3. 参考资料",content:"# Mysql 配置\n\n> 版本：\n\n\n# 1. 基本配置\n\n[mysqld]\n# GENERAL\n# -------------------------------------------------------------------------------\ndatadir = /var/lib/mysql\nsocket  = /var/lib/mysql/mysql.sock\npid_file = /var/lib/mysql/mysql.pid\nuser = mysql\nport = 3306\ndefault_storage_engine = InnoDB\ndefault_time_zone = '+8：00'\ncharacter_set_server = utf8mb4\ncollation_server = utf8mb4_0900_ai_ci\n\n# LOG\n# -------------------------------------------------------------------------------\nlog_error = /var/log/mysql/mysql-error.log\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/mysql-slow.log\n\n# InnoDB\n# -------------------------------------------------------------------------------\ninnodb_buffer_pool_size = <value>\ninnodb_log_file_size = <value>\ninnodb_file_per_table = 1\ninnodb_flush_method = O_DIRECT\n\n# MyIsam\n# -------------------------------------------------------------------------------\nkey_buffer_size = <value>\n\n# OTHER\n# -------------------------------------------------------------------------------\ntmp_table_size = 32M\nmax_heap_table_size = 32M\nmax_connections = <value>\nopen_files_limit = 65535\n\n[client]\nsocket  = /var/lib/mysql/mysql.sock\nport = 3306\n\n\n\n# 2. 配置项说明\n\n[client]\n# 服务端口号，默认 3306\nport = 3306\n\n# socket 文件\nsocket = /var/lib/mysql/mysql.sock\n\n\n\n[mysqld]\n\n# GENERAL\n# -------------------------------------------------------------------------------\n\n# socket 文件\nsocket = /var/lib/mysql/mysql.sock\n\n# PID 文件\npid_file = /var/lib/mysql/mysql.pid\n\n# 启动 mysql 服务进程的用户\nuser = mysql\n\n# 服务端口号，默认 3306\nport = 3306\n\n# 默认时区\ndefault_time_zone = '+8：00'\n\n# Mysql 服务 ID，单点服务时没必要设置\nserver-id = 1\n\n# 事务隔离级别，默认为可重复读（REPEATABLE-READ）。（此级别下可能参数很多间隙锁，影响性能，但是修改又影响主从复制及灾难恢复，建议还是修改代码逻辑吧）\n# 隔离级别可选项目：READ-UNCOMMITTED READ-COMMITTED REPEATABLE-READ SERIALIZABLE\ntransaction_isolation = REPEATABLE-READ\n\n# 目录配置\n# -------------------------------------------------------------------------------\n\n# mysql 安装根目录\nbasedir = /usr/local/mysql-5.7.21\n\n# mysql 数据文件所在目录\ndatadir = /var/lib/mysql\n\n# 临时目录 比如 load data infile 会用到，一般都是使用/tmp\ntmpdir = /tmp\n\n# 数据库引擎配置\n# -------------------------------------------------------------------------------\n\n# mysql 5.1 之后，默认引擎是 InnoDB\ndefault_storage_engine = InnoDB\n\n# 内存临时表默认引擎，默认 InnoDB\ndefault_tmp_storage_engine = InnoDB\n\n# mysql 5.7 新增特性，磁盘临时表默认引擎，默认 InnoDB\ninternal_tmp_disk_storage_engine = InnoDB\n\n# 字符集配置\n# -------------------------------------------------------------------------------\n\n# 数据库默认字符集，主流字符集支持一些特殊表情符号（特殊表情符占用 4 个字节）\ncharacter_set_server = utf8mb4\n\n# 数据库字符集对应一些排序等规则，注意要和 character_set_server 对应\ncollation-server = utf8mb4_0900_ai_ci\n\n# 设置 client 连接 mysql 时的字符集，防止乱码\n# init_connect='SET NAMES utf8'\n\n# 是否对 sql 语句大小写敏感，默认值为 0，1 表示不敏感\nlower_case_table_names = 1\n\n# 数据库连接配置\n# -------------------------------------------------------------------------------\n\n# 最大连接数，可设最大值 16384，一般考虑根据同时在线人数设置一个比较综合的数字，鉴于该数值增大并不太消耗系统资源，建议直接设 10000\n# 如果在访问时经常出现 Too Many Connections 的错误提示，则需要增大该参数值\nmax_connections = 10000\n\n# 默认值 100，最大错误连接数，如果有超出该参数值个数的中断错误连接，则该主机将被禁止连接。如需对该主机进行解禁，执行：FLUSH HOST\n# 考虑高并发场景下的容错，建议加大。\nmax_connect_errors = 10000\n\n# MySQL 打开的文件描述符限制，默认最小 1024;\n# 当 open_files_limit 没有被配置的时候，比较 max_connections\\*5 和 ulimit -n 的值，哪个大用哪个，\n# 当 open_file_limit 被配置的时候，比较 open_files_limit 和 max_connections\\*5 的值，哪个大用哪个。\n# 注意：仍然可能出现报错信息 Can't create a new thread；此时观察系统 cat /proc/mysql 进程号/limits，观察进程 ulimit 限制情况\n# 过小的话，考虑修改系统配置表，/etc/security/limits.conf 和 /etc/security/limits.d/90-nproc.conf\nopen_files_limit = 65535\n\n# 超时配置\n# -------------------------------------------------------------------------------\n\n# MySQL 默认的 wait_timeout 值为 8 个小时，interactive_timeout 参数需要同时配置才能生效\n# MySQL 连接闲置超过一定时间后(单位：秒，此处为 1800 秒)将会被强行关闭\ninteractive_timeout = 1800\nwait_timeout = 1800\n\n# 在 MySQL 暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中\n# 官方建议 back_log = 50 + (max_connections / 5)，封顶数为 900\nback_log = 900\n\n# 数据库数据交换配置\n# -------------------------------------------------------------------------------\n# 该参数限制服务器端，接受的数据包大小，如果有 BLOB 子段，建议增大此值，避免写入或者更新出错。有 BLOB 子段，建议改为 1024M\nmax_allowed_packet = 128M\n\n# 内存、cache 与 buffer 设置\n\n# 内存临时表的最大值，默认 16M，此处设置成 64M\ntmp_table_size = 64M\n\n# 用户创建的内存表的大小，默认 16M，往往和 tmp_table_size 一起设置，限制用户临时表大小。\n# 超限的话，MySQL 就会自动地把它转化为基于磁盘的 MyISAM 表，存储在指定的 tmpdir 目录下，增大 IO 压力，建议内存大，增大该数值。\nmax_heap_table_size = 64M\n\n# 表示这个 mysql 版本是否支持查询缓存。ps：SHOW STATUS LIKE 'qcache%'，与缓存相关的状态变量。\n# have_query_cache\n\n# 这个系统变量控制着查询缓存功能的开启和关闭，0 表示关闭，1 表示打开，2 表示只要 select 中明确指定 SQL_CACHE 才缓存。\n# 看业务场景决定是否使用缓存，不使用，下面就不用配置了。\n# Mysql8 不支持\nquery_cache_type = 0\n\n# 默认值 1M，优点是查询缓存可以极大的提高服务器速度，如果你有大量的相同的查询并且很少修改表。\n# 缺点：在你表经常变化的情况下或者如果你的查询原文每次都不同，查询缓存也许引起性能下降而不是性能提升。\n# Mysql8 不支持\nquery_cache_size = 64M\n\n# 只有小于此设定值的结果才会被缓冲，保护查询缓冲，防止一个极大的结果集将其他所有的查询结果都覆盖。\nquery_cache_limit = 2M\n\n# 每个被缓存的结果集要占用的最小内存，默认值 4kb，一般不怎么调整。\n# 如果 Qcache_free_blocks 值过大，可能是 query_cache_min_res_unit 值过大，应该调小些\n# query_cache_min_res_unit 的估计值：(query_cache_size - Qcache_free_memory) / Qcache_queries_in_cache\nquery_cache_min_res_unit = 4kb\n\n# 在一个事务中 binlog 为了记录 SQL 状态所持有的 cache 大小\n# 如果你经常使用大的、多声明的事务，你可以增加此值来获取更大的性能。\n# 所有从事务来的状态都将被缓冲在 binlog 缓冲中然后在提交后一次性写入到 binlog 中\n# 如果事务比此值大，会使用磁盘上的临时文件来替代。\n# 此缓冲在每个连接的事务第一次更新状态时被创建\nbinlog_cache_size = 1M\n\n# 日志配置\n# -------------------------------------------------------------------------------\n\n# 日志文件相关设置，一般只开启三种日志，错误日志，慢查询日志，二进制日志。普通查询日志不开启。\n# 普通查询日志，默认值 off，不开启\ngeneral_log = 0\n\n# 普通查询日志存放地址\ngeneral_log_file = /usr/local/mysql-5.7.21/log/mysql-general.log\n\n# 全局动态变量，默认 3，范围：1 ～ 3\n# 表示错误日志记录的信息，1：只记录 error 信息；2：记录 error 和 warnings 信息；3：记录 error、warnings 和普通的 notes 信息。\nlog_error_verbosity = 2\n\n# 错误日志文件地址\nlog_error = /usr/local/mysql-5.7.21/log/mysql-error.log\n\n# 开启慢查询\nslow_query_log = 1\n\n# 开启慢查询时间，此处为 1 秒，达到此值才记录数据\nlong_query_time = 3\n\n# 检索行数达到此数值，才记录慢查询日志中\nmin_examined_row_limit = 100\n\n# mysql 5.6.5 新增，用来表示每分钟允许记录到 slow log 的且未使用索引的 SQL 语句次数，默认值为 0，不限制。\nlog_throttle_queries_not_using_indexes = 0\n\n# 慢查询日志文件地址\nslow_query_log_file = /var/log/mysql/mysql-slow.log\n\n# 开启记录没有使用索引查询语句\nlog-queries-not-using-indexes = 1\n\n# 开启二进制日志\nlog_bin = /usr/local/mysql-5.7.21/log/mysql-bin.log\n\n# mysql 清除过期日志的时间，默认值 0，不自动清理，而是使用滚动循环的方式。\nexpire_logs_days = 0\n\n# 如果二进制日志写入的内容超出给定值，日志就会发生滚动。你不能将该变量设置为大于 1GB 或小于 4096 字节。 默认值是 1GB。\nmax_binlog_size = 1000M\n\n# binlog 的格式也有三种：STATEMENT，ROW，MIXED。mysql 5.7.7 后，默认值从 MIXED 改为 ROW\n# 关于 binlog 日志格式问题，请查阅网络资料\nbinlog_format = row\n\n# 默认值 N=1，使 binlog 在每 N 次 binlog 写入后与硬盘同步，ps：1 最慢\n# sync_binlog = 1\n\n# MyISAM 引擎配置\n# -------------------------------------------------------------------------------\n\n# 指定索引缓冲区的大小，为 MYISAM 数据表开启供线程共享的索引缓存，对 INNODB 引擎无效。相当影响 MyISAM 的性能。\n# 不要将其设置大于你可用内存的 30%，因为一部分内存同样被 OS 用来缓冲行数据\n# 甚至在你并不使用 MyISAM 表的情况下，你也需要仍旧设置起 8-64M 内存由于它同样会被内部临时磁盘表使用。\n# 默认值 8M，建议值：对于内存在 4GB 左右的服务器该参数可设置为 256M 或 384M。注意：该参数值设置的过大反而会是服务器整体效率降低！\nkey_buffer_size = 64M\n\n# 为每个扫描 MyISAM 的线程分配参数设置的内存大小缓冲区。\n# 默认值 128kb，建议值：16G 内存建议 1M，4G：128kb 或者 256kb 吧\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 128kb*连接数；极端情况 128kb*maxconnectiosns，会超级大，所以要考虑日常平均连接数。\n# 一般不需要太关心该数值，稍微增大就可以了，\nread_buffer_size = 262144\n\n# 支持任何存储引擎\n# MySQL 的随机读缓冲区大小，适当增大，可以提高性能。\n# 默认值 256kb；建议值：得参考连接数，16G 内存，有人推荐 8M\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 128kb*连接数；极端情况 128kb*maxconnectiosns，会超级大，所以要考虑日常平均连接数。\nread_rnd_buffer_size = 1M\n\n# order by 或 group by 时用到\n# 支持所有引擎，innodb 和 myisam 有自己的 innodb_sort_buffer_size 和 myisam_sort_buffer_size 设置\n# 默认值 256kb；建议值：得参考连接数，16G 内存，有人推荐 8M。\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 1M*连接数；极端情况 1M*maxconnectiosns，会超级大。所以要考虑日常平均连接数。\nsort_buffer_size = 1M\n\n# 此缓冲被使用来优化全联合(full JOINs 不带索引的联合)\n# 类似的联合在极大多数情况下有非常糟糕的性能表现，但是将此值设大能够减轻性能影响。\n# 通过 “Select_full_join” 状态变量查看全联合的数量\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 1M*连接数；极端情况 1M*maxconnectiosns，会超级大。所以要考虑日常平均连接数。\n# 默认值 256kb;建议值：16G 内存，设置 8M。\njoin_buffer_size = 1M\n\n# 缓存 linux 文件描述符信息，加快数据文件打开速度\n# 它影响 myisam 表的打开关闭，但是不影响 innodb 表的打开关闭。\n# 默认值 2000，建议值：根据状态变量 Opened_tables 去设定\ntable_open_cache = 2000\n\n# 缓存表定义的相关信息，加快读取表信息速度\n# 默认值 1400，最大值 2000，建议值：基本不改。\ntable_definition_cache = 1400\n\n# 该参数是 myssql 5.6 后引入的，目的是提高并发。\n# 默认值 1，建议值：cpu 核数，并且<=16\ntable_open_cache_instances = 2\n\n# 当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁。可重用，减小了系统开销。\n# 默认值为 9，建议值：两种取值方式，方式一，根据物理内存，1G —> 8；2G —> 16； 3G —> 32； >3G —> 64；\n# 方式二，根据 show status like 'threads%'，查看 Threads_connected 值。\nthread_cache_size = 16\n\n# 默认值 256k，建议值：16/32G 内存，512kb，其他一般不改变，如果报错：Thread stack overrun，就增大看看，\n# 注意，每个线程分配内存空间，所以总内存空间。。。你懂得。\nthread_stack = 512k\n\n# InnoDB 引擎配置\n# -------------------------------------------------------------------------------\n\n# 说明：该参数可以提升扩展性和刷脏页性能。\n# 默认值 1，建议值：4-8；并且必须小于 innodb_buffer_pool_instances\ninnodb_page_cleaners = 4\n\n# 说明：一般 8k 和 16k 中选择，8k 的话，cpu 消耗小些，selcet 效率高一点，一般不用改\n# 默认值：16k；建议值：不改，\ninnodb_page_size = 16384\n\n# 说明：InnoDB 使用一个缓冲池来保存索引和原始数据，不像 MyISAM。这里你设置越大，你在存取表里面数据时所需要的磁盘 I/O 越少。\n# 在一个独立使用的数据库服务器上，你可以设置这个变量到服务器物理内存大小的 60%-80%\n# 注意别设置的过大，会导致 system 的 swap 空间被占用，导致操作系统变慢，从而减低 sql 查询的效率\n# 默认值：128M，建议值：物理内存的 60%-80%\ninnodb_buffer_pool_size = 512M\n\n# 说明：只有当设置 innodb_buffer_pool_size 值大于 1G 时才有意义，小于 1G，instances 默认为 1，大于 1G，instances 默认为 8\n# 但是网络上有评价，最佳性能，每个实例至少 1G 大小。\n# 默认值：1 或 8，建议值：innodb_buffer_pool_size/innodb_buffer_pool_instances >= 1G\ninnodb_buffer_pool_instances = 1\n\n# 说明：mysql 5.7 新特性，defines the chunk size for online InnoDB buffer pool resizing operations。\n# 实际缓冲区大小必须为 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances*倍数，取略大于 innodb_buffer_pool_size\n# 默认值 128M，建议值：默认值就好，乱改反而容易出问题，它会影响实际 buffer pool 大小。\ninnodb_buffer_pool_chunk_size = 128M\n\n# 在启动时把热数据加载到内存。默认值为 on，不修改\ninnodb_buffer_pool_load_at_startup = 1\n\n# 在关闭时把热数据 dump 到本地磁盘。默认值为 on，不修改\ninnodb_buffer_pool_dump_at_shutdown = 1\n\n# 说明：影响 Innodb 缓冲区的刷新算法，建议从小到大配置，直到 zero free pages；innodb_lru_scan_depth \\* innodb_buffer_pool_instances defines the amount of work performed by the page cleaner thread each second。\n# 默认值 1024，建议值： 未知\ninnodb_lru_scan_depth = 1024\n\n# 说明：事务等待获取资源等待的最长时间，单位为秒，看具体业务情况，一般默认值就好\n# 默认值：50，建议值：看业务。\ninnodb_lock_wait_timeout = 60\n\n# 说明：设置了 Mysql 后台任务（例如页刷新和 merge dadta from buffer pool）每秒 io 操作的上限。\n# 默认值：200，建议值：方法一，单盘 sata 设 100，sas10，raid10 设 200，ssd 设 2000，fushion-io 设 50000；方法二，通过测试工具获得磁盘 io 性能后，设置 IOPS 数值/2。\ninnodb_io_capacity = 2000\n\n# 说明：该参数是所有缓冲区线程 io 操作的总上限。\n# 默认值：innodb_io_capacity 的两倍。建议值：例如用 iometer 测试后的 iops 数值就好\ninnodb_io_capacity_max = 4000\n\n# 说明：控制着 innodb 数据文件及 redo log 的打开、刷写模式，三种模式：fdatasync(默认)，O_DSYNC，O_DIRECT\n# fdatasync：数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->os buffer->磁盘；\n# O_DSYNC： 数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->磁盘；\n# O_DIRECT： 数据文件，buffer pool->磁盘； 日志文件，buffer pool->os buffer->磁盘；\n# 默认值为空，建议值：使用 SAN 或者 raid，建议用 O_DIRECT，不懂测试的话，默认生产上使用 O_DIRECT\ninnodb_flush_method = O_DIRECT\n\n# 说明：mysql5.7 之后默认开启，意思是，每张表一个独立表空间。\n# 默认值 1，开启\ninnodb_file_per_table = 1\n\n# 说明：The path where InnoDB creates undo tablespaces。通常等于 undo log 文件的存放目录。\n# 默认值 ./;自行设置\ninnodb_undo_directory = /usr/local/mysql-5.7.21/log\n\n# 说明：The number of undo tablespaces used by InnoDB 等于 undo log 文件数量。5.7.21 后开始弃用\n# 默认值为 0，建议默认值就好，不用调整了。\ninnodb_undo_tablespaces = 0\n\n# 说明：定义 undo 使用的回滚段数量。5.7.19 后弃用\n# 默认值 128，建议不动，以后弃用了。\ninnodb_undo_logs = 128\n\n# 说明：5.7.5 后开始使用，在线收缩 undo log 使用的空间。\n# 默认值：关闭，建议值：开启\ninnodb_undo_log_truncate = 1\n\n# 说明：结合 innodb_undo_log_truncate，实现 undo 空间收缩功能\n# 默认值：1G，建议值，不改。\ninnodb_max_undo_log_size = 1G\n\n# 说明：重作日志文件的存放目录\ninnodb_log_group_home_dir = /usr/local/mysql-5.7.21/log\n\n# 说明：日志文件的大小\n# 默认值：48M，建议值：根据你系统的磁盘空间和日志增长情况调整大小\ninnodb_log_file_size = 128M\n\n# 说明：日志组中的文件数量，mysql 以循环方式写入日志\n# 默认值 2，建议值：根据你系统的磁盘空间和日志增长情况调整大小\ninnodb_log_files_in_group = 3\n\n# 此参数确定些日志文件所用的内存大小，以 M 为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL 开发人员建议设置为 1－8M 之间\ninnodb_log_buffer_size = 16M\n\n# 说明：可以控制 log 从系统 buffer 刷入磁盘文件的刷新频率，增大可减轻系统负荷\n# 默认值是 1；建议值不改。系统性能一般够用。\ninnodb_flush_log_at_timeout = 1\n\n# 说明：参数可设为 0，1，2；\n# 参数 0：表示每秒将 log buffer 内容刷新到系统 buffer 中，再调用系统 flush 操作写入磁盘文件。\n# 参数 1：表示每次事物提交，将 log buffer 内容刷新到系统 buffer 中，再调用系统 flush 操作写入磁盘文件。\n# 参数 2：表示每次事物提交，将 log buffer 内容刷新到系统 buffer 中，隔 1 秒后再调用系统 flush 操作写入磁盘文件。\ninnodb_flush_log_at_trx_commit = 1\n\n# 说明：限制 Innodb 能打开的表的数据，如果库里的表特别多的情况，请增加这个。\n# 值默认是 2000，建议值：参考数据库表总数再进行调整，一般够用不用调整。\ninnodb_open_files = 8192\n\n# innodb 处理 io 读写的后台并发线程数量，根据 cpu 核来确认，取值范围：1-64\n# 默认值：4，建议值：与逻辑 cpu 数量的一半保持一致。\ninnodb_read_io_threads = 4\ninnodb_write_io_threads = 4\n\n# 默认设置为 0，表示不限制并发数，这里推荐设置为 0，更好去发挥 CPU 多核处理能力，提高并发量\ninnodb_thread_concurrency = 0\n\n# 默认值为 4，建议不变。InnoDB 中的清除操作是一类定期回收无用数据的操作。mysql 5.5 之后，支持多线程清除操作。\ninnodb_purge_threads = 4\n\n# 说明：mysql 缓冲区分为 new blocks 和 old blocks；此参数表示 old blocks 占比；\n# 默认值：37，建议值，一般不动\ninnodb_old_blocks_pct = 37\n\n# 说明：新数据被载入缓冲池，进入 old pages 链区，当 1 秒后再次访问，则提升进入 new pages 链区。\n# 默认值：1000\ninnodb_old_blocks_time=1000\n\n# 说明：开启异步 io，可以提高并发性，默认开启。\n# 默认值为 1，建议不动\ninnodb_use_native_aio = 1\n\n# 说明：默认为空，使用 data 目录，一般不改。\ninnodb_data_home_dir=/usr/local/mysql-5.7.21/data\n\n# 说明：Defines the name，size，and attributes of InnoDB system tablespace data files。\n# 默认值，不指定，默认为 ibdata1：12M：autoextend\ninnodb_data_file_path = ibdata1：12M：autoextend\n\n# 说明：设置了 InnoDB 存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，除非你的数据对象及其多，否则一般默认不改。\n# innodb_additional_mem_pool_size = 16M\n# 说明：The crash recovery mode。只有紧急情况需要恢复数据的时候，才改为大于 1-6 之间数值，含义查下官网。\n# 默认值为 0；\n#innodb_force_recovery = 0\n\n\n\n[mysqldump]\n\n# quick 选项强制 mysqldump 从服务器查询取得记录直接输出而不是取得所有记录后将它们缓存到内存中\nquick\n\nmax_allowed_packet = 16M\n\n\n\n[mysql]\n\n# mysql 命令行工具不使用自动补全功能，建议还是改为\n# no-auto-rehash\nauto-rehash\n\n# socket 文件\nsocket = /var/lib/mysql/mysql.sock\n\n\n\n# 3. 参考资料\n\n * 《高性能 MySQL》\n * Mysql 配置文件/etc/my.cnf 解析",normalizedContent:"# mysql 配置\n\n> 版本：\n\n\n# 1. 基本配置\n\n[mysqld]\n# general\n# -------------------------------------------------------------------------------\ndatadir = /var/lib/mysql\nsocket  = /var/lib/mysql/mysql.sock\npid_file = /var/lib/mysql/mysql.pid\nuser = mysql\nport = 3306\ndefault_storage_engine = innodb\ndefault_time_zone = '+8：00'\ncharacter_set_server = utf8mb4\ncollation_server = utf8mb4_0900_ai_ci\n\n# log\n# -------------------------------------------------------------------------------\nlog_error = /var/log/mysql/mysql-error.log\nslow_query_log = 1\nslow_query_log_file = /var/log/mysql/mysql-slow.log\n\n# innodb\n# -------------------------------------------------------------------------------\ninnodb_buffer_pool_size = <value>\ninnodb_log_file_size = <value>\ninnodb_file_per_table = 1\ninnodb_flush_method = o_direct\n\n# myisam\n# -------------------------------------------------------------------------------\nkey_buffer_size = <value>\n\n# other\n# -------------------------------------------------------------------------------\ntmp_table_size = 32m\nmax_heap_table_size = 32m\nmax_connections = <value>\nopen_files_limit = 65535\n\n[client]\nsocket  = /var/lib/mysql/mysql.sock\nport = 3306\n\n\n\n# 2. 配置项说明\n\n[client]\n# 服务端口号，默认 3306\nport = 3306\n\n# socket 文件\nsocket = /var/lib/mysql/mysql.sock\n\n\n\n[mysqld]\n\n# general\n# -------------------------------------------------------------------------------\n\n# socket 文件\nsocket = /var/lib/mysql/mysql.sock\n\n# pid 文件\npid_file = /var/lib/mysql/mysql.pid\n\n# 启动 mysql 服务进程的用户\nuser = mysql\n\n# 服务端口号，默认 3306\nport = 3306\n\n# 默认时区\ndefault_time_zone = '+8：00'\n\n# mysql 服务 id，单点服务时没必要设置\nserver-id = 1\n\n# 事务隔离级别，默认为可重复读（repeatable-read）。（此级别下可能参数很多间隙锁，影响性能，但是修改又影响主从复制及灾难恢复，建议还是修改代码逻辑吧）\n# 隔离级别可选项目：read-uncommitted read-committed repeatable-read serializable\ntransaction_isolation = repeatable-read\n\n# 目录配置\n# -------------------------------------------------------------------------------\n\n# mysql 安装根目录\nbasedir = /usr/local/mysql-5.7.21\n\n# mysql 数据文件所在目录\ndatadir = /var/lib/mysql\n\n# 临时目录 比如 load data infile 会用到，一般都是使用/tmp\ntmpdir = /tmp\n\n# 数据库引擎配置\n# -------------------------------------------------------------------------------\n\n# mysql 5.1 之后，默认引擎是 innodb\ndefault_storage_engine = innodb\n\n# 内存临时表默认引擎，默认 innodb\ndefault_tmp_storage_engine = innodb\n\n# mysql 5.7 新增特性，磁盘临时表默认引擎，默认 innodb\ninternal_tmp_disk_storage_engine = innodb\n\n# 字符集配置\n# -------------------------------------------------------------------------------\n\n# 数据库默认字符集，主流字符集支持一些特殊表情符号（特殊表情符占用 4 个字节）\ncharacter_set_server = utf8mb4\n\n# 数据库字符集对应一些排序等规则，注意要和 character_set_server 对应\ncollation-server = utf8mb4_0900_ai_ci\n\n# 设置 client 连接 mysql 时的字符集，防止乱码\n# init_connect='set names utf8'\n\n# 是否对 sql 语句大小写敏感，默认值为 0，1 表示不敏感\nlower_case_table_names = 1\n\n# 数据库连接配置\n# -------------------------------------------------------------------------------\n\n# 最大连接数，可设最大值 16384，一般考虑根据同时在线人数设置一个比较综合的数字，鉴于该数值增大并不太消耗系统资源，建议直接设 10000\n# 如果在访问时经常出现 too many connections 的错误提示，则需要增大该参数值\nmax_connections = 10000\n\n# 默认值 100，最大错误连接数，如果有超出该参数值个数的中断错误连接，则该主机将被禁止连接。如需对该主机进行解禁，执行：flush host\n# 考虑高并发场景下的容错，建议加大。\nmax_connect_errors = 10000\n\n# mysql 打开的文件描述符限制，默认最小 1024;\n# 当 open_files_limit 没有被配置的时候，比较 max_connections\\*5 和 ulimit -n 的值，哪个大用哪个，\n# 当 open_file_limit 被配置的时候，比较 open_files_limit 和 max_connections\\*5 的值，哪个大用哪个。\n# 注意：仍然可能出现报错信息 can't create a new thread；此时观察系统 cat /proc/mysql 进程号/limits，观察进程 ulimit 限制情况\n# 过小的话，考虑修改系统配置表，/etc/security/limits.conf 和 /etc/security/limits.d/90-nproc.conf\nopen_files_limit = 65535\n\n# 超时配置\n# -------------------------------------------------------------------------------\n\n# mysql 默认的 wait_timeout 值为 8 个小时，interactive_timeout 参数需要同时配置才能生效\n# mysql 连接闲置超过一定时间后(单位：秒，此处为 1800 秒)将会被强行关闭\ninteractive_timeout = 1800\nwait_timeout = 1800\n\n# 在 mysql 暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中\n# 官方建议 back_log = 50 + (max_connections / 5)，封顶数为 900\nback_log = 900\n\n# 数据库数据交换配置\n# -------------------------------------------------------------------------------\n# 该参数限制服务器端，接受的数据包大小，如果有 blob 子段，建议增大此值，避免写入或者更新出错。有 blob 子段，建议改为 1024m\nmax_allowed_packet = 128m\n\n# 内存、cache 与 buffer 设置\n\n# 内存临时表的最大值，默认 16m，此处设置成 64m\ntmp_table_size = 64m\n\n# 用户创建的内存表的大小，默认 16m，往往和 tmp_table_size 一起设置，限制用户临时表大小。\n# 超限的话，mysql 就会自动地把它转化为基于磁盘的 myisam 表，存储在指定的 tmpdir 目录下，增大 io 压力，建议内存大，增大该数值。\nmax_heap_table_size = 64m\n\n# 表示这个 mysql 版本是否支持查询缓存。ps：show status like 'qcache%'，与缓存相关的状态变量。\n# have_query_cache\n\n# 这个系统变量控制着查询缓存功能的开启和关闭，0 表示关闭，1 表示打开，2 表示只要 select 中明确指定 sql_cache 才缓存。\n# 看业务场景决定是否使用缓存，不使用，下面就不用配置了。\n# mysql8 不支持\nquery_cache_type = 0\n\n# 默认值 1m，优点是查询缓存可以极大的提高服务器速度，如果你有大量的相同的查询并且很少修改表。\n# 缺点：在你表经常变化的情况下或者如果你的查询原文每次都不同，查询缓存也许引起性能下降而不是性能提升。\n# mysql8 不支持\nquery_cache_size = 64m\n\n# 只有小于此设定值的结果才会被缓冲，保护查询缓冲，防止一个极大的结果集将其他所有的查询结果都覆盖。\nquery_cache_limit = 2m\n\n# 每个被缓存的结果集要占用的最小内存，默认值 4kb，一般不怎么调整。\n# 如果 qcache_free_blocks 值过大，可能是 query_cache_min_res_unit 值过大，应该调小些\n# query_cache_min_res_unit 的估计值：(query_cache_size - qcache_free_memory) / qcache_queries_in_cache\nquery_cache_min_res_unit = 4kb\n\n# 在一个事务中 binlog 为了记录 sql 状态所持有的 cache 大小\n# 如果你经常使用大的、多声明的事务，你可以增加此值来获取更大的性能。\n# 所有从事务来的状态都将被缓冲在 binlog 缓冲中然后在提交后一次性写入到 binlog 中\n# 如果事务比此值大，会使用磁盘上的临时文件来替代。\n# 此缓冲在每个连接的事务第一次更新状态时被创建\nbinlog_cache_size = 1m\n\n# 日志配置\n# -------------------------------------------------------------------------------\n\n# 日志文件相关设置，一般只开启三种日志，错误日志，慢查询日志，二进制日志。普通查询日志不开启。\n# 普通查询日志，默认值 off，不开启\ngeneral_log = 0\n\n# 普通查询日志存放地址\ngeneral_log_file = /usr/local/mysql-5.7.21/log/mysql-general.log\n\n# 全局动态变量，默认 3，范围：1 ～ 3\n# 表示错误日志记录的信息，1：只记录 error 信息；2：记录 error 和 warnings 信息；3：记录 error、warnings 和普通的 notes 信息。\nlog_error_verbosity = 2\n\n# 错误日志文件地址\nlog_error = /usr/local/mysql-5.7.21/log/mysql-error.log\n\n# 开启慢查询\nslow_query_log = 1\n\n# 开启慢查询时间，此处为 1 秒，达到此值才记录数据\nlong_query_time = 3\n\n# 检索行数达到此数值，才记录慢查询日志中\nmin_examined_row_limit = 100\n\n# mysql 5.6.5 新增，用来表示每分钟允许记录到 slow log 的且未使用索引的 sql 语句次数，默认值为 0，不限制。\nlog_throttle_queries_not_using_indexes = 0\n\n# 慢查询日志文件地址\nslow_query_log_file = /var/log/mysql/mysql-slow.log\n\n# 开启记录没有使用索引查询语句\nlog-queries-not-using-indexes = 1\n\n# 开启二进制日志\nlog_bin = /usr/local/mysql-5.7.21/log/mysql-bin.log\n\n# mysql 清除过期日志的时间，默认值 0，不自动清理，而是使用滚动循环的方式。\nexpire_logs_days = 0\n\n# 如果二进制日志写入的内容超出给定值，日志就会发生滚动。你不能将该变量设置为大于 1gb 或小于 4096 字节。 默认值是 1gb。\nmax_binlog_size = 1000m\n\n# binlog 的格式也有三种：statement，row，mixed。mysql 5.7.7 后，默认值从 mixed 改为 row\n# 关于 binlog 日志格式问题，请查阅网络资料\nbinlog_format = row\n\n# 默认值 n=1，使 binlog 在每 n 次 binlog 写入后与硬盘同步，ps：1 最慢\n# sync_binlog = 1\n\n# myisam 引擎配置\n# -------------------------------------------------------------------------------\n\n# 指定索引缓冲区的大小，为 myisam 数据表开启供线程共享的索引缓存，对 innodb 引擎无效。相当影响 myisam 的性能。\n# 不要将其设置大于你可用内存的 30%，因为一部分内存同样被 os 用来缓冲行数据\n# 甚至在你并不使用 myisam 表的情况下，你也需要仍旧设置起 8-64m 内存由于它同样会被内部临时磁盘表使用。\n# 默认值 8m，建议值：对于内存在 4gb 左右的服务器该参数可设置为 256m 或 384m。注意：该参数值设置的过大反而会是服务器整体效率降低！\nkey_buffer_size = 64m\n\n# 为每个扫描 myisam 的线程分配参数设置的内存大小缓冲区。\n# 默认值 128kb，建议值：16g 内存建议 1m，4g：128kb 或者 256kb 吧\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 128kb*连接数；极端情况 128kb*maxconnectiosns，会超级大，所以要考虑日常平均连接数。\n# 一般不需要太关心该数值，稍微增大就可以了，\nread_buffer_size = 262144\n\n# 支持任何存储引擎\n# mysql 的随机读缓冲区大小，适当增大，可以提高性能。\n# 默认值 256kb；建议值：得参考连接数，16g 内存，有人推荐 8m\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 128kb*连接数；极端情况 128kb*maxconnectiosns，会超级大，所以要考虑日常平均连接数。\nread_rnd_buffer_size = 1m\n\n# order by 或 group by 时用到\n# 支持所有引擎，innodb 和 myisam 有自己的 innodb_sort_buffer_size 和 myisam_sort_buffer_size 设置\n# 默认值 256kb；建议值：得参考连接数，16g 内存，有人推荐 8m。\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 1m*连接数；极端情况 1m*maxconnectiosns，会超级大。所以要考虑日常平均连接数。\nsort_buffer_size = 1m\n\n# 此缓冲被使用来优化全联合(full joins 不带索引的联合)\n# 类似的联合在极大多数情况下有非常糟糕的性能表现，但是将此值设大能够减轻性能影响。\n# 通过 “select_full_join” 状态变量查看全联合的数量\n# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 1m*连接数；极端情况 1m*maxconnectiosns，会超级大。所以要考虑日常平均连接数。\n# 默认值 256kb;建议值：16g 内存，设置 8m。\njoin_buffer_size = 1m\n\n# 缓存 linux 文件描述符信息，加快数据文件打开速度\n# 它影响 myisam 表的打开关闭，但是不影响 innodb 表的打开关闭。\n# 默认值 2000，建议值：根据状态变量 opened_tables 去设定\ntable_open_cache = 2000\n\n# 缓存表定义的相关信息，加快读取表信息速度\n# 默认值 1400，最大值 2000，建议值：基本不改。\ntable_definition_cache = 1400\n\n# 该参数是 myssql 5.6 后引入的，目的是提高并发。\n# 默认值 1，建议值：cpu 核数，并且<=16\ntable_open_cache_instances = 2\n\n# 当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁。可重用，减小了系统开销。\n# 默认值为 9，建议值：两种取值方式，方式一，根据物理内存，1g —> 8；2g —> 16； 3g —> 32； >3g —> 64；\n# 方式二，根据 show status like 'threads%'，查看 threads_connected 值。\nthread_cache_size = 16\n\n# 默认值 256k，建议值：16/32g 内存，512kb，其他一般不改变，如果报错：thread stack overrun，就增大看看，\n# 注意，每个线程分配内存空间，所以总内存空间。。。你懂得。\nthread_stack = 512k\n\n# innodb 引擎配置\n# -------------------------------------------------------------------------------\n\n# 说明：该参数可以提升扩展性和刷脏页性能。\n# 默认值 1，建议值：4-8；并且必须小于 innodb_buffer_pool_instances\ninnodb_page_cleaners = 4\n\n# 说明：一般 8k 和 16k 中选择，8k 的话，cpu 消耗小些，selcet 效率高一点，一般不用改\n# 默认值：16k；建议值：不改，\ninnodb_page_size = 16384\n\n# 说明：innodb 使用一个缓冲池来保存索引和原始数据，不像 myisam。这里你设置越大，你在存取表里面数据时所需要的磁盘 i/o 越少。\n# 在一个独立使用的数据库服务器上，你可以设置这个变量到服务器物理内存大小的 60%-80%\n# 注意别设置的过大，会导致 system 的 swap 空间被占用，导致操作系统变慢，从而减低 sql 查询的效率\n# 默认值：128m，建议值：物理内存的 60%-80%\ninnodb_buffer_pool_size = 512m\n\n# 说明：只有当设置 innodb_buffer_pool_size 值大于 1g 时才有意义，小于 1g，instances 默认为 1，大于 1g，instances 默认为 8\n# 但是网络上有评价，最佳性能，每个实例至少 1g 大小。\n# 默认值：1 或 8，建议值：innodb_buffer_pool_size/innodb_buffer_pool_instances >= 1g\ninnodb_buffer_pool_instances = 1\n\n# 说明：mysql 5.7 新特性，defines the chunk size for online innodb buffer pool resizing operations。\n# 实际缓冲区大小必须为 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances*倍数，取略大于 innodb_buffer_pool_size\n# 默认值 128m，建议值：默认值就好，乱改反而容易出问题，它会影响实际 buffer pool 大小。\ninnodb_buffer_pool_chunk_size = 128m\n\n# 在启动时把热数据加载到内存。默认值为 on，不修改\ninnodb_buffer_pool_load_at_startup = 1\n\n# 在关闭时把热数据 dump 到本地磁盘。默认值为 on，不修改\ninnodb_buffer_pool_dump_at_shutdown = 1\n\n# 说明：影响 innodb 缓冲区的刷新算法，建议从小到大配置，直到 zero free pages；innodb_lru_scan_depth \\* innodb_buffer_pool_instances defines the amount of work performed by the page cleaner thread each second。\n# 默认值 1024，建议值： 未知\ninnodb_lru_scan_depth = 1024\n\n# 说明：事务等待获取资源等待的最长时间，单位为秒，看具体业务情况，一般默认值就好\n# 默认值：50，建议值：看业务。\ninnodb_lock_wait_timeout = 60\n\n# 说明：设置了 mysql 后台任务（例如页刷新和 merge dadta from buffer pool）每秒 io 操作的上限。\n# 默认值：200，建议值：方法一，单盘 sata 设 100，sas10，raid10 设 200，ssd 设 2000，fushion-io 设 50000；方法二，通过测试工具获得磁盘 io 性能后，设置 iops 数值/2。\ninnodb_io_capacity = 2000\n\n# 说明：该参数是所有缓冲区线程 io 操作的总上限。\n# 默认值：innodb_io_capacity 的两倍。建议值：例如用 iometer 测试后的 iops 数值就好\ninnodb_io_capacity_max = 4000\n\n# 说明：控制着 innodb 数据文件及 redo log 的打开、刷写模式，三种模式：fdatasync(默认)，o_dsync，o_direct\n# fdatasync：数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->os buffer->磁盘；\n# o_dsync： 数据文件，buffer pool->os buffer->磁盘；日志文件，buffer pool->磁盘；\n# o_direct： 数据文件，buffer pool->磁盘； 日志文件，buffer pool->os buffer->磁盘；\n# 默认值为空，建议值：使用 san 或者 raid，建议用 o_direct，不懂测试的话，默认生产上使用 o_direct\ninnodb_flush_method = o_direct\n\n# 说明：mysql5.7 之后默认开启，意思是，每张表一个独立表空间。\n# 默认值 1，开启\ninnodb_file_per_table = 1\n\n# 说明：the path where innodb creates undo tablespaces。通常等于 undo log 文件的存放目录。\n# 默认值 ./;自行设置\ninnodb_undo_directory = /usr/local/mysql-5.7.21/log\n\n# 说明：the number of undo tablespaces used by innodb 等于 undo log 文件数量。5.7.21 后开始弃用\n# 默认值为 0，建议默认值就好，不用调整了。\ninnodb_undo_tablespaces = 0\n\n# 说明：定义 undo 使用的回滚段数量。5.7.19 后弃用\n# 默认值 128，建议不动，以后弃用了。\ninnodb_undo_logs = 128\n\n# 说明：5.7.5 后开始使用，在线收缩 undo log 使用的空间。\n# 默认值：关闭，建议值：开启\ninnodb_undo_log_truncate = 1\n\n# 说明：结合 innodb_undo_log_truncate，实现 undo 空间收缩功能\n# 默认值：1g，建议值，不改。\ninnodb_max_undo_log_size = 1g\n\n# 说明：重作日志文件的存放目录\ninnodb_log_group_home_dir = /usr/local/mysql-5.7.21/log\n\n# 说明：日志文件的大小\n# 默认值：48m，建议值：根据你系统的磁盘空间和日志增长情况调整大小\ninnodb_log_file_size = 128m\n\n# 说明：日志组中的文件数量，mysql 以循环方式写入日志\n# 默认值 2，建议值：根据你系统的磁盘空间和日志增长情况调整大小\ninnodb_log_files_in_group = 3\n\n# 此参数确定些日志文件所用的内存大小，以 m 为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。mysql 开发人员建议设置为 1－8m 之间\ninnodb_log_buffer_size = 16m\n\n# 说明：可以控制 log 从系统 buffer 刷入磁盘文件的刷新频率，增大可减轻系统负荷\n# 默认值是 1；建议值不改。系统性能一般够用。\ninnodb_flush_log_at_timeout = 1\n\n# 说明：参数可设为 0，1，2；\n# 参数 0：表示每秒将 log buffer 内容刷新到系统 buffer 中，再调用系统 flush 操作写入磁盘文件。\n# 参数 1：表示每次事物提交，将 log buffer 内容刷新到系统 buffer 中，再调用系统 flush 操作写入磁盘文件。\n# 参数 2：表示每次事物提交，将 log buffer 内容刷新到系统 buffer 中，隔 1 秒后再调用系统 flush 操作写入磁盘文件。\ninnodb_flush_log_at_trx_commit = 1\n\n# 说明：限制 innodb 能打开的表的数据，如果库里的表特别多的情况，请增加这个。\n# 值默认是 2000，建议值：参考数据库表总数再进行调整，一般够用不用调整。\ninnodb_open_files = 8192\n\n# innodb 处理 io 读写的后台并发线程数量，根据 cpu 核来确认，取值范围：1-64\n# 默认值：4，建议值：与逻辑 cpu 数量的一半保持一致。\ninnodb_read_io_threads = 4\ninnodb_write_io_threads = 4\n\n# 默认设置为 0，表示不限制并发数，这里推荐设置为 0，更好去发挥 cpu 多核处理能力，提高并发量\ninnodb_thread_concurrency = 0\n\n# 默认值为 4，建议不变。innodb 中的清除操作是一类定期回收无用数据的操作。mysql 5.5 之后，支持多线程清除操作。\ninnodb_purge_threads = 4\n\n# 说明：mysql 缓冲区分为 new blocks 和 old blocks；此参数表示 old blocks 占比；\n# 默认值：37，建议值，一般不动\ninnodb_old_blocks_pct = 37\n\n# 说明：新数据被载入缓冲池，进入 old pages 链区，当 1 秒后再次访问，则提升进入 new pages 链区。\n# 默认值：1000\ninnodb_old_blocks_time=1000\n\n# 说明：开启异步 io，可以提高并发性，默认开启。\n# 默认值为 1，建议不动\ninnodb_use_native_aio = 1\n\n# 说明：默认为空，使用 data 目录，一般不改。\ninnodb_data_home_dir=/usr/local/mysql-5.7.21/data\n\n# 说明：defines the name，size，and attributes of innodb system tablespace data files。\n# 默认值，不指定，默认为 ibdata1：12m：autoextend\ninnodb_data_file_path = ibdata1：12m：autoextend\n\n# 说明：设置了 innodb 存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，除非你的数据对象及其多，否则一般默认不改。\n# innodb_additional_mem_pool_size = 16m\n# 说明：the crash recovery mode。只有紧急情况需要恢复数据的时候，才改为大于 1-6 之间数值，含义查下官网。\n# 默认值为 0；\n#innodb_force_recovery = 0\n\n\n\n[mysqldump]\n\n# quick 选项强制 mysqldump 从服务器查询取得记录直接输出而不是取得所有记录后将它们缓存到内存中\nquick\n\nmax_allowed_packet = 16m\n\n\n\n[mysql]\n\n# mysql 命令行工具不使用自动补全功能，建议还是改为\n# no-auto-rehash\nauto-rehash\n\n# socket 文件\nsocket = /var/lib/mysql/mysql.sock\n\n\n\n# 3. 参考资料\n\n * 《高性能 mysql》\n * mysql 配置文件/etc/my.cnf 解析",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 常见问题",frontmatter:{title:"Mysql 常见问题",date:"2020-09-12T10:43:53.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql","FAQ"],permalink:"/pages/7b0caf/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/99.Mysql%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98.html",relativePath:"12.数据库/03.关系型数据库/02.Mysql/99.Mysql常见问题.md",key:"v-4954d596",path:"/pages/7b0caf/",headers:[{level:2,title:"1. 为什么表数据删掉一半，表文件大小不变",slug:"_1-为什么表数据删掉一半-表文件大小不变",normalizedTitle:"1. 为什么表数据删掉一半，表文件大小不变",charIndex:48},{level:2,title:"2. 参考资料",slug:"_2-参考资料",normalizedTitle:"2. 参考资料",charIndex:747}],headersStr:"1. 为什么表数据删掉一半，表文件大小不变 2. 参考资料",content:"# Mysql 常见问题\n\n> 📦 本文以及示例源码已归档在 db-tutorial\n\n\n# 1. 为什么表数据删掉一半，表文件大小不变\n\n【问题】数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？\n\n表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：\n\n 1. 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；\n 2. 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。\n\n从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。\n\n我建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。\n\n所以，将 innodb_file_per_table 设置为 ON，是推荐做法，我们接下来的讨论都是基于这个设置展开的。\n\n我们在删除整个表的时候，可以使用 drop table 命令回收表空间。但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。\n\n插入和删除操作可能会造成空洞。\n\n * 插入时，如果插入位置所在页已满，需要申请新页面。\n * 删除时，不会删除所在页，而是将记录在页面的位置标记为可重用。\n\n所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。\n\n要达到收缩空洞的目的，可以使用重建表的方式。\n\n\n# 2. 参考资料\n\n * 《高性能 MySQL》\n * MySQL 实战 45 讲",normalizedContent:"# mysql 常见问题\n\n> 📦 本文以及示例源码已归档在 db-tutorial\n\n\n# 1. 为什么表数据删掉一半，表文件大小不变\n\n【问题】数据库占用空间太大，我把一个最大的表删掉了一半的数据，怎么表文件的大小还是没变？\n\n表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：\n\n 1. 这个参数设置为 off 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；\n 2. 这个参数设置为 on 表示的是，每个 innodb 表数据存储在一个以 .ibd 为后缀的文件中。\n\n从 mysql 5.6.6 版本开始，它的默认值就是 on 了。\n\n我建议你不论使用 mysql 的哪个版本，都将这个值设置为 on。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。\n\n所以，将 innodb_file_per_table 设置为 on，是推荐做法，我们接下来的讨论都是基于这个设置展开的。\n\n我们在删除整个表的时候，可以使用 drop table 命令回收表空间。但是，我们遇到的更多的删除数据的场景是删除某些行，这时就遇到了我们文章开头的问题：表中的数据被删除了，但是表空间却没有被回收。\n\n插入和删除操作可能会造成空洞。\n\n * 插入时，如果插入位置所在页已满，需要申请新页面。\n * 删除时，不会删除所在页，而是将记录在页面的位置标记为可重用。\n\n所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。\n\n要达到收缩空洞的目的，可以使用重建表的方式。\n\n\n# 2. 参考资料\n\n * 《高性能 mysql》\n * mysql 实战 45 讲",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Mysql 教程",frontmatter:{title:"Mysql 教程",date:"2020-02-10T14:27:39.000Z",categories:["数据库","关系型数据库","Mysql"],tags:["数据库","关系型数据库","Mysql"],permalink:"/pages/a5b63b/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/02.Mysql/",relativePath:"12.数据库/03.关系型数据库/02.Mysql/README.md",key:"v-74c83129",path:"/pages/a5b63b/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:17},{level:3,title:"Mysql 应用指南",slug:"mysql-应用指南",normalizedTitle:"mysql 应用指南",charIndex:27},{level:3,title:"Mysql 工作流",slug:"mysql-工作流",normalizedTitle:"mysql 工作流",charIndex:42},{level:3,title:"Mysql 事务",slug:"mysql-事务",normalizedTitle:"mysql 事务",charIndex:56},{level:3,title:"Mysql 锁",slug:"mysql-锁",normalizedTitle:"mysql 锁",charIndex:110},{level:3,title:"Mysql 索引",slug:"mysql-索引",normalizedTitle:"mysql 索引",charIndex:163},{level:3,title:"Mysql 性能优化",slug:"mysql-性能优化",normalizedTitle:"mysql 性能优化",charIndex:202},{level:3,title:"Mysql 运维 🔨",slug:"mysql-运维-🔨",normalizedTitle:"mysql 运维 🔨",charIndex:217},{level:3,title:"Mysql 配置 🔨",slug:"mysql-配置-🔨",normalizedTitle:"mysql 配置 🔨",charIndex:233},{level:3,title:"Mysql 常见问题",slug:"mysql-常见问题",normalizedTitle:"mysql 常见问题",charIndex:249},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:365},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:605}],headersStr:"📖 内容 Mysql 应用指南 Mysql 工作流 Mysql 事务 Mysql 锁 Mysql 索引 Mysql 性能优化 Mysql 运维 🔨 Mysql 配置 🔨 Mysql 常见问题 📚 资料 🚪 传送",content:"# Mysql 教程\n\n\n\n\n# 📖 内容\n\n\n# Mysql 应用指南\n\n\n# Mysql 工作流\n\n\n# Mysql 事务\n\n> 关键词：ACID、AUTOCOMMIT、事务隔离级别、死锁、分布式事务\n\n\n\n\n# Mysql 锁\n\n> 关键词：乐观锁、表级锁、行级锁、意向锁、MVCC、Next-key 锁\n\n\n\n\n# Mysql 索引\n\n> 关键词：Hash、B 树、聚簇索引、回表\n\n\n\n\n# Mysql 性能优化\n\n\n# Mysql 运维 🔨\n\n\n# Mysql 配置 🔨\n\n\n# Mysql 常见问题\n\n----------------------------------------\n\n相关扩展知识：\n\n * 关系型数据库面试总结 💯\n * SQL Cheat Sheet\n * 分布式事务基本原理\n\n\n# 📚 资料\n\n * 官方\n   * Mysql 官网\n   * Mysql 官方文档\n   * Mysql 官方文档之命令行客户端\n * 书籍\n   * 《高性能 MySQL》 - 经典，适合 DBA 或作为开发者的参考手册\n   * 《MySQL 必知必会》 - 适合入门者\n * 教程\n   * MySQL 实战 45 讲\n   * runoob.com MySQL 教程\n   * mysql-tutorial\n * 更多资源\n   * awesome-mysql\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# mysql 教程\n\n\n\n\n# 📖 内容\n\n\n# mysql 应用指南\n\n\n# mysql 工作流\n\n\n# mysql 事务\n\n> 关键词：acid、autocommit、事务隔离级别、死锁、分布式事务\n\n\n\n\n# mysql 锁\n\n> 关键词：乐观锁、表级锁、行级锁、意向锁、mvcc、next-key 锁\n\n\n\n\n# mysql 索引\n\n> 关键词：hash、b 树、聚簇索引、回表\n\n\n\n\n# mysql 性能优化\n\n\n# mysql 运维 🔨\n\n\n# mysql 配置 🔨\n\n\n# mysql 常见问题\n\n----------------------------------------\n\n相关扩展知识：\n\n * 关系型数据库面试总结 💯\n * sql cheat sheet\n * 分布式事务基本原理\n\n\n# 📚 资料\n\n * 官方\n   * mysql 官网\n   * mysql 官方文档\n   * mysql 官方文档之命令行客户端\n * 书籍\n   * 《高性能 mysql》 - 经典，适合 dba 或作为开发者的参考手册\n   * 《mysql 必知必会》 - 适合入门者\n * 教程\n   * mysql 实战 45 讲\n   * runoob.com mysql 教程\n   * mysql-tutorial\n * 更多资源\n   * awesome-mysql\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"PostgreSQL 应用指南",frontmatter:{title:"PostgreSQL 应用指南",date:"2019-08-22T09:02:39.000Z",categories:["数据库","关系型数据库","其他"],tags:["数据库","关系型数据库","PostgreSQL"],permalink:"/pages/52609d/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/99.%E5%85%B6%E4%BB%96/01.PostgreSQL.html",relativePath:"12.数据库/03.关系型数据库/99.其他/01.PostgreSQL.md",key:"v-5a900e2a",path:"/pages/52609d/",headers:[{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:85},{level:2,title:"添加新用户和新数据库",slug:"添加新用户和新数据库",normalizedTitle:"添加新用户和新数据库",charIndex:612},{level:2,title:"登录数据库",slug:"登录数据库",normalizedTitle:"登录数据库",charIndex:866},{level:2,title:"控制台命令",slug:"控制台命令",normalizedTitle:"控制台命令",charIndex:1897},{level:2,title:"数据库操作",slug:"数据库操作",normalizedTitle:"数据库操作",charIndex:1247},{level:2,title:"备份和恢复",slug:"备份和恢复",normalizedTitle:"备份和恢复",charIndex:2901},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:3037},{level:2,title:"🚪 传送门",slug:"传送门",normalizedTitle:"🚪 传送门",charIndex:3166}],headersStr:"安装 添加新用户和新数据库 登录数据库 控制台命令 数据库操作 备份和恢复 参考资料 🚪 传送门",content:"# PostgreSQL 应用指南\n\n> PostgreSQL 是一个关系型数据库（RDBM）。\n> \n> 关键词：Database, RDBM, psql\n\n\n\n\n# 安装\n\n> 本文仅以运行在 Centos 环境下举例。\n\n进入官方下载页面，根据操作系统选择合适版本。\n\n官方下载页面要求用户选择相应版本，然后动态的给出安装提示，如下图所示：\n\n\n\n前 3 步要求用户选择，后 4 步是根据选择动态提示的安装步骤\n\n（1）选择 PostgreSQL 版本\n\n（2）选择平台\n\n（3）选择架构\n\n（4）安装 PostgreSQL 的 rpm 仓库（为了识别下载源）\n\nyum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm\n\n\n（5）安装客户端\n\nyum install postgresql10\n\n\n（6）安装服务端（可选的）\n\nyum install postgresql10-server\n\n\n（7）设置开机启动（可选的）\n\n/usr/pgsql-10/bin/postgresql-10-setup initdb\nsystemctl enable postgresql-10\nsystemctl start postgresql-10\n\n\n\n# 添加新用户和新数据库\n\n初次安装后，默认生成一个名为 postgres 的数据库和一个名为 postgres 的数据库用户。这里需要注意的是，同时还生成了一个名为 postgres 的 Linux 系统用户。\n\n首先，新建一个 Linux 新用户，可以取你想要的名字，这里为 dbuser。\n\nsudo adduser dbuser\n\n\n使用 psql 命令登录 PostgreSQL 控制台：\n\nsudo -u postgres psql\n\n\n这时相当于系统用户 postgres 以同名数据库用户的身份，登录数据库，这是不用输入密码的。如果一切正常，系统提示符会变为\"postgres=#\"，表示这时已经进入了数据库控制台。以下的命令都在控制台内完成。\n\n（1）使用 \\password 命令，为 postgres 用户设置一个密码。\n\npostgres=# \\password postgres\n\n\n（2）创建数据库用户 dbuser（刚才创建的是 Linux 系统用户），并设置密码。\n\nCREATE USER dbuser WITH PASSWORD 'password';\n\n\n（3）创建用户数据库，这里为 exampledb，并指定所有者为 dbuser。\n\nCREATE DATABASE exampledb OWNER dbuser;\n\n\n（4）将 exampledb 数据库的所有权限都赋予 dbuser，否则 dbuser 只能登录控制台，没有任何数据库操作权限。\n\nGRANT ALL PRIVILEGES ON DATABASE exampledb to dbuser;\n\n\n（5）使用\\q 命令退出控制台（也可以直接按 ctrl+D）。\n\n\n# 登录数据库\n\n添加新用户和新数据库以后，就要以新用户的名义登录数据库，这时使用的是 psql 命令。\n\npsql -U dbuser -d exampledb -h 127.0.0.1 -p 5432\n\n\n上面命令的参数含义如下：-U 指定用户，-d 指定数据库，-h 指定服务器，-p 指定端口。\n\n输入上面命令以后，系统会提示输入 dbuser 用户的密码。输入正确，就可以登录控制台了。\n\npsql 命令存在简写形式。如果当前 Linux 系统用户，同时也是 PostgreSQL 用户，则可以省略用户名（-U 参数的部分）。举例来说，我的 Linux 系统用户名为 ruanyf，且 PostgreSQL 数据库存在同名用户，则我以 ruanyf 身份登录 Linux 系统后，可以直接使用下面的命令登录数据库，且不需要密码。\n\npsql exampledb\n\n\n此时，如果 PostgreSQL 内部还存在与当前系统用户同名的数据库，则连数据库名都可以省略。比如，假定存在一个叫做 ruanyf 的数据库，则直接键入 psql 就可以登录该数据库。\n\npsql\n\n另外，如果要恢复外部数据，可以使用下面的命令。\n\npsql exampledb < exampledb.sql\n\n\n\n# 控制台命令\n\n除了前面已经用到的 \\password 命令（设置密码）和 \\q 命令（退出）以外，控制台还提供一系列其他命令。\n\n\\password           设置密码\n\\q                  退出\n\\h                  查看SQL命令的解释，比如\\h select\n\\?                  查看psql命令列表\n\\l                  列出所有数据库\n\\c [database_name]  连接其他数据库\n\\d                  列出当前数据库的所有表格\n\\d [table_name]     列出某一张表格的结构\n\\x                  对数据做展开操作\n\\du                 列出所有用户\n\n\n\n# 数据库操作\n\n基本的数据库操作，就是使用一般的 SQL 语言。\n\n# 创建新表\nCREATE TABLE user_tbl(name VARCHAR(20), signup_date DATE);\n# 插入数据\nINSERT INTO user_tbl(name, signup_date) VALUES('张三', '2013-12-22');\n# 选择记录\nSELECT * FROM user_tbl;\n# 更新数据\nUPDATE user_tbl set name = '李四' WHERE name = '张三';\n# 删除记录\nDELETE FROM user_tbl WHERE name = '李四' ;\n# 添加栏位\nALTER TABLE user_tbl ADD email VARCHAR(40);\n# 更新结构\nALTER TABLE user_tbl ALTER COLUMN signup_date SET NOT NULL;\n# 更名栏位\nALTER TABLE user_tbl RENAME COLUMN signup_date TO signup;\n# 删除栏位\nALTER TABLE user_tbl DROP COLUMN email;\n# 表格更名\nALTER TABLE user_tbl RENAME TO backup_tbl;\n# 删除表格\nDROP TABLE IF EXISTS backup_tbl;\n\n\n\n# 备份和恢复\n\npg_dump --format=t -d db_name -U user_name -h 127.0.0.1 -O -W  > dump.sql\npsql -h 127.0.0.1 -U user_name db_name < dump.sql\n\n\n\n# 参考资料\n\n * https://www.postgresql.org/download/\n * http://www.ruanyifeng.com/blog/2013/12/getting_started_with_postgresql.html\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |",normalizedContent:"# postgresql 应用指南\n\n> postgresql 是一个关系型数据库（rdbm）。\n> \n> 关键词：database, rdbm, psql\n\n\n\n\n# 安装\n\n> 本文仅以运行在 centos 环境下举例。\n\n进入官方下载页面，根据操作系统选择合适版本。\n\n官方下载页面要求用户选择相应版本，然后动态的给出安装提示，如下图所示：\n\n\n\n前 3 步要求用户选择，后 4 步是根据选择动态提示的安装步骤\n\n（1）选择 postgresql 版本\n\n（2）选择平台\n\n（3）选择架构\n\n（4）安装 postgresql 的 rpm 仓库（为了识别下载源）\n\nyum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm\n\n\n（5）安装客户端\n\nyum install postgresql10\n\n\n（6）安装服务端（可选的）\n\nyum install postgresql10-server\n\n\n（7）设置开机启动（可选的）\n\n/usr/pgsql-10/bin/postgresql-10-setup initdb\nsystemctl enable postgresql-10\nsystemctl start postgresql-10\n\n\n\n# 添加新用户和新数据库\n\n初次安装后，默认生成一个名为 postgres 的数据库和一个名为 postgres 的数据库用户。这里需要注意的是，同时还生成了一个名为 postgres 的 linux 系统用户。\n\n首先，新建一个 linux 新用户，可以取你想要的名字，这里为 dbuser。\n\nsudo adduser dbuser\n\n\n使用 psql 命令登录 postgresql 控制台：\n\nsudo -u postgres psql\n\n\n这时相当于系统用户 postgres 以同名数据库用户的身份，登录数据库，这是不用输入密码的。如果一切正常，系统提示符会变为\"postgres=#\"，表示这时已经进入了数据库控制台。以下的命令都在控制台内完成。\n\n（1）使用 \\password 命令，为 postgres 用户设置一个密码。\n\npostgres=# \\password postgres\n\n\n（2）创建数据库用户 dbuser（刚才创建的是 linux 系统用户），并设置密码。\n\ncreate user dbuser with password 'password';\n\n\n（3）创建用户数据库，这里为 exampledb，并指定所有者为 dbuser。\n\ncreate database exampledb owner dbuser;\n\n\n（4）将 exampledb 数据库的所有权限都赋予 dbuser，否则 dbuser 只能登录控制台，没有任何数据库操作权限。\n\ngrant all privileges on database exampledb to dbuser;\n\n\n（5）使用\\q 命令退出控制台（也可以直接按 ctrl+d）。\n\n\n# 登录数据库\n\n添加新用户和新数据库以后，就要以新用户的名义登录数据库，这时使用的是 psql 命令。\n\npsql -u dbuser -d exampledb -h 127.0.0.1 -p 5432\n\n\n上面命令的参数含义如下：-u 指定用户，-d 指定数据库，-h 指定服务器，-p 指定端口。\n\n输入上面命令以后，系统会提示输入 dbuser 用户的密码。输入正确，就可以登录控制台了。\n\npsql 命令存在简写形式。如果当前 linux 系统用户，同时也是 postgresql 用户，则可以省略用户名（-u 参数的部分）。举例来说，我的 linux 系统用户名为 ruanyf，且 postgresql 数据库存在同名用户，则我以 ruanyf 身份登录 linux 系统后，可以直接使用下面的命令登录数据库，且不需要密码。\n\npsql exampledb\n\n\n此时，如果 postgresql 内部还存在与当前系统用户同名的数据库，则连数据库名都可以省略。比如，假定存在一个叫做 ruanyf 的数据库，则直接键入 psql 就可以登录该数据库。\n\npsql\n\n另外，如果要恢复外部数据，可以使用下面的命令。\n\npsql exampledb < exampledb.sql\n\n\n\n# 控制台命令\n\n除了前面已经用到的 \\password 命令（设置密码）和 \\q 命令（退出）以外，控制台还提供一系列其他命令。\n\n\\password           设置密码\n\\q                  退出\n\\h                  查看sql命令的解释，比如\\h select\n\\?                  查看psql命令列表\n\\l                  列出所有数据库\n\\c [database_name]  连接其他数据库\n\\d                  列出当前数据库的所有表格\n\\d [table_name]     列出某一张表格的结构\n\\x                  对数据做展开操作\n\\du                 列出所有用户\n\n\n\n# 数据库操作\n\n基本的数据库操作，就是使用一般的 sql 语言。\n\n# 创建新表\ncreate table user_tbl(name varchar(20), signup_date date);\n# 插入数据\ninsert into user_tbl(name, signup_date) values('张三', '2013-12-22');\n# 选择记录\nselect * from user_tbl;\n# 更新数据\nupdate user_tbl set name = '李四' where name = '张三';\n# 删除记录\ndelete from user_tbl where name = '李四' ;\n# 添加栏位\nalter table user_tbl add email varchar(40);\n# 更新结构\nalter table user_tbl alter column signup_date set not null;\n# 更名栏位\nalter table user_tbl rename column signup_date to signup;\n# 删除栏位\nalter table user_tbl drop column email;\n# 表格更名\nalter table user_tbl rename to backup_tbl;\n# 删除表格\ndrop table if exists backup_tbl;\n\n\n\n# 备份和恢复\n\npg_dump --format=t -d db_name -u user_name -h 127.0.0.1 -o -w  > dump.sql\npsql -h 127.0.0.1 -u user_name db_name < dump.sql\n\n\n\n# 参考资料\n\n * https://www.postgresql.org/download/\n * http://www.ruanyifeng.com/blog/2013/12/getting_started_with_postgresql.html\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"H2 应用指南",frontmatter:{title:"H2 应用指南",date:"2019-08-22T09:02:39.000Z",categories:["数据库","关系型数据库","其他"],tags:["数据库","关系型数据库","H2"],permalink:"/pages/f27c0c/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/99.%E5%85%B6%E4%BB%96/02.H2.html",relativePath:"12.数据库/03.关系型数据库/99.其他/02.H2.md",key:"v-7b0ce341",path:"/pages/f27c0c/",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:14},{level:2,title:"使用说明",slug:"使用说明",normalizedTitle:"使用说明",charIndex:145},{level:3,title:"H2 控制台应用",slug:"h2-控制台应用",normalizedTitle:"h2 控制台应用",charIndex:154},{level:3,title:"嵌入式应用",slug:"嵌入式应用",normalizedTitle:"嵌入式应用",charIndex:389},{level:4,title:"JDBC API",slug:"jdbc-api",normalizedTitle:"jdbc api",charIndex:398},{level:4,title:"连接池",slug:"连接池",normalizedTitle:"连接池",charIndex:521},{level:4,title:"Maven",slug:"maven",normalizedTitle:"maven",charIndex:740},{level:4,title:"Hibernate",slug:"hibernate",normalizedTitle:"hibernate",charIndex:893},{level:4,title:"TopLink 和 Glassfish",slug:"toplink-和-glassfish",normalizedTitle:"toplink 和 glassfish",charIndex:1043},{level:3,title:"运行方式",slug:"运行方式",normalizedTitle:"运行方式",charIndex:1197},{level:4,title:"嵌入式",slug:"嵌入式",normalizedTitle:"嵌入式",charIndex:27},{level:4,title:"内存式",slug:"内存式",normalizedTitle:"内存式",charIndex:1393},{level:4,title:"服务模式",slug:"服务模式",normalizedTitle:"服务模式",charIndex:1573},{level:4,title:"启动服务",slug:"启动服务",normalizedTitle:"启动服务",charIndex:1806},{level:4,title:"设置",slug:"设置",normalizedTitle:"设置",charIndex:2075},{level:4,title:"连接字符串参数",slug:"连接字符串参数",normalizedTitle:"连接字符串参数",charIndex:2172},{level:4,title:"maven 方式",slug:"maven-方式",normalizedTitle:"maven 方式",charIndex:2569},{level:2,title:"Spring 整合 H2",slug:"spring-整合-h2",normalizedTitle:"spring 整合 h2",charIndex:3256},{level:2,title:"H2 SQL",slug:"h2-sql",normalizedTitle:"h2 sql",charIndex:4911},{level:3,title:"SELECT",slug:"select",normalizedTitle:"select",charIndex:4922},{level:3,title:"INSERT",slug:"insert",normalizedTitle:"insert",charIndex:4935},{level:3,title:"UPDATE",slug:"update",normalizedTitle:"update",charIndex:4948},{level:3,title:"DELETE",slug:"delete",normalizedTitle:"delete",charIndex:4961},{level:3,title:"BACKUP",slug:"backup",normalizedTitle:"backup",charIndex:4974},{level:3,title:"EXPLAIN",slug:"explain",normalizedTitle:"explain",charIndex:4987},{level:3,title:"RUNSCRIPT",slug:"runscript",normalizedTitle:"runscript",charIndex:5010},{level:3,title:"SCRIPT",slug:"script",normalizedTitle:"script",charIndex:5013},{level:3,title:"SHOW",slug:"show",normalizedTitle:"show",charIndex:5068},{level:3,title:"ALTER",slug:"alter",normalizedTitle:"alter",charIndex:5079},{level:4,title:"ALTER INDEX RENAME",slug:"alter-index-rename",normalizedTitle:"alter index rename",charIndex:5088},{level:4,title:"ALTER SCHEMA RENAME",slug:"alter-schema-rename",normalizedTitle:"alter schema rename",charIndex:5112},{level:4,title:"ALTER SEQUENCE",slug:"alter-sequence",normalizedTitle:"alter sequence",charIndex:5137},{level:4,title:"ALTER TABLE",slug:"alter-table",normalizedTitle:"alter table",charIndex:5157},{level:5,title:"增加约束",slug:"增加约束",normalizedTitle:"增加约束",charIndex:5174},{level:5,title:"修改列",slug:"修改列",normalizedTitle:"修改列",charIndex:5184},{level:5,title:"删除列",slug:"删除列",normalizedTitle:"删除列",charIndex:5193},{level:5,title:"删除序列",slug:"删除序列",normalizedTitle:"删除序列",charIndex:5202},{level:4,title:"ALTER USER",slug:"alter-user",normalizedTitle:"alter user",charIndex:5212},{level:5,title:"修改用户名",slug:"修改用户名",normalizedTitle:"修改用户名",charIndex:5226},{level:5,title:"修改用户密码",slug:"修改用户密码",normalizedTitle:"修改用户密码",charIndex:5237},{level:4,title:"ALTER VIEW",slug:"alter-view",normalizedTitle:"alter view",charIndex:5249},{level:3,title:"COMMENT",slug:"comment",normalizedTitle:"comment",charIndex:5266},{level:3,title:"CREATE CONSTANT",slug:"create-constant",normalizedTitle:"create constant",charIndex:5280},{level:3,title:"CREATE INDEX",slug:"create-index",normalizedTitle:"create index",charIndex:5302},{level:3,title:"CREATE ROLE",slug:"create-role",normalizedTitle:"create role",charIndex:5321},{level:3,title:"CREATE SCHEMA",slug:"create-schema",normalizedTitle:"create schema",charIndex:5339},{level:3,title:"CREATE SEQUENCE",slug:"create-sequence",normalizedTitle:"create sequence",charIndex:5359},{level:3,title:"CREATE TABLE",slug:"create-table",normalizedTitle:"create table",charIndex:5381},{level:3,title:"CREATE TRIGGER",slug:"create-trigger",normalizedTitle:"create trigger",charIndex:5400},{level:3,title:"CREATE USER",slug:"create-user",normalizedTitle:"create user",charIndex:5421},{level:3,title:"CREATE VIEW",slug:"create-view",normalizedTitle:"create view",charIndex:5439},{level:3,title:"DROP",slug:"drop",normalizedTitle:"drop",charIndex:5457},{level:3,title:"GRANT RIGHT",slug:"grant-right",normalizedTitle:"grant right",charIndex:5468},{level:4,title:"复制角色的权限",slug:"复制角色的权限",normalizedTitle:"复制角色的权限",charIndex:5526},{level:3,title:"REVOKE RIGHT",slug:"revoke-right",normalizedTitle:"revoke right",charIndex:5540},{level:4,title:"移除授权",slug:"移除授权",normalizedTitle:"移除授权",charIndex:5556},{level:4,title:"移除角色具有的权限",slug:"移除角色具有的权限",normalizedTitle:"移除角色具有的权限",charIndex:5566},{level:3,title:"ROLLBACK",slug:"rollback",normalizedTitle:"rollback",charIndex:5582},{level:4,title:"从某个还原点（savepoint）回滚",slug:"从某个还原点-savepoint-回滚",normalizedTitle:"从某个还原点（savepoint）回滚",charIndex:5594},{level:4,title:"回滚事务",slug:"回滚事务",normalizedTitle:"回滚事务",charIndex:5619},{level:4,title:"创建 savepoint",slug:"创建-savepoint",normalizedTitle:"创建 savepoint",charIndex:5629},{level:2,title:"数据类型",slug:"数据类型",normalizedTitle:"数据类型",charIndex:5648},{level:3,title:"INT Type",slug:"int-type",normalizedTitle:"int type",charIndex:5659},{level:2,title:"集群",slug:"集群",normalizedTitle:"集群",charIndex:5674},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:6274},{level:2,title:"🚪 传送门",slug:"传送门",normalizedTitle:"🚪 传送门",charIndex:6337}],headersStr:"概述 使用说明 H2 控制台应用 嵌入式应用 JDBC API 连接池 Maven Hibernate TopLink 和 Glassfish 运行方式 嵌入式 内存式 服务模式 启动服务 设置 连接字符串参数 maven 方式 Spring 整合 H2 H2 SQL SELECT INSERT UPDATE DELETE BACKUP EXPLAIN RUNSCRIPT SCRIPT SHOW ALTER ALTER INDEX RENAME ALTER SCHEMA RENAME ALTER SEQUENCE ALTER TABLE 增加约束 修改列 删除列 删除序列 ALTER USER 修改用户名 修改用户密码 ALTER VIEW COMMENT CREATE CONSTANT CREATE INDEX CREATE ROLE CREATE SCHEMA CREATE SEQUENCE CREATE TABLE CREATE TRIGGER CREATE USER CREATE VIEW DROP GRANT RIGHT 复制角色的权限 REVOKE RIGHT 移除授权 移除角色具有的权限 ROLLBACK 从某个还原点（savepoint）回滚 回滚事务 创建 savepoint 数据类型 INT Type 集群 参考资料 🚪 传送门",content:'# H2 应用指南\n\n\n# 概述\n\nH2 是一个开源的嵌入式数据库引擎，采用 java 语言编写，不受平台的限制。同时 H2 提供了一个十分方便的 web 控制台用于操作和管理数据库内容。H2 还提供兼容模式，可以兼容一些主流的数据库，因此采用 H2 作为开发期的数据库非常方便。\n\n\n# 使用说明\n\n\n# H2 控制台应用\n\nH2 允许用户通过浏览器接口方式访问 SQL 数据库。\n\n 1. 进入官方下载地址，选择合适版本，下载并安装到本地。\n 2. 启动方式：在 bin 目录下，双击 jar 包；执行 java -jar h2*.jar；执行脚本：h2.bat 或 h2.sh。\n 3. 在浏览器中访问：http://localhost:8082，应该可以看到下图中的页面：\n\n\n\n点击 Connect ，可以进入操作界面：\n\n\n\n操作界面十分简单，不一一细说。\n\n\n# 嵌入式应用\n\n# JDBC API\n\nConnection conn = DriverManager.\n    getConnection("jdbc:h2:~/test");\nconn.close();\n\n\n> 详见：Using the JDBC API\n\n# 连接池\n\nimport org.h2.jdbcx.JdbcConnectionPool;\nJdbcConnectionPool cp = JdbcConnectionPool.\ncreate("jdbc:h2:~/test", "sa", "sa");\nConnection conn = cp.getConnection();\nconn.close(); cp.dispose();\n\n\n> 详见：Connection Pool\n\n# Maven\n\n<dependency>\n    <groupId>com.h2database</groupId>\n    <artifactId>h2</artifactId>\n    <version>1.4.197</version>\n</dependency>\n\n\n> 详见：Maven 2\n\n# Hibernate\n\nhibernate.cfg.xml (or use the HSQLDialect):\n\n<property name="dialect">\n    org.hibernate.dialect.H2Dialect\n</property>\n\n\n> 详见：Hibernate\n\n# TopLink 和 Glassfish\n\nDatasource class: org.h2.jdbcx.JdbcDataSource oracle.toplink.essentials.platform.database.H2Platform\n\n> 详见：TopLink and Glassfish\n\n\n# 运行方式\n\n# 嵌入式\n\n数据库持久化存储为单个文件。\n\n连接字符串：\\~/.h2/DBName 表示数据库文件的存储位置，如果第一次连接则会自动创建数据库。\n\n * jdbc:h2:\\~/test - \'test\' 在用户根目录下\n * jdbc:h2:/data/test - \'test\' 在 /data 目录下\n * jdbc:h2:test - \'test\' 在当前工作目录\n\n# 内存式\n\n数据库只在内存中运行，关闭连接后数据库将被清空，适合测试环境\n\n连接字符串：jdbc:h2:mem:DBName;DB_CLOSE_DELAY=-1\n\n如果不指定 DBName，则以私有方式启动，只允许一个连接。\n\n * jdbc:h2:mem:test - 一个进程中有多个连接\n * jdbc:h2:mem: - 未命名的私有库，一个连接\n\n# 服务模式\n\nH2 支持三种服务模式：\n\n * web server：此种运行方式支持使用浏览器访问 H2 Console\n * TCP server：支持客户端/服务器端的连接方式\n * PG server：支持 PostgreSQL 客户端\n\n启动 tcp 服务连接字符串示例：\n\n * jdbc:h2:tcp://localhost/\\~/test - 用户根目录\n * jdbc:h2:tcp://localhost//data/test - 绝对路径\n\n# 启动服务\n\n执行 java -cp *.jar org.h2.tools.Server\n\n执行如下命令，获取选项列表及默认值\n\njava -cp h2*.jar org.h2.tools.Server -?\n\n\n常见的选项如下：\n\n * -web：启动支持 H2 Console 的服务\n * -webPort <port>：服务启动端口，默认为 8082\n * -browser：启动 H2 Console web 管理页面\n * -tcp：使用 TCP server 模式启动\n * -pg：使用 PG server 模式启动\n\n# 设置\n\n * jdbc:h2:..;MODE=MySQL 兼容模式（或 HSQLDB 等）\n * jdbc:h2:..;TRACE_LEVEL_FILE=3 记录到 *.trace.db\n\n# 连接字符串参数\n\n * DB_CLOSE_DELAY - 要求最后一个正在连接的连接断开后，不要关闭数据库\n * MODE=MySQL - 兼容模式，H2 兼容多种数据库，该值可以为：DB2、Derby、HSQLDB、MSSQLServer、MySQL、Oracle、PostgreSQL\n * AUTO_RECONNECT=TRUE - 连接丢失后自动重新连接\n * AUTO_SERVER=TRUE - 启动自动混合模式，允许开启多个连接，该参数不支持在内存中运行模式\n * TRACE_LEVEL_SYSTEM_OUT、TRACE_LEVEL_FILE - 输出跟踪日志到控制台或文件， 取值 0 为 OFF，1 为 ERROR（默认值），2 为 INFO，3 为 DEBUG\n * SET TRACE_MAX_FILE_SIZE mb - 设置跟踪日志文件的大小，默认为 16M\n\n# maven 方式\n\n此外，使用 maven 也可以启动 H2 服务。添加以下插件\n\n<plugin>\n  <groupId>org.codehaus.mojo</groupId>\n  <artifactId>exec-maven-plugin</artifactId>\n  <executions>\n    <execution>\n      <goals>\n        <goal>java</goal>\n      </goals>\n    </execution>\n  </executions>\n  <configuration>\n    <mainClass>org.h2.tools.Server</mainClass>\n    <arguments>\n      <argument>-web</argument>\n      <argument>-webPort</argument>\n      <argument>8090</argument>\n      <argument>-browser</argument>\n    </arguments>\n  </configuration>\n</plugin>\n\n\n在命令行中执行如下命令启动 H2 Console\n\nmvn exec:java\n\n\n或者建立一个 bat 文件\n\n@echo off\ncall mvn exec:java\npause\n\n\n此操作相当于执行了如下命令：\n\njava -jar h2-1.3.168.jar -web -webPort 8090 -browser\n\n\n\n# Spring 整合 H2\n\n（1）添加依赖\n\n<dependency>\n  <groupId>com.h2database</groupId>\n  <artifactId>h2</artifactId>\n  <version>1.4.194</version>\n</dependency>\n\n\n（2）spring 配置\n\n<?xml version="1.0" encoding="UTF-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n       xmlns:jdbc="http://www.springframework.org/schema/jdbc"\n       xsi:schemaLocation="http://www.springframework.org/schema/beans\n            http://www.springframework.org/schema/beans/spring-beans-3.2.xsd\n            http://www.springframework.org/schema/jdbc\n            http://www.springframework.org/schema/jdbc/spring-jdbc.xsd">\n\n  \x3c!--配置数据源--\x3e\n  <bean id="dataSource" class="org.h2.jdbcx.JdbcConnectionPool"\n        destroy-method="dispose">\n    <constructor-arg>\n      <bean class="org.h2.jdbcx.JdbcDataSource">\n        \x3c!-- 内存模式 --\x3e\n        <property name="URL" value="jdbc:h2:mem:test"/>\n        \x3c!-- 文件模式 --\x3e\n        \x3c!-- <property name="URL" value="jdbc:h2:testRestDB" /> --\x3e\n        <property name="user" value="root"/>\n        <property name="password" value="root"/>\n      </bean>\n    </constructor-arg>\n  </bean>\n\n  \x3c!-- JDBC模板 --\x3e\n  <bean id="jdbcTemplate" class="org.springframework.jdbc.core.JdbcTemplate">\n    <constructor-arg ref="dataSource"/>\n  </bean>\n  <bean id="myJdbcTemplate" class="org.zp.notes.spring.jdbc.MyJdbcTemplate">\n    <property name="jdbcTemplate" ref="jdbcTemplate"/>\n  </bean>\n\n  \x3c!-- 初始化数据表结构 --\x3e\n  <jdbc:initialize-database data-source="dataSource" ignore-failures="ALL">\n    <jdbc:script location="classpath:sql/h2/create_table_student.sql"/>\n  </jdbc:initialize-database>\n</beans>\n\n\n\n# H2 SQL\n\n\n# SELECT\n\n\n\n\n# INSERT\n\n\n\n\n# UPDATE\n\n\n\n\n# DELETE\n\n\n\n\n# BACKUP\n\n\n\n\n# EXPLAIN\n\n\n\n7、MERGE\n\n\n# RUNSCRIPT\n\n运行 sql 脚本文件\n\n\n\n\n# SCRIPT\n\n根据数据库创建 sql 脚本\n\n\n\n\n# SHOW\n\n\n\n\n# ALTER\n\n# ALTER INDEX RENAME\n\n\n\n# ALTER SCHEMA RENAME\n\n\n\n# ALTER SEQUENCE\n\n\n\n# ALTER TABLE\n\n\n\n# 增加约束\n\n\n\n# 修改列\n\n\n\n# 删除列\n\n\n\n# 删除序列\n\n\n\n# ALTER USER\n\n# 修改用户名\n\n\n\n# 修改用户密码\n\n\n\n# ALTER VIEW\n\n\n\n\n# COMMENT\n\n\n\n\n# CREATE CONSTANT\n\n\n\n\n# CREATE INDEX\n\n\n\n\n# CREATE ROLE\n\n\n\n\n# CREATE SCHEMA\n\n\n\n\n# CREATE SEQUENCE\n\n\n\n\n# CREATE TABLE\n\n\n\n\n# CREATE TRIGGER\n\n\n\n\n# CREATE USER\n\n\n\n\n# CREATE VIEW\n\n\n\n\n# DROP\n\n\n\n\n# GRANT RIGHT\n\n给 schema 授权授权\n\n\n\n给 schema 授权给 schema 授权\n\n\n\n# 复制角色的权限\n\n\n\n\n# REVOKE RIGHT\n\n# 移除授权\n\n\n\n# 移除角色具有的权限\n\n\n\n\n# ROLLBACK\n\n# 从某个还原点（savepoint）回滚\n\n\n\n# 回滚事务\n\n\n\n# 创建 savepoint\n\n\n\n\n# 数据类型\n\n\n\n\n# INT Type\n\n\n\n\n# 集群\n\nH2 支持两台服务器运行两个数据库成为集群，两个数据库互为备份，如果一个服务器失效，另一个服务器仍然可以工作。另外只有服务模式支持集群配置。\n\nH2 可以通过 CreateCluster 工具创建集群，示例步骤如下（在在一台服务器上模拟两个数据库组成集群）：\n\n * 创建目录\n   * 创建两个服务器工作的目录\n * 启动 tcp 服务\n   * 执行如下命令分别在 9101、9102 端口启动两个使用 tcp 服务模式的数据库\n * 使用 CreateCluster 工具创建集群\n   * 如果两个数据库不存在，该命令将会自动创建数据库。如果一个数据库失效，可以先删除坏的数据库文件，重新启动数据库，然后重新运行 CreateCluster 工具\n * 连接数据库现在可以使用如下连接字符串连接集群数据库\n   * 监控集群运行状态\n   * 可以使用如下命令查看配置的集群服务器是否都在运行\n * 限制\n   * H2 的集群并不支持针对事务的负载均衡，所以很多操作会使两个数据库产生不一致的结果\n * 执行如下操作时请小心：\n   * 自动增长列和标识列不支持集群，当插入数据时，序列值需要手动创建不支持 SET AUTOCOMMIT FALSE 语句；\n   * 如果需要设置成为不自动提交，可以执行方法 Connection.setAutoCommit(false)\n\n\n# 参考资料\n\n * h2database 官网\n * Java 嵌入式数据库 H2 学习总结(一)——H2 数据库入门\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |',normalizedContent:'# h2 应用指南\n\n\n# 概述\n\nh2 是一个开源的嵌入式数据库引擎，采用 java 语言编写，不受平台的限制。同时 h2 提供了一个十分方便的 web 控制台用于操作和管理数据库内容。h2 还提供兼容模式，可以兼容一些主流的数据库，因此采用 h2 作为开发期的数据库非常方便。\n\n\n# 使用说明\n\n\n# h2 控制台应用\n\nh2 允许用户通过浏览器接口方式访问 sql 数据库。\n\n 1. 进入官方下载地址，选择合适版本，下载并安装到本地。\n 2. 启动方式：在 bin 目录下，双击 jar 包；执行 java -jar h2*.jar；执行脚本：h2.bat 或 h2.sh。\n 3. 在浏览器中访问：http://localhost:8082，应该可以看到下图中的页面：\n\n\n\n点击 connect ，可以进入操作界面：\n\n\n\n操作界面十分简单，不一一细说。\n\n\n# 嵌入式应用\n\n# jdbc api\n\nconnection conn = drivermanager.\n    getconnection("jdbc:h2:~/test");\nconn.close();\n\n\n> 详见：using the jdbc api\n\n# 连接池\n\nimport org.h2.jdbcx.jdbcconnectionpool;\njdbcconnectionpool cp = jdbcconnectionpool.\ncreate("jdbc:h2:~/test", "sa", "sa");\nconnection conn = cp.getconnection();\nconn.close(); cp.dispose();\n\n\n> 详见：connection pool\n\n# maven\n\n<dependency>\n    <groupid>com.h2database</groupid>\n    <artifactid>h2</artifactid>\n    <version>1.4.197</version>\n</dependency>\n\n\n> 详见：maven 2\n\n# hibernate\n\nhibernate.cfg.xml (or use the hsqldialect):\n\n<property name="dialect">\n    org.hibernate.dialect.h2dialect\n</property>\n\n\n> 详见：hibernate\n\n# toplink 和 glassfish\n\ndatasource class: org.h2.jdbcx.jdbcdatasource oracle.toplink.essentials.platform.database.h2platform\n\n> 详见：toplink and glassfish\n\n\n# 运行方式\n\n# 嵌入式\n\n数据库持久化存储为单个文件。\n\n连接字符串：\\~/.h2/dbname 表示数据库文件的存储位置，如果第一次连接则会自动创建数据库。\n\n * jdbc:h2:\\~/test - \'test\' 在用户根目录下\n * jdbc:h2:/data/test - \'test\' 在 /data 目录下\n * jdbc:h2:test - \'test\' 在当前工作目录\n\n# 内存式\n\n数据库只在内存中运行，关闭连接后数据库将被清空，适合测试环境\n\n连接字符串：jdbc:h2:mem:dbname;db_close_delay=-1\n\n如果不指定 dbname，则以私有方式启动，只允许一个连接。\n\n * jdbc:h2:mem:test - 一个进程中有多个连接\n * jdbc:h2:mem: - 未命名的私有库，一个连接\n\n# 服务模式\n\nh2 支持三种服务模式：\n\n * web server：此种运行方式支持使用浏览器访问 h2 console\n * tcp server：支持客户端/服务器端的连接方式\n * pg server：支持 postgresql 客户端\n\n启动 tcp 服务连接字符串示例：\n\n * jdbc:h2:tcp://localhost/\\~/test - 用户根目录\n * jdbc:h2:tcp://localhost//data/test - 绝对路径\n\n# 启动服务\n\n执行 java -cp *.jar org.h2.tools.server\n\n执行如下命令，获取选项列表及默认值\n\njava -cp h2*.jar org.h2.tools.server -?\n\n\n常见的选项如下：\n\n * -web：启动支持 h2 console 的服务\n * -webport <port>：服务启动端口，默认为 8082\n * -browser：启动 h2 console web 管理页面\n * -tcp：使用 tcp server 模式启动\n * -pg：使用 pg server 模式启动\n\n# 设置\n\n * jdbc:h2:..;mode=mysql 兼容模式（或 hsqldb 等）\n * jdbc:h2:..;trace_level_file=3 记录到 *.trace.db\n\n# 连接字符串参数\n\n * db_close_delay - 要求最后一个正在连接的连接断开后，不要关闭数据库\n * mode=mysql - 兼容模式，h2 兼容多种数据库，该值可以为：db2、derby、hsqldb、mssqlserver、mysql、oracle、postgresql\n * auto_reconnect=true - 连接丢失后自动重新连接\n * auto_server=true - 启动自动混合模式，允许开启多个连接，该参数不支持在内存中运行模式\n * trace_level_system_out、trace_level_file - 输出跟踪日志到控制台或文件， 取值 0 为 off，1 为 error（默认值），2 为 info，3 为 debug\n * set trace_max_file_size mb - 设置跟踪日志文件的大小，默认为 16m\n\n# maven 方式\n\n此外，使用 maven 也可以启动 h2 服务。添加以下插件\n\n<plugin>\n  <groupid>org.codehaus.mojo</groupid>\n  <artifactid>exec-maven-plugin</artifactid>\n  <executions>\n    <execution>\n      <goals>\n        <goal>java</goal>\n      </goals>\n    </execution>\n  </executions>\n  <configuration>\n    <mainclass>org.h2.tools.server</mainclass>\n    <arguments>\n      <argument>-web</argument>\n      <argument>-webport</argument>\n      <argument>8090</argument>\n      <argument>-browser</argument>\n    </arguments>\n  </configuration>\n</plugin>\n\n\n在命令行中执行如下命令启动 h2 console\n\nmvn exec:java\n\n\n或者建立一个 bat 文件\n\n@echo off\ncall mvn exec:java\npause\n\n\n此操作相当于执行了如下命令：\n\njava -jar h2-1.3.168.jar -web -webport 8090 -browser\n\n\n\n# spring 整合 h2\n\n（1）添加依赖\n\n<dependency>\n  <groupid>com.h2database</groupid>\n  <artifactid>h2</artifactid>\n  <version>1.4.194</version>\n</dependency>\n\n\n（2）spring 配置\n\n<?xml version="1.0" encoding="utf-8"?>\n<beans xmlns="http://www.springframework.org/schema/beans"\n       xmlns:xsi="http://www.w3.org/2001/xmlschema-instance"\n       xmlns:jdbc="http://www.springframework.org/schema/jdbc"\n       xsi:schemalocation="http://www.springframework.org/schema/beans\n            http://www.springframework.org/schema/beans/spring-beans-3.2.xsd\n            http://www.springframework.org/schema/jdbc\n            http://www.springframework.org/schema/jdbc/spring-jdbc.xsd">\n\n  \x3c!--配置数据源--\x3e\n  <bean id="datasource" class="org.h2.jdbcx.jdbcconnectionpool"\n        destroy-method="dispose">\n    <constructor-arg>\n      <bean class="org.h2.jdbcx.jdbcdatasource">\n        \x3c!-- 内存模式 --\x3e\n        <property name="url" value="jdbc:h2:mem:test"/>\n        \x3c!-- 文件模式 --\x3e\n        \x3c!-- <property name="url" value="jdbc:h2:testrestdb" /> --\x3e\n        <property name="user" value="root"/>\n        <property name="password" value="root"/>\n      </bean>\n    </constructor-arg>\n  </bean>\n\n  \x3c!-- jdbc模板 --\x3e\n  <bean id="jdbctemplate" class="org.springframework.jdbc.core.jdbctemplate">\n    <constructor-arg ref="datasource"/>\n  </bean>\n  <bean id="myjdbctemplate" class="org.zp.notes.spring.jdbc.myjdbctemplate">\n    <property name="jdbctemplate" ref="jdbctemplate"/>\n  </bean>\n\n  \x3c!-- 初始化数据表结构 --\x3e\n  <jdbc:initialize-database data-source="datasource" ignore-failures="all">\n    <jdbc:script location="classpath:sql/h2/create_table_student.sql"/>\n  </jdbc:initialize-database>\n</beans>\n\n\n\n# h2 sql\n\n\n# select\n\n\n\n\n# insert\n\n\n\n\n# update\n\n\n\n\n# delete\n\n\n\n\n# backup\n\n\n\n\n# explain\n\n\n\n7、merge\n\n\n# runscript\n\n运行 sql 脚本文件\n\n\n\n\n# script\n\n根据数据库创建 sql 脚本\n\n\n\n\n# show\n\n\n\n\n# alter\n\n# alter index rename\n\n\n\n# alter schema rename\n\n\n\n# alter sequence\n\n\n\n# alter table\n\n\n\n# 增加约束\n\n\n\n# 修改列\n\n\n\n# 删除列\n\n\n\n# 删除序列\n\n\n\n# alter user\n\n# 修改用户名\n\n\n\n# 修改用户密码\n\n\n\n# alter view\n\n\n\n\n# comment\n\n\n\n\n# create constant\n\n\n\n\n# create index\n\n\n\n\n# create role\n\n\n\n\n# create schema\n\n\n\n\n# create sequence\n\n\n\n\n# create table\n\n\n\n\n# create trigger\n\n\n\n\n# create user\n\n\n\n\n# create view\n\n\n\n\n# drop\n\n\n\n\n# grant right\n\n给 schema 授权授权\n\n\n\n给 schema 授权给 schema 授权\n\n\n\n# 复制角色的权限\n\n\n\n\n# revoke right\n\n# 移除授权\n\n\n\n# 移除角色具有的权限\n\n\n\n\n# rollback\n\n# 从某个还原点（savepoint）回滚\n\n\n\n# 回滚事务\n\n\n\n# 创建 savepoint\n\n\n\n\n# 数据类型\n\n\n\n\n# int type\n\n\n\n\n# 集群\n\nh2 支持两台服务器运行两个数据库成为集群，两个数据库互为备份，如果一个服务器失效，另一个服务器仍然可以工作。另外只有服务模式支持集群配置。\n\nh2 可以通过 createcluster 工具创建集群，示例步骤如下（在在一台服务器上模拟两个数据库组成集群）：\n\n * 创建目录\n   * 创建两个服务器工作的目录\n * 启动 tcp 服务\n   * 执行如下命令分别在 9101、9102 端口启动两个使用 tcp 服务模式的数据库\n * 使用 createcluster 工具创建集群\n   * 如果两个数据库不存在，该命令将会自动创建数据库。如果一个数据库失效，可以先删除坏的数据库文件，重新启动数据库，然后重新运行 createcluster 工具\n * 连接数据库现在可以使用如下连接字符串连接集群数据库\n   * 监控集群运行状态\n   * 可以使用如下命令查看配置的集群服务器是否都在运行\n * 限制\n   * h2 的集群并不支持针对事务的负载均衡，所以很多操作会使两个数据库产生不一致的结果\n * 执行如下操作时请小心：\n   * 自动增长列和标识列不支持集群，当插入数据时，序列值需要手动创建不支持 set autocommit false 语句；\n   * 如果需要设置成为不自动提交，可以执行方法 connection.setautocommit(false)\n\n\n# 参考资料\n\n * h2database 官网\n * java 嵌入式数据库 h2 学习总结(一)——h2 数据库入门\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"sqlite",frontmatter:{title:"sqlite",date:"2019-08-22T09:02:39.000Z",categories:["数据库","关系型数据库","其他"],tags:["数据库","关系型数据库","SQLite"],permalink:"/pages/bdcd7e/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/99.%E5%85%B6%E4%BB%96/03.Sqlite.html",relativePath:"12.数据库/03.关系型数据库/99.其他/03.Sqlite.md",key:"v-28fda0bf",path:"/pages/bdcd7e/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:72},{level:3,title:"优点",slug:"优点",normalizedTitle:"优点",charIndex:79},{level:3,title:"局限",slug:"局限",normalizedTitle:"局限",charIndex:456},{level:3,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:145},{level:2,title:"语法",slug:"语法",normalizedTitle:"语法",charIndex:1046},{level:3,title:"大小写敏感",slug:"大小写敏感",normalizedTitle:"大小写敏感",charIndex:1138},{level:3,title:"注释",slug:"注释",normalizedTitle:"注释",charIndex:1216},{level:3,title:"创建数据库",slug:"创建数据库",normalizedTitle:"创建数据库",charIndex:1254},{level:3,title:"查看数据库",slug:"查看数据库",normalizedTitle:"查看数据库",charIndex:1422},{level:3,title:"退出数据库",slug:"退出数据库",normalizedTitle:"退出数据库",charIndex:1597},{level:3,title:"附加数据库",slug:"附加数据库",normalizedTitle:"附加数据库",charIndex:1623},{level:3,title:"分离数据库",slug:"分离数据库",normalizedTitle:"分离数据库",charIndex:2089},{level:3,title:"备份数据库",slug:"备份数据库",normalizedTitle:"备份数据库",charIndex:2570},{level:3,title:"恢复数据库",slug:"恢复数据库",normalizedTitle:"恢复数据库",charIndex:2653},{level:2,title:"数据类型",slug:"数据类型",normalizedTitle:"数据类型",charIndex:2726},{level:3,title:"SQLite 存储类",slug:"sqlite-存储类",normalizedTitle:"sqlite 存储类",charIndex:2796},{level:3,title:"SQLite 亲和(Affinity)类型",slug:"sqlite-亲和-affinity-类型",normalizedTitle:"sqlite 亲和(affinity)类型",charIndex:3122},{level:3,title:"SQLite 亲和类型(Affinity)及类型名称",slug:"sqlite-亲和类型-affinity-及类型名称",normalizedTitle:"sqlite 亲和类型(affinity)及类型名称",charIndex:3806},{level:3,title:"Boolean 数据类型",slug:"boolean-数据类型",normalizedTitle:"boolean 数据类型",charIndex:4379},{level:3,title:"Date 与 Time 数据类型",slug:"date-与-time-数据类型",normalizedTitle:"date 与 time 数据类型",charIndex:4454},{level:2,title:"SQLite 命令",slug:"sqlite-命令",normalizedTitle:"sqlite 命令",charIndex:4750},{level:3,title:"快速开始",slug:"快速开始",normalizedTitle:"快速开始",charIndex:4764},{level:4,title:"进入 SQLite 控制台",slug:"进入-sqlite-控制台",normalizedTitle:"进入 sqlite 控制台",charIndex:4772},{level:4,title:"进入 SQLite 控制台并指定数据库",slug:"进入-sqlite-控制台并指定数据库",normalizedTitle:"进入 sqlite 控制台并指定数据库",charIndex:4925},{level:4,title:"退出 SQLite 控制台",slug:"退出-sqlite-控制台",normalizedTitle:"退出 sqlite 控制台",charIndex:5092},{level:4,title:"查看命令帮助",slug:"查看命令帮助",normalizedTitle:"查看命令帮助",charIndex:5124},{level:3,title:"常用命令清单",slug:"常用命令清单",normalizedTitle:"常用命令清单",charIndex:5150},{level:3,title:"实战",slug:"实战",normalizedTitle:"实战",charIndex:6586},{level:4,title:"格式化输出",slug:"格式化输出",normalizedTitle:"格式化输出",charIndex:6592},{level:4,title:"输出结果到文件",slug:"输出结果到文件",normalizedTitle:"输出结果到文件",charIndex:6666},{level:2,title:"JAVA Client",slug:"java-client",normalizedTitle:"java client",charIndex:6842},{level:3,title:"如何指定数据库文件",slug:"如何指定数据库文件",normalizedTitle:"如何指定数据库文件",charIndex:8534},{level:3,title:"如何使用内存数据库",slug:"如何使用内存数据库",normalizedTitle:"如何使用内存数据库",charIndex:8777},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:8870},{level:2,title:"🚪 传送门",slug:"传送门",normalizedTitle:"🚪 传送门",charIndex:9069}],headersStr:"简介 优点 局限 安装 语法 大小写敏感 注释 创建数据库 查看数据库 退出数据库 附加数据库 分离数据库 备份数据库 恢复数据库 数据类型 SQLite 存储类 SQLite 亲和(Affinity)类型 SQLite 亲和类型(Affinity)及类型名称 Boolean 数据类型 Date 与 Time 数据类型 SQLite 命令 快速开始 进入 SQLite 控制台 进入 SQLite 控制台并指定数据库 退出 SQLite 控制台 查看命令帮助 常用命令清单 实战 格式化输出 输出结果到文件 JAVA Client 如何指定数据库文件 如何使用内存数据库 参考资料 🚪 传送门",content:'# SQLite\n\n> SQLite 是一个实现了自给自足的、无服务器的、零配置的、事务性的 SQL 数据库引擎。 👉 完整示例源码\n\n\n# 简介\n\n\n# 优点\n\n * SQLite 是自给自足的，这意味着不需要任何外部的依赖。\n * SQLite 是无服务器的、零配置的，这意味着不需要安装或管理。\n * SQLite 事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。\n * SQLite 是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于 250KiB。\n * SQLite 支持 SQL92（SQL2）标准的大多数查询语言的功能。\n * 一个完整的 SQLite 数据库是存储在一个单一的跨平台的磁盘文件。\n * SQLite 使用 ANSI-C 编写的，并提供了简单和易于使用的 API。\n * SQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。\n\n\n# 局限\n\n特性                 描述\nRIGHT OUTER JOIN   只实现了 LEFT OUTER JOIN。\nFULL OUTER JOIN    只实现了 LEFT OUTER JOIN。\nALTER TABLE        支持 RENAME TABLE 和 ALTER TABLE 的 ADD COLUMN variants 命令，不支持\n                   DROP COLUMN、ALTER COLUMN、ADD CONSTRAINT。\nTrigger 支持         支持 FOR EACH ROW 触发器，但不支持 FOR EACH STATEMENT 触发器。\nVIEWs              在 SQLite 中，视图是只读的。您不可以在视图上执行 DELETE、INSERT 或 UPDATE 语句。\nGRANT 和 REVOKE     可以应用的唯一的访问权限是底层操作系统的正常文件访问权限。\n\n\n# 安装\n\nSqlite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。\n\n一般，Linux 和 Mac 上会预安装 sqlite。如果没有安装，可以在官方下载地址下载合适安装版本，自行安装。\n\n\n# 语法\n\n> 这里不会详细列举所有 SQL 语法，仅列举 SQLite 除标准 SQL 以外的，一些自身特殊的 SQL 语法。\n> \n> 👉 扩展阅读：标准 SQL 基本语法\n\n\n# 大小写敏感\n\nSQLite 是不区分大小写的，但也有一些命令是大小写敏感的，比如 GLOB 和 glob 在 SQLite 的语句中有不同的含义。\n\n\n# 注释\n\n-- 单行注释\n/*\n 多行注释1\n 多行注释2\n */\n\n\n\n# 创建数据库\n\n如下，创建一个名为 test 的数据库：\n\n$ sqlite3 test.db\nSQLite version 3.7.17 2013-05-20 00:56:22\nEnter ".help" for instructions\nEnter SQL statements terminated with a ";"\n\n\n\n# 查看数据库\n\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n\n\n\n# 退出数据库\n\nsqlite> .quit\n\n\n\n# 附加数据库\n\n假设这样一种情况，当在同一时间有多个数据库可用，您想使用其中的任何一个。\n\nSQLite 的 ATTACH DATABASE 语句是用来选择一个特定的数据库，使用该命令后，所有的 SQLite 语句将在附加的数据库下执行。\n\nsqlite> ATTACH DATABASE \'test.db\' AS \'test\';\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n2    test             /root/test.db\n\n\n> 🔔 注意：数据库名 main 和 temp 被保留用于主数据库和存储临时表及其他临时数据对象的数据库。这两个数据库名称可用于每个数据库连接，且不应该被用于附加，否则将得到一个警告消息。\n\n\n# 分离数据库\n\nSQLite 的 DETACH DATABASE 语句是用来把命名数据库从一个数据库连接分离和游离出来，连接是之前使用 ATTACH 语句附加的。\n\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n2    test             /root/test.db\nsqlite> DETACH DATABASE \'test\';\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n\n\n\n# 备份数据库\n\n如下，备份 test 数据库到 /home/test.sql\n\nsqlite3 test.db .dump > /home/test.sql\n\n\n\n# 恢复数据库\n\n如下，根据 /home/test.sql 恢复 test 数据库\n\nsqlite3 test.db < test.sql\n\n\n\n# 数据类型\n\nSQLite 使用一个更普遍的动态类型系统。在 SQLite 中，值的数据类型与值本身是相关的，而不是与它的容器相关。\n\n\n# SQLite 存储类\n\n每个存储在 SQLite 数据库中的值都具有以下存储类之一：\n\n存储类       描述\nNULL      值是一个 NULL 值。\nINTEGER   值是一个带符号的整数，根据值的大小存储在 1、2、3、4、6 或 8 字节中。\nREAL      值是一个浮点值，存储为 8 字节的 IEEE 浮点数字。\nTEXT      值是一个文本字符串，使用数据库编码（UTF-8、UTF-16BE 或 UTF-16LE）存储。\nBLOB      值是一个 blob 数据，完全根据它的输入存储。\n\nSQLite 的存储类稍微比数据类型更普遍。INTEGER 存储类，例如，包含 6 种不同的不同长度的整数数据类型。\n\n\n# SQLite 亲和(Affinity)类型\n\nSQLite 支持列的亲和类型概念。任何列仍然可以存储任何类型的数据，当数据插入时，该字段的数据将会优先采用亲缘类型作为该值的存储方式。SQLite 目前的版本支持以下五种亲缘类型：\n\n亲和类型      描述\nTEXT      数值型数据在被插入之前，需要先被转换为文本格式，之后再插入到目标字段中。\nNUMERIC   当文本数据被插入到亲缘性为 NUMERIC 的字段中时，如果转换操作不会导致数据信息丢失以及完全可逆，那么 SQLite\n          就会将该文本数据转换为 INTEGER 或 REAL 类型的数据，如果转换失败，SQLite 仍会以 TEXT\n          方式存储该数据。对于 NULL 或 BLOB 类型的新数据，SQLite 将不做任何转换，直接以 NULL 或 BLOB\n          的方式存储该数据。需要额外说明的是，对于浮点格式的常量文本，如"30000.0"，如果该值可以转换为 INTEGER\n          同时又不会丢失数值信息，那么 SQLite 就会将其转换为 INTEGER 的存储方式。\nINTEGER   对于亲缘类型为 INTEGER 的字段，其规则等同于 NUMERIC，唯一差别是在执行 CAST 表达式时。\nREAL      其规则基本等同于 NUMERIC，唯一的差别是不会将"30000.0"这样的文本数据转换为 INTEGER 存储方式。\nNONE      不做任何的转换，直接以该数据所属的数据类型进行存储。\n\n\n# SQLite 亲和类型(Affinity)及类型名称\n\n下表列出了当创建 SQLite3 表时可使用的各种数据类型名称，同时也显示了相应的亲和类型：\n\n数据类型                                                           亲和类型\nINT, INTEGER, TINYINT, SMALLINT, MEDIUMINT, BIGINT, UNSIGNED   INTEGER\nBIG INT, INT2, INT8\nCHARACTER(20), VARCHAR(255), VARYING CHARACTER(255),           TEXT\nNCHAR(55), NATIVE CHARACTER(70), NVARCHAR(100), TEXT, CLOB\nBLOB, no datatype specified                                    NONE\nREAL, DOUBLE, DOUBLE PRECISION, FLOAT                          REAL\nNUMERIC, DECIMAL(10,5), BOOLEAN, DATE, DATETIME                NUMERIC\n\n\n# Boolean 数据类型\n\nSQLite 没有单独的 Boolean 存储类。相反，布尔值被存储为整数 0（false）和 1（true）。\n\n\n# Date 与 Time 数据类型\n\nSQLite 没有一个单独的用于存储日期和/或时间的存储类，但 SQLite 能够把日期和时间存储为 TEXT、REAL 或 INTEGER 值。\n\n存储类       日期格式\nTEXT      格式为 "YYYY-MM-DD HH:MM:SS.SSS" 的日期。\nREAL      从公元前 4714 年 11 月 24 日格林尼治时间的正午开始算起的天数。\nINTEGER   从 1970-01-01 00:00:00 UTC 算起的秒数。\n\n您可以以任何上述格式来存储日期和时间，并且可以使用内置的日期和时间函数来自由转换不同格式。\n\n\n# SQLite 命令\n\n\n# 快速开始\n\n# 进入 SQLite 控制台\n\n$ sqlite3\nSQLite version 3.7.17 2013-05-20 00:56:22\nEnter ".help" for instructions\nEnter SQL statements terminated with a ";"\nsqlite>\n\n\n# 进入 SQLite 控制台并指定数据库\n\n$ sqlite3 test.db\nSQLite version 3.7.17 2013-05-20 00:56:22\nEnter ".help" for instructions\nEnter SQL statements terminated with a ";"\nsqlite>\n\n\n# 退出 SQLite 控制台\n\nsqlite>.quit\n\n\n# 查看命令帮助\n\nsqlite>.help\n\n\n\n# 常用命令清单\n\n命令                      描述\n.backup ?DB? FILE       备份 DB 数据库（默认是 "main"）到 FILE 文件。\n.bail ON|OFF            发生错误后停止。默认为 OFF。\n.databases              列出数据库的名称及其所依附的文件。\n.dump ?TABLE?           以 SQL 文本格式转储数据库。如果指定了 TABLE 表，则只转储匹配 LIKE 模式的 TABLE 表。\n.echo ON|OFF            开启或关闭 echo 命令。\n.exit                   退出 SQLite 提示符。\n.explain ON|OFF         开启或关闭适合于 EXPLAIN 的输出模式。如果没有带参数，则为 EXPLAIN on，及开启 EXPLAIN。\n.header(s) ON|OFF       开启或关闭头部显示。\n.help                   显示消息。\n.import FILE TABLE      导入来自 FILE 文件的数据到 TABLE 表中。\n.indices ?TABLE?        显示所有索引的名称。如果指定了 TABLE 表，则只显示匹配 LIKE 模式的 TABLE 表的索引。\n.load FILE ?ENTRY?      加载一个扩展库。\n.log FILE|off           开启或关闭日志。FILE 文件可以是 stderr（标准错误）/stdout（标准输出）。\n.mode MODE              设置输出模式，MODE 可以是下列之一：csv 逗号分隔的值column 左对齐的列html HTML 的\n.nullvalue STRING       在 NULL 值的地方输出 STRING 字符串。\n.output FILENAME        发送输出到 FILENAME 文件。\n.output stdout          发送输出到屏幕。\n.print STRING...        逐字地输出 STRING 字符串。\n.prompt MAIN CONTINUE   替换标准提示符。\n.quit                   退出 SQLite 提示符。\n.read FILENAME          执行 FILENAME 文件中的 SQL。\n.schema ?TABLE?         显示 CREATE 语句。如果指定了 TABLE 表，则只显示匹配 LIKE 模式的 TABLE 表。\n.separator STRING       改变输出模式和 .import 所使用的分隔符。\n.show                   显示各种设置的当前值。\n.stats ON|OFF           开启或关闭统计。\n.tables ?PATTERN?       列出匹配 LIKE 模式的表的名称。\n.timeout MS             尝试打开锁定的表 MS 毫秒。\n.width NUM NUM          为 "column" 模式设置列宽度。\n.timer ON|OFF           开启或关闭 CPU 定时器。\n\n\n# 实战\n\n# 格式化输出\n\nsqlite>.header on\nsqlite>.mode column\nsqlite>.timer on\nsqlite>\n\n\n# 输出结果到文件\n\nsqlite> .mode list\nsqlite> .separator |\nsqlite> .output teyptest_file_1.txt\nsqlite> select * from tbl1;\nsqlite> .exit\n$ cat test_file_1.txt\nhello|10\ngoodbye|20\n$\n\n\n\n# JAVA Client\n\n（1）在官方下载地址下载 sqlite-jdbc-(VERSION).jar ，然后将 jar 包放在项目中的 classpath。\n\n（2）通过 API 打开一个 SQLite 数据库连接。\n\n执行方法：\n\n> javac Sample.java\n> java -classpath ".;sqlite-jdbc-(VERSION).jar" Sample   # in Windows\nor\n> java -classpath ".:sqlite-jdbc-(VERSION).jar" Sample   # in Mac or Linux\nname = leo\nid = 1\nname = yui\nid = 2\n\n\n示例：\n\npublic class Sample {\n    public static void main(String[] args) {\n        Connection connection = null;\n        try {\n            // 创建数据库连接\n            connection = DriverManager.getConnection("jdbc:sqlite:sample.db");\n            Statement statement = connection.createStatement();\n            statement.setQueryTimeout(30);  // 设置 sql 执行超时时间为 30s\n\n            statement.executeUpdate("drop table if exists person");\n            statement.executeUpdate("create table person (id integer, name string)");\n            statement.executeUpdate("insert into person values(1, \'leo\')");\n            statement.executeUpdate("insert into person values(2, \'yui\')");\n            ResultSet rs = statement.executeQuery("select * from person");\n            while (rs.next()) {\n                // 读取结果集\n                System.out.println("name = " + rs.getString("name"));\n                System.out.println("id = " + rs.getInt("id"));\n            }\n        } catch (SQLException e) {\n            // 如果错误信息是 "out of memory"，可能是找不到数据库文件\n            System.err.println(e.getMessage());\n        } finally {\n            try {\n                if (connection != null) {\n                    connection.close();\n                }\n            } catch (SQLException e) {\n                // 关闭连接失败\n                System.err.println(e.getMessage());\n            }\n        }\n    }\n}\n\n\n\n# 如何指定数据库文件\n\nWindows\n\nConnection connection = DriverManager.getConnection("jdbc:sqlite:C:/work/mydatabase.db");\n\n\nUnix (Linux, Mac OS X, etc)\n\nConnection connection = DriverManager.getConnection("jdbc:sqlite:/home/leo/work/mydatabase.db");\n\n\n\n# 如何使用内存数据库\n\nConnection connection = DriverManager.getConnection("jdbc:sqlite::memory:");\n\n\n\n# 参考资料\n\n * SQLite 官网\n * SQLite 官方文档\n * SQLite 官方命令行手册\n * http://www.runoob.com/sqlite/sqlite-commands.html\n * https://github.com/xerial/sqlite-jdbc\n * http://www.runoob.com/sqlite/sqlite-java.html\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |',normalizedContent:'# sqlite\n\n> sqlite 是一个实现了自给自足的、无服务器的、零配置的、事务性的 sql 数据库引擎。 👉 完整示例源码\n\n\n# 简介\n\n\n# 优点\n\n * sqlite 是自给自足的，这意味着不需要任何外部的依赖。\n * sqlite 是无服务器的、零配置的，这意味着不需要安装或管理。\n * sqlite 事务是完全兼容 acid 的，允许从多个进程或线程安全访问。\n * sqlite 是非常小的，是轻量级的，完全配置时小于 400kib，省略可选功能配置时小于 250kib。\n * sqlite 支持 sql92（sql2）标准的大多数查询语言的功能。\n * 一个完整的 sqlite 数据库是存储在一个单一的跨平台的磁盘文件。\n * sqlite 使用 ansi-c 编写的，并提供了简单和易于使用的 api。\n * sqlite 可在 unix（linux, mac os-x, android, ios）和 windows（win32, wince, winrt）中运行。\n\n\n# 局限\n\n特性                 描述\nright outer join   只实现了 left outer join。\nfull outer join    只实现了 left outer join。\nalter table        支持 rename table 和 alter table 的 add column variants 命令，不支持\n                   drop column、alter column、add constraint。\ntrigger 支持         支持 for each row 触发器，但不支持 for each statement 触发器。\nviews              在 sqlite 中，视图是只读的。您不可以在视图上执行 delete、insert 或 update 语句。\ngrant 和 revoke     可以应用的唯一的访问权限是底层操作系统的正常文件访问权限。\n\n\n# 安装\n\nsqlite 可在 unix（linux, mac os-x, android, ios）和 windows（win32, wince, winrt）中运行。\n\n一般，linux 和 mac 上会预安装 sqlite。如果没有安装，可以在官方下载地址下载合适安装版本，自行安装。\n\n\n# 语法\n\n> 这里不会详细列举所有 sql 语法，仅列举 sqlite 除标准 sql 以外的，一些自身特殊的 sql 语法。\n> \n> 👉 扩展阅读：标准 sql 基本语法\n\n\n# 大小写敏感\n\nsqlite 是不区分大小写的，但也有一些命令是大小写敏感的，比如 glob 和 glob 在 sqlite 的语句中有不同的含义。\n\n\n# 注释\n\n-- 单行注释\n/*\n 多行注释1\n 多行注释2\n */\n\n\n\n# 创建数据库\n\n如下，创建一个名为 test 的数据库：\n\n$ sqlite3 test.db\nsqlite version 3.7.17 2013-05-20 00:56:22\nenter ".help" for instructions\nenter sql statements terminated with a ";"\n\n\n\n# 查看数据库\n\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n\n\n\n# 退出数据库\n\nsqlite> .quit\n\n\n\n# 附加数据库\n\n假设这样一种情况，当在同一时间有多个数据库可用，您想使用其中的任何一个。\n\nsqlite 的 attach database 语句是用来选择一个特定的数据库，使用该命令后，所有的 sqlite 语句将在附加的数据库下执行。\n\nsqlite> attach database \'test.db\' as \'test\';\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n2    test             /root/test.db\n\n\n> 🔔 注意：数据库名 main 和 temp 被保留用于主数据库和存储临时表及其他临时数据对象的数据库。这两个数据库名称可用于每个数据库连接，且不应该被用于附加，否则将得到一个警告消息。\n\n\n# 分离数据库\n\nsqlite 的 detach database 语句是用来把命名数据库从一个数据库连接分离和游离出来，连接是之前使用 attach 语句附加的。\n\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n2    test             /root/test.db\nsqlite> detach database \'test\';\nsqlite> .databases\nseq  name             file\n---  ---------------  ----------------------------------------------------------\n0    main             /root/test.db\n\n\n\n# 备份数据库\n\n如下，备份 test 数据库到 /home/test.sql\n\nsqlite3 test.db .dump > /home/test.sql\n\n\n\n# 恢复数据库\n\n如下，根据 /home/test.sql 恢复 test 数据库\n\nsqlite3 test.db < test.sql\n\n\n\n# 数据类型\n\nsqlite 使用一个更普遍的动态类型系统。在 sqlite 中，值的数据类型与值本身是相关的，而不是与它的容器相关。\n\n\n# sqlite 存储类\n\n每个存储在 sqlite 数据库中的值都具有以下存储类之一：\n\n存储类       描述\nnull      值是一个 null 值。\ninteger   值是一个带符号的整数，根据值的大小存储在 1、2、3、4、6 或 8 字节中。\nreal      值是一个浮点值，存储为 8 字节的 ieee 浮点数字。\ntext      值是一个文本字符串，使用数据库编码（utf-8、utf-16be 或 utf-16le）存储。\nblob      值是一个 blob 数据，完全根据它的输入存储。\n\nsqlite 的存储类稍微比数据类型更普遍。integer 存储类，例如，包含 6 种不同的不同长度的整数数据类型。\n\n\n# sqlite 亲和(affinity)类型\n\nsqlite 支持列的亲和类型概念。任何列仍然可以存储任何类型的数据，当数据插入时，该字段的数据将会优先采用亲缘类型作为该值的存储方式。sqlite 目前的版本支持以下五种亲缘类型：\n\n亲和类型      描述\ntext      数值型数据在被插入之前，需要先被转换为文本格式，之后再插入到目标字段中。\nnumeric   当文本数据被插入到亲缘性为 numeric 的字段中时，如果转换操作不会导致数据信息丢失以及完全可逆，那么 sqlite\n          就会将该文本数据转换为 integer 或 real 类型的数据，如果转换失败，sqlite 仍会以 text\n          方式存储该数据。对于 null 或 blob 类型的新数据，sqlite 将不做任何转换，直接以 null 或 blob\n          的方式存储该数据。需要额外说明的是，对于浮点格式的常量文本，如"30000.0"，如果该值可以转换为 integer\n          同时又不会丢失数值信息，那么 sqlite 就会将其转换为 integer 的存储方式。\ninteger   对于亲缘类型为 integer 的字段，其规则等同于 numeric，唯一差别是在执行 cast 表达式时。\nreal      其规则基本等同于 numeric，唯一的差别是不会将"30000.0"这样的文本数据转换为 integer 存储方式。\nnone      不做任何的转换，直接以该数据所属的数据类型进行存储。\n\n\n# sqlite 亲和类型(affinity)及类型名称\n\n下表列出了当创建 sqlite3 表时可使用的各种数据类型名称，同时也显示了相应的亲和类型：\n\n数据类型                                                           亲和类型\nint, integer, tinyint, smallint, mediumint, bigint, unsigned   integer\nbig int, int2, int8\ncharacter(20), varchar(255), varying character(255),           text\nnchar(55), native character(70), nvarchar(100), text, clob\nblob, no datatype specified                                    none\nreal, double, double precision, float                          real\nnumeric, decimal(10,5), boolean, date, datetime                numeric\n\n\n# boolean 数据类型\n\nsqlite 没有单独的 boolean 存储类。相反，布尔值被存储为整数 0（false）和 1（true）。\n\n\n# date 与 time 数据类型\n\nsqlite 没有一个单独的用于存储日期和/或时间的存储类，但 sqlite 能够把日期和时间存储为 text、real 或 integer 值。\n\n存储类       日期格式\ntext      格式为 "yyyy-mm-dd hh:mm:ss.sss" 的日期。\nreal      从公元前 4714 年 11 月 24 日格林尼治时间的正午开始算起的天数。\ninteger   从 1970-01-01 00:00:00 utc 算起的秒数。\n\n您可以以任何上述格式来存储日期和时间，并且可以使用内置的日期和时间函数来自由转换不同格式。\n\n\n# sqlite 命令\n\n\n# 快速开始\n\n# 进入 sqlite 控制台\n\n$ sqlite3\nsqlite version 3.7.17 2013-05-20 00:56:22\nenter ".help" for instructions\nenter sql statements terminated with a ";"\nsqlite>\n\n\n# 进入 sqlite 控制台并指定数据库\n\n$ sqlite3 test.db\nsqlite version 3.7.17 2013-05-20 00:56:22\nenter ".help" for instructions\nenter sql statements terminated with a ";"\nsqlite>\n\n\n# 退出 sqlite 控制台\n\nsqlite>.quit\n\n\n# 查看命令帮助\n\nsqlite>.help\n\n\n\n# 常用命令清单\n\n命令                      描述\n.backup ?db? file       备份 db 数据库（默认是 "main"）到 file 文件。\n.bail on|off            发生错误后停止。默认为 off。\n.databases              列出数据库的名称及其所依附的文件。\n.dump ?table?           以 sql 文本格式转储数据库。如果指定了 table 表，则只转储匹配 like 模式的 table 表。\n.echo on|off            开启或关闭 echo 命令。\n.exit                   退出 sqlite 提示符。\n.explain on|off         开启或关闭适合于 explain 的输出模式。如果没有带参数，则为 explain on，及开启 explain。\n.header(s) on|off       开启或关闭头部显示。\n.help                   显示消息。\n.import file table      导入来自 file 文件的数据到 table 表中。\n.indices ?table?        显示所有索引的名称。如果指定了 table 表，则只显示匹配 like 模式的 table 表的索引。\n.load file ?entry?      加载一个扩展库。\n.log file|off           开启或关闭日志。file 文件可以是 stderr（标准错误）/stdout（标准输出）。\n.mode mode              设置输出模式，mode 可以是下列之一：csv 逗号分隔的值column 左对齐的列html html 的\n.nullvalue string       在 null 值的地方输出 string 字符串。\n.output filename        发送输出到 filename 文件。\n.output stdout          发送输出到屏幕。\n.print string...        逐字地输出 string 字符串。\n.prompt main continue   替换标准提示符。\n.quit                   退出 sqlite 提示符。\n.read filename          执行 filename 文件中的 sql。\n.schema ?table?         显示 create 语句。如果指定了 table 表，则只显示匹配 like 模式的 table 表。\n.separator string       改变输出模式和 .import 所使用的分隔符。\n.show                   显示各种设置的当前值。\n.stats on|off           开启或关闭统计。\n.tables ?pattern?       列出匹配 like 模式的表的名称。\n.timeout ms             尝试打开锁定的表 ms 毫秒。\n.width num num          为 "column" 模式设置列宽度。\n.timer on|off           开启或关闭 cpu 定时器。\n\n\n# 实战\n\n# 格式化输出\n\nsqlite>.header on\nsqlite>.mode column\nsqlite>.timer on\nsqlite>\n\n\n# 输出结果到文件\n\nsqlite> .mode list\nsqlite> .separator |\nsqlite> .output teyptest_file_1.txt\nsqlite> select * from tbl1;\nsqlite> .exit\n$ cat test_file_1.txt\nhello|10\ngoodbye|20\n$\n\n\n\n# java client\n\n（1）在官方下载地址下载 sqlite-jdbc-(version).jar ，然后将 jar 包放在项目中的 classpath。\n\n（2）通过 api 打开一个 sqlite 数据库连接。\n\n执行方法：\n\n> javac sample.java\n> java -classpath ".;sqlite-jdbc-(version).jar" sample   # in windows\nor\n> java -classpath ".:sqlite-jdbc-(version).jar" sample   # in mac or linux\nname = leo\nid = 1\nname = yui\nid = 2\n\n\n示例：\n\npublic class sample {\n    public static void main(string[] args) {\n        connection connection = null;\n        try {\n            // 创建数据库连接\n            connection = drivermanager.getconnection("jdbc:sqlite:sample.db");\n            statement statement = connection.createstatement();\n            statement.setquerytimeout(30);  // 设置 sql 执行超时时间为 30s\n\n            statement.executeupdate("drop table if exists person");\n            statement.executeupdate("create table person (id integer, name string)");\n            statement.executeupdate("insert into person values(1, \'leo\')");\n            statement.executeupdate("insert into person values(2, \'yui\')");\n            resultset rs = statement.executequery("select * from person");\n            while (rs.next()) {\n                // 读取结果集\n                system.out.println("name = " + rs.getstring("name"));\n                system.out.println("id = " + rs.getint("id"));\n            }\n        } catch (sqlexception e) {\n            // 如果错误信息是 "out of memory"，可能是找不到数据库文件\n            system.err.println(e.getmessage());\n        } finally {\n            try {\n                if (connection != null) {\n                    connection.close();\n                }\n            } catch (sqlexception e) {\n                // 关闭连接失败\n                system.err.println(e.getmessage());\n            }\n        }\n    }\n}\n\n\n\n# 如何指定数据库文件\n\nwindows\n\nconnection connection = drivermanager.getconnection("jdbc:sqlite:c:/work/mydatabase.db");\n\n\nunix (linux, mac os x, etc)\n\nconnection connection = drivermanager.getconnection("jdbc:sqlite:/home/leo/work/mydatabase.db");\n\n\n\n# 如何使用内存数据库\n\nconnection connection = drivermanager.getconnection("jdbc:sqlite::memory:");\n\n\n\n# 参考资料\n\n * sqlite 官网\n * sqlite 官方文档\n * sqlite 官方命令行手册\n * http://www.runoob.com/sqlite/sqlite-commands.html\n * https://github.com/xerial/sqlite-jdbc\n * http://www.runoob.com/sqlite/sqlite-java.html\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"关系型数据库其他知识",frontmatter:{title:"关系型数据库其他知识",date:"2022-04-11T16:52:35.000Z",categories:["数据库","关系型数据库","其他"],tags:["数据库","关系型数据库"],permalink:"/pages/ca9888/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/99.%E5%85%B6%E4%BB%96/",relativePath:"12.数据库/03.关系型数据库/99.其他/README.md",key:"v-19684eb4",path:"/pages/ca9888/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:17},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:73},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:83}],headersStr:"📖 内容 📚 资料 🚪 传送",content:"# 关系型数据库其他知识\n\n\n# 📖 内容\n\n * PostgreSQL 应用指南\n * H2 应用指南\n * SqLite 应用指南\n\n\n# 📚 资料\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# 关系型数据库其他知识\n\n\n# 📖 内容\n\n * postgresql 应用指南\n * h2 应用指南\n * sqlite 应用指南\n\n\n# 📚 资料\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"关系型数据库",frontmatter:{title:"关系型数据库",date:"2022-04-11T16:52:35.000Z",categories:["数据库","关系型数据库"],tags:["数据库","关系型数据库"],permalink:"/pages/bb43eb/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/03.%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/",relativePath:"12.数据库/03.关系型数据库/README.md",key:"v-44b09ccb",path:"/pages/bb43eb/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:13},{level:3,title:"公共知识",slug:"公共知识",normalizedTitle:"公共知识",charIndex:23},{level:3,title:"Mysql",slug:"mysql",normalizedTitle:"mysql",charIndex:107},{level:3,title:"其他",slug:"其他",normalizedTitle:"其他",charIndex:387},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:440},{level:3,title:"综合",slug:"综合",normalizedTitle:"综合",charIndex:450},{level:3,title:"Mysql",slug:"mysql-2",normalizedTitle:"mysql",charIndex:107},{level:3,title:"其他",slug:"其他-2",normalizedTitle:"其他",charIndex:387},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:793}],headersStr:"📖 内容 公共知识 Mysql 其他 📚 资料 综合 Mysql 其他 🚪 传送",content:"# 关系型数据库\n\n\n# 📖 内容\n\n\n# 公共知识\n\n * 关系型数据库面试总结 💯\n * SQL Cheat Sheet 是一个 SQL 入门教程。\n * 扩展 SQL 是一个 SQL 入门教程。\n\n\n# Mysql\n\n\n\n * Mysql 应用指南 ⚡\n * Mysql 工作流 - 关键词：连接、缓存、语法分析、优化、执行引擎、redo log、bin log、两阶段提交\n * Mysql 事务 - 关键词：ACID、AUTOCOMMIT、事务隔离级别、死锁、分布式事务\n * Mysql 锁 - 关键词：乐观锁、表级锁、行级锁、意向锁、MVCC、Next-key 锁\n * Mysql 索引 - 关键词：Hash、B 树、聚簇索引、回表\n * Mysql 性能优化\n * Mysql 运维 🔨\n * Mysql 配置 🔨\n * Mysql 问题\n\n\n# 其他\n\n * PostgreSQL 应用指南\n * H2 应用指南\n * SqLite 应用指南\n\n\n# 📚 资料\n\n\n# 综合\n\n * 《数据库的索引设计与优化》\n * 《SQL 必知必会》 - SQL 的基本概念和语法【入门】\n\n\n# Mysql\n\n * 官方\n   * Mysql 官网\n   * Mysql 官方文档\n   * Mysql 官方文档之命令行客户端\n * 书籍\n   * 《高性能 MySQL》 - 经典，适合 DBA 或作为开发者的参考手册\n   * 《MySQL 必知必会》 - 适合入门者\n * 教程\n   * MySQL 实战 45 讲\n   * runoob.com MySQL 教程\n   * mysql-tutorial\n * 更多资源\n   * awesome-mysql\n\n\n# 其他\n\n * 《Oracle Database 9i/10g/11g 编程艺术》\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# 关系型数据库\n\n\n# 📖 内容\n\n\n# 公共知识\n\n * 关系型数据库面试总结 💯\n * sql cheat sheet 是一个 sql 入门教程。\n * 扩展 sql 是一个 sql 入门教程。\n\n\n# mysql\n\n\n\n * mysql 应用指南 ⚡\n * mysql 工作流 - 关键词：连接、缓存、语法分析、优化、执行引擎、redo log、bin log、两阶段提交\n * mysql 事务 - 关键词：acid、autocommit、事务隔离级别、死锁、分布式事务\n * mysql 锁 - 关键词：乐观锁、表级锁、行级锁、意向锁、mvcc、next-key 锁\n * mysql 索引 - 关键词：hash、b 树、聚簇索引、回表\n * mysql 性能优化\n * mysql 运维 🔨\n * mysql 配置 🔨\n * mysql 问题\n\n\n# 其他\n\n * postgresql 应用指南\n * h2 应用指南\n * sqlite 应用指南\n\n\n# 📚 资料\n\n\n# 综合\n\n * 《数据库的索引设计与优化》\n * 《sql 必知必会》 - sql 的基本概念和语法【入门】\n\n\n# mysql\n\n * 官方\n   * mysql 官网\n   * mysql 官方文档\n   * mysql 官方文档之命令行客户端\n * 书籍\n   * 《高性能 mysql》 - 经典，适合 dba 或作为开发者的参考手册\n   * 《mysql 必知必会》 - 适合入门者\n * 教程\n   * mysql 实战 45 讲\n   * runoob.com mysql 教程\n   * mysql-tutorial\n * 更多资源\n   * awesome-mysql\n\n\n# 其他\n\n * 《oracle database 9i/10g/11g 编程艺术》\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 应用指南",frontmatter:{title:"MongoDB 应用指南",date:"2020-09-07T07:54:19.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB"],permalink:"/pages/3288f3/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/01.MongoDB%E5%BA%94%E7%94%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/01.MongoDB应用指南.md",key:"v-62392692",path:"/pages/3288f3/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:19},{level:3,title:"MongoDB 发展",slug:"mongodb-发展",normalizedTitle:"mongodb 发展",charIndex:178},{level:3,title:"MongoDB 和 RDBMS",slug:"mongodb-和-rdbms",normalizedTitle:"mongodb 和 rdbms",charIndex:273},{level:3,title:"MongoDB 特性",slug:"mongodb-特性",normalizedTitle:"mongodb 特性",charIndex:652},{level:2,title:"MongoDB 概念",slug:"mongodb-概念",normalizedTitle:"mongodb 概念",charIndex:748},{level:3,title:"数据库",slug:"数据库",normalizedTitle:"数据库",charIndex:44},{level:3,title:"文档",slug:"文档",normalizedTitle:"文档",charIndex:105},{level:3,title:"集合",slug:"集合",normalizedTitle:"集合",charIndex:865},{level:3,title:"元数据",slug:"元数据",normalizedTitle:"元数据",charIndex:2594},{level:2,title:"MongoDB 数据类型",slug:"mongodb-数据类型",normalizedTitle:"mongodb 数据类型",charIndex:3092},{level:2,title:"MongoDB CRUD",slug:"mongodb-crud",normalizedTitle:"mongodb crud",charIndex:3848},{level:3,title:"数据库操作",slug:"数据库操作",normalizedTitle:"数据库操作",charIndex:3865},{level:4,title:"查看所有数据库",slug:"查看所有数据库",normalizedTitle:"查看所有数据库",charIndex:3874},{level:4,title:"创建数据库",slug:"创建数据库",normalizedTitle:"创建数据库",charIndex:3896},{level:4,title:"删除数据库",slug:"删除数据库",normalizedTitle:"删除数据库",charIndex:4249},{level:3,title:"集合操作",slug:"集合操作",normalizedTitle:"集合操作",charIndex:4288},{level:4,title:"查看集合",slug:"查看集合",normalizedTitle:"查看集合",charIndex:4296},{level:4,title:"创建集合",slug:"创建集合",normalizedTitle:"创建集合",charIndex:4323},{level:4,title:"删除集合",slug:"删除集合",normalizedTitle:"删除集合",charIndex:4899},{level:3,title:"插入文档操作",slug:"插入文档操作",normalizedTitle:"插入文档操作",charIndex:4959},{level:3,title:"查询文档操作",slug:"查询文档操作",normalizedTitle:"查询文档操作",charIndex:5539},{level:4,title:"查询条件",slug:"查询条件",normalizedTitle:"查询条件",charIndex:5607},{level:4,title:"查询逻辑条件",slug:"查询逻辑条件",normalizedTitle:"查询逻辑条件",charIndex:6768},{level:4,title:"模糊查询",slug:"模糊查询",normalizedTitle:"模糊查询",charIndex:7058},{level:4,title:"Limit() 方法",slug:"limit-方法",normalizedTitle:"limit() 方法",charIndex:7228},{level:4,title:"Skip() 方法",slug:"skip-方法",normalizedTitle:"skip() 方法",charIndex:7401},{level:4,title:"Sort() 方法",slug:"sort-方法",normalizedTitle:"sort() 方法",charIndex:7574},{level:3,title:"更新文档操作",slug:"更新文档操作",normalizedTitle:"更新文档操作",charIndex:7835},{level:3,title:"删除文档操作",slug:"删除文档操作",normalizedTitle:"删除文档操作",charIndex:9212},{level:3,title:"索引操作",slug:"索引操作",normalizedTitle:"索引操作",charIndex:9399},{level:2,title:"MongoDB 聚合操作",slug:"mongodb-聚合操作",normalizedTitle:"mongodb 聚合操作",charIndex:10938},{level:3,title:"管道",slug:"管道",normalizedTitle:"管道",charIndex:11036},{level:3,title:"聚合步骤",slug:"聚合步骤",normalizedTitle:"聚合步骤",charIndex:11228},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:13583}],headersStr:"简介 MongoDB 发展 MongoDB 和 RDBMS MongoDB 特性 MongoDB 概念 数据库 文档 集合 元数据 MongoDB 数据类型 MongoDB CRUD 数据库操作 查看所有数据库 创建数据库 删除数据库 集合操作 查看集合 创建集合 删除集合 插入文档操作 查询文档操作 查询条件 查询逻辑条件 模糊查询 Limit() 方法 Skip() 方法 Sort() 方法 更新文档操作 删除文档操作 索引操作 MongoDB 聚合操作 管道 聚合步骤 参考资料",content:'# MongoDB 应用指南\n\n\n# 简介\n\nMongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\n\nMongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。\n\n\n# MongoDB 发展\n\n * 1.x - 支持复制和分片\n * 2.x - 更丰富的数据库功能\n * 3.x - WiredTiger 和周边生态\n * 4.x - 支持分布式事务\n\n\n# MongoDB 和 RDBMS\n\n特性        MONGODB                       RDBMS\n数据模型      文档模型                          关系型\nCRUD 操作   MQL/SQL                       SQL\n高可用       复制集                           集群模式\n扩展性       支持分片                          数据分区\n扩繁方式      垂直扩展+水平扩展                     垂直扩展\n索引类型      B 树、全文索引、地理位置索引、多键索引、TTL 索引   B 树\n数据容量      没有理论上限                        千万、亿\n\n\n# MongoDB 特性\n\n * 数据是 JSON 结构\n   * 支持结构化、半结构化数据模型\n   * 可以动态响应结构变化\n * 通过副本机制提供高可用\n * 通过分片提供扩容能力\n\n\n# MongoDB 概念\n\nSQL 术语/概念     MONGODB 术语/概念   解释/说明\ndatabase      database        数据库\ntable         collection      数据库表/集合\nrow           document        数据记录行/文档\ncolumn        field           数据字段/域\nindex         index           索引\ntable joins                   表连接,MongoDB 不支持\nprimary key   primary key     主键,MongoDB 自动将_id 字段设置为主键\n\n\n# 数据库\n\n一个 MongoDB 中可以建立多个数据库。\n\nMongoDB 的默认数据库为"db"，该数据库存储在 data 目录中。\n\nMongoDB 的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。\n\n"show dbs" 命令可以显示所有数据的列表。\n\n$ ./mongo\nMongoDBshell version: 3.0.6\nconnecting to: test\n> show dbs\nlocal  0.078GB\ntest   0.078GB\n>\n\n\n执行 "db" 命令可以显示当前数据库对象或集合。\n\n$ ./mongo\nMongoDBshell version: 3.0.6\nconnecting to: test\n> db\ntest\n>\n\n\n运行"use"命令，可以连接到一个指定的数据库。\n\n> use local\nswitched to db local\n> db\nlocal\n>\n\n\n数据库也通过名字来标识。数据库名可以是满足以下条件的任意 UTF-8 字符串。\n\n * 不能是空字符串（"")。\n * 不得含有 \' \'（空格)、.、\\$、/、\\和 \\0 (空字符)。\n * 应全部小写。\n * 最多 64 字节。\n\n有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。\n\n * admin：从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。\n * local：这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合\n * config：当 Mongo 用于分片设置时，config 数据库在内部使用，用于保存分片的相关信息。\n\n\n# 文档\n\n文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。\n\n需要注意的是：\n\n * 文档中的键/值对是有序的。\n * 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。\n * MongoDB 区分类型和大小写。\n * MongoDB 的文档不能有重复的键。\n * 文档的键是字符串。除了少数例外情况，键可以使用任意 UTF-8 字符。\n\n文档键命名规范：\n\n * 键不能含有 \\0 (空字符)。这个字符用来表示键的结尾。\n * . 和 $ 有特别的意义，只有在特定环境下才能使用。\n * 以下划线 _ 开头的键是保留的(不是严格要求的)。\n\n\n# 集合\n\n集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。\n\n集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。\n\n合法的集合名：\n\n * 集合名不能是空字符串""。\n * 集合名不能含有 \\0 字符（空字符)，这个字符表示集合名的结尾。\n * 集合名不能以"system."开头，这是为系统集合保留的前缀。\n * 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现 $。\n\n\n# 元数据\n\n数据库的信息是存储在集合中。它们使用了系统的命名空间：dbname.system.*\n\n在 MongoDB 数据库中名字空间 <dbname>.system.* 是包含多种系统信息的特殊集合(Collection)，如下:\n\n集合命名空间                     描述\ndbname.system.namespaces   列出所有名字空间。\ndbname.system.indexes      列出所有索引。\ndbname.system.profile      包含数据库概要(profile)信息。\ndbname.system.users        列出所有可访问数据库的用户。\ndbname.local.sources       包含复制对端（slave）的服务器信息和状态。\n\n对于修改系统集合中的对象有如下限制。\n\n在 system.indexes 插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的 drop index 命令将自动更新相关信息)。system.users 是可修改的。system.profile 是可删除的。\n\n\n# MongoDB 数据类型\n\n数据类型                 描述\nString               字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。\nInteger              整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。\nBoolean              布尔值。用于存储布尔值（真/假）。\nDouble               双精度浮点值。用于存储浮点值。\nMin/Max keys         将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。\nArray                用于将数组或列表或多个值存储为一个键。\nTimestamp            时间戳。记录文档修改或添加的具体时间。\nObject               用于内嵌文档。\nNull                 用于创建空值。\nSymbol               符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。\nDate                 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。\nObject ID            对象 ID。用于创建文档的 ID。\nBinary Data          二进制数据。用于存储二进制数据。\nCode                 代码类型。用于在文档中存储 JavaScript 代码。\nRegular expression   正则表达式类型。用于存储正则表达式。\n\n\n# MongoDB CRUD\n\n\n# 数据库操作\n\n# 查看所有数据库\n\nshow dbs\n\n\n# 创建数据库\n\nuse <database>\n\n\n如果数据库不存在，则创建数据库，否则切换到指定数据库。\n\n【示例】创建数据库，并插入一条数据\n\n刚创建的数据库 test 并不在数据库的列表中， 要显示它，需要插入一些数据\n\n> use test\nswitched to db test\n>\n> show dbs\nadmin   0.000GB\nconfig  0.000GB\nlocal   0.000GB\n> db.test.insert({"name":"mongodb"})\nWriteResult({ "nInserted" : 1 })\n> show dbs\nadmin   0.000GB\nconfig  0.000GB\nlocal   0.000GB\ntest    0.000GB\n\n\n# 删除数据库\n\n删除当前数据库\n\ndb.dropDatabase()\n\n\n\n# 集合操作\n\n# 查看集合\n\nshow collections\n\n\n# 创建集合\n\ndb.createCollection(name, options)\n\n\n参数说明：\n\n * name: 要创建的集合名称\n * options: 可选参数, 指定有关内存大小及索引的选项\n\noptions 可以是如下参数：\n\n字段            类型   描述\ncapped        布尔   （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。\n                   当该值为 true 时，必须指定 size 参数。\nautoIndexId   布尔   3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。\nsize          数值   （可选）为固定集合指定一个最大值，即字节数。 如果 capped 为 true，也需要指定该字段。\nmax           数值   （可选）指定固定集合中包含文档的最大数量。\n\n在插入文档时，MongoDB 首先检查固定集合的 size 字段，然后检查 max 字段。\n\n> db.createCollection("collection")\n{ "ok" : 1 }\n> show collections\ncollection\n\n\n# 删除集合\n\n> db.collection.drop()\ntrue\n> show collections\n>\n\n\n\n# 插入文档操作\n\nMongoDB 使用 insert() 方法完成插入操作。\n\n语法格式\n\n# 插入单条记录\ndb.<集合>.insertOne(<JSON>)\n# 插入多条记录\ndb.<集合>.insertMany([<JSON 1>, <JSON 2>, ..., <JSON N>])\n\n\n【示例】insertOne\n\n> db.color.insertOne({name: "red"})\n{\n        "acknowledged" : true,\n        "insertedId" : ObjectId("5f533ae4e8f16647950fdf43")\n}\n\n\n【示例】insertMany\n\n> db.color.insertMany([\n  {\n    "name": "yellow"\n  },\n  {\n    "name": "blue"\n  }\n])\n{\n        "acknowledged" : true,\n        "insertedIds" : [\n                ObjectId("5f533bcae8f16647950fdf44"),\n                ObjectId("5f533bcae8f16647950fdf45")\n        ]\n}\n>\n\n\n\n# 查询文档操作\n\nMongoDB 使用 find() 方法完成查询文档操作。\n\n语法格式\n\ndb.<集合>.find(<JSON>)\n\n\n查询条件也是 json 形式，如果不设置查询条件，即为全量查询。\n\n# 查询条件\n\n操作                      格式                                      范例                                         RDBMS 中的类似语句\n等于                      {<key>:<value>}                         db.book.find({"pageCount": {$eq: 0}})      where pageCount = 0\n不等于                     {<key>:{$ne:<value>}}                   db.book.find({"pageCount": {$ne: 0}})      where likes != 50\n大于                      {<key>:{$gt:<value>}}                   db.book.find({"pageCount": {$gt: 0}})      where likes > 50\n{<key>:{$gt:<value>}}   db.book.find({"pageCount": {$gt: 0}})   where likes > 50                           大于或等于\n小于                      {<key>:{$lt:<value>}}                   db.book.find({"pageCount": {$lt: 200}})    where likes < 50\n小于或等于                   {<key>:{$lte:<value>}}                  db.book.find({"pageCount": {$lte: 200}})   where likes <= 50\n\n> 说明：\n> \n> $eq  --------  equal  =\n> $ne ----------- not equal  !=\n> $gt -------- greater than  >\n> $gte --------- gt equal  >=\n> $lt -------- less than  <\n> $lte --------- lt equal  <=\n\n【示例】\n\n\n\n# 统计匹配查询条件的记录数\n> db.book.find({"status": "MEAP"}).count()\n68\n\n\n# 查询逻辑条件\n\n（1）and 条件\n\nMongoDB 的 find() 方法可以传入多个键(key)，每个键(key)以逗号隔开，即常规 SQL 的 AND 条件。\n\n语法格式如下：\n\n> db.col.find({key1:value1, key2:value2}).pretty()\n\n\n（2）or 条件\n\nMongoDB OR 条件语句使用了关键字 $or,语法格式如下：\n\n>db.col.find(\n   {\n      $or: [\n         {key1: value1}, {key2:value2}\n      ]\n   }\n).pretty()\n\n\n# 模糊查询\n\n查询 title 包含"教"字的文档：\n\ndb.col.find({ title: /教/ })\n\n\n查询 title 字段以"教"字开头的文档：\n\ndb.col.find({ title: /^教/ })\n\n\n查询 titl e 字段以"教"字结尾的文档：\n\ndb.col.find({ title: /教$/ })\n\n\n# Limit() 方法\n\n如果你需要在 MongoDB 中读取指定数量的数据记录，可以使用 MongoDB 的 Limit 方法，limit()方法接受一个数字参数，该参数指定从 MongoDB 中读取的记录条数。\n\nlimit()方法基本语法如下所示：\n\n>db.COLLECTION_NAME.find().limit(NUMBER)\n\n\n# Skip() 方法\n\n我们除了可以使用 limit()方法来读取指定数量的数据外，还可以使用 skip()方法来跳过指定数量的数据，skip 方法同样接受一个数字参数作为跳过的记录条数。\n\nskip() 方法脚本语法格式如下：\n\n>db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER)\n\n\n# Sort() 方法\n\n在 MongoDB 中使用 sort() 方法对数据进行排序，sort() 方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而 -1 是用于降序排列。\n\nsort()方法基本语法如下所示：\n\n>db.COLLECTION_NAME.find().sort({KEY:1})\n\n\n> 注意：skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()。\n\n\n# 更新文档操作\n\nupdate() 方法用于更新已存在的文档。语法格式如下：\n\ndb.collection.update(\n   <query>,\n   <update>,\n   {\n     upsert: <boolean>,\n     multi: <boolean>,\n     writeConcern: <document>\n   }\n)\n\n\n参数说明：\n\n * query : update 的查询条件，类似 sql update 查询内 where 后面的。\n * update : update 的对象和一些更新的操作符（如$,$inc...）等，也可以理解为 sql update 查询内 set 后面的\n * upsert : 可选，这个参数的意思是，如果不存在 update 的记录，是否插入 objNew,true 为插入，默认是 false，不插入。\n * multi : 可选，mongodb 默认是 false,只更新找到的第一条记录，如果这个参数为 true,就把按条件查出来多条记录全部更新。\n * writeConcern :可选，抛出异常的级别。\n\n【示例】更新文档\n\ndb.collection.update({ title: \'MongoDB 教程\' }, { $set: { title: \'MongoDB\' } })\n\n\n【示例】更新多条相同文档\n\n以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置 multi 参数为 true。\n\ndb.collection.update(\n  { title: \'MongoDB 教程\' },\n  { $set: { title: \'MongoDB\' } },\n  { multi: true }\n)\n\n\n【示例】更多实例\n\n只更新第一条记录：\n\ndb.collection.update({ count: { $gt: 1 } }, { $set: { test2: \'OK\' } })\n\n\n全部更新：\n\ndb.collection.update(\n  { count: { $gt: 3 } },\n  { $set: { test2: \'OK\' } },\n  false,\n  true\n)\n\n\n只添加第一条：\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'OK\' } },\n  true,\n  false\n)\n\n\n全部添加进去:\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'OK\' } },\n  true,\n  false\n)\n\n\n全部更新：\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'OK\' } },\n  true,\n  false\n)\n\n\n只更新第一条记录：\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'OK\' } },\n  true,\n  false\n)\n\n\n\n# 删除文档操作\n\n官方推荐使用 deleteOne() 和 deleteMany() 方法删除数据。\n\n删除 status 等于 A 的全部文档：\n\ndb.collection.deleteMany({ status: \'A\' })\n\n\n删除 status 等于 D 的一个文档：\n\ndb.collection.deleteOne({ status: \'D\' })\n\n\n\n# 索引操作\n\n索引通常能够极大的提高查询的效率，如果没有索引，MongoDB 在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。\n\n这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。\n\n索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。\n\nMongoDB 使用 createIndex() 方法来创建索引。\n\ncreateIndex()方法基本语法格式如下所示：\n\n>db.collection.createIndex(keys, options)\n\n\n语法中 Key 值为你要创建的索引字段，1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1 即可。\n\n>db.col.createIndex({"title":1})\n\n\ncreateIndex() 方法中你也可以设置使用多个字段创建索引（关系型数据库中称作复合索引）。\n\n>db.col.createIndex({"title":1,"description":-1})\n\n\ncreateIndex() 接收可选参数，可选参数列表如下：\n\nPARAMETER            TYPE            DESCRIPTION\nbackground           Boolean         建索引过程会阻塞其它数据库操作，background 可指定以后台方式创建索引，即增加 "background"\n                                     可选参数。 "background" 默认值为false。\nunique               Boolean         建立的索引是否唯一。指定为 true 创建唯一索引。默认值为false.\nname                 string          索引的名称。如果未指定，MongoDB 的通过连接索引的字段名和排序顺序生成一个索引名称。\ndropDups             Boolean         **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false。\nsparse               Boolean         对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为 true\n                                     的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireAfterSeconds   integer         指定一个以秒为单位的数值，完成 TTL 设定，设定集合的生存时间。\nv                    index version   索引的版本号。默认的索引版本取决于 mongod 创建索引时运行的版本。\nweights              document        索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language     string          对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override    string          对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的 language，默认值为 language.\n\n\n# MongoDB 聚合操作\n\nMongoDB 中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似 sql 语句中的 count(*)。\n\n\n# 管道\n\n整个聚合运算过程称为管道，它是由多个步骤组成，每个管道\n\n * 接受一系列文档（原始数据）；\n * 每个步骤对这些文档进行一系列运算；\n * 结果文档输出给下一个步骤；\n\n聚合操作的基本格式\n\npipeline = [$stage1, $stage1, ..., $stageN];\n\ndb.<集合>.aggregate(pipeline, {options});\n\n\n\n# 聚合步骤\n\n步骤                 作用     SQL 等价运算符\n$match             过滤     WHERE\n$project           投影     AS\n$sort              排序     ORDER BY\n$group             分组     GROUP BY\n$skip / $limit     结果限制   SKIP / LIMIT\n$lookup            左外连接   LEFT OUTER JOIN\n$unwind            展开数组   N/A\n$graphLookup       图搜索    N/A\n$facet / $bucket   分面搜索   N/A\n\n【示例】\n\n> db.collection.insertMany([{"title":"MongoDB Overview","description":"MongoDB is no sql database","by_user":"collection","tagsr":["mongodb","database","NoSQL"],"likes":"100"},{"title":"NoSQL Overview","description":"No sql database is very fast","by_user":"collection","tagsr":["mongodb","database","NoSQL"],"likes":"10"},{"title":"Neo4j Overview","description":"Neo4j is no sql database","by_user":"Neo4j","tagsr":["neo4j","database","NoSQL"],"likes":"750"}])\n> db.collection.aggregate([{$group : {_id : "$by_user", num_tutorial : {$sum : 1}}}])\n{ "_id" : null, "num_tutorial" : 3 }\n{ "_id" : "Neo4j", "num_tutorial" : 1 }\n{ "_id" : "collection", "num_tutorial" : 2 }\n\n\n下表展示了一些聚合的表达式:\n\n表达式         描述                        实例\n$sum        计算总和。                     db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$sum : "$likes"}}}])\n$avg        计算平均值                     db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$avg : "$likes"}}}])\n$min        获取集合中所有文档对应值得最小值。         db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$min : "$likes"}}}])\n$max        获取集合中所有文档对应值得最大值。         db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$max : "$likes"}}}])\n$push       在结果文档中插入值到一个数组中。          db.mycol.aggregate([{$group : {_id : "$by_user", url :\n                                      {$push: "$url"}}}])\n$addToSet   在结果文档中插入值到一个数组中，但不创建副本。   db.mycol.aggregate([{$group : {_id : "$by_user", url :\n                                      {$addToSet : "$url"}}}])\n$first      根据资源文档的排序获取第一个文档数据。       db.mycol.aggregate([{$group : {_id : "$by_user", first_url :\n                                      {$first : "$url"}}}])\n$last       根据资源文档的排序获取最后一个文档数据       db.mycol.aggregate([{$group : {_id : "$by_user", last_url :\n                                      {$last : "$url"}}}])\n\n\n# 参考资料\n\n * MongoDB 官网\n * MongoDB Github\n * MongoDB 教程',normalizedContent:'# mongodb 应用指南\n\n\n# 简介\n\nmongodb 是一个基于分布式文件存储的数据库。由 c++ 语言编写。旨在为 web 应用提供可扩展的高性能数据存储解决方案。\n\nmongodb 将数据存储为一个文档，数据结构由键值(key=>value)对组成。mongodb 文档类似于 json 对象。字段值可以包含其他文档，数组及文档数组。\n\n\n# mongodb 发展\n\n * 1.x - 支持复制和分片\n * 2.x - 更丰富的数据库功能\n * 3.x - wiredtiger 和周边生态\n * 4.x - 支持分布式事务\n\n\n# mongodb 和 rdbms\n\n特性        mongodb                       rdbms\n数据模型      文档模型                          关系型\ncrud 操作   mql/sql                       sql\n高可用       复制集                           集群模式\n扩展性       支持分片                          数据分区\n扩繁方式      垂直扩展+水平扩展                     垂直扩展\n索引类型      b 树、全文索引、地理位置索引、多键索引、ttl 索引   b 树\n数据容量      没有理论上限                        千万、亿\n\n\n# mongodb 特性\n\n * 数据是 json 结构\n   * 支持结构化、半结构化数据模型\n   * 可以动态响应结构变化\n * 通过副本机制提供高可用\n * 通过分片提供扩容能力\n\n\n# mongodb 概念\n\nsql 术语/概念     mongodb 术语/概念   解释/说明\ndatabase      database        数据库\ntable         collection      数据库表/集合\nrow           document        数据记录行/文档\ncolumn        field           数据字段/域\nindex         index           索引\ntable joins                   表连接,mongodb 不支持\nprimary key   primary key     主键,mongodb 自动将_id 字段设置为主键\n\n\n# 数据库\n\n一个 mongodb 中可以建立多个数据库。\n\nmongodb 的默认数据库为"db"，该数据库存储在 data 目录中。\n\nmongodb 的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。\n\n"show dbs" 命令可以显示所有数据的列表。\n\n$ ./mongo\nmongodbshell version: 3.0.6\nconnecting to: test\n> show dbs\nlocal  0.078gb\ntest   0.078gb\n>\n\n\n执行 "db" 命令可以显示当前数据库对象或集合。\n\n$ ./mongo\nmongodbshell version: 3.0.6\nconnecting to: test\n> db\ntest\n>\n\n\n运行"use"命令，可以连接到一个指定的数据库。\n\n> use local\nswitched to db local\n> db\nlocal\n>\n\n\n数据库也通过名字来标识。数据库名可以是满足以下条件的任意 utf-8 字符串。\n\n * 不能是空字符串（"")。\n * 不得含有 \' \'（空格)、.、\\$、/、\\和 \\0 (空字符)。\n * 应全部小写。\n * 最多 64 字节。\n\n有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。\n\n * admin：从权限的角度来看，这是"root"数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。\n * local：这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合\n * config：当 mongo 用于分片设置时，config 数据库在内部使用，用于保存分片的相关信息。\n\n\n# 文档\n\n文档是一组键值(key-value)对(即 bson)。mongodb 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 mongodb 非常突出的特点。\n\n需要注意的是：\n\n * 文档中的键/值对是有序的。\n * 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。\n * mongodb 区分类型和大小写。\n * mongodb 的文档不能有重复的键。\n * 文档的键是字符串。除了少数例外情况，键可以使用任意 utf-8 字符。\n\n文档键命名规范：\n\n * 键不能含有 \\0 (空字符)。这个字符用来表示键的结尾。\n * . 和 $ 有特别的意义，只有在特定环境下才能使用。\n * 以下划线 _ 开头的键是保留的(不是严格要求的)。\n\n\n# 集合\n\n集合就是 mongodb 文档组，类似于 rdbms （关系数据库管理系统：relational database management system)中的表格。\n\n集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。\n\n合法的集合名：\n\n * 集合名不能是空字符串""。\n * 集合名不能含有 \\0 字符（空字符)，这个字符表示集合名的结尾。\n * 集合名不能以"system."开头，这是为系统集合保留的前缀。\n * 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现 $。\n\n\n# 元数据\n\n数据库的信息是存储在集合中。它们使用了系统的命名空间：dbname.system.*\n\n在 mongodb 数据库中名字空间 <dbname>.system.* 是包含多种系统信息的特殊集合(collection)，如下:\n\n集合命名空间                     描述\ndbname.system.namespaces   列出所有名字空间。\ndbname.system.indexes      列出所有索引。\ndbname.system.profile      包含数据库概要(profile)信息。\ndbname.system.users        列出所有可访问数据库的用户。\ndbname.local.sources       包含复制对端（slave）的服务器信息和状态。\n\n对于修改系统集合中的对象有如下限制。\n\n在 system.indexes 插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的 drop index 命令将自动更新相关信息)。system.users 是可修改的。system.profile 是可删除的。\n\n\n# mongodb 数据类型\n\n数据类型                 描述\nstring               字符串。存储数据常用的数据类型。在 mongodb 中，utf-8 编码的字符串才是合法的。\ninteger              整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。\nboolean              布尔值。用于存储布尔值（真/假）。\ndouble               双精度浮点值。用于存储浮点值。\nmin/max keys         将一个值与 bson（二进制的 json）元素的最低值和最高值相对比。\narray                用于将数组或列表或多个值存储为一个键。\ntimestamp            时间戳。记录文档修改或添加的具体时间。\nobject               用于内嵌文档。\nnull                 用于创建空值。\nsymbol               符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。\ndate                 日期时间。用 unix 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 date 对象，传入年月日信息。\nobject id            对象 id。用于创建文档的 id。\nbinary data          二进制数据。用于存储二进制数据。\ncode                 代码类型。用于在文档中存储 javascript 代码。\nregular expression   正则表达式类型。用于存储正则表达式。\n\n\n# mongodb crud\n\n\n# 数据库操作\n\n# 查看所有数据库\n\nshow dbs\n\n\n# 创建数据库\n\nuse <database>\n\n\n如果数据库不存在，则创建数据库，否则切换到指定数据库。\n\n【示例】创建数据库，并插入一条数据\n\n刚创建的数据库 test 并不在数据库的列表中， 要显示它，需要插入一些数据\n\n> use test\nswitched to db test\n>\n> show dbs\nadmin   0.000gb\nconfig  0.000gb\nlocal   0.000gb\n> db.test.insert({"name":"mongodb"})\nwriteresult({ "ninserted" : 1 })\n> show dbs\nadmin   0.000gb\nconfig  0.000gb\nlocal   0.000gb\ntest    0.000gb\n\n\n# 删除数据库\n\n删除当前数据库\n\ndb.dropdatabase()\n\n\n\n# 集合操作\n\n# 查看集合\n\nshow collections\n\n\n# 创建集合\n\ndb.createcollection(name, options)\n\n\n参数说明：\n\n * name: 要创建的集合名称\n * options: 可选参数, 指定有关内存大小及索引的选项\n\noptions 可以是如下参数：\n\n字段            类型   描述\ncapped        布尔   （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。\n                   当该值为 true 时，必须指定 size 参数。\nautoindexid   布尔   3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。\nsize          数值   （可选）为固定集合指定一个最大值，即字节数。 如果 capped 为 true，也需要指定该字段。\nmax           数值   （可选）指定固定集合中包含文档的最大数量。\n\n在插入文档时，mongodb 首先检查固定集合的 size 字段，然后检查 max 字段。\n\n> db.createcollection("collection")\n{ "ok" : 1 }\n> show collections\ncollection\n\n\n# 删除集合\n\n> db.collection.drop()\ntrue\n> show collections\n>\n\n\n\n# 插入文档操作\n\nmongodb 使用 insert() 方法完成插入操作。\n\n语法格式\n\n# 插入单条记录\ndb.<集合>.insertone(<json>)\n# 插入多条记录\ndb.<集合>.insertmany([<json 1>, <json 2>, ..., <json n>])\n\n\n【示例】insertone\n\n> db.color.insertone({name: "red"})\n{\n        "acknowledged" : true,\n        "insertedid" : objectid("5f533ae4e8f16647950fdf43")\n}\n\n\n【示例】insertmany\n\n> db.color.insertmany([\n  {\n    "name": "yellow"\n  },\n  {\n    "name": "blue"\n  }\n])\n{\n        "acknowledged" : true,\n        "insertedids" : [\n                objectid("5f533bcae8f16647950fdf44"),\n                objectid("5f533bcae8f16647950fdf45")\n        ]\n}\n>\n\n\n\n# 查询文档操作\n\nmongodb 使用 find() 方法完成查询文档操作。\n\n语法格式\n\ndb.<集合>.find(<json>)\n\n\n查询条件也是 json 形式，如果不设置查询条件，即为全量查询。\n\n# 查询条件\n\n操作                      格式                                      范例                                         rdbms 中的类似语句\n等于                      {<key>:<value>}                         db.book.find({"pagecount": {$eq: 0}})      where pagecount = 0\n不等于                     {<key>:{$ne:<value>}}                   db.book.find({"pagecount": {$ne: 0}})      where likes != 50\n大于                      {<key>:{$gt:<value>}}                   db.book.find({"pagecount": {$gt: 0}})      where likes > 50\n{<key>:{$gt:<value>}}   db.book.find({"pagecount": {$gt: 0}})   where likes > 50                           大于或等于\n小于                      {<key>:{$lt:<value>}}                   db.book.find({"pagecount": {$lt: 200}})    where likes < 50\n小于或等于                   {<key>:{$lte:<value>}}                  db.book.find({"pagecount": {$lte: 200}})   where likes <= 50\n\n> 说明：\n> \n> $eq  --------  equal  =\n> $ne ----------- not equal  !=\n> $gt -------- greater than  >\n> $gte --------- gt equal  >=\n> $lt -------- less than  <\n> $lte --------- lt equal  <=\n\n【示例】\n\n\n\n# 统计匹配查询条件的记录数\n> db.book.find({"status": "meap"}).count()\n68\n\n\n# 查询逻辑条件\n\n（1）and 条件\n\nmongodb 的 find() 方法可以传入多个键(key)，每个键(key)以逗号隔开，即常规 sql 的 and 条件。\n\n语法格式如下：\n\n> db.col.find({key1:value1, key2:value2}).pretty()\n\n\n（2）or 条件\n\nmongodb or 条件语句使用了关键字 $or,语法格式如下：\n\n>db.col.find(\n   {\n      $or: [\n         {key1: value1}, {key2:value2}\n      ]\n   }\n).pretty()\n\n\n# 模糊查询\n\n查询 title 包含"教"字的文档：\n\ndb.col.find({ title: /教/ })\n\n\n查询 title 字段以"教"字开头的文档：\n\ndb.col.find({ title: /^教/ })\n\n\n查询 titl e 字段以"教"字结尾的文档：\n\ndb.col.find({ title: /教$/ })\n\n\n# limit() 方法\n\n如果你需要在 mongodb 中读取指定数量的数据记录，可以使用 mongodb 的 limit 方法，limit()方法接受一个数字参数，该参数指定从 mongodb 中读取的记录条数。\n\nlimit()方法基本语法如下所示：\n\n>db.collection_name.find().limit(number)\n\n\n# skip() 方法\n\n我们除了可以使用 limit()方法来读取指定数量的数据外，还可以使用 skip()方法来跳过指定数量的数据，skip 方法同样接受一个数字参数作为跳过的记录条数。\n\nskip() 方法脚本语法格式如下：\n\n>db.collection_name.find().limit(number).skip(number)\n\n\n# sort() 方法\n\n在 mongodb 中使用 sort() 方法对数据进行排序，sort() 方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而 -1 是用于降序排列。\n\nsort()方法基本语法如下所示：\n\n>db.collection_name.find().sort({key:1})\n\n\n> 注意：skip(), limilt(), sort()三个放在一起执行的时候，执行的顺序是先 sort(), 然后是 skip()，最后是显示的 limit()。\n\n\n# 更新文档操作\n\nupdate() 方法用于更新已存在的文档。语法格式如下：\n\ndb.collection.update(\n   <query>,\n   <update>,\n   {\n     upsert: <boolean>,\n     multi: <boolean>,\n     writeconcern: <document>\n   }\n)\n\n\n参数说明：\n\n * query : update 的查询条件，类似 sql update 查询内 where 后面的。\n * update : update 的对象和一些更新的操作符（如$,$inc...）等，也可以理解为 sql update 查询内 set 后面的\n * upsert : 可选，这个参数的意思是，如果不存在 update 的记录，是否插入 objnew,true 为插入，默认是 false，不插入。\n * multi : 可选，mongodb 默认是 false,只更新找到的第一条记录，如果这个参数为 true,就把按条件查出来多条记录全部更新。\n * writeconcern :可选，抛出异常的级别。\n\n【示例】更新文档\n\ndb.collection.update({ title: \'mongodb 教程\' }, { $set: { title: \'mongodb\' } })\n\n\n【示例】更新多条相同文档\n\n以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置 multi 参数为 true。\n\ndb.collection.update(\n  { title: \'mongodb 教程\' },\n  { $set: { title: \'mongodb\' } },\n  { multi: true }\n)\n\n\n【示例】更多实例\n\n只更新第一条记录：\n\ndb.collection.update({ count: { $gt: 1 } }, { $set: { test2: \'ok\' } })\n\n\n全部更新：\n\ndb.collection.update(\n  { count: { $gt: 3 } },\n  { $set: { test2: \'ok\' } },\n  false,\n  true\n)\n\n\n只添加第一条：\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'ok\' } },\n  true,\n  false\n)\n\n\n全部添加进去:\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'ok\' } },\n  true,\n  false\n)\n\n\n全部更新：\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'ok\' } },\n  true,\n  false\n)\n\n\n只更新第一条记录：\n\ndb.collection.update(\n  { count: { $gt: 4 } },\n  { $set: { test5: \'ok\' } },\n  true,\n  false\n)\n\n\n\n# 删除文档操作\n\n官方推荐使用 deleteone() 和 deletemany() 方法删除数据。\n\n删除 status 等于 a 的全部文档：\n\ndb.collection.deletemany({ status: \'a\' })\n\n\n删除 status 等于 d 的一个文档：\n\ndb.collection.deleteone({ status: \'d\' })\n\n\n\n# 索引操作\n\n索引通常能够极大的提高查询的效率，如果没有索引，mongodb 在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。\n\n这种扫描全集合的查询效率是非常低的，特别在处理大量的数据时，查询可以要花费几十秒甚至几分钟，这对网站的性能是非常致命的。\n\n索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。\n\nmongodb 使用 createindex() 方法来创建索引。\n\ncreateindex()方法基本语法格式如下所示：\n\n>db.collection.createindex(keys, options)\n\n\n语法中 key 值为你要创建的索引字段，1 为指定按升序创建索引，如果你想按降序来创建索引指定为 -1 即可。\n\n>db.col.createindex({"title":1})\n\n\ncreateindex() 方法中你也可以设置使用多个字段创建索引（关系型数据库中称作复合索引）。\n\n>db.col.createindex({"title":1,"description":-1})\n\n\ncreateindex() 接收可选参数，可选参数列表如下：\n\nparameter            type            description\nbackground           boolean         建索引过程会阻塞其它数据库操作，background 可指定以后台方式创建索引，即增加 "background"\n                                     可选参数。 "background" 默认值为false。\nunique               boolean         建立的索引是否唯一。指定为 true 创建唯一索引。默认值为false.\nname                 string          索引的名称。如果未指定，mongodb 的通过连接索引的字段名和排序顺序生成一个索引名称。\ndropdups             boolean         **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false。\nsparse               boolean         对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为 true\n                                     的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireafterseconds   integer         指定一个以秒为单位的数值，完成 ttl 设定，设定集合的生存时间。\nv                    index version   索引的版本号。默认的索引版本取决于 mongod 创建索引时运行的版本。\nweights              document        索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language     string          对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override    string          对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的 language，默认值为 language.\n\n\n# mongodb 聚合操作\n\nmongodb 中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似 sql 语句中的 count(*)。\n\n\n# 管道\n\n整个聚合运算过程称为管道，它是由多个步骤组成，每个管道\n\n * 接受一系列文档（原始数据）；\n * 每个步骤对这些文档进行一系列运算；\n * 结果文档输出给下一个步骤；\n\n聚合操作的基本格式\n\npipeline = [$stage1, $stage1, ..., $stagen];\n\ndb.<集合>.aggregate(pipeline, {options});\n\n\n\n# 聚合步骤\n\n步骤                 作用     sql 等价运算符\n$match             过滤     where\n$project           投影     as\n$sort              排序     order by\n$group             分组     group by\n$skip / $limit     结果限制   skip / limit\n$lookup            左外连接   left outer join\n$unwind            展开数组   n/a\n$graphlookup       图搜索    n/a\n$facet / $bucket   分面搜索   n/a\n\n【示例】\n\n> db.collection.insertmany([{"title":"mongodb overview","description":"mongodb is no sql database","by_user":"collection","tagsr":["mongodb","database","nosql"],"likes":"100"},{"title":"nosql overview","description":"no sql database is very fast","by_user":"collection","tagsr":["mongodb","database","nosql"],"likes":"10"},{"title":"neo4j overview","description":"neo4j is no sql database","by_user":"neo4j","tagsr":["neo4j","database","nosql"],"likes":"750"}])\n> db.collection.aggregate([{$group : {_id : "$by_user", num_tutorial : {$sum : 1}}}])\n{ "_id" : null, "num_tutorial" : 3 }\n{ "_id" : "neo4j", "num_tutorial" : 1 }\n{ "_id" : "collection", "num_tutorial" : 2 }\n\n\n下表展示了一些聚合的表达式:\n\n表达式         描述                        实例\n$sum        计算总和。                     db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$sum : "$likes"}}}])\n$avg        计算平均值                     db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$avg : "$likes"}}}])\n$min        获取集合中所有文档对应值得最小值。         db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$min : "$likes"}}}])\n$max        获取集合中所有文档对应值得最大值。         db.mycol.aggregate([{$group : {_id : "$by_user",\n                                      num_tutorial : {$max : "$likes"}}}])\n$push       在结果文档中插入值到一个数组中。          db.mycol.aggregate([{$group : {_id : "$by_user", url :\n                                      {$push: "$url"}}}])\n$addtoset   在结果文档中插入值到一个数组中，但不创建副本。   db.mycol.aggregate([{$group : {_id : "$by_user", url :\n                                      {$addtoset : "$url"}}}])\n$first      根据资源文档的排序获取第一个文档数据。       db.mycol.aggregate([{$group : {_id : "$by_user", first_url :\n                                      {$first : "$url"}}}])\n$last       根据资源文档的排序获取最后一个文档数据       db.mycol.aggregate([{$group : {_id : "$by_user", last_url :\n                                      {$last : "$url"}}}])\n\n\n# 参考资料\n\n * mongodb 官网\n * mongodb github\n * mongodb 教程',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 的 CRUD 操作",frontmatter:{title:"MongoDB 的 CRUD 操作",date:"2020-09-25T21:23:41.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB"],permalink:"/pages/7efbac/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/02.MongoDB%E7%9A%84CRUD%E6%93%8D%E4%BD%9C.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/02.MongoDB的CRUD操作.md",key:"v-62ac19f1",path:"/pages/7efbac/",headers:[{level:2,title:"一、基本 CRUD 操作",slug:"一、基本-crud-操作",normalizedTitle:"一、基本 crud 操作",charIndex:24},{level:3,title:"Create 操作",slug:"create-操作",normalizedTitle:"create 操作",charIndex:79},{level:3,title:"Read 操作",slug:"read-操作",normalizedTitle:"read 操作",charIndex:983},{level:3,title:"Update 操作",slug:"update-操作",normalizedTitle:"update 操作",charIndex:1046},{level:3,title:"Delete 操作",slug:"delete-操作",normalizedTitle:"delete 操作",charIndex:3207},{level:2,title:"二、批量写操作",slug:"二、批量写操作",normalizedTitle:"二、批量写操作",charIndex:3397},{level:3,title:"有序和无序的操作",slug:"有序和无序的操作",normalizedTitle:"有序和无序的操作",charIndex:3516},{level:3,title:"bulkWrite() 方法",slug:"bulkwrite-方法",normalizedTitle:"bulkwrite() 方法",charIndex:3431},{level:3,title:"批量写操作策略",slug:"批量写操作策略",normalizedTitle:"批量写操作策略",charIndex:4653},{level:4,title:"预拆分 collection",slug:"预拆分-collection",normalizedTitle:"预拆分 collection",charIndex:4718},{level:4,title:"无序写操作",slug:"无序写操作",normalizedTitle:"无序写操作",charIndex:3767},{level:4,title:"避免单调节流",slug:"避免单调节流",normalizedTitle:"避免单调节流",charIndex:4996},{level:2,title:"SQL 和 MongoDB 对比",slug:"sql-和-mongodb-对比",normalizedTitle:"sql 和 mongodb 对比",charIndex:5236},{level:3,title:"术语和概念",slug:"术语和概念",normalizedTitle:"术语和概念",charIndex:5257},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:6095}],headersStr:"一、基本 CRUD 操作 Create 操作 Read 操作 Update 操作 Delete 操作 二、批量写操作 有序和无序的操作 bulkWrite() 方法 批量写操作策略 预拆分 collection 无序写操作 避免单调节流 SQL 和 MongoDB 对比 术语和概念 参考资料",content:"# MongoDB 的 CRUD 操作\n\n\n# 一、基本 CRUD 操作\n\nMongoDB 的 CRUD 操作是针对 document 的读写操作。\n\n\n# Create 操作\n\nMongoDB 提供以下操作向一个 collection 插入 document\n\n * db.collection.insertOne()：插入一条 document\n * db.collection.insertMany()：插入多条 document\n\n> 注：以上操作都是原子操作。\n\n\n\n插入操作的特性：\n\n * MongoDB 中的所有写操作都是单个文档级别的原子操作。\n * 如果要插入的 collection 当前不存在，则插入操作会自动创建 collection。\n * 在 MongoDB 中，存储在集合中的每个文档都需要一个唯一的 _id 字段作为主键。如果插入的文档省略 _id 字段，则 MongoDB 驱动程序会自动为 _id 字段生成 ObjectId。\n * 可以 MongoDB 写入操作的确认级别来控制写入行为。\n\n【示例】插入一条 document 示例\n\ndb.inventory.insertOne({\n  item: 'canvas',\n  qty: 100,\n  tags: ['cotton'],\n  size: { h: 28, w: 35.5, uom: 'cm' }\n})\n\n\n【示例】插入多条 document 示例\n\ndb.inventory.insertMany([\n  {\n    item: 'journal',\n    qty: 25,\n    tags: ['blank', 'red'],\n    size: { h: 14, w: 21, uom: 'cm' }\n  },\n  {\n    item: 'mat',\n    qty: 85,\n    tags: ['gray'],\n    size: { h: 27.9, w: 35.5, uom: 'cm' }\n  },\n  {\n    item: 'mousepad',\n    qty: 25,\n    tags: ['gel', 'blue'],\n    size: { h: 19, w: 22.85, uom: 'cm' }\n  }\n])\n\n\n\n# Read 操作\n\nMongoDB 提供 db.collection.find() 方法来检索 document。\n\n\n\n\n# Update 操作\n\nMongoDB 提供以下操作来更新 collection 中的 document\n\n * db.collection.updateOne()：更新一条 document\n * db.collection.updateMany()：更新多条 document\n * db.collection.replaceOne()：替换一条 document\n\n语法格式：\n\n * db.collection.updateOne(<filter>, <update>, <options>)\n * db.collection.updateMany(<filter>, <update>, <options>)\n * db.collection.replaceOne(<filter>, <update>, <options>)\n\n\n\n【示例】插入测试数据\n\ndb.inventory.insertMany([\n  {\n    item: 'canvas',\n    qty: 100,\n    size: { h: 28, w: 35.5, uom: 'cm' },\n    status: 'A'\n  },\n  { item: 'journal', qty: 25, size: { h: 14, w: 21, uom: 'cm' }, status: 'A' },\n  { item: 'mat', qty: 85, size: { h: 27.9, w: 35.5, uom: 'cm' }, status: 'A' },\n  {\n    item: 'mousepad',\n    qty: 25,\n    size: { h: 19, w: 22.85, uom: 'cm' },\n    status: 'P'\n  },\n  {\n    item: 'notebook',\n    qty: 50,\n    size: { h: 8.5, w: 11, uom: 'in' },\n    status: 'P'\n  },\n  { item: 'paper', qty: 100, size: { h: 8.5, w: 11, uom: 'in' }, status: 'D' },\n  {\n    item: 'planner',\n    qty: 75,\n    size: { h: 22.85, w: 30, uom: 'cm' },\n    status: 'D'\n  },\n  {\n    item: 'postcard',\n    qty: 45,\n    size: { h: 10, w: 15.25, uom: 'cm' },\n    status: 'A'\n  },\n  {\n    item: 'sketchbook',\n    qty: 80,\n    size: { h: 14, w: 21, uom: 'cm' },\n    status: 'A'\n  },\n  {\n    item: 'sketch pad',\n    qty: 95,\n    size: { h: 22.85, w: 30.5, uom: 'cm' },\n    status: 'A'\n  }\n])\n\n\n【示例】更新一条 document\n\ndb.inventory.updateOne(\n  { item: 'paper' },\n  {\n    $set: { 'size.uom': 'cm', status: 'P' },\n    $currentDate: { lastModified: true }\n  }\n)\n\n\n【示例】更新多条 document\n\ndb.inventory.updateMany(\n  { qty: { $lt: 50 } },\n  {\n    $set: { 'size.uom': 'in', status: 'P' },\n    $currentDate: { lastModified: true }\n  }\n)\n\n\n【示例】替换一条 document\n\ndb.inventory.replaceOne(\n  { item: 'paper' },\n  {\n    item: 'paper',\n    instock: [\n      { warehouse: 'A', qty: 60 },\n      { warehouse: 'B', qty: 40 }\n    ]\n  }\n)\n\n\n更新操作的特性：\n\n * MongoDB 中的所有写操作都是单个文档级别的原子操作。\n * 一旦设置了，就无法更新或替换 _id 字段。\n * 除以下情况外，MongoDB 会在执行写操作后保留文档字段的顺序：\n   * _id 字段始终是文档中的第一个字段。\n   * 包括重命名字段名称的更新可能导致文档中字段的重新排序。\n * 如果更新操作中包含 upsert : true 并且没有 document 匹配过滤器，MongoDB 会新插入一个 document；如果有匹配的 document，MongoDB 会修改或替换这些 document。\n\n\n# Delete 操作\n\nMongoDB 提供以下操作来删除 collection 中的 document\n\n * db.collection.deleteOne()：删除一条 document\n * db.collection.deleteMany()：删除多条 document\n\n\n\n删除操作的特性：\n\n * MongoDB 中的所有写操作都是单个文档级别的原子操作。\n\n\n# 二、批量写操作\n\nMongoDB 通过 db.collection.bulkWrite() 方法来支持批量写操作（包括批量插入、更新、删除）。\n\n此外，db.collection.insertMany() 方法支持批量插入操作。\n\n\n# 有序和无序的操作\n\n批量写操作可以有序或无序。\n\n * 对于有序列表，MongoDB 串行执行操作。如果在写操作的处理过程中发生错误，MongoDB 将不处理列表中剩余的写操作。\n * 对于无序列表，MongoDB 可以并行执行操作，但是不能保证此行为。如果在写操作的处理过程中发生错误，MongoDB 将继续处理列表中剩余的写操作。\n\n在分片集合上执行操作的有序列表通常比执行无序列表要慢，因为对于有序列表，每个操作必须等待上一个操作完成。\n\n默认情况下，bulkWrite() 执行有序操作。要指定无序写操作，请在选项文档中设置 ordered : false。\n\n\n# bulkWrite() 方法\n\nbulkWrite() 支持以下写操作：\n\n * insertOne\n * updateOne\n * updateMany\n * replaceOne\n * deleteOne\n * deleteMany\n\n【示例】批量写操作示例\n\ntry {\n  db.characters.bulkWrite([\n    {\n      insertOne: {\n        document: {\n          _id: 4,\n          char: 'Dithras',\n          class: 'barbarian',\n          lvl: 4\n        }\n      }\n    },\n    {\n      insertOne: {\n        document: {\n          _id: 5,\n          char: 'Taeln',\n          class: 'fighter',\n          lvl: 3\n        }\n      }\n    },\n    {\n      updateOne: {\n        filter: { char: 'Eldon' },\n        update: { $set: { status: 'Critical Injury' } }\n      }\n    },\n    { deleteOne: { filter: { char: 'Brisbane' } } },\n    {\n      replaceOne: {\n        filter: { char: 'Meldane' },\n        replacement: { char: 'Tanys', class: 'oracle', lvl: 4 }\n      }\n    }\n  ])\n} catch (e) {\n  print(e)\n}\n\n\n\n# 批量写操作策略\n\n大量的插入操作（包括初始数据插入或常规数据导入）可能会影响分片集群的性能。对于批量插入，请考虑以下策略：\n\n# 预拆分 collection\n\n如果分片集合为空，则该集合只有一个初始 chunk，该 chunk 位于单个分片上。然后，MongoDB 必须花一些时间来接收数据，创建拆分并将拆分的块分发到可用的分片。为了避免这种性能成本，您可以按照拆分群集中的拆分块中的说明预拆分 collection。\n\n# 无序写操作\n\n要提高对分片集群的写入性能，请使用 bulkWrite()，并将可选参数顺序设置为 false。mongos 可以尝试同时将写入操作发送到多个分片。对于空集合，首先按照分片群集中的分割 chunk 中的说明预拆分 collection。\n\n# 避免单调节流\n\n如果在一次插入操作中，分片 key 单调递增，那么所有的插入数据都会存入 collection 的最后一个 chunk，也就是存入一个分片中。因此，集群的插入容量将永远不会超过该单个分片的插入容量。\n\n如果插入量大于单个分片可以处理的插入量，并且无法避免单调递增的分片键，那么请考虑对应用程序进行以下修改：\n\n * 反转分片密钥的二进制位。这样可以保留信息，并避免将插入顺序与值序列的增加关联起来。\n * 交换第一个和最后一个 16 位字以“随机”插入。\n\n\n# SQL 和 MongoDB 对比\n\n\n# 术语和概念\n\nSQL 术语和概念                     MONGODB 术语和概念\ndatabase                      database\ntable                         collection\nrow                           document 或 BSON\ncolumn                        field\nindex                         index\ntable joins                   $lookup、嵌入式文档\nprimary key                   primary key\n                              MongoDB 中自动设置主键为 _id 字段\naggregation (e.g. group by)   aggregation pipeline\n                              参考 SQL to Aggregation Mapping Chart.\nSELECT INTO NEW_TABLE         $out\n                              参考 SQL to Aggregation Mapping Chart\nMERGE INTO TABLE              $merge (MongoDB 4.2 开始支持)\n                              参考 SQL to Aggregation Mapping Chart.\nUNION ALL                     $unionWith (MongoDB 4.4 开始支持)\ntransactions                  transactions\n\n\n# 参考资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课",normalizedContent:"# mongodb 的 crud 操作\n\n\n# 一、基本 crud 操作\n\nmongodb 的 crud 操作是针对 document 的读写操作。\n\n\n# create 操作\n\nmongodb 提供以下操作向一个 collection 插入 document\n\n * db.collection.insertone()：插入一条 document\n * db.collection.insertmany()：插入多条 document\n\n> 注：以上操作都是原子操作。\n\n\n\n插入操作的特性：\n\n * mongodb 中的所有写操作都是单个文档级别的原子操作。\n * 如果要插入的 collection 当前不存在，则插入操作会自动创建 collection。\n * 在 mongodb 中，存储在集合中的每个文档都需要一个唯一的 _id 字段作为主键。如果插入的文档省略 _id 字段，则 mongodb 驱动程序会自动为 _id 字段生成 objectid。\n * 可以 mongodb 写入操作的确认级别来控制写入行为。\n\n【示例】插入一条 document 示例\n\ndb.inventory.insertone({\n  item: 'canvas',\n  qty: 100,\n  tags: ['cotton'],\n  size: { h: 28, w: 35.5, uom: 'cm' }\n})\n\n\n【示例】插入多条 document 示例\n\ndb.inventory.insertmany([\n  {\n    item: 'journal',\n    qty: 25,\n    tags: ['blank', 'red'],\n    size: { h: 14, w: 21, uom: 'cm' }\n  },\n  {\n    item: 'mat',\n    qty: 85,\n    tags: ['gray'],\n    size: { h: 27.9, w: 35.5, uom: 'cm' }\n  },\n  {\n    item: 'mousepad',\n    qty: 25,\n    tags: ['gel', 'blue'],\n    size: { h: 19, w: 22.85, uom: 'cm' }\n  }\n])\n\n\n\n# read 操作\n\nmongodb 提供 db.collection.find() 方法来检索 document。\n\n\n\n\n# update 操作\n\nmongodb 提供以下操作来更新 collection 中的 document\n\n * db.collection.updateone()：更新一条 document\n * db.collection.updatemany()：更新多条 document\n * db.collection.replaceone()：替换一条 document\n\n语法格式：\n\n * db.collection.updateone(<filter>, <update>, <options>)\n * db.collection.updatemany(<filter>, <update>, <options>)\n * db.collection.replaceone(<filter>, <update>, <options>)\n\n\n\n【示例】插入测试数据\n\ndb.inventory.insertmany([\n  {\n    item: 'canvas',\n    qty: 100,\n    size: { h: 28, w: 35.5, uom: 'cm' },\n    status: 'a'\n  },\n  { item: 'journal', qty: 25, size: { h: 14, w: 21, uom: 'cm' }, status: 'a' },\n  { item: 'mat', qty: 85, size: { h: 27.9, w: 35.5, uom: 'cm' }, status: 'a' },\n  {\n    item: 'mousepad',\n    qty: 25,\n    size: { h: 19, w: 22.85, uom: 'cm' },\n    status: 'p'\n  },\n  {\n    item: 'notebook',\n    qty: 50,\n    size: { h: 8.5, w: 11, uom: 'in' },\n    status: 'p'\n  },\n  { item: 'paper', qty: 100, size: { h: 8.5, w: 11, uom: 'in' }, status: 'd' },\n  {\n    item: 'planner',\n    qty: 75,\n    size: { h: 22.85, w: 30, uom: 'cm' },\n    status: 'd'\n  },\n  {\n    item: 'postcard',\n    qty: 45,\n    size: { h: 10, w: 15.25, uom: 'cm' },\n    status: 'a'\n  },\n  {\n    item: 'sketchbook',\n    qty: 80,\n    size: { h: 14, w: 21, uom: 'cm' },\n    status: 'a'\n  },\n  {\n    item: 'sketch pad',\n    qty: 95,\n    size: { h: 22.85, w: 30.5, uom: 'cm' },\n    status: 'a'\n  }\n])\n\n\n【示例】更新一条 document\n\ndb.inventory.updateone(\n  { item: 'paper' },\n  {\n    $set: { 'size.uom': 'cm', status: 'p' },\n    $currentdate: { lastmodified: true }\n  }\n)\n\n\n【示例】更新多条 document\n\ndb.inventory.updatemany(\n  { qty: { $lt: 50 } },\n  {\n    $set: { 'size.uom': 'in', status: 'p' },\n    $currentdate: { lastmodified: true }\n  }\n)\n\n\n【示例】替换一条 document\n\ndb.inventory.replaceone(\n  { item: 'paper' },\n  {\n    item: 'paper',\n    instock: [\n      { warehouse: 'a', qty: 60 },\n      { warehouse: 'b', qty: 40 }\n    ]\n  }\n)\n\n\n更新操作的特性：\n\n * mongodb 中的所有写操作都是单个文档级别的原子操作。\n * 一旦设置了，就无法更新或替换 _id 字段。\n * 除以下情况外，mongodb 会在执行写操作后保留文档字段的顺序：\n   * _id 字段始终是文档中的第一个字段。\n   * 包括重命名字段名称的更新可能导致文档中字段的重新排序。\n * 如果更新操作中包含 upsert : true 并且没有 document 匹配过滤器，mongodb 会新插入一个 document；如果有匹配的 document，mongodb 会修改或替换这些 document。\n\n\n# delete 操作\n\nmongodb 提供以下操作来删除 collection 中的 document\n\n * db.collection.deleteone()：删除一条 document\n * db.collection.deletemany()：删除多条 document\n\n\n\n删除操作的特性：\n\n * mongodb 中的所有写操作都是单个文档级别的原子操作。\n\n\n# 二、批量写操作\n\nmongodb 通过 db.collection.bulkwrite() 方法来支持批量写操作（包括批量插入、更新、删除）。\n\n此外，db.collection.insertmany() 方法支持批量插入操作。\n\n\n# 有序和无序的操作\n\n批量写操作可以有序或无序。\n\n * 对于有序列表，mongodb 串行执行操作。如果在写操作的处理过程中发生错误，mongodb 将不处理列表中剩余的写操作。\n * 对于无序列表，mongodb 可以并行执行操作，但是不能保证此行为。如果在写操作的处理过程中发生错误，mongodb 将继续处理列表中剩余的写操作。\n\n在分片集合上执行操作的有序列表通常比执行无序列表要慢，因为对于有序列表，每个操作必须等待上一个操作完成。\n\n默认情况下，bulkwrite() 执行有序操作。要指定无序写操作，请在选项文档中设置 ordered : false。\n\n\n# bulkwrite() 方法\n\nbulkwrite() 支持以下写操作：\n\n * insertone\n * updateone\n * updatemany\n * replaceone\n * deleteone\n * deletemany\n\n【示例】批量写操作示例\n\ntry {\n  db.characters.bulkwrite([\n    {\n      insertone: {\n        document: {\n          _id: 4,\n          char: 'dithras',\n          class: 'barbarian',\n          lvl: 4\n        }\n      }\n    },\n    {\n      insertone: {\n        document: {\n          _id: 5,\n          char: 'taeln',\n          class: 'fighter',\n          lvl: 3\n        }\n      }\n    },\n    {\n      updateone: {\n        filter: { char: 'eldon' },\n        update: { $set: { status: 'critical injury' } }\n      }\n    },\n    { deleteone: { filter: { char: 'brisbane' } } },\n    {\n      replaceone: {\n        filter: { char: 'meldane' },\n        replacement: { char: 'tanys', class: 'oracle', lvl: 4 }\n      }\n    }\n  ])\n} catch (e) {\n  print(e)\n}\n\n\n\n# 批量写操作策略\n\n大量的插入操作（包括初始数据插入或常规数据导入）可能会影响分片集群的性能。对于批量插入，请考虑以下策略：\n\n# 预拆分 collection\n\n如果分片集合为空，则该集合只有一个初始 chunk，该 chunk 位于单个分片上。然后，mongodb 必须花一些时间来接收数据，创建拆分并将拆分的块分发到可用的分片。为了避免这种性能成本，您可以按照拆分群集中的拆分块中的说明预拆分 collection。\n\n# 无序写操作\n\n要提高对分片集群的写入性能，请使用 bulkwrite()，并将可选参数顺序设置为 false。mongos 可以尝试同时将写入操作发送到多个分片。对于空集合，首先按照分片群集中的分割 chunk 中的说明预拆分 collection。\n\n# 避免单调节流\n\n如果在一次插入操作中，分片 key 单调递增，那么所有的插入数据都会存入 collection 的最后一个 chunk，也就是存入一个分片中。因此，集群的插入容量将永远不会超过该单个分片的插入容量。\n\n如果插入量大于单个分片可以处理的插入量，并且无法避免单调递增的分片键，那么请考虑对应用程序进行以下修改：\n\n * 反转分片密钥的二进制位。这样可以保留信息，并避免将插入顺序与值序列的增加关联起来。\n * 交换第一个和最后一个 16 位字以“随机”插入。\n\n\n# sql 和 mongodb 对比\n\n\n# 术语和概念\n\nsql 术语和概念                     mongodb 术语和概念\ndatabase                      database\ntable                         collection\nrow                           document 或 bson\ncolumn                        field\nindex                         index\ntable joins                   $lookup、嵌入式文档\nprimary key                   primary key\n                              mongodb 中自动设置主键为 _id 字段\naggregation (e.g. group by)   aggregation pipeline\n                              参考 sql to aggregation mapping chart.\nselect into new_table         $out\n                              参考 sql to aggregation mapping chart\nmerge into table              $merge (mongodb 4.2 开始支持)\n                              参考 sql to aggregation mapping chart.\nunion all                     $unionwith (mongodb 4.4 开始支持)\ntransactions                  transactions\n\n\n# 参考资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 的聚合操作",frontmatter:{title:"MongoDB 的聚合操作",date:"2020-09-21T21:22:57.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","聚合"],permalink:"/pages/75daa5/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/03.MongoDB%E7%9A%84%E8%81%9A%E5%90%88%E6%93%8D%E4%BD%9C.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/03.MongoDB的聚合操作.md",key:"v-0f37026c",path:"/pages/75daa5/",headers:[{level:2,title:"Pipeline",slug:"pipeline",normalizedTitle:"pipeline",charIndex:137},{level:3,title:"Pipeline 简介",slug:"pipeline-简介",normalizedTitle:"pipeline 简介",charIndex:150},{level:3,title:"Pipeline 优化",slug:"pipeline-优化",normalizedTitle:"pipeline 优化",charIndex:868},{level:4,title:"投影优化",slug:"投影优化",normalizedTitle:"投影优化",charIndex:883},{level:4,title:"Pipeline 串行优化",slug:"pipeline-串行优化",normalizedTitle:"pipeline 串行优化",charIndex:933},{level:4,title:"Pipeline 并行优化",slug:"pipeline-并行优化",normalizedTitle:"pipeline 并行优化",charIndex:2147},{level:5,title:"$sort + $limit",slug:"sort-limit",normalizedTitle:"$sort + $limit",charIndex:2217},{level:5,title:"$limit + $limit",slug:"limit-limit",normalizedTitle:"$limit + $limit",charIndex:2710},{level:5,title:"$skip + $skip",slug:"skip-skip",normalizedTitle:"$skip + $skip",charIndex:2832},{level:5,title:"$match + $match",slug:"match-match",normalizedTitle:"$match + $match",charIndex:2944},{level:5,title:"$lookup + $unwind",slug:"lookup-unwind",normalizedTitle:"$lookup + $unwind",charIndex:3144},{level:3,title:"Pipeline 限制",slug:"pipeline-限制",normalizedTitle:"pipeline 限制",charIndex:3614},{level:2,title:"Map-Reduce",slug:"map-reduce",normalizedTitle:"map-reduce",charIndex:3691},{level:2,title:"单一目的聚合方法",slug:"单一目的聚合方法",normalizedTitle:"单一目的聚合方法",charIndex:123},{level:2,title:"SQL 和 MongoDB 聚合对比",slug:"sql-和-mongodb-聚合对比",normalizedTitle:"sql 和 mongodb 聚合对比",charIndex:4488},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7427}],headersStr:"Pipeline Pipeline 简介 Pipeline 优化 投影优化 Pipeline 串行优化 Pipeline 并行优化 $sort + $limit $limit + $limit $skip + $skip $match + $match $lookup + $unwind Pipeline 限制 Map-Reduce 单一目的聚合方法 SQL 和 MongoDB 聚合对比 参考资料",content:"# MongoDB 的聚合操作\n\n聚合操作处理数据记录并返回计算结果。聚合操作将来自多个 document 的值分组，并可以对分组的数据执行各种操作以返回单个结果。 MongoDB 提供了三种执行聚合的方式：聚合管道，map-reduce 函数和单一目的聚合方法。\n\n\n# Pipeline\n\n\n# Pipeline 简介\n\nMongoDB 的聚合框架以数据处理管道（Pipeline）的概念为模型。\n\nMongoDB 通过 db.collection.aggregate() 方法支持聚合操作。并提供了 aggregate 命令来执行 pipeline。\n\nMongoDB Pipeline 由多个阶段（stages）组成。每个阶段在 document 通过 pipeline 时都会对其进行转换。pipeline 阶段不需要为每个输入 document 都生成一个输出 document。例如，某些阶段可能会生成新 document 或过滤 document。\n\n同一个阶段可以在 pipeline 中出现多次，但 $out、$merge,和 $geoNear 阶段除外。所有可用 pipeline 阶段可以参考：Aggregation Pipeline Stages。\n\n\n\n * 第一阶段：$match 阶段按状态字段过滤 document，然后将状态等于“ A”的那些 document 传递到下一阶段。\n * 第二阶段：$group 阶段按 cust_id 字段对 document 进行分组，以计算每个唯一 cust_id 的金额总和。\n\n最基本的管道阶段提供过滤器，其操作类似于查询和 document 转换（修改输出 document 形式）。\n\n其他管道操作提供了用于按特定字段对 document 进行分组和排序的工具，以及用于汇总数组（包括 document 数组）内容的工具。另外，管道阶段可以将运算符用于诸如计算平均值或连接字符串之类的任务。\n\n聚合管道也可以在分片 collection 上操作。\n\n\n# Pipeline 优化\n\n# 投影优化\n\nPipeline 可以确定是否仅需要 document 中必填字段即可获得结果。\n\n# Pipeline 串行优化\n\n（$project、$unset、$addFields、$set） + $match 串行优化\n\n对于包含投影阶段（$project 或 $unset 或 $addFields 或 $set），且后续跟随着 $match 阶段的 Pipeline ，MongoDB 会将所有 $match 阶段中不需要在投影阶段中计算出的值的过滤器，移动一个在投影阶段之前的新 $match 阶段。\n\n如果 Pipeline 包含多个投影阶段 和 / 或 $match 阶段，则 MongoDB 将为每个 $match 阶段执行此优化，将每个 $match 过滤器移动到该过滤器不依赖的所有投影阶段之前。\n\n【示例】Pipeline 串行优化示例\n\n优化前：\n\n{ $addFields: {\n    maxTime: { $max: \"$times\" },\n    minTime: { $min: \"$times\" }\n} },\n{ $project: {\n    _id: 1, name: 1, times: 1, maxTime: 1, minTime: 1,\n    avgTime: { $avg: [\"$maxTime\", \"$minTime\"] }\n} },\n{ $match: {\n    name: \"Joe Schmoe\",\n    maxTime: { $lt: 20 },\n    minTime: { $gt: 5 },\n    avgTime: { $gt: 7 }\n} }\n\n\n优化后：\n\n{ $match: { name: \"Joe Schmoe\" } },\n{ $addFields: {\n    maxTime: { $max: \"$times\" },\n    minTime: { $min: \"$times\" }\n} },\n{ $match: { maxTime: { $lt: 20 }, minTime: { $gt: 5 } } },\n{ $project: {\n    _id: 1, name: 1, times: 1, maxTime: 1, minTime: 1,\n    avgTime: { $avg: [\"$maxTime\", \"$minTime\"] }\n} },\n{ $match: { avgTime: { $gt: 7 } } }\n\n\n说明：\n\n{ name: \"Joe Schmoe\" } 不需要计算任何投影阶段的值，所以可以放在最前面。\n\n{ avgTime: { $gt: 7 } } 依赖 $project 阶段的 avgTime 字段，所以不能移动。\n\nmaxTime 和 minTime 字段被 $addFields 阶段所依赖，但自身不依赖其他，所以会新建一个 $match 阶段，并将其置于 $project 阶段之前。\n\n# Pipeline 并行优化\n\n如果可能，优化阶段会将 Pipeline 阶段合并到其前身。通常，合并发生在任意序列重新排序优化之后。\n\n# $sort + $limit\n\n当 $sort 在 $limit 之前时，如果没有中间阶段修改文档数量（例如 $unwind、$group），则优化程序可以将 $limit 合并到 $sort 中。如果有管道阶段更改了 $sort 和 $limit 阶段之间的文档数，则 MongoDB 不会将 $limit 合并到 $sort 中。\n\n【示例】$sort + $limit\n\n优化前：\n\n{ $sort : { age : -1 } },\n{ $project : { age : 1, status : 1, name : 1 } },\n{ $limit: 5 }\n\n\n优化后：\n\n{\n    \"$sort\" : {\n       \"sortKey\" : {\n          \"age\" : -1\n       },\n       \"limit\" : NumberLong(5)\n    }\n},\n{ \"$project\" : {\n         \"age\" : 1,\n         \"status\" : 1,\n         \"name\" : 1\n  }\n}\n\n\n# $limit + $limit\n\n如果一个 $limit 紧随另一个 $limit，那么它们可以合并为一。\n\n优化前：\n\n{ $limit: 100 },\n{ $limit: 10 }\n\n\n优化后：\n\n{\n  $limit: 10\n}\n\n\n# $skip + $skip\n\n如果一个 $skip 紧随另一个 $skip ，那么它们可以合并为一。\n\n优化前：\n\n{ $skip: 5 },\n{ $skip: 2 }\n\n\n优化后：\n\n{\n  $skip: 7\n}\n\n\n# $match + $match\n\n如果一个 $skip 紧随另一个 $skip ，那么它们可以通过 $and 合并为一。\n\n优化前：\n\n{ $match: { year: 2014 } },\n{ $match: { status: \"A\" } }\n\n\n优化后：\n\n{\n  $match: {\n    $and: [{ year: 2014 }, { status: 'A' }]\n  }\n}\n\n\n# $lookup + $unwind\n\n如果一个 $unwind 紧随另一个 $lookup，并且 $unwind 在 $lookup 的 as 字段上运行时，优化程序可以将 $unwind 合并到 $lookup 阶段。这样可以避免创建较大的中间文档。\n\n优化前：\n\n{\n  $lookup: {\n    from: \"otherCollection\",\n    as: \"resultingArray\",\n    localField: \"x\",\n    foreignField: \"y\"\n  }\n},\n{ $unwind: \"$resultingArray\"}\n\n\n优化后：\n\n{\n  $lookup: {\n    from: \"otherCollection\",\n    as: \"resultingArray\",\n    localField: \"x\",\n    foreignField: \"y\",\n    unwinding: { preserveNullAndEmptyArrays: false }\n  }\n}\n\n\n\n# Pipeline 限制\n\n结果集中的每个文档均受 BSON 文档大小限制（当前为 16 MB）\n\nPipeline 的内存限制为 100 MB。\n\n\n# Map-Reduce\n\n> 聚合 pipeline 比 map-reduce 提供更好的性能和更一致的接口。\n\nMap-reduce 是一种数据处理范式，用于将大量数据汇总为有用的聚合结果。为了执行 map-reduce 操作，MongoDB 提供了 mapReduce 数据库命令。\n\n\n\n在上面的操作中，MongoDB 将 map 阶段应用于每个输入 document（即 collection 中与查询条件匹配的 document）。 map 函数分发出多个键-值对。对于具有多个值的那些键，MongoDB 应用 reduce 阶段，该阶段收集并汇总聚合的数据。然后，MongoDB 将结果存储在 collection 中。可选地，reduce 函数的输出可以通过 finalize 函数来进一步汇总聚合结果。\n\nMongoDB 中的所有 map-reduce 函数都是 JavaScript，并在 mongod 进程中运行。 Map-reduce 操作将单个 collection 的 document 作为输入，并且可以在开始 map 阶段之前执行任意排序和限制。 mapReduce 可以将 map-reduce 操作的结果作为 document 返回，也可以将结果写入 collection。\n\n\n# 单一目的聚合方法\n\nMongoDB 支持一下单一目的的聚合操作：\n\n * db.collection.estimatedDocumentCount()\n * db.collection.count()\n * db.collection.distinct()\n\n所有这些操作都汇总了单个 collection 中的 document。尽管这些操作提供了对常见聚合过程的简单访问，但是它们相比聚合 pipeline 和 map-reduce，缺少灵活性和丰富的功能性。\n\n\n\n\n# SQL 和 MongoDB 聚合对比\n\nMongoDB pipeline 提供了许多等价于 SQL 中常见聚合语句的操作。\n\n下表概述了常见的 SQL 聚合语句或函数和 MongoDB 聚合操作的映射表：\n\nSQL TERMS, FUNCTIONS, AND CONCEPTS   MONGODB AGGREGATION OPERATORS\nWHERE                                $match\nGROUP BY                             $group\nHAVING                               $match\nSELECT                               $project\nORDER BY                             $sort\nLIMIT                                $limit\nSUM()                                $sum\nCOUNT()                              $sum$sortByCount\nJOIN                                 $lookup\nSELECT INTO NEW_TABLE                $out\nMERGE INTO TABLE                     $merge (Available starting in MongoDB 4.2)\nUNION ALL                            $unionWith (Available starting in MongoDB 4.4)\n\n【示例】\n\ndb.orders.insertMany([\n  {\n    _id: 1,\n    cust_id: 'Ant O. Knee',\n    ord_date: new Date('2020-03-01'),\n    price: 25,\n    items: [\n      { sku: 'oranges', qty: 5, price: 2.5 },\n      { sku: 'apples', qty: 5, price: 2.5 }\n    ],\n    status: 'A'\n  },\n  {\n    _id: 2,\n    cust_id: 'Ant O. Knee',\n    ord_date: new Date('2020-03-08'),\n    price: 70,\n    items: [\n      { sku: 'oranges', qty: 8, price: 2.5 },\n      { sku: 'chocolates', qty: 5, price: 10 }\n    ],\n    status: 'A'\n  },\n  {\n    _id: 3,\n    cust_id: 'Busby Bee',\n    ord_date: new Date('2020-03-08'),\n    price: 50,\n    items: [\n      { sku: 'oranges', qty: 10, price: 2.5 },\n      { sku: 'pears', qty: 10, price: 2.5 }\n    ],\n    status: 'A'\n  },\n  {\n    _id: 4,\n    cust_id: 'Busby Bee',\n    ord_date: new Date('2020-03-18'),\n    price: 25,\n    items: [{ sku: 'oranges', qty: 10, price: 2.5 }],\n    status: 'A'\n  },\n  {\n    _id: 5,\n    cust_id: 'Busby Bee',\n    ord_date: new Date('2020-03-19'),\n    price: 50,\n    items: [{ sku: 'chocolates', qty: 5, price: 10 }],\n    status: 'A'\n  },\n  {\n    _id: 6,\n    cust_id: 'Cam Elot',\n    ord_date: new Date('2020-03-19'),\n    price: 35,\n    items: [\n      { sku: 'carrots', qty: 10, price: 1.0 },\n      { sku: 'apples', qty: 10, price: 2.5 }\n    ],\n    status: 'A'\n  },\n  {\n    _id: 7,\n    cust_id: 'Cam Elot',\n    ord_date: new Date('2020-03-20'),\n    price: 25,\n    items: [{ sku: 'oranges', qty: 10, price: 2.5 }],\n    status: 'A'\n  },\n  {\n    _id: 8,\n    cust_id: 'Don Quis',\n    ord_date: new Date('2020-03-20'),\n    price: 75,\n    items: [\n      { sku: 'chocolates', qty: 5, price: 10 },\n      { sku: 'apples', qty: 10, price: 2.5 }\n    ],\n    status: 'A'\n  },\n  {\n    _id: 9,\n    cust_id: 'Don Quis',\n    ord_date: new Date('2020-03-20'),\n    price: 55,\n    items: [\n      { sku: 'carrots', qty: 5, price: 1.0 },\n      { sku: 'apples', qty: 10, price: 2.5 },\n      { sku: 'oranges', qty: 10, price: 2.5 }\n    ],\n    status: 'A'\n  },\n  {\n    _id: 10,\n    cust_id: 'Don Quis',\n    ord_date: new Date('2020-03-23'),\n    price: 25,\n    items: [{ sku: 'oranges', qty: 10, price: 2.5 }],\n    status: 'A'\n  }\n])\n\n\nSQL 和 MongoDB 聚合方式对比：\n\n\n\n\n# 参考资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课",normalizedContent:"# mongodb 的聚合操作\n\n聚合操作处理数据记录并返回计算结果。聚合操作将来自多个 document 的值分组，并可以对分组的数据执行各种操作以返回单个结果。 mongodb 提供了三种执行聚合的方式：聚合管道，map-reduce 函数和单一目的聚合方法。\n\n\n# pipeline\n\n\n# pipeline 简介\n\nmongodb 的聚合框架以数据处理管道（pipeline）的概念为模型。\n\nmongodb 通过 db.collection.aggregate() 方法支持聚合操作。并提供了 aggregate 命令来执行 pipeline。\n\nmongodb pipeline 由多个阶段（stages）组成。每个阶段在 document 通过 pipeline 时都会对其进行转换。pipeline 阶段不需要为每个输入 document 都生成一个输出 document。例如，某些阶段可能会生成新 document 或过滤 document。\n\n同一个阶段可以在 pipeline 中出现多次，但 $out、$merge,和 $geonear 阶段除外。所有可用 pipeline 阶段可以参考：aggregation pipeline stages。\n\n\n\n * 第一阶段：$match 阶段按状态字段过滤 document，然后将状态等于“ a”的那些 document 传递到下一阶段。\n * 第二阶段：$group 阶段按 cust_id 字段对 document 进行分组，以计算每个唯一 cust_id 的金额总和。\n\n最基本的管道阶段提供过滤器，其操作类似于查询和 document 转换（修改输出 document 形式）。\n\n其他管道操作提供了用于按特定字段对 document 进行分组和排序的工具，以及用于汇总数组（包括 document 数组）内容的工具。另外，管道阶段可以将运算符用于诸如计算平均值或连接字符串之类的任务。\n\n聚合管道也可以在分片 collection 上操作。\n\n\n# pipeline 优化\n\n# 投影优化\n\npipeline 可以确定是否仅需要 document 中必填字段即可获得结果。\n\n# pipeline 串行优化\n\n（$project、$unset、$addfields、$set） + $match 串行优化\n\n对于包含投影阶段（$project 或 $unset 或 $addfields 或 $set），且后续跟随着 $match 阶段的 pipeline ，mongodb 会将所有 $match 阶段中不需要在投影阶段中计算出的值的过滤器，移动一个在投影阶段之前的新 $match 阶段。\n\n如果 pipeline 包含多个投影阶段 和 / 或 $match 阶段，则 mongodb 将为每个 $match 阶段执行此优化，将每个 $match 过滤器移动到该过滤器不依赖的所有投影阶段之前。\n\n【示例】pipeline 串行优化示例\n\n优化前：\n\n{ $addfields: {\n    maxtime: { $max: \"$times\" },\n    mintime: { $min: \"$times\" }\n} },\n{ $project: {\n    _id: 1, name: 1, times: 1, maxtime: 1, mintime: 1,\n    avgtime: { $avg: [\"$maxtime\", \"$mintime\"] }\n} },\n{ $match: {\n    name: \"joe schmoe\",\n    maxtime: { $lt: 20 },\n    mintime: { $gt: 5 },\n    avgtime: { $gt: 7 }\n} }\n\n\n优化后：\n\n{ $match: { name: \"joe schmoe\" } },\n{ $addfields: {\n    maxtime: { $max: \"$times\" },\n    mintime: { $min: \"$times\" }\n} },\n{ $match: { maxtime: { $lt: 20 }, mintime: { $gt: 5 } } },\n{ $project: {\n    _id: 1, name: 1, times: 1, maxtime: 1, mintime: 1,\n    avgtime: { $avg: [\"$maxtime\", \"$mintime\"] }\n} },\n{ $match: { avgtime: { $gt: 7 } } }\n\n\n说明：\n\n{ name: \"joe schmoe\" } 不需要计算任何投影阶段的值，所以可以放在最前面。\n\n{ avgtime: { $gt: 7 } } 依赖 $project 阶段的 avgtime 字段，所以不能移动。\n\nmaxtime 和 mintime 字段被 $addfields 阶段所依赖，但自身不依赖其他，所以会新建一个 $match 阶段，并将其置于 $project 阶段之前。\n\n# pipeline 并行优化\n\n如果可能，优化阶段会将 pipeline 阶段合并到其前身。通常，合并发生在任意序列重新排序优化之后。\n\n# $sort + $limit\n\n当 $sort 在 $limit 之前时，如果没有中间阶段修改文档数量（例如 $unwind、$group），则优化程序可以将 $limit 合并到 $sort 中。如果有管道阶段更改了 $sort 和 $limit 阶段之间的文档数，则 mongodb 不会将 $limit 合并到 $sort 中。\n\n【示例】$sort + $limit\n\n优化前：\n\n{ $sort : { age : -1 } },\n{ $project : { age : 1, status : 1, name : 1 } },\n{ $limit: 5 }\n\n\n优化后：\n\n{\n    \"$sort\" : {\n       \"sortkey\" : {\n          \"age\" : -1\n       },\n       \"limit\" : numberlong(5)\n    }\n},\n{ \"$project\" : {\n         \"age\" : 1,\n         \"status\" : 1,\n         \"name\" : 1\n  }\n}\n\n\n# $limit + $limit\n\n如果一个 $limit 紧随另一个 $limit，那么它们可以合并为一。\n\n优化前：\n\n{ $limit: 100 },\n{ $limit: 10 }\n\n\n优化后：\n\n{\n  $limit: 10\n}\n\n\n# $skip + $skip\n\n如果一个 $skip 紧随另一个 $skip ，那么它们可以合并为一。\n\n优化前：\n\n{ $skip: 5 },\n{ $skip: 2 }\n\n\n优化后：\n\n{\n  $skip: 7\n}\n\n\n# $match + $match\n\n如果一个 $skip 紧随另一个 $skip ，那么它们可以通过 $and 合并为一。\n\n优化前：\n\n{ $match: { year: 2014 } },\n{ $match: { status: \"a\" } }\n\n\n优化后：\n\n{\n  $match: {\n    $and: [{ year: 2014 }, { status: 'a' }]\n  }\n}\n\n\n# $lookup + $unwind\n\n如果一个 $unwind 紧随另一个 $lookup，并且 $unwind 在 $lookup 的 as 字段上运行时，优化程序可以将 $unwind 合并到 $lookup 阶段。这样可以避免创建较大的中间文档。\n\n优化前：\n\n{\n  $lookup: {\n    from: \"othercollection\",\n    as: \"resultingarray\",\n    localfield: \"x\",\n    foreignfield: \"y\"\n  }\n},\n{ $unwind: \"$resultingarray\"}\n\n\n优化后：\n\n{\n  $lookup: {\n    from: \"othercollection\",\n    as: \"resultingarray\",\n    localfield: \"x\",\n    foreignfield: \"y\",\n    unwinding: { preservenullandemptyarrays: false }\n  }\n}\n\n\n\n# pipeline 限制\n\n结果集中的每个文档均受 bson 文档大小限制（当前为 16 mb）\n\npipeline 的内存限制为 100 mb。\n\n\n# map-reduce\n\n> 聚合 pipeline 比 map-reduce 提供更好的性能和更一致的接口。\n\nmap-reduce 是一种数据处理范式，用于将大量数据汇总为有用的聚合结果。为了执行 map-reduce 操作，mongodb 提供了 mapreduce 数据库命令。\n\n\n\n在上面的操作中，mongodb 将 map 阶段应用于每个输入 document（即 collection 中与查询条件匹配的 document）。 map 函数分发出多个键-值对。对于具有多个值的那些键，mongodb 应用 reduce 阶段，该阶段收集并汇总聚合的数据。然后，mongodb 将结果存储在 collection 中。可选地，reduce 函数的输出可以通过 finalize 函数来进一步汇总聚合结果。\n\nmongodb 中的所有 map-reduce 函数都是 javascript，并在 mongod 进程中运行。 map-reduce 操作将单个 collection 的 document 作为输入，并且可以在开始 map 阶段之前执行任意排序和限制。 mapreduce 可以将 map-reduce 操作的结果作为 document 返回，也可以将结果写入 collection。\n\n\n# 单一目的聚合方法\n\nmongodb 支持一下单一目的的聚合操作：\n\n * db.collection.estimateddocumentcount()\n * db.collection.count()\n * db.collection.distinct()\n\n所有这些操作都汇总了单个 collection 中的 document。尽管这些操作提供了对常见聚合过程的简单访问，但是它们相比聚合 pipeline 和 map-reduce，缺少灵活性和丰富的功能性。\n\n\n\n\n# sql 和 mongodb 聚合对比\n\nmongodb pipeline 提供了许多等价于 sql 中常见聚合语句的操作。\n\n下表概述了常见的 sql 聚合语句或函数和 mongodb 聚合操作的映射表：\n\nsql terms, functions, and concepts   mongodb aggregation operators\nwhere                                $match\ngroup by                             $group\nhaving                               $match\nselect                               $project\norder by                             $sort\nlimit                                $limit\nsum()                                $sum\ncount()                              $sum$sortbycount\njoin                                 $lookup\nselect into new_table                $out\nmerge into table                     $merge (available starting in mongodb 4.2)\nunion all                            $unionwith (available starting in mongodb 4.4)\n\n【示例】\n\ndb.orders.insertmany([\n  {\n    _id: 1,\n    cust_id: 'ant o. knee',\n    ord_date: new date('2020-03-01'),\n    price: 25,\n    items: [\n      { sku: 'oranges', qty: 5, price: 2.5 },\n      { sku: 'apples', qty: 5, price: 2.5 }\n    ],\n    status: 'a'\n  },\n  {\n    _id: 2,\n    cust_id: 'ant o. knee',\n    ord_date: new date('2020-03-08'),\n    price: 70,\n    items: [\n      { sku: 'oranges', qty: 8, price: 2.5 },\n      { sku: 'chocolates', qty: 5, price: 10 }\n    ],\n    status: 'a'\n  },\n  {\n    _id: 3,\n    cust_id: 'busby bee',\n    ord_date: new date('2020-03-08'),\n    price: 50,\n    items: [\n      { sku: 'oranges', qty: 10, price: 2.5 },\n      { sku: 'pears', qty: 10, price: 2.5 }\n    ],\n    status: 'a'\n  },\n  {\n    _id: 4,\n    cust_id: 'busby bee',\n    ord_date: new date('2020-03-18'),\n    price: 25,\n    items: [{ sku: 'oranges', qty: 10, price: 2.5 }],\n    status: 'a'\n  },\n  {\n    _id: 5,\n    cust_id: 'busby bee',\n    ord_date: new date('2020-03-19'),\n    price: 50,\n    items: [{ sku: 'chocolates', qty: 5, price: 10 }],\n    status: 'a'\n  },\n  {\n    _id: 6,\n    cust_id: 'cam elot',\n    ord_date: new date('2020-03-19'),\n    price: 35,\n    items: [\n      { sku: 'carrots', qty: 10, price: 1.0 },\n      { sku: 'apples', qty: 10, price: 2.5 }\n    ],\n    status: 'a'\n  },\n  {\n    _id: 7,\n    cust_id: 'cam elot',\n    ord_date: new date('2020-03-20'),\n    price: 25,\n    items: [{ sku: 'oranges', qty: 10, price: 2.5 }],\n    status: 'a'\n  },\n  {\n    _id: 8,\n    cust_id: 'don quis',\n    ord_date: new date('2020-03-20'),\n    price: 75,\n    items: [\n      { sku: 'chocolates', qty: 5, price: 10 },\n      { sku: 'apples', qty: 10, price: 2.5 }\n    ],\n    status: 'a'\n  },\n  {\n    _id: 9,\n    cust_id: 'don quis',\n    ord_date: new date('2020-03-20'),\n    price: 55,\n    items: [\n      { sku: 'carrots', qty: 5, price: 1.0 },\n      { sku: 'apples', qty: 10, price: 2.5 },\n      { sku: 'oranges', qty: 10, price: 2.5 }\n    ],\n    status: 'a'\n  },\n  {\n    _id: 10,\n    cust_id: 'don quis',\n    ord_date: new date('2020-03-23'),\n    price: 25,\n    items: [{ sku: 'oranges', qty: 10, price: 2.5 }],\n    status: 'a'\n  }\n])\n\n\nsql 和 mongodb 聚合方式对比：\n\n\n\n\n# 参考资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 事务",frontmatter:{title:"MongoDB 事务",date:"2020-09-20T23:12:17.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","事务"],permalink:"/pages/4574fe/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/04.MongoDB%E4%BA%8B%E5%8A%A1.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/04.MongoDB事务.md",key:"v-f8f28ef4",path:"/pages/4574fe/",headersStr:null,content:"# MongoDB 事务\n\nwriteConcern 可以决定写操作到达多少个节点才算成功。\n\n * 默认：多节点复制集不做任何设定，所以是有可能丢失数据。\n * w: \"majority\"：大部分节点确认，就视为写成功\n * w: \"all\"：全部节点确认，才视为写成功\n\njournal 则定义如何才算成功。取值包括：\n\n * true：写操作落到 journal 文件中才算成功；\n * false：写操作达到内存即算作成功。\n\n【示例】在集群中使用 writeConcern 参数\n\ndb.transaction.insert({ count: 1 }, { writeConcern: { w: 'majoriy' } })\ndb.transaction.insert({ count: 1 }, { writeConcern: { w: '4' } })\ndb.transaction.insert({ count: 1 }, { writeConcern: { w: 'all' } })\n\n\n【示例】配置延迟节点，模拟网络延迟\n\nconf=rs.conf()\nconf.memebers[2].slaveDelay=5\nconf.memebers[2].priority=0\nrs.reconfig(conf)\n",normalizedContent:"# mongodb 事务\n\nwriteconcern 可以决定写操作到达多少个节点才算成功。\n\n * 默认：多节点复制集不做任何设定，所以是有可能丢失数据。\n * w: \"majority\"：大部分节点确认，就视为写成功\n * w: \"all\"：全部节点确认，才视为写成功\n\njournal 则定义如何才算成功。取值包括：\n\n * true：写操作落到 journal 文件中才算成功；\n * false：写操作达到内存即算作成功。\n\n【示例】在集群中使用 writeconcern 参数\n\ndb.transaction.insert({ count: 1 }, { writeconcern: { w: 'majoriy' } })\ndb.transaction.insert({ count: 1 }, { writeconcern: { w: '4' } })\ndb.transaction.insert({ count: 1 }, { writeconcern: { w: 'all' } })\n\n\n【示例】配置延迟节点，模拟网络延迟\n\nconf=rs.conf()\nconf.memebers[2].slavedelay=5\nconf.memebers[2].priority=0\nrs.reconfig(conf)\n",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 建模",frontmatter:{title:"MongoDB 建模",date:"2020-09-09T20:47:14.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","建模"],permalink:"/pages/562f99/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/05.MongoDB%E5%BB%BA%E6%A8%A1.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/05.MongoDB建模.md",key:"v-925d1ece",path:"/pages/562f99/",headers:[{level:2,title:"MongoDB 数据建模入门",slug:"mongodb-数据建模入门",normalizedTitle:"mongodb 数据建模入门",charIndex:295},{level:3,title:"（一）定义数据集",slug:"一-定义数据集",normalizedTitle:"（一）定义数据集",charIndex:390},{level:3,title:"（二）思考 JSON 结构",slug:"二-思考-json-结构",normalizedTitle:"（二）思考 json 结构",charIndex:992},{level:3,title:"（三）确定哪些字段作为嵌入式数据",slug:"三-确定哪些字段作为嵌入式数据",normalizedTitle:"（三）确定哪些字段作为嵌入式数据",charIndex:1564},{level:2,title:"数据模型简介",slug:"数据模型简介",normalizedTitle:"数据模型简介",charIndex:2357},{level:3,title:"灵活的 Schema",slug:"灵活的-schema",normalizedTitle:"灵活的 schema",charIndex:2461},{level:3,title:"Document 结构",slug:"document-结构",normalizedTitle:"document 结构",charIndex:2865},{level:4,title:"嵌入式数据模型",slug:"嵌入式数据模型",normalizedTitle:"嵌入式数据模型",charIndex:2880},{level:4,title:"引用式数据模型",slug:"引用式数据模型",normalizedTitle:"引用式数据模型",charIndex:3136},{level:3,title:"原子写操作",slug:"原子写操作",normalizedTitle:"原子写操作",charIndex:3580},{level:4,title:"单 document 的原子性",slug:"单-document-的原子性",normalizedTitle:"单 document 的原子性",charIndex:3589},{level:4,title:"多 document 事务",slug:"多-document-事务",normalizedTitle:"多 document 事务",charIndex:3854},{level:3,title:"数据使用和性能",slug:"数据使用和性能",normalizedTitle:"数据使用和性能",charIndex:4229},{level:2,title:"Schema 校验",slug:"schema-校验",normalizedTitle:"schema 校验",charIndex:4355},{level:3,title:"指定校验规则",slug:"指定校验规则",normalizedTitle:"指定校验规则",charIndex:4369},{level:3,title:"JSON Schema",slug:"json-schema",normalizedTitle:"json schema",charIndex:4693},{level:3,title:"其它查询表达式",slug:"其它查询表达式",normalizedTitle:"其它查询表达式",charIndex:5992},{level:3,title:"行为",slug:"行为",normalizedTitle:"行为",charIndex:6386},{level:4,title:"现有的 document",slug:"现有的-document",normalizedTitle:"现有的 document",charIndex:6427},{level:4,title:"接受或拒绝无效的 document",slug:"接受或拒绝无效的-document",normalizedTitle:"接受或拒绝无效的 document",charIndex:7576},{level:4,title:"限制",slug:"限制",normalizedTitle:"限制",charIndex:82},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:8886}],headersStr:"MongoDB 数据建模入门 （一）定义数据集 （二）思考 JSON 结构 （三）确定哪些字段作为嵌入式数据 数据模型简介 灵活的 Schema Document 结构 嵌入式数据模型 引用式数据模型 原子写操作 单 document 的原子性 多 document 事务 数据使用和性能 Schema 校验 指定校验规则 JSON Schema 其它查询表达式 行为 现有的 document 接受或拒绝无效的 document 限制 参考资料",content:"# MongoDB 建模\n\nMongoDB 的数据模式是一种灵活模式，关系型数据库要求你在插入数据之前必须先定义好一个表的模式结构，而 MongoDB 的集合则并不限制 document 结构。这种灵活性让对象和数据库文档之间的映射变得很容易。即使数据记录之间有很大的变化，每个文档也可以很好的映射到各条不同的记录。 当然在实际使用中，同一个集合中的文档往往都有一个比较类似的结构。\n\n数据模型设计中最具挑战性的是在应用程序需求，数据库引擎性能要求和数据读写模式之间做权衡考量。当设计数据模型的时候，一定要考虑应用程序对数据的使用模式（如查询，更新和处理）以及数据本身的天然结构。\n\n\n# MongoDB 数据建模入门\n\n> 参考：https://docs.mongodb.com/guides/server/introduction/#what-you-ll-need\n\n\n# （一）定义数据集\n\n当需要建立数据存储时，首先应该思考以下问题：需要存储哪些数据？这些字段之间如何关联？\n\n这是一个数据建模的过程。目标是将业务需求抽象为逻辑模型。\n\n假设这样一个场景：我们需要建立数据库以跟踪物料及其数量，大小，标签和等级。\n\n如果是存储在 RDBMS，可能以下的数据表：\n\nNAME       QUANTITY   SIZE          STATUS   TAGS                       RATING\njournal    25         14x21,cm      A        brown, lined               9\nnotebook   50         8.5x11,in     A        college-ruled,perforated   8\npaper      100        8.5x11,in     D        watercolor                 10\nplanner    75         22.85x30,cm   D        2019                       10\npostcard   45         10x,cm        D        double-sided,white         2\n\n\n# （二）思考 JSON 结构\n\n从上例中可以看出，表似乎是存储数据的好地方，但该数据集中的字段需要多个值，如果在单个列中建模，则不容易搜索或显示（对于 例如–大小和标签）。\n\n在 SQL 数据库中，您可以通过创建关系表来解决此问题。\n\n在 MongoDB 中，数据存储为文档（document）。 这些文档以 JSON（JavaScript 对象表示法）格式存储在 MongoDB 中。 JSON 文档支持嵌入式字段，因此相关数据和数据列表可以与文档一起存储，而不是与外部表一起存储。\n\nJSON 格式为键/值对。 在 JSON 文档中，字段名和值用冒号分隔，字段名和值对用逗号分隔，并且字段集封装在“大括号”（{}）中。\n\n如果要开始对上面的行之一进行建模，例如此行：\n\nNAME       QUANTITY   SIZE        STATUS   TAGS                       RATING\nnotebook   50         8.5x11,in   A        college-ruled,perforated   8\n\n您可以从 name 和 quantity 字段开始。 在 JSON 中，这些字段如下所示：\n\n{ \"name\": \"notebook\", \"qty\": 50 }\n\n\n\n# （三）确定哪些字段作为嵌入式数据\n\n接下来，需要确定哪些字段可能需要多个值。可以考虑将这些字段作为嵌入式文档或嵌入式文档中的 列表/数组 对象。\n\n例如，在上面的示例中，size 可能包含三个字段：\n\n{ \"h\": 11, \"w\": 8.5, \"uom\": \"in\" }\n\n\nAnd some items have multiple ratings, so ratings might be represented as a list of documents containing the field scores:\n\n[{ \"score\": 8 }, { \"score\": 9 }]\n\n\nAnd you might need to handle multiple tags per item. So you might store them in a list too.\n\n[\"college-ruled\", \"perforated\"]\n\n\nFinally, a JSON document that stores an inventory item might look like this:\n\n{\n  \"name\": \"notebook\",\n  \"qty\": 50,\n  \"rating\": [{ \"score\": 8 }, { \"score\": 9 }],\n  \"size\": { \"height\": 11, \"width\": 8.5, \"unit\": \"in\" },\n  \"status\": \"A\",\n  \"tags\": [\"college-ruled\", \"perforated\"]\n}\n\n\nThis looks very different from the tabular data structure you started with in Step 1.\n\n\n# 数据模型简介\n\n数据建模中的关键挑战是平衡应用程序的需求、数据库引擎的性能以及数据检索模式。 在设计数据模型时，始终需要考虑数据的应用程序使用情况（即数据的查询，更新和处理）以及数据本身的固有结构。\n\n\n# 灵活的 Schema\n\n在关系型数据库中，必须在插入数据之前确定并声明表的结构。而 MongoDB 的 collection 默认情况下不需要其文档具有相同的架构。也就是说：\n\n同一个 collection 中的 document 不需要具有相同的 field 集，并且 field 的数据类型可以在集合中的不同文档之间有所不同。\n\n要更改 collection 中的 document 结构，例如添加新 field，删除现有 field 或将 field 值更改为新类型，只需要将文档更新为新结构即可。\n\n这种灵活性有助于将 document 映射到实体或对象。每个 document 都可以匹配所表示实体的数据字段，即使该文档与集合中的其他文档有很大的不同。但是，实际上，集合中的文档具有相似的结构，并且您可以在更新和插入操作期间对 collection 强制执行 document 校验规则。\n\n\n# Document 结构\n\n# 嵌入式数据模型\n\n嵌入式 document 通过将相关数据存储在单个 document 结构中来捕获数据之间的关系。 MongoDB document 可以将 document 结构嵌入到另一个 document 中的字段或数组中。这些非规范化的数据模型允许应用程序在单个数据库操作中检索和操纵相关数据。\n\n\n\n对于 MongoDB 中的很多场景，非规范化数据模型都是最佳的。\n\n> 嵌入式 document 有大小限制：必须小于 16 MB。\n> \n> 如果是较大的二进制数据，可以考虑 GridFS。\n\n# 引用式数据模型\n\n引用通过包含从一个 document 到另一个 document 的链接或引用来存储数据之间的关系。 应用程序可以解析这些引用以访问相关数据。 广义上讲，这些是规范化的数据模型。\n\n\n\n通常，在以下场景使用引用式的数据模型：\n\n * 嵌入时会导致数据重复，但无法提供足够的读取性能优势，无法胜过重复的含义。\n * 代表更复杂的多对多关系。\n * 为大规模分层数据集建模。\n\n为了 join collection，MongoDB 支持聚合 stage：\n\n * $lookup（MongoDB 3.2 开始支持）\n * $graphLookup（MongoDB 3.4 开始支持）\n\nMongoDB 还提供了引用来支持跨集合 join 数据：\n\n * 引用数据模型示例，参考：Model One-to-Many Relationships with Document References.\n * 更多树形模型，参考：Model Tree Structures.\n\n\n# 原子写操作\n\n# 单 document 的原子性\n\n在 MongoDB 中，针对单个 document 的写操作是原子性的，即使该 document 中嵌入了多个子 document。 具有嵌入数据的非规范化数据模型将所有相关数据合并在一个 document 中，而不是在多个 document 和 collection 中进行规范化。 该数据模型有助于原子操作。 当单个写入操作（例如 db.collection.updateMany()）修改多个 document 时，每个 document 的独立修改是原子的，但整个操作不是原子的。\n\n# 多 document 事务\n\n对于需要对多个 document（在单个或多个集合中）进行读写原子性的情况，MongoDB 支持多 document 事务。\n\n * 在版本 4.0 中，MongoDB 在副本集上支持多 document 事务。\n * 在版本 4.2 中，MongoDB 引入了分布式事务，它增加了对分片群集上多 document 事务的支持，并合并了对副本集上多 document 事务的现有支持。\n\n> 在大多数情况下，多 document 事务会比单 document 的写入产生更高的性能消耗，并且多 document 事务的可用性不能替代高效的结构设计。 在许多情况下，非规范化数据模型（嵌入式 document 和数组）仍是最佳选择。 也就是说，合理的数据建模，将最大程度地减少对多 document 事务的需求。\n\n\n# 数据使用和性能\n\n在设计数据模型时，请考虑应用程序将如何使用您的数据库。 例如，如果您的应用程序仅使用最近插入的 document，请考虑使用上限集合。 或者，如果您的应用程序主要是对 collection 的读取操作，则添加索引以提高性能。\n\n\n# Schema 校验\n\n\n# 指定校验规则\n\n如果创建新 collection 时要指定校验规则，需要在使用 db.createCollection() 时指定 validator 选项。\n\n如果要将 document 校验添加到现有 collection 中，需要使用带有 validator 选项的 collMod 命令。\n\nMongoDB 还提供以下相关选项：\n\n * validationLevel 选项（用于确定 MongoDB 在更新过程中，对现有 document 应用校验规则的严格程度）\n * validationAction 选项（用于确定 MongoDB 发现违反校验规则的 document 时，是选择报错并拒绝，还是接受数据但在日志中告警）。\n\n\n# JSON Schema\n\n从 3.6 版本开始，MongoDB 开始支持 JSON Schema 校验。\n\n可以通过在 validator 表达式中使用 $jsonSchema 操作来指定 JSON Schema 校验。\n\n【示例】\n\ndb.createCollection('students', {\n  validator: {\n    $jsonSchema: {\n      bsonType: 'object',\n      required: ['name', 'year', 'major', 'address'],\n      properties: {\n        name: {\n          bsonType: 'string',\n          description: 'must be a string and is required'\n        },\n        year: {\n          bsonType: 'int',\n          minimum: 2017,\n          maximum: 3017,\n          description: 'must be an integer in [ 2017, 3017 ] and is required'\n        },\n        major: {\n          enum: ['Math', 'English', 'Computer Science', 'History', null],\n          description: 'can only be one of the enum values and is required'\n        },\n        gpa: {\n          bsonType: ['double'],\n          description: 'must be a double if the field exists'\n        },\n        address: {\n          bsonType: 'object',\n          required: ['city'],\n          properties: {\n            street: {\n              bsonType: 'string',\n              description: 'must be a string if the field exists'\n            },\n            city: {\n              bsonType: 'string',\n              description: 'must be a string and is required'\n            }\n          }\n        }\n      }\n    }\n  }\n})\n\n\n\n# 其它查询表达式\n\n除了使用 $jsonSchema 查询运算符的 JSON Schema 校验外，MongoDB 还支持其它查询运算符的校验，但以下情况除外：\n\n * $near,\n * $nearSphere,\n * $text,\n * $where, and\n * 带有 $function 表达式的 $expr\n\n【示例】查询表达式中指定校验规则\n\ndb.createCollection('contacts', {\n  validator: {\n    $or: [\n      { phone: { $type: 'string' } },\n      { email: { $regex: /@mongodb\\.com$/ } },\n      { status: { $in: ['Unknown', 'Incomplete'] } }\n    ]\n  }\n})\n\n\n\n# 行为\n\n校验发生在更新和插入期间。添加校验规则到 collection 时，不会对现有的 document 进行校验，除非发生修改操作。\n\n# 现有的 document\n\nvalidationLevel 选项确定 MongoDB 进行规则校验时执行的操作：\n\n * 如果 validationLevel 是 strict（严格级别。这是 MongoDB 默认级别），则 MongoDB 将校验规则应用于所有插入和更新。\n * 如果 validationLevel 是 moderate（中等级别），则 MongoDB 只对已满足校验条件的现有文档的插入和更新操作进行校验；对不符合校验标准的现有文档的更新操作不进行校验。\n\n【示例】\n\n下面是一个正常的插入操作：\n\ndb.contacts.insert([\n  {\n    _id: 1,\n    name: 'Anne',\n    phone: '+1 555 123 456',\n    city: 'London',\n    status: 'Complete'\n  },\n  { _id: 2, name: 'Ivan', city: 'Vancouver' }\n])\n\n\n在 collection 上配置一个校验规则：\n\ndb.runCommand({\n  collMod: 'contacts',\n  validator: {\n    $jsonSchema: {\n      bsonType: 'object',\n      required: ['phone', 'name'],\n      properties: {\n        phone: {\n          bsonType: 'string',\n          description: 'must be a string and is required'\n        },\n        name: {\n          bsonType: 'string',\n          description: 'must be a string and is required'\n        }\n      }\n    }\n  },\n  validationLevel: 'moderate'\n})\n\n\n则 contacts collection 现在添加了含中等级别（moderate） validationLevel 的 validator：\n\n * 如果尝试更新 _id为 1 的文档，则 MongoDB 将应用校验规则，因为现有文档符合条件。\n\n * 相反，MongoDB 不会将校验 _id 为 2 的文档，因为它不符合校验规则。\n\n如果要完全禁用校验，可以将 validationLevel 置为 off。\n\n# 接受或拒绝无效的 document\n\n * 如果 validationAction 是 Error（默认），则 MongoDB 拒绝任何违反校验规则的插入或更新。\n * 如果 validationAction 是 Warn，MongoDB 会记录所有的违规，但允许进行插入或更新。\n\n【示例】\n\n创建集合时，配置 validationAction 为 warn。\n\ndb.createCollection('contacts2', {\n  validator: {\n    $jsonSchema: {\n      bsonType: 'object',\n      required: ['phone'],\n      properties: {\n        phone: {\n          bsonType: 'string',\n          description: 'must be a string and is required'\n        },\n        email: {\n          bsonType: 'string',\n          pattern: '@mongodb.com$',\n          description:\n            'must be a string and match the regular expression pattern'\n        },\n        status: {\n          enum: ['Unknown', 'Incomplete'],\n          description: 'can only be one of the enum values'\n        }\n      }\n    }\n  },\n  validationAction: 'warn'\n})\n\n\n尝试插入一条违规记录\n\n> db.contacts2.insert( { name: \"Amanda\", status: \"Updated\" } )\nWriteResult({ \"nInserted\" : 1 })\n\n\nMongoDB 允许这条操作执行，但是服务器会记录下告警信息。\n\n{\"t\":{\"$date\":\"2020-09-11T16:35:57.754+08:00\"},\"s\":\"W\",  \"c\":\"STORAGE\",  \"id\":20294,   \"ctx\":\"conn14\",\"msg\":\"Document would fail validation\",\"attr\":{\"namespace\":\"test.contacts2\",\"document\":{\"_id\":{\"$oid\":\"5f5b36ed8ea53d62a0b51c4e\"},\"name\":\"Amanda\",\"status\":\"Updated\"}}}\n\n\n# 限制\n\n不能在 admin、local、config 这几个特殊的数据库中指定校验规则。\n\n不能在 system.* collection 中指定校验。\n\n\n# 参考资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课",normalizedContent:"# mongodb 建模\n\nmongodb 的数据模式是一种灵活模式，关系型数据库要求你在插入数据之前必须先定义好一个表的模式结构，而 mongodb 的集合则并不限制 document 结构。这种灵活性让对象和数据库文档之间的映射变得很容易。即使数据记录之间有很大的变化，每个文档也可以很好的映射到各条不同的记录。 当然在实际使用中，同一个集合中的文档往往都有一个比较类似的结构。\n\n数据模型设计中最具挑战性的是在应用程序需求，数据库引擎性能要求和数据读写模式之间做权衡考量。当设计数据模型的时候，一定要考虑应用程序对数据的使用模式（如查询，更新和处理）以及数据本身的天然结构。\n\n\n# mongodb 数据建模入门\n\n> 参考：https://docs.mongodb.com/guides/server/introduction/#what-you-ll-need\n\n\n# （一）定义数据集\n\n当需要建立数据存储时，首先应该思考以下问题：需要存储哪些数据？这些字段之间如何关联？\n\n这是一个数据建模的过程。目标是将业务需求抽象为逻辑模型。\n\n假设这样一个场景：我们需要建立数据库以跟踪物料及其数量，大小，标签和等级。\n\n如果是存储在 rdbms，可能以下的数据表：\n\nname       quantity   size          status   tags                       rating\njournal    25         14x21,cm      a        brown, lined               9\nnotebook   50         8.5x11,in     a        college-ruled,perforated   8\npaper      100        8.5x11,in     d        watercolor                 10\nplanner    75         22.85x30,cm   d        2019                       10\npostcard   45         10x,cm        d        double-sided,white         2\n\n\n# （二）思考 json 结构\n\n从上例中可以看出，表似乎是存储数据的好地方，但该数据集中的字段需要多个值，如果在单个列中建模，则不容易搜索或显示（对于 例如–大小和标签）。\n\n在 sql 数据库中，您可以通过创建关系表来解决此问题。\n\n在 mongodb 中，数据存储为文档（document）。 这些文档以 json（javascript 对象表示法）格式存储在 mongodb 中。 json 文档支持嵌入式字段，因此相关数据和数据列表可以与文档一起存储，而不是与外部表一起存储。\n\njson 格式为键/值对。 在 json 文档中，字段名和值用冒号分隔，字段名和值对用逗号分隔，并且字段集封装在“大括号”（{}）中。\n\n如果要开始对上面的行之一进行建模，例如此行：\n\nname       quantity   size        status   tags                       rating\nnotebook   50         8.5x11,in   a        college-ruled,perforated   8\n\n您可以从 name 和 quantity 字段开始。 在 json 中，这些字段如下所示：\n\n{ \"name\": \"notebook\", \"qty\": 50 }\n\n\n\n# （三）确定哪些字段作为嵌入式数据\n\n接下来，需要确定哪些字段可能需要多个值。可以考虑将这些字段作为嵌入式文档或嵌入式文档中的 列表/数组 对象。\n\n例如，在上面的示例中，size 可能包含三个字段：\n\n{ \"h\": 11, \"w\": 8.5, \"uom\": \"in\" }\n\n\nand some items have multiple ratings, so ratings might be represented as a list of documents containing the field scores:\n\n[{ \"score\": 8 }, { \"score\": 9 }]\n\n\nand you might need to handle multiple tags per item. so you might store them in a list too.\n\n[\"college-ruled\", \"perforated\"]\n\n\nfinally, a json document that stores an inventory item might look like this:\n\n{\n  \"name\": \"notebook\",\n  \"qty\": 50,\n  \"rating\": [{ \"score\": 8 }, { \"score\": 9 }],\n  \"size\": { \"height\": 11, \"width\": 8.5, \"unit\": \"in\" },\n  \"status\": \"a\",\n  \"tags\": [\"college-ruled\", \"perforated\"]\n}\n\n\nthis looks very different from the tabular data structure you started with in step 1.\n\n\n# 数据模型简介\n\n数据建模中的关键挑战是平衡应用程序的需求、数据库引擎的性能以及数据检索模式。 在设计数据模型时，始终需要考虑数据的应用程序使用情况（即数据的查询，更新和处理）以及数据本身的固有结构。\n\n\n# 灵活的 schema\n\n在关系型数据库中，必须在插入数据之前确定并声明表的结构。而 mongodb 的 collection 默认情况下不需要其文档具有相同的架构。也就是说：\n\n同一个 collection 中的 document 不需要具有相同的 field 集，并且 field 的数据类型可以在集合中的不同文档之间有所不同。\n\n要更改 collection 中的 document 结构，例如添加新 field，删除现有 field 或将 field 值更改为新类型，只需要将文档更新为新结构即可。\n\n这种灵活性有助于将 document 映射到实体或对象。每个 document 都可以匹配所表示实体的数据字段，即使该文档与集合中的其他文档有很大的不同。但是，实际上，集合中的文档具有相似的结构，并且您可以在更新和插入操作期间对 collection 强制执行 document 校验规则。\n\n\n# document 结构\n\n# 嵌入式数据模型\n\n嵌入式 document 通过将相关数据存储在单个 document 结构中来捕获数据之间的关系。 mongodb document 可以将 document 结构嵌入到另一个 document 中的字段或数组中。这些非规范化的数据模型允许应用程序在单个数据库操作中检索和操纵相关数据。\n\n\n\n对于 mongodb 中的很多场景，非规范化数据模型都是最佳的。\n\n> 嵌入式 document 有大小限制：必须小于 16 mb。\n> \n> 如果是较大的二进制数据，可以考虑 gridfs。\n\n# 引用式数据模型\n\n引用通过包含从一个 document 到另一个 document 的链接或引用来存储数据之间的关系。 应用程序可以解析这些引用以访问相关数据。 广义上讲，这些是规范化的数据模型。\n\n\n\n通常，在以下场景使用引用式的数据模型：\n\n * 嵌入时会导致数据重复，但无法提供足够的读取性能优势，无法胜过重复的含义。\n * 代表更复杂的多对多关系。\n * 为大规模分层数据集建模。\n\n为了 join collection，mongodb 支持聚合 stage：\n\n * $lookup（mongodb 3.2 开始支持）\n * $graphlookup（mongodb 3.4 开始支持）\n\nmongodb 还提供了引用来支持跨集合 join 数据：\n\n * 引用数据模型示例，参考：model one-to-many relationships with document references.\n * 更多树形模型，参考：model tree structures.\n\n\n# 原子写操作\n\n# 单 document 的原子性\n\n在 mongodb 中，针对单个 document 的写操作是原子性的，即使该 document 中嵌入了多个子 document。 具有嵌入数据的非规范化数据模型将所有相关数据合并在一个 document 中，而不是在多个 document 和 collection 中进行规范化。 该数据模型有助于原子操作。 当单个写入操作（例如 db.collection.updatemany()）修改多个 document 时，每个 document 的独立修改是原子的，但整个操作不是原子的。\n\n# 多 document 事务\n\n对于需要对多个 document（在单个或多个集合中）进行读写原子性的情况，mongodb 支持多 document 事务。\n\n * 在版本 4.0 中，mongodb 在副本集上支持多 document 事务。\n * 在版本 4.2 中，mongodb 引入了分布式事务，它增加了对分片群集上多 document 事务的支持，并合并了对副本集上多 document 事务的现有支持。\n\n> 在大多数情况下，多 document 事务会比单 document 的写入产生更高的性能消耗，并且多 document 事务的可用性不能替代高效的结构设计。 在许多情况下，非规范化数据模型（嵌入式 document 和数组）仍是最佳选择。 也就是说，合理的数据建模，将最大程度地减少对多 document 事务的需求。\n\n\n# 数据使用和性能\n\n在设计数据模型时，请考虑应用程序将如何使用您的数据库。 例如，如果您的应用程序仅使用最近插入的 document，请考虑使用上限集合。 或者，如果您的应用程序主要是对 collection 的读取操作，则添加索引以提高性能。\n\n\n# schema 校验\n\n\n# 指定校验规则\n\n如果创建新 collection 时要指定校验规则，需要在使用 db.createcollection() 时指定 validator 选项。\n\n如果要将 document 校验添加到现有 collection 中，需要使用带有 validator 选项的 collmod 命令。\n\nmongodb 还提供以下相关选项：\n\n * validationlevel 选项（用于确定 mongodb 在更新过程中，对现有 document 应用校验规则的严格程度）\n * validationaction 选项（用于确定 mongodb 发现违反校验规则的 document 时，是选择报错并拒绝，还是接受数据但在日志中告警）。\n\n\n# json schema\n\n从 3.6 版本开始，mongodb 开始支持 json schema 校验。\n\n可以通过在 validator 表达式中使用 $jsonschema 操作来指定 json schema 校验。\n\n【示例】\n\ndb.createcollection('students', {\n  validator: {\n    $jsonschema: {\n      bsontype: 'object',\n      required: ['name', 'year', 'major', 'address'],\n      properties: {\n        name: {\n          bsontype: 'string',\n          description: 'must be a string and is required'\n        },\n        year: {\n          bsontype: 'int',\n          minimum: 2017,\n          maximum: 3017,\n          description: 'must be an integer in [ 2017, 3017 ] and is required'\n        },\n        major: {\n          enum: ['math', 'english', 'computer science', 'history', null],\n          description: 'can only be one of the enum values and is required'\n        },\n        gpa: {\n          bsontype: ['double'],\n          description: 'must be a double if the field exists'\n        },\n        address: {\n          bsontype: 'object',\n          required: ['city'],\n          properties: {\n            street: {\n              bsontype: 'string',\n              description: 'must be a string if the field exists'\n            },\n            city: {\n              bsontype: 'string',\n              description: 'must be a string and is required'\n            }\n          }\n        }\n      }\n    }\n  }\n})\n\n\n\n# 其它查询表达式\n\n除了使用 $jsonschema 查询运算符的 json schema 校验外，mongodb 还支持其它查询运算符的校验，但以下情况除外：\n\n * $near,\n * $nearsphere,\n * $text,\n * $where, and\n * 带有 $function 表达式的 $expr\n\n【示例】查询表达式中指定校验规则\n\ndb.createcollection('contacts', {\n  validator: {\n    $or: [\n      { phone: { $type: 'string' } },\n      { email: { $regex: /@mongodb\\.com$/ } },\n      { status: { $in: ['unknown', 'incomplete'] } }\n    ]\n  }\n})\n\n\n\n# 行为\n\n校验发生在更新和插入期间。添加校验规则到 collection 时，不会对现有的 document 进行校验，除非发生修改操作。\n\n# 现有的 document\n\nvalidationlevel 选项确定 mongodb 进行规则校验时执行的操作：\n\n * 如果 validationlevel 是 strict（严格级别。这是 mongodb 默认级别），则 mongodb 将校验规则应用于所有插入和更新。\n * 如果 validationlevel 是 moderate（中等级别），则 mongodb 只对已满足校验条件的现有文档的插入和更新操作进行校验；对不符合校验标准的现有文档的更新操作不进行校验。\n\n【示例】\n\n下面是一个正常的插入操作：\n\ndb.contacts.insert([\n  {\n    _id: 1,\n    name: 'anne',\n    phone: '+1 555 123 456',\n    city: 'london',\n    status: 'complete'\n  },\n  { _id: 2, name: 'ivan', city: 'vancouver' }\n])\n\n\n在 collection 上配置一个校验规则：\n\ndb.runcommand({\n  collmod: 'contacts',\n  validator: {\n    $jsonschema: {\n      bsontype: 'object',\n      required: ['phone', 'name'],\n      properties: {\n        phone: {\n          bsontype: 'string',\n          description: 'must be a string and is required'\n        },\n        name: {\n          bsontype: 'string',\n          description: 'must be a string and is required'\n        }\n      }\n    }\n  },\n  validationlevel: 'moderate'\n})\n\n\n则 contacts collection 现在添加了含中等级别（moderate） validationlevel 的 validator：\n\n * 如果尝试更新 _id为 1 的文档，则 mongodb 将应用校验规则，因为现有文档符合条件。\n\n * 相反，mongodb 不会将校验 _id 为 2 的文档，因为它不符合校验规则。\n\n如果要完全禁用校验，可以将 validationlevel 置为 off。\n\n# 接受或拒绝无效的 document\n\n * 如果 validationaction 是 error（默认），则 mongodb 拒绝任何违反校验规则的插入或更新。\n * 如果 validationaction 是 warn，mongodb 会记录所有的违规，但允许进行插入或更新。\n\n【示例】\n\n创建集合时，配置 validationaction 为 warn。\n\ndb.createcollection('contacts2', {\n  validator: {\n    $jsonschema: {\n      bsontype: 'object',\n      required: ['phone'],\n      properties: {\n        phone: {\n          bsontype: 'string',\n          description: 'must be a string and is required'\n        },\n        email: {\n          bsontype: 'string',\n          pattern: '@mongodb.com$',\n          description:\n            'must be a string and match the regular expression pattern'\n        },\n        status: {\n          enum: ['unknown', 'incomplete'],\n          description: 'can only be one of the enum values'\n        }\n      }\n    }\n  },\n  validationaction: 'warn'\n})\n\n\n尝试插入一条违规记录\n\n> db.contacts2.insert( { name: \"amanda\", status: \"updated\" } )\nwriteresult({ \"ninserted\" : 1 })\n\n\nmongodb 允许这条操作执行，但是服务器会记录下告警信息。\n\n{\"t\":{\"$date\":\"2020-09-11t16:35:57.754+08:00\"},\"s\":\"w\",  \"c\":\"storage\",  \"id\":20294,   \"ctx\":\"conn14\",\"msg\":\"document would fail validation\",\"attr\":{\"namespace\":\"test.contacts2\",\"document\":{\"_id\":{\"$oid\":\"5f5b36ed8ea53d62a0b51c4e\"},\"name\":\"amanda\",\"status\":\"updated\"}}}\n\n\n# 限制\n\n不能在 admin、local、config 这几个特殊的数据库中指定校验规则。\n\n不能在 system.* collection 中指定校验。\n\n\n# 参考资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 建模示例",frontmatter:{title:"MongoDB 建模示例",date:"2020-09-12T10:43:53.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","建模"],permalink:"/pages/88c7d3/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/06.MongoDB%E5%BB%BA%E6%A8%A1%E7%A4%BA%E4%BE%8B.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/06.MongoDB建模示例.md",key:"v-2d5a6152",path:"/pages/88c7d3/",headers:[{level:2,title:"关系型模型",slug:"关系型模型",normalizedTitle:"关系型模型",charIndex:19},{level:3,title:"嵌入式文档一对一关系模型",slug:"嵌入式文档一对一关系模型",normalizedTitle:"嵌入式文档一对一关系模型",charIndex:29},{level:4,title:"嵌入式文档一对一关系模型 - 嵌入式文档模式",slug:"嵌入式文档一对一关系模型-嵌入式文档模式",normalizedTitle:"嵌入式文档一对一关系模型 - 嵌入式文档模式",charIndex:45},{level:4,title:"嵌入式文档一对一关系模型 - 子集模式",slug:"嵌入式文档一对一关系模型-子集模式",normalizedTitle:"嵌入式文档一对一关系模型 - 子集模式",charIndex:466},{level:3,title:"嵌入式文档一对多关系模型",slug:"嵌入式文档一对多关系模型",normalizedTitle:"嵌入式文档一对多关系模型",charIndex:3099},{level:4,title:"嵌入式文档一对多关系模型 - 嵌入式文档模式",slug:"嵌入式文档一对多关系模型-嵌入式文档模式",normalizedTitle:"嵌入式文档一对多关系模型 - 嵌入式文档模式",charIndex:3115},{level:4,title:"嵌入式文档一对多关系模型 - 子集模式",slug:"嵌入式文档一对多关系模型-子集模式",normalizedTitle:"嵌入式文档一对多关系模型 - 子集模式",charIndex:3788},{level:3,title:"引用式文档一对多关系模型",slug:"引用式文档一对多关系模型",normalizedTitle:"引用式文档一对多关系模型",charIndex:5670},{level:2,title:"树形结构模型",slug:"树形结构模型",normalizedTitle:"树形结构模型",charIndex:7560},{level:3,title:"具有父节点的树形结构模型",slug:"具有父节点的树形结构模型",normalizedTitle:"具有父节点的树形结构模型",charIndex:7573},{level:3,title:"具有子节点的树形结构模型",slug:"具有子节点的树形结构模型",normalizedTitle:"具有子节点的树形结构模型",charIndex:8181},{level:3,title:"具有祖先的树形结构模型",slug:"具有祖先的树形结构模型",normalizedTitle:"具有祖先的树形结构模型",charIndex:8800},{level:3,title:"具有实体化路径的树形结构模型",slug:"具有实体化路径的树形结构模型",normalizedTitle:"具有实体化路径的树形结构模型",charIndex:9685},{level:3,title:"具有嵌套集的树形结构模型",slug:"具有嵌套集的树形结构模型",normalizedTitle:"具有嵌套集的树形结构模型",charIndex:10414},{level:2,title:"设计模式",slug:"设计模式",normalizedTitle:"设计模式",charIndex:11020},{level:3,title:"大文档，很多列，很多索引",slug:"大文档-很多列-很多索引",normalizedTitle:"大文档，很多列，很多索引",charIndex:11029},{level:3,title:"管理文档不同版本",slug:"管理文档不同版本",normalizedTitle:"管理文档不同版本",charIndex:11059},{level:3,title:"统计网页点击量",slug:"统计网页点击量",normalizedTitle:"统计网页点击量",charIndex:11163},{level:3,title:"精确统计",slug:"精确统计",normalizedTitle:"精确统计",charIndex:11248},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:11269}],headersStr:"关系型模型 嵌入式文档一对一关系模型 嵌入式文档一对一关系模型 - 嵌入式文档模式 嵌入式文档一对一关系模型 - 子集模式 嵌入式文档一对多关系模型 嵌入式文档一对多关系模型 - 嵌入式文档模式 嵌入式文档一对多关系模型 - 子集模式 引用式文档一对多关系模型 树形结构模型 具有父节点的树形结构模型 具有子节点的树形结构模型 具有祖先的树形结构模型 具有实体化路径的树形结构模型 具有嵌套集的树形结构模型 设计模式 大文档，很多列，很多索引 管理文档不同版本 统计网页点击量 精确统计 参考资料",content:'# MongoDB 建模示例\n\n\n# 关系型模型\n\n\n# 嵌入式文档一对一关系模型\n\n# 嵌入式文档一对一关系模型 - 嵌入式文档模式\n\n// patron document\n{\n   _id: "joe",\n   name: "Joe Bookreader"\n}\n\n// address document\n{\n   patron_id: "joe", // reference to patron document\n   street: "123 Fake Street",\n   city: "Faketon",\n   state: "MA",\n   zip: "12345"\n}\n\n\n合并为：\n\n{\n  "_id": "joe",\n  "name": "Joe Bookreader",\n  "address": {\n    "street": "123 Fake Street",\n    "city": "Faketon",\n    "state": "MA",\n    "zip": "12345"\n  }\n}\n\n\n# 嵌入式文档一对一关系模型 - 子集模式\n\n假设，有一个用于描述电影信息的 collection 定义：\n\n{\n  "_id": 1,\n  "title": "The Arrival of a Train",\n  "year": 1896,\n  "runtime": 1,\n  "released": ISODate("01-25-1896"),\n  "poster": "http://ia.media-imdb.com/images/M/MV5BMjEyNDk5MDYzOV5BMl5BanBnXkFtZTgwNjIxMTEwMzE@._V1_SX300.jpg",\n  "plot": "A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, ...",\n  "fullplot": "A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, the line dissolves. The doors of the railway-cars open, and people on the platform help passengers to get off.",\n  "lastupdated": ISODate("2015-08-15T10:06:53"),\n  "type": "movie",\n  "directors": ["Auguste Lumière", "Louis Lumière"],\n  "imdb": {\n    "rating": 7.3,\n    "votes": 5043,\n    "id": 12\n  },\n  "countries": ["France"],\n  "genres": ["Documentary", "Short"],\n  "tomatoes": {\n    "viewer": {\n      "rating": 3.7,\n      "numReviews": 59\n    },\n    "lastUpdated": ISODate("2020-01-09T00:02:53")\n  }\n}\n\n\n在应用中，有的场景只需要显示电影的简单浏览信息，不需要显示类似 fullplot、poster 这样的详细信息。因为，我们可以考虑将原结构一份为二，并通过 id 字段关联起来。\n\n用于展示摘要信息的 movie collection\n\n// movie collection\n\n{\n  "_id": 1,\n  "title": "The Arrival of a Train",\n  "year": 1896,\n  "runtime": 1,\n  "released": ISODate("1896-01-25"),\n  "type": "movie",\n  "directors": ["Auguste Lumière", "Louis Lumière"],\n  "countries": ["France"],\n  "genres": ["Documentary", "Short"]\n}\n\n\n用于展示细节信息的 movie_details collection\n\n// movie_details collection\n\n{\n  "_id": 156,\n  "movie_id": 1, // reference to the movie collection\n  "poster": "http://ia.media-imdb.com/images/M/MV5BMjEyNDk5MDYzOV5BMl5BanBnXkFtZTgwNjIxMTEwMzE@._V1_SX300.jpg",\n  "plot": "A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, ...",\n  "fullplot": "A group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. When the train stops at the platform, the line dissolves. The doors of the railway-cars open, and people on the platform help passengers to get off.",\n  "lastupdated": ISODate("2015-08-15T10:06:53"),\n  "imdb": {\n    "rating": 7.3,\n    "votes": 5043,\n    "id": 12\n  },\n  "tomatoes": {\n    "viewer": {\n      "rating": 3.7,\n      "numReviews": 59\n    },\n    "lastUpdated": ISODate("2020-01-29T00:02:53")\n  }\n}\n\n\n\n# 嵌入式文档一对多关系模型\n\n# 嵌入式文档一对多关系模型 - 嵌入式文档模式\n\n// patron document\n{\n   _id: "joe",\n   name: "Joe Bookreader"\n}\n\n// address documents\n{\n   patron_id: "joe", // reference to patron document\n   street: "123 Fake Street",\n   city: "Faketon",\n   state: "MA",\n   zip: "12345"\n}\n\n{\n   patron_id: "joe",\n   street: "1 Some Other Street",\n   city: "Boston",\n   state: "MA",\n   zip: "12345"\n}\n\n\n合并为：\n\n{\n  "_id": "joe",\n  "name": "Joe Bookreader",\n  "addresses": [\n    {\n      "street": "123 Fake Street",\n      "city": "Faketon",\n      "state": "MA",\n      "zip": "12345"\n    },\n    {\n      "street": "1 Some Other Street",\n      "city": "Boston",\n      "state": "MA",\n      "zip": "12345"\n    }\n  ]\n}\n\n\n# 嵌入式文档一对多关系模型 - 子集模式\n\n考虑一个电商网站用于表示商品的 collection：\n\n{\n  "_id": 1,\n  "name": "Super Widget",\n  "description": "This is the most useful item in your toolbox.",\n  "price": { "value": NumberDecimal("119.99"), "currency": "USD" },\n  "reviews": [\n    {\n      "review_id": 786,\n      "review_author": "Kristina",\n      "review_text": "This is indeed an amazing widget.",\n      "published_date": ISODate("2019-02-18")\n    },\n    {\n      "review_id": 785,\n      "review_author": "Trina",\n      "review_text": "Nice product. Slow shipping.",\n      "published_date": ISODate("2019-02-17")\n    },\n    ...{\n      "review_id": 1,\n      "review_author": "Hans",\n      "review_text": "Meh, it\'s okay.",\n      "published_date": ISODate("2017-12-06")\n    }\n  ]\n}\n\n\n评论按时间倒序排列。 当用户访问产品页面时，应用程序将加载十条最近的评论。可以将集合分为两个集合，而不是与产品一起存储所有评论：\n\n产品集合存储有关每个产品的信息，包括产品的十个最新评论：\n\n{\n  "_id": 1,\n  "name": "Super Widget",\n  "description": "This is the most useful item in your toolbox.",\n  "price": { "value": NumberDecimal("119.99"), "currency": "USD" },\n  "reviews": [\n    {\n      "review_id": 786,\n      "review_author": "Kristina",\n      "review_text": "This is indeed an amazing widget.",\n      "published_date": ISODate("2019-02-18")\n    }\n    ...\n    {\n      "review_id": 776,\n      "review_author": "Pablo",\n      "review_text": "Amazing!",\n      "published_date": ISODate("2019-02-16")\n    }\n  ]\n}\n\n\nreview collection 存储所有的评论\n\n{\n  "review_id": 786,\n  "product_id": 1,\n  "review_author": "Kristina",\n  "review_text": "This is indeed an amazing widget.",\n  "published_date": ISODate("2019-02-18")\n}\n{\n  "review_id": 785,\n  "product_id": 1,\n  "review_author": "Trina",\n  "review_text": "Nice product. Slow shipping.",\n  "published_date": ISODate("2019-02-17")\n}\n...\n{\n  "review_id": 1,\n  "product_id": 1,\n  "review_author": "Hans",\n  "review_text": "Meh, it\'s okay.",\n  "published_date": ISODate("2017-12-06")\n}\n\n\n\n# 引用式文档一对多关系模型\n\n考虑以下映射出版商和书籍关系的示例。\n\n该示例说明了引用式文档的优点，以避免重复发布者信息。\n\n{\n   title: "MongoDB: The Definitive Guide",\n   author: [ "Kristina Chodorow", "Mike Dirolf" ],\n   published_date: ISODate("2010-09-24"),\n   pages: 216,\n   language: "English",\n   publisher: {\n              name: "O\'Reilly Media",\n              founded: 1980,\n              location: "CA"\n            }\n}\n\n{\n   title: "50 Tips and Tricks for MongoDB Developer",\n   author: "Kristina Chodorow",\n   published_date: ISODate("2011-05-06"),\n   pages: 68,\n   language: "English",\n   publisher: {\n              name: "O\'Reilly Media",\n              founded: 1980,\n              location: "CA"\n            }\n}\n\n\n为避免重复出版商数据，可以使用引用型文档，并将出版商信息与书本分开保存。 使用引用时，关系的增长决定了将引用存储在何处。 如果每个出版商的图书数量很少且增长有限，则有时将图书参考存储在出版商文档中可能会很有用。 否则，如果每个发布者的书籍数量不受限制，则此数据模型将导致可变的，不断增长的数组，如以下示例所示：\n\n{\n   name: "O\'Reilly Media",\n   founded: 1980,\n   location: "CA",\n   books: [123456789, 234567890, ...]\n}\n\n{\n    _id: 123456789,\n    title: "MongoDB: The Definitive Guide",\n    author: [ "Kristina Chodorow", "Mike Dirolf" ],\n    published_date: ISODate("2010-09-24"),\n    pages: 216,\n    language: "English"\n}\n\n{\n   _id: 234567890,\n   title: "50 Tips and Tricks for MongoDB Developer",\n   author: "Kristina Chodorow",\n   published_date: ISODate("2011-05-06"),\n   pages: 68,\n   language: "English"\n}\n\n\n为了避免可变的，增长的数组，请将发行者参考存储在书籍文档中：\n\n{\n   _id: "oreilly",\n   name: "O\'Reilly Media",\n   founded: 1980,\n   location: "CA"\n}\n\n{\n   _id: 123456789,\n   title: "MongoDB: The Definitive Guide",\n   author: [ "Kristina Chodorow", "Mike Dirolf" ],\n   published_date: ISODate("2010-09-24"),\n   pages: 216,\n   language: "English",\n   publisher_id: "oreilly"\n}\n\n{\n   _id: 234567890,\n   title: "50 Tips and Tricks for MongoDB Developer",\n   author: "Kristina Chodorow",\n   published_date: ISODate("2011-05-06"),\n   pages: 68,\n   language: "English",\n   publisher_id: "oreilly"\n}\n\n\n\n# 树形结构模型\n\n\n\n\n# 具有父节点的树形结构模型\n\n上图结构可以用父引用来表示：\n\ndb.categories.insertMany([\n  { "_id": "MongoDB", "parent": "Databases" },\n  { "_id": "dbm", "parent": "Databases" },\n  { "_id": "Databases", "parent": "Programming" },\n  { "_id": "Languages", "parent": "Programming" },\n  { "_id": "Programming", "parent": "Books" },\n  { "_id": "Books", "parent": null }\n])\n\n\n * 检索节点的父节点：\n   \n   db.categories.findOne( { _id: "MongoDB" } ).parent\n   \n\n * 可以在父字段上创建索引以启用父节点的快速搜索：\n   \n   db.categories.createIndex( { parent: 1 } )\n   \n\n * 可以通过父字段查询找到其直接子节点：\n   \n   db.categories.find( { parent: "Databases" } )\n   \n\n * 检索子树，可以参考： $graphLookup.\n\n\n# 具有子节点的树形结构模型\n\ndb.categories.insertMany([\n  { "_id": "MongoDB", "children": [] },\n  { "_id": "dbm", "children": [] },\n  { "_id": "Databases", "children": ["MongoDB", "dbm"] },\n  { "_id": "Languages", "children": [] },\n  { "_id": "Programming", "children": ["Databases", "Languages"] },\n  { "_id": "Books", "children": ["Programming"] }\n])\n\n\n * 检索节点的 children：\n   \n   db.categories.findOne( { _id: "Databases" } ).children\n   \n\n * 可以在 children 字段上创建索引以启用子节点的快速搜索：\n   \n   db.categories.createIndex( { children: 1 } )\n   \n\n * 可以在 children 字段中查询节点，以找到其父节点及其兄弟节点：\n   \n   db.categories.find( { children: "MongoDB" } )\n   \n\n\n# 具有祖先的树形结构模型\n\ndb.categories.insertMany([\n  {\n    "_id": "MongoDB",\n    "ancestors": ["Books", "Programming", "Databases"],\n    "parent": "Databases"\n  },\n  {\n    "_id": "dbm",\n    "ancestors": ["Books", "Programming", "Databases"],\n    "parent": "Databases"\n  },\n  {\n    "_id": "Databases",\n    "ancestors": ["Books", "Programming"],\n    "parent": "Programming"\n  },\n  {\n    "_id": "Languages",\n    "ancestors": ["Books", "Programming"],\n    "parent": "Programming"\n  },\n  { "_id": "Programming", "ancestors": ["Books"], "parent": "Books" },\n  { "_id": "Books", "ancestors": [], "parent": null }\n])\n\n\n * 检索节点的祖先或路径的查询是快速而直接的：\n   \n   db.categories.findOne({ "_id": "MongoDB" }).ancestors\n   \n\n * 可以在 ancestors 字段上创建索引，以启用祖先节点的快速搜索：\n   \n   db.categories.createIndex({ "ancestors": 1 })\n   \n\n * 可以通过 ancestors 字段查询查找其所有后代：\n   \n   db.categories.find({ "ancestors": "Programming" })\n   \n\n\n# 具有实体化路径的树形结构模型\n\ndb.categories.insertMany([\n  { "_id": "Books", "path": null },\n  { "_id": "Programming", "path": ",Books," },\n  { "_id": "Databases", "path": ",Books,Programming," },\n  { "_id": "Languages", "path": ",Books,Programming," },\n  { "_id": "MongoDB", "path": ",Books,Programming,Databases," },\n  { "_id": "dbm", "path": ",Books,Programming,Databases," }\n])\n\n\n * 可以查询以检索整个树，并按字段路径排序：\n   \n   db.categories.find().sort( { path: 1 } )\n   \n\n * 可以在 path 字段上使用正则表达式来查找 Programming 的后代\n   \n   db.categories.find( { path: /,Programming,/ } )\n   \n\n * 可以检索 Books 的后代，其中 Books 也位于层次结构的最高级别：\n   \n   db.categories.find( { path: /^,Books,/ } )\n   \n\n * 要在 path 字段上创建索引，请使用以下调用：\n   \n   db.categories.createIndex( { path: 1 } )\n   \n\n\n# 具有嵌套集的树形结构模型\n\n\n\ndb.categories.insertMany([\n  { _id: \'Books\', parent: 0, left: 1, right: 12 },\n  { _id: \'Programming\', parent: \'Books\', left: 2, right: 11 },\n  { _id: \'Languages\', parent: \'Programming\', left: 3, right: 4 },\n  { _id: \'Databases\', parent: \'Programming\', left: 5, right: 10 },\n  { _id: \'MongoDB\', parent: \'Databases\', left: 6, right: 7 },\n  { _id: \'dbm\', parent: \'Databases\', left: 8, right: 9 }\n])\n\n\n可以查询以检索节点的后代：\n\nvar databaseCategory = db.categories.findOne({ _id: \'Databases\' })\ndb.categories.find({\n  left: { $gt: databaseCategory.left },\n  right: { $lt: databaseCategory.right }\n})\n\n\n\n# 设计模式\n\n\n# 大文档，很多列，很多索引\n\n解决方案是：列转行\n\n\n\n\n# 管理文档不同版本\n\nMongoDB 文档格式非常灵活，势必会带来版本维护上的难度。\n\n解决方案是：可以增加一个版本号字段\n\n * 快速过滤掉不需要升级的文档\n * 升级时，对不同版本的文档做不同处理\n\n\n# 统计网页点击量\n\n统计数据精确性要求并不是十分重要。\n\n解决方案：用近似计算\n\n每隔 10 次写一次：\n\n{ "$inc": { "views": 1 } }\n\n\n\n# 精确统计\n\n解决方案：使用预聚合\n\n\n# 参考资料\n\n * Data Model Examples and Patterns',normalizedContent:'# mongodb 建模示例\n\n\n# 关系型模型\n\n\n# 嵌入式文档一对一关系模型\n\n# 嵌入式文档一对一关系模型 - 嵌入式文档模式\n\n// patron document\n{\n   _id: "joe",\n   name: "joe bookreader"\n}\n\n// address document\n{\n   patron_id: "joe", // reference to patron document\n   street: "123 fake street",\n   city: "faketon",\n   state: "ma",\n   zip: "12345"\n}\n\n\n合并为：\n\n{\n  "_id": "joe",\n  "name": "joe bookreader",\n  "address": {\n    "street": "123 fake street",\n    "city": "faketon",\n    "state": "ma",\n    "zip": "12345"\n  }\n}\n\n\n# 嵌入式文档一对一关系模型 - 子集模式\n\n假设，有一个用于描述电影信息的 collection 定义：\n\n{\n  "_id": 1,\n  "title": "the arrival of a train",\n  "year": 1896,\n  "runtime": 1,\n  "released": isodate("01-25-1896"),\n  "poster": "http://ia.media-imdb.com/images/m/mv5bmjeyndk5mdyzov5bml5banbnxkftztgwnjixmtewmze@._v1_sx300.jpg",\n  "plot": "a group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. when the train stops at the platform, ...",\n  "fullplot": "a group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. when the train stops at the platform, the line dissolves. the doors of the railway-cars open, and people on the platform help passengers to get off.",\n  "lastupdated": isodate("2015-08-15t10:06:53"),\n  "type": "movie",\n  "directors": ["auguste lumiere", "louis lumiere"],\n  "imdb": {\n    "rating": 7.3,\n    "votes": 5043,\n    "id": 12\n  },\n  "countries": ["france"],\n  "genres": ["documentary", "short"],\n  "tomatoes": {\n    "viewer": {\n      "rating": 3.7,\n      "numreviews": 59\n    },\n    "lastupdated": isodate("2020-01-09t00:02:53")\n  }\n}\n\n\n在应用中，有的场景只需要显示电影的简单浏览信息，不需要显示类似 fullplot、poster 这样的详细信息。因为，我们可以考虑将原结构一份为二，并通过 id 字段关联起来。\n\n用于展示摘要信息的 movie collection\n\n// movie collection\n\n{\n  "_id": 1,\n  "title": "the arrival of a train",\n  "year": 1896,\n  "runtime": 1,\n  "released": isodate("1896-01-25"),\n  "type": "movie",\n  "directors": ["auguste lumiere", "louis lumiere"],\n  "countries": ["france"],\n  "genres": ["documentary", "short"]\n}\n\n\n用于展示细节信息的 movie_details collection\n\n// movie_details collection\n\n{\n  "_id": 156,\n  "movie_id": 1, // reference to the movie collection\n  "poster": "http://ia.media-imdb.com/images/m/mv5bmjeyndk5mdyzov5bml5banbnxkftztgwnjixmtewmze@._v1_sx300.jpg",\n  "plot": "a group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. when the train stops at the platform, ...",\n  "fullplot": "a group of people are standing in a straight line along the platform of a railway station, waiting for a train, which is seen coming at some distance. when the train stops at the platform, the line dissolves. the doors of the railway-cars open, and people on the platform help passengers to get off.",\n  "lastupdated": isodate("2015-08-15t10:06:53"),\n  "imdb": {\n    "rating": 7.3,\n    "votes": 5043,\n    "id": 12\n  },\n  "tomatoes": {\n    "viewer": {\n      "rating": 3.7,\n      "numreviews": 59\n    },\n    "lastupdated": isodate("2020-01-29t00:02:53")\n  }\n}\n\n\n\n# 嵌入式文档一对多关系模型\n\n# 嵌入式文档一对多关系模型 - 嵌入式文档模式\n\n// patron document\n{\n   _id: "joe",\n   name: "joe bookreader"\n}\n\n// address documents\n{\n   patron_id: "joe", // reference to patron document\n   street: "123 fake street",\n   city: "faketon",\n   state: "ma",\n   zip: "12345"\n}\n\n{\n   patron_id: "joe",\n   street: "1 some other street",\n   city: "boston",\n   state: "ma",\n   zip: "12345"\n}\n\n\n合并为：\n\n{\n  "_id": "joe",\n  "name": "joe bookreader",\n  "addresses": [\n    {\n      "street": "123 fake street",\n      "city": "faketon",\n      "state": "ma",\n      "zip": "12345"\n    },\n    {\n      "street": "1 some other street",\n      "city": "boston",\n      "state": "ma",\n      "zip": "12345"\n    }\n  ]\n}\n\n\n# 嵌入式文档一对多关系模型 - 子集模式\n\n考虑一个电商网站用于表示商品的 collection：\n\n{\n  "_id": 1,\n  "name": "super widget",\n  "description": "this is the most useful item in your toolbox.",\n  "price": { "value": numberdecimal("119.99"), "currency": "usd" },\n  "reviews": [\n    {\n      "review_id": 786,\n      "review_author": "kristina",\n      "review_text": "this is indeed an amazing widget.",\n      "published_date": isodate("2019-02-18")\n    },\n    {\n      "review_id": 785,\n      "review_author": "trina",\n      "review_text": "nice product. slow shipping.",\n      "published_date": isodate("2019-02-17")\n    },\n    ...{\n      "review_id": 1,\n      "review_author": "hans",\n      "review_text": "meh, it\'s okay.",\n      "published_date": isodate("2017-12-06")\n    }\n  ]\n}\n\n\n评论按时间倒序排列。 当用户访问产品页面时，应用程序将加载十条最近的评论。可以将集合分为两个集合，而不是与产品一起存储所有评论：\n\n产品集合存储有关每个产品的信息，包括产品的十个最新评论：\n\n{\n  "_id": 1,\n  "name": "super widget",\n  "description": "this is the most useful item in your toolbox.",\n  "price": { "value": numberdecimal("119.99"), "currency": "usd" },\n  "reviews": [\n    {\n      "review_id": 786,\n      "review_author": "kristina",\n      "review_text": "this is indeed an amazing widget.",\n      "published_date": isodate("2019-02-18")\n    }\n    ...\n    {\n      "review_id": 776,\n      "review_author": "pablo",\n      "review_text": "amazing!",\n      "published_date": isodate("2019-02-16")\n    }\n  ]\n}\n\n\nreview collection 存储所有的评论\n\n{\n  "review_id": 786,\n  "product_id": 1,\n  "review_author": "kristina",\n  "review_text": "this is indeed an amazing widget.",\n  "published_date": isodate("2019-02-18")\n}\n{\n  "review_id": 785,\n  "product_id": 1,\n  "review_author": "trina",\n  "review_text": "nice product. slow shipping.",\n  "published_date": isodate("2019-02-17")\n}\n...\n{\n  "review_id": 1,\n  "product_id": 1,\n  "review_author": "hans",\n  "review_text": "meh, it\'s okay.",\n  "published_date": isodate("2017-12-06")\n}\n\n\n\n# 引用式文档一对多关系模型\n\n考虑以下映射出版商和书籍关系的示例。\n\n该示例说明了引用式文档的优点，以避免重复发布者信息。\n\n{\n   title: "mongodb: the definitive guide",\n   author: [ "kristina chodorow", "mike dirolf" ],\n   published_date: isodate("2010-09-24"),\n   pages: 216,\n   language: "english",\n   publisher: {\n              name: "o\'reilly media",\n              founded: 1980,\n              location: "ca"\n            }\n}\n\n{\n   title: "50 tips and tricks for mongodb developer",\n   author: "kristina chodorow",\n   published_date: isodate("2011-05-06"),\n   pages: 68,\n   language: "english",\n   publisher: {\n              name: "o\'reilly media",\n              founded: 1980,\n              location: "ca"\n            }\n}\n\n\n为避免重复出版商数据，可以使用引用型文档，并将出版商信息与书本分开保存。 使用引用时，关系的增长决定了将引用存储在何处。 如果每个出版商的图书数量很少且增长有限，则有时将图书参考存储在出版商文档中可能会很有用。 否则，如果每个发布者的书籍数量不受限制，则此数据模型将导致可变的，不断增长的数组，如以下示例所示：\n\n{\n   name: "o\'reilly media",\n   founded: 1980,\n   location: "ca",\n   books: [123456789, 234567890, ...]\n}\n\n{\n    _id: 123456789,\n    title: "mongodb: the definitive guide",\n    author: [ "kristina chodorow", "mike dirolf" ],\n    published_date: isodate("2010-09-24"),\n    pages: 216,\n    language: "english"\n}\n\n{\n   _id: 234567890,\n   title: "50 tips and tricks for mongodb developer",\n   author: "kristina chodorow",\n   published_date: isodate("2011-05-06"),\n   pages: 68,\n   language: "english"\n}\n\n\n为了避免可变的，增长的数组，请将发行者参考存储在书籍文档中：\n\n{\n   _id: "oreilly",\n   name: "o\'reilly media",\n   founded: 1980,\n   location: "ca"\n}\n\n{\n   _id: 123456789,\n   title: "mongodb: the definitive guide",\n   author: [ "kristina chodorow", "mike dirolf" ],\n   published_date: isodate("2010-09-24"),\n   pages: 216,\n   language: "english",\n   publisher_id: "oreilly"\n}\n\n{\n   _id: 234567890,\n   title: "50 tips and tricks for mongodb developer",\n   author: "kristina chodorow",\n   published_date: isodate("2011-05-06"),\n   pages: 68,\n   language: "english",\n   publisher_id: "oreilly"\n}\n\n\n\n# 树形结构模型\n\n\n\n\n# 具有父节点的树形结构模型\n\n上图结构可以用父引用来表示：\n\ndb.categories.insertmany([\n  { "_id": "mongodb", "parent": "databases" },\n  { "_id": "dbm", "parent": "databases" },\n  { "_id": "databases", "parent": "programming" },\n  { "_id": "languages", "parent": "programming" },\n  { "_id": "programming", "parent": "books" },\n  { "_id": "books", "parent": null }\n])\n\n\n * 检索节点的父节点：\n   \n   db.categories.findone( { _id: "mongodb" } ).parent\n   \n\n * 可以在父字段上创建索引以启用父节点的快速搜索：\n   \n   db.categories.createindex( { parent: 1 } )\n   \n\n * 可以通过父字段查询找到其直接子节点：\n   \n   db.categories.find( { parent: "databases" } )\n   \n\n * 检索子树，可以参考： $graphlookup.\n\n\n# 具有子节点的树形结构模型\n\ndb.categories.insertmany([\n  { "_id": "mongodb", "children": [] },\n  { "_id": "dbm", "children": [] },\n  { "_id": "databases", "children": ["mongodb", "dbm"] },\n  { "_id": "languages", "children": [] },\n  { "_id": "programming", "children": ["databases", "languages"] },\n  { "_id": "books", "children": ["programming"] }\n])\n\n\n * 检索节点的 children：\n   \n   db.categories.findone( { _id: "databases" } ).children\n   \n\n * 可以在 children 字段上创建索引以启用子节点的快速搜索：\n   \n   db.categories.createindex( { children: 1 } )\n   \n\n * 可以在 children 字段中查询节点，以找到其父节点及其兄弟节点：\n   \n   db.categories.find( { children: "mongodb" } )\n   \n\n\n# 具有祖先的树形结构模型\n\ndb.categories.insertmany([\n  {\n    "_id": "mongodb",\n    "ancestors": ["books", "programming", "databases"],\n    "parent": "databases"\n  },\n  {\n    "_id": "dbm",\n    "ancestors": ["books", "programming", "databases"],\n    "parent": "databases"\n  },\n  {\n    "_id": "databases",\n    "ancestors": ["books", "programming"],\n    "parent": "programming"\n  },\n  {\n    "_id": "languages",\n    "ancestors": ["books", "programming"],\n    "parent": "programming"\n  },\n  { "_id": "programming", "ancestors": ["books"], "parent": "books" },\n  { "_id": "books", "ancestors": [], "parent": null }\n])\n\n\n * 检索节点的祖先或路径的查询是快速而直接的：\n   \n   db.categories.findone({ "_id": "mongodb" }).ancestors\n   \n\n * 可以在 ancestors 字段上创建索引，以启用祖先节点的快速搜索：\n   \n   db.categories.createindex({ "ancestors": 1 })\n   \n\n * 可以通过 ancestors 字段查询查找其所有后代：\n   \n   db.categories.find({ "ancestors": "programming" })\n   \n\n\n# 具有实体化路径的树形结构模型\n\ndb.categories.insertmany([\n  { "_id": "books", "path": null },\n  { "_id": "programming", "path": ",books," },\n  { "_id": "databases", "path": ",books,programming," },\n  { "_id": "languages", "path": ",books,programming," },\n  { "_id": "mongodb", "path": ",books,programming,databases," },\n  { "_id": "dbm", "path": ",books,programming,databases," }\n])\n\n\n * 可以查询以检索整个树，并按字段路径排序：\n   \n   db.categories.find().sort( { path: 1 } )\n   \n\n * 可以在 path 字段上使用正则表达式来查找 programming 的后代\n   \n   db.categories.find( { path: /,programming,/ } )\n   \n\n * 可以检索 books 的后代，其中 books 也位于层次结构的最高级别：\n   \n   db.categories.find( { path: /^,books,/ } )\n   \n\n * 要在 path 字段上创建索引，请使用以下调用：\n   \n   db.categories.createindex( { path: 1 } )\n   \n\n\n# 具有嵌套集的树形结构模型\n\n\n\ndb.categories.insertmany([\n  { _id: \'books\', parent: 0, left: 1, right: 12 },\n  { _id: \'programming\', parent: \'books\', left: 2, right: 11 },\n  { _id: \'languages\', parent: \'programming\', left: 3, right: 4 },\n  { _id: \'databases\', parent: \'programming\', left: 5, right: 10 },\n  { _id: \'mongodb\', parent: \'databases\', left: 6, right: 7 },\n  { _id: \'dbm\', parent: \'databases\', left: 8, right: 9 }\n])\n\n\n可以查询以检索节点的后代：\n\nvar databasecategory = db.categories.findone({ _id: \'databases\' })\ndb.categories.find({\n  left: { $gt: databasecategory.left },\n  right: { $lt: databasecategory.right }\n})\n\n\n\n# 设计模式\n\n\n# 大文档，很多列，很多索引\n\n解决方案是：列转行\n\n\n\n\n# 管理文档不同版本\n\nmongodb 文档格式非常灵活，势必会带来版本维护上的难度。\n\n解决方案是：可以增加一个版本号字段\n\n * 快速过滤掉不需要升级的文档\n * 升级时，对不同版本的文档做不同处理\n\n\n# 统计网页点击量\n\n统计数据精确性要求并不是十分重要。\n\n解决方案：用近似计算\n\n每隔 10 次写一次：\n\n{ "$inc": { "views": 1 } }\n\n\n\n# 精确统计\n\n解决方案：使用预聚合\n\n\n# 参考资料\n\n * data model examples and patterns',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 索引",frontmatter:{title:"MongoDB 索引",date:"2020-09-21T21:22:57.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","索引"],permalink:"/pages/10c674/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/07.MongoDB%E7%B4%A2%E5%BC%95.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/07.MongoDB索引.md",key:"v-2310a086",path:"/pages/10c674/",headers:[{level:2,title:"MongoDB 索引简介",slug:"mongodb-索引简介",normalizedTitle:"mongodb 索引简介",charIndex:17},{level:3,title:"索引的作用",slug:"索引的作用",normalizedTitle:"索引的作用",charIndex:34},{level:3,title:"createIndex() 方法",slug:"createindex-方法",normalizedTitle:"createindex() 方法",charIndex:284},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1542}],headersStr:"MongoDB 索引简介 索引的作用 createIndex() 方法 参考资料",content:'# MongoDB 索引\n\n\n# MongoDB 索引简介\n\n\n# 索引的作用\n\nMongoDB 在 collection 数据级别上定义索引。\n\n索引通常能够极大的提高查询的效率。如果没有索引，MongoDB 在读取数据时必须扫描 collection 中的每个 document 并选取那些符合查询条件的记录。\n\n这种扫描全集合的查询是非常低效的，特别是在处理大量的数据时。查询可能要花费几十秒甚至几分钟，这种性能开销是不可接受的。\n\n索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。\n\n\n\n\n# createIndex() 方法\n\nMongoDB 使用 createIndex() 方法来创建索引。\n\ncreateIndex() 语法如下：\n\ndb.collection.createIndex( <key and index type specification>, <options> )\n\n\ncreateIndex() 可选参数列表如下：\n\nPARAMETER            TYPE            DESCRIPTION\nbackground           Boolean         建索引过程会阻塞其它数据库操作，background 可指定以后台方式创建索引，即增加 "background"\n                                     可选参数。 "background" 默认值为false。\nunique               Boolean         建立的索引是否唯一。指定为 true 创建唯一索引。默认值为false.\nname                 string          索引的名称。如果未指定，MongoDB 的通过连接索引的字段名和排序顺序生成一个索引名称。\ndropDups             Boolean         **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false.\nsparse               Boolean         对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为 true\n                                     的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireAfterSeconds   integer         指定一个以秒为单位的数值，完成 TTL 设定，设定集合的生存时间。\nv                    index version   索引的版本号。默认的索引版本取决于 mongod 创建索引时运行的版本。\nweights              document        索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language     string          对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override    string          对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的 language，默认值为 language.\n\n【示例】使用 name 作为索引，并且按照降序排序\n\ndb.collection.createIndex( { name: -1 } )\n\n\n\n# 参考资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课',normalizedContent:'# mongodb 索引\n\n\n# mongodb 索引简介\n\n\n# 索引的作用\n\nmongodb 在 collection 数据级别上定义索引。\n\n索引通常能够极大的提高查询的效率。如果没有索引，mongodb 在读取数据时必须扫描 collection 中的每个 document 并选取那些符合查询条件的记录。\n\n这种扫描全集合的查询是非常低效的，特别是在处理大量的数据时。查询可能要花费几十秒甚至几分钟，这种性能开销是不可接受的。\n\n索引是特殊的数据结构，索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构。\n\n\n\n\n# createindex() 方法\n\nmongodb 使用 createindex() 方法来创建索引。\n\ncreateindex() 语法如下：\n\ndb.collection.createindex( <key and index type specification>, <options> )\n\n\ncreateindex() 可选参数列表如下：\n\nparameter            type            description\nbackground           boolean         建索引过程会阻塞其它数据库操作，background 可指定以后台方式创建索引，即增加 "background"\n                                     可选参数。 "background" 默认值为false。\nunique               boolean         建立的索引是否唯一。指定为 true 创建唯一索引。默认值为false.\nname                 string          索引的名称。如果未指定，mongodb 的通过连接索引的字段名和排序顺序生成一个索引名称。\ndropdups             boolean         **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false.\nsparse               boolean         对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为 true\n                                     的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false.\nexpireafterseconds   integer         指定一个以秒为单位的数值，完成 ttl 设定，设定集合的生存时间。\nv                    index version   索引的版本号。默认的索引版本取决于 mongod 创建索引时运行的版本。\nweights              document        索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。\ndefault_language     string          对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语\nlanguage_override    string          对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的 language，默认值为 language.\n\n【示例】使用 name 作为索引，并且按照降序排序\n\ndb.collection.createindex( { name: -1 } )\n\n\n\n# 参考资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 复制",frontmatter:{title:"MongoDB 复制",date:"2020-09-20T23:12:17.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","复制"],permalink:"/pages/505407/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/08.MongoDB%E5%A4%8D%E5%88%B6.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/08.MongoDB复制.md",key:"v-0b02d45b",path:"/pages/505407/",headers:[{level:2,title:"副本和可用性",slug:"副本和可用性",normalizedTitle:"副本和可用性",charIndex:17},{level:2,title:"MongoDB 副本",slug:"mongodb-副本",normalizedTitle:"mongodb 副本",charIndex:214},{level:2,title:"异步复制",slug:"异步复制",normalizedTitle:"异步复制",charIndex:689},{level:3,title:"慢操作",slug:"慢操作",normalizedTitle:"慢操作",charIndex:698},{level:3,title:"复制延迟和流控",slug:"复制延迟和流控",normalizedTitle:"复制延迟和流控",charIndex:970},{level:2,title:"故障转移",slug:"故障转移",normalizedTitle:"故障转移",charIndex:1291},{level:2,title:"读操作",slug:"读操作",normalizedTitle:"读操作",charIndex:1935},{level:3,title:"读优先",slug:"读优先",normalizedTitle:"读优先",charIndex:1943},{level:3,title:"数据可见性",slug:"数据可见性",normalizedTitle:"数据可见性",charIndex:2085},{level:3,title:"镜像读取",slug:"镜像读取",normalizedTitle:"镜像读取",charIndex:1880},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:2645}],headersStr:"副本和可用性 MongoDB 副本 异步复制 慢操作 复制延迟和流控 故障转移 读操作 读优先 数据可见性 镜像读取 参考资料",content:'# MongoDB 复制\n\n\n# 副本和可用性\n\n副本可以提供冗余并提高数据可用性。在不同数据库服务器上使用多个数据副本，可以提供一定程度的容错能力，以防止单个数据库服务器宕机时，数据丢失。\n\n在某些情况下，副本还可以提供更大的读取吞吐量。因为客户端可以将读取操作发送到不同的服务器。在不同数据中心中维护数据副本可以提高数据本地性和分布式应用程序的可用性。您还可以维护其他副本以用于专用目的：例如灾难恢复，报告或备份。\n\n\n# MongoDB 副本\n\nMongoDB 中的副本集是一组维护相同数据集的 mongod 进程。一个副本集包含多个数据承载节点和一个仲裁器节点（可选）。在数据承载节点中，只有一个成员被视为主要节点，而其他节点则被视为次要节点。\n\n主节点负责接收所有写操作。副本集只能有一个主副本，能够以 { w: "majority" } 来确认集群中节点的写操作成功情况；尽管在某些情况下，另一个 MongoDB 实例可能会暂时认为自己也是主要的。主节点在其操作日志（即 oplog）中记录了对其数据集的所有更改。\n\n\n\n从节点复制主节点的操作日志，并将操作应用于其数据集，以便同步主节点的数据。如果主节点不可用，则符合条件的从节点将选举新的主节点。\n\n\n\n在某些情况下（例如，有一个主节点和一个从节点，但由于成本限制，禁止添加另一个从节点），您可以选择将 mongod 实例作为仲裁节点添加到副本集。仲裁节点参加选举但不保存数据（即不提供数据冗余）。\n\n\n\n仲裁节点将永远是仲裁节点。在选举期间，主节点可能会降级成为次节点，而次节点可能会升级成为主节点。\n\n\n# 异步复制\n\n\n# 慢操作\n\n从节点复制主节点的操作日志，并将操作异步应用于其数据集。通过从节点同步主节点的数据集，即使一个或多个成员失败，副本集（MongoDB 集群）也可以继续运行。\n\n从 4.2 版本开始，副本集的从节点记录慢操作（操作时间比设置的阈值长）的日志条目。这些慢操作在 REPL 组件下的 诊断日志 中记录了日志消息，并使用了文本 op: <oplog entry> 花费了 <num>ms。这些慢操作日志条目仅取决于慢操作阈值，而不取决于日志级别（在系统级别或组件级别），配置级别或运行缓慢的采样率。探查器不会捕获缓慢的操作日志条目。\n\n\n# 复制延迟和流控\n\n复制延迟（Replication lag）是指将主节点上的写操作复制到从节点上所花费的时间。较短的延迟时间是可以接受的，但是随着复制延迟的增加，可能会出现严重的问题：比如在主节点上的缓存压力。\n\n从 MongoDB 4.2 开始，管理员可以限制主节点的写入速率，使得大多数延迟时间保持在可配置的最大值 flowControlTargetLagSeconds 以下。\n\n默认情况下，流控是开启的。\n\n启用流控后，随着延迟时间越来越接近 flowControlTargetLagSeconds，主对象上的写操作必须先获得令牌，然后才能进行锁定并执行写操作。通过限制每秒发出的令牌数量，流控机制尝试将延迟保持在目标以下。\n\n\n# 故障转移\n\n当主节点与集群中的其他成员通信的时间超过配置的 electionTimeoutMillis（默认为 10 秒）时，符合选举要求的从节点将要求选举，并提名自己为新的主节点。集群尝试完成选举新主节点并恢复正常工作。\n\n\n\n选举完成前，副本集无法处理写入操作。如果将副本集配置为：在主节点处于脱机状态时，在次节点上运行，则副本集可以继续提供读取查询。\n\n假设副本配置采用默认配置，则集群选择新节点的时间通常不应超过 12 秒，这包括：将主节点标记为不可用并完成选举所需的时间。可以通过修改 settings.electionTimeoutMillis 配置选项来调整此时间。网络延迟等因素可能会延长完成选举所需的时间，进而影响集群在没有主节点的情况下可以运行的时间。这些因素取决于集群实际的情况。\n\n将默认为 10 秒的 electionTimeoutMillis 选项数值缩小，可以更快地检测到主要故障。但是，由于网络延迟等因素，集群可能会更频繁地进行选举，即使该主节点实际上处于健康状态。这可能导致 w : 1 写操作的回滚次数增加。\n\n应用程序的连接逻辑应包括对自动故障转移和后续选举的容错处理。从 MongoDB 3.6 开始，MongoDB 驱动程序可以检测到主节点的失联，并可以自动重试一次某些写入操作。\n\n从 MongoDB4.4 开始，MongoDB 提供镜像读取：将可选举的从节点的最近访问的数据，预热为缓存。预热从节点的缓存可以帮助在选举后更快地恢复。\n\n\n# 读操作\n\n\n# 读优先\n\n默认情况下，客户端从主节点读取数据；但是，客户端可以指定读取首选项，以将读取操作发送到从节点。\n\n\n\n异步复制到从节点意味着向从节点读取数据可能会返回与主节点不一致的数据。\n\n包含读取操作的多文档事务必须使用读取主节点优先。给定事务中的所有操作必须路由到同一成员。\n\n\n# 数据可见性\n\n根据读取的关注点，客户端可以在持久化写入前查看写入结果：\n\n * 不管写的 write concern 如何设置，其他使用 "local" 或 "available" 的读配置的客户端都可以向发布客户端确认写操作之前看到写操作的结果。\n * 使用 "local" 或 "available" 读取配置的客户端可以读取数据，这些数据随后可能会在副本集故障转移期间回滚。\n\n对于多文档事务中的操作，当事务提交时，在事务中进行的所有数据更改都将保存，并在事务外部可见。也就是说，事务在回滚其他事务时将不会提交其某些更改。在提交事务前，事务外部看不到在事务中进行的数据更改。\n\n但是，当事务写入多个分片时，并非所有外部读操作都需要等待已提交事务的结果在所有分片上可见。例如，如果提交了一个事务，并且在分片 A 上可以看到写 1，但是在分片 B 上还看不到写 2，则在 "local" 读配置级别，外部读取可以读取写 1 的结果而看不到写 2。\n\n\n# 镜像读取\n\n从 MongoDB 4.4 开始，MongoDB 提供镜像读取以预热可选从节点（即优先级大于 0 的成员）的缓存。使用镜像读取（默认情况下已启用），主节点可以镜像它接收到的一部分操作，并将其发送给可选择的从节点的子集。子集的大小是可配置的。\n\n\n# 参考资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课',normalizedContent:'# mongodb 复制\n\n\n# 副本和可用性\n\n副本可以提供冗余并提高数据可用性。在不同数据库服务器上使用多个数据副本，可以提供一定程度的容错能力，以防止单个数据库服务器宕机时，数据丢失。\n\n在某些情况下，副本还可以提供更大的读取吞吐量。因为客户端可以将读取操作发送到不同的服务器。在不同数据中心中维护数据副本可以提高数据本地性和分布式应用程序的可用性。您还可以维护其他副本以用于专用目的：例如灾难恢复，报告或备份。\n\n\n# mongodb 副本\n\nmongodb 中的副本集是一组维护相同数据集的 mongod 进程。一个副本集包含多个数据承载节点和一个仲裁器节点（可选）。在数据承载节点中，只有一个成员被视为主要节点，而其他节点则被视为次要节点。\n\n主节点负责接收所有写操作。副本集只能有一个主副本，能够以 { w: "majority" } 来确认集群中节点的写操作成功情况；尽管在某些情况下，另一个 mongodb 实例可能会暂时认为自己也是主要的。主节点在其操作日志（即 oplog）中记录了对其数据集的所有更改。\n\n\n\n从节点复制主节点的操作日志，并将操作应用于其数据集，以便同步主节点的数据。如果主节点不可用，则符合条件的从节点将选举新的主节点。\n\n\n\n在某些情况下（例如，有一个主节点和一个从节点，但由于成本限制，禁止添加另一个从节点），您可以选择将 mongod 实例作为仲裁节点添加到副本集。仲裁节点参加选举但不保存数据（即不提供数据冗余）。\n\n\n\n仲裁节点将永远是仲裁节点。在选举期间，主节点可能会降级成为次节点，而次节点可能会升级成为主节点。\n\n\n# 异步复制\n\n\n# 慢操作\n\n从节点复制主节点的操作日志，并将操作异步应用于其数据集。通过从节点同步主节点的数据集，即使一个或多个成员失败，副本集（mongodb 集群）也可以继续运行。\n\n从 4.2 版本开始，副本集的从节点记录慢操作（操作时间比设置的阈值长）的日志条目。这些慢操作在 repl 组件下的 诊断日志 中记录了日志消息，并使用了文本 op: <oplog entry> 花费了 <num>ms。这些慢操作日志条目仅取决于慢操作阈值，而不取决于日志级别（在系统级别或组件级别），配置级别或运行缓慢的采样率。探查器不会捕获缓慢的操作日志条目。\n\n\n# 复制延迟和流控\n\n复制延迟（replication lag）是指将主节点上的写操作复制到从节点上所花费的时间。较短的延迟时间是可以接受的，但是随着复制延迟的增加，可能会出现严重的问题：比如在主节点上的缓存压力。\n\n从 mongodb 4.2 开始，管理员可以限制主节点的写入速率，使得大多数延迟时间保持在可配置的最大值 flowcontroltargetlagseconds 以下。\n\n默认情况下，流控是开启的。\n\n启用流控后，随着延迟时间越来越接近 flowcontroltargetlagseconds，主对象上的写操作必须先获得令牌，然后才能进行锁定并执行写操作。通过限制每秒发出的令牌数量，流控机制尝试将延迟保持在目标以下。\n\n\n# 故障转移\n\n当主节点与集群中的其他成员通信的时间超过配置的 electiontimeoutmillis（默认为 10 秒）时，符合选举要求的从节点将要求选举，并提名自己为新的主节点。集群尝试完成选举新主节点并恢复正常工作。\n\n\n\n选举完成前，副本集无法处理写入操作。如果将副本集配置为：在主节点处于脱机状态时，在次节点上运行，则副本集可以继续提供读取查询。\n\n假设副本配置采用默认配置，则集群选择新节点的时间通常不应超过 12 秒，这包括：将主节点标记为不可用并完成选举所需的时间。可以通过修改 settings.electiontimeoutmillis 配置选项来调整此时间。网络延迟等因素可能会延长完成选举所需的时间，进而影响集群在没有主节点的情况下可以运行的时间。这些因素取决于集群实际的情况。\n\n将默认为 10 秒的 electiontimeoutmillis 选项数值缩小，可以更快地检测到主要故障。但是，由于网络延迟等因素，集群可能会更频繁地进行选举，即使该主节点实际上处于健康状态。这可能导致 w : 1 写操作的回滚次数增加。\n\n应用程序的连接逻辑应包括对自动故障转移和后续选举的容错处理。从 mongodb 3.6 开始，mongodb 驱动程序可以检测到主节点的失联，并可以自动重试一次某些写入操作。\n\n从 mongodb4.4 开始，mongodb 提供镜像读取：将可选举的从节点的最近访问的数据，预热为缓存。预热从节点的缓存可以帮助在选举后更快地恢复。\n\n\n# 读操作\n\n\n# 读优先\n\n默认情况下，客户端从主节点读取数据；但是，客户端可以指定读取首选项，以将读取操作发送到从节点。\n\n\n\n异步复制到从节点意味着向从节点读取数据可能会返回与主节点不一致的数据。\n\n包含读取操作的多文档事务必须使用读取主节点优先。给定事务中的所有操作必须路由到同一成员。\n\n\n# 数据可见性\n\n根据读取的关注点，客户端可以在持久化写入前查看写入结果：\n\n * 不管写的 write concern 如何设置，其他使用 "local" 或 "available" 的读配置的客户端都可以向发布客户端确认写操作之前看到写操作的结果。\n * 使用 "local" 或 "available" 读取配置的客户端可以读取数据，这些数据随后可能会在副本集故障转移期间回滚。\n\n对于多文档事务中的操作，当事务提交时，在事务中进行的所有数据更改都将保存，并在事务外部可见。也就是说，事务在回滚其他事务时将不会提交其某些更改。在提交事务前，事务外部看不到在事务中进行的数据更改。\n\n但是，当事务写入多个分片时，并非所有外部读操作都需要等待已提交事务的结果在所有分片上可见。例如，如果提交了一个事务，并且在分片 a 上可以看到写 1，但是在分片 b 上还看不到写 2，则在 "local" 读配置级别，外部读取可以读取写 1 的结果而看不到写 2。\n\n\n# 镜像读取\n\n从 mongodb 4.4 开始，mongodb 提供镜像读取以预热可选从节点（即优先级大于 0 的成员）的缓存。使用镜像读取（默认情况下已启用），主节点可以镜像它接收到的一部分操作，并将其发送给可选择的从节点的子集。子集的大小是可配置的。\n\n\n# 参考资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 分片",frontmatter:{title:"MongoDB 分片",date:"2020-09-20T23:12:17.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","分片"],permalink:"/pages/ad08f5/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/09.MongoDB%E5%88%86%E7%89%87.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/09.MongoDB分片.md",key:"v-1211efa8",path:"/pages/ad08f5/",headers:[{level:2,title:"分片集群简介",slug:"分片集群简介",normalizedTitle:"分片集群简介",charIndex:17},{level:3,title:"分片集群特点",slug:"分片集群特点",normalizedTitle:"分片集群特点",charIndex:107},{level:3,title:"分片集群组件",slug:"分片集群组件",normalizedTitle:"分片集群组件",charIndex:151},{level:3,title:"分片集群的分布",slug:"分片集群的分布",normalizedTitle:"分片集群的分布",charIndex:361},{level:3,title:"路由节点 mongos",slug:"路由节点-mongos",normalizedTitle:"路由节点 mongos",charIndex:606},{level:2,title:"分片 Key",slug:"分片-key",normalizedTitle:"分片 key",charIndex:857},{level:2,title:"分片策略",slug:"分片策略",normalizedTitle:"分片策略",charIndex:1606},{level:3,title:"Hash 分片",slug:"hash-分片",normalizedTitle:"hash 分片",charIndex:1721},{level:3,title:"范围分片",slug:"范围分片",normalizedTitle:"范围分片",charIndex:1729},{level:2,title:"分片集群中的区域",slug:"分片集群中的区域",normalizedTitle:"分片集群中的区域",charIndex:2173},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:2459}],headersStr:"分片集群简介 分片集群特点 分片集群组件 分片集群的分布 路由节点 mongos 分片 Key 分片策略 Hash 分片 范围分片 分片集群中的区域 参考资料",content:"# MongoDB 分片\n\n\n# 分片集群简介\n\n当 MongoDB 需要存储海量数据时，单节点不足以存储全量数据，且可能无法提供令人满意的吞吐量。所以，可以通过 MongoDB 分片机制来支持水平扩展。\n\n\n# 分片集群特点\n\n对应用完全透明\n\n数据自动均衡\n\n动态扩容\n\n提供三种分片方式\n\n\n# 分片集群组件\n\nMongoDB 分片集群含以下组件：\n\n * shard：每个分片包含分片数据的子集。每个分片都可以部署为副本集。\n * mongos：mongos 充当查询路由器，在客户端应用程序和分片集群之间提供接口。从 MongoDB 4.4 开始，mongos 可以支持 hedged reads 以最大程度地减少延迟。\n * config servers：提供集群元数据存储和分片数据分布的映射。\n\n\n\n\n# 分片集群的分布\n\nMongoDB 复制集以 collection 为单位，将数据分布在集群中的各个分片上。最多允许 1024 个分片。\n\nMongoDB 复制集的分片之间数据不重复，只有当所有分片都正常时，才能完整工作。\n\nMongoDB 数据库可以同时包含分片和未分片的集合的 collection。分片 collection 会分布在集群中各节点上。而未分片的 collection 存储在主节点上。每个数据库都有其自己的主节点。\n\n分片和未分片的 collection：\n\n\n\n\n# 路由节点 mongos\n\n要连接 MongoDB 分片集群，必须连接到 mongos 路由器。这包括分片和未分片的 collection。客户端不应该连接到单个分片节点进行读写操作。\n\n连接 mongos 的方式和连接 mongod 相同，例如通过 mongo shell 或 MongoDB 驱动程序。\n\n\n\n路由节点的作用：\n\n * 提供集群的单一入口\n * 转发应用端请求\n * 选择合适数据节点进行读写\n * 合并多个数据节点的返回\n\n一般，路由节点 mongos 建议至少 2 个。\n\n\n# 分片 Key\n\nMongoDB 使用分片 Key 在各个分片之间分发 collection 的 document。分片 Key 由 document 中的一个或多个字段组成。\n\n * 从 MongoDB 4.4 开始，分片 collection 中的 document 可能缺少分片 Key 字段。在跨分片分布文档时，缺少分片 Key 字段将被视为具有空值，但在路由查询时则不会。\n\n * 在 MongoDB 4.2 及更早版本中，分片 Key 字段必须在每个 document 中存在一个分片 collection。\n\n在分片 collection 时选择分片 Key。\n\n * 从 MongoDB 4.4 开始，您可以通过在现有 Key 中添加一个或多个后缀字段来优化 collection 的分片 Key。\n * 在 MongoDB 4.2 和更低版本中，无法在分片后更改分片 Key 的选择。\n\ndocument 的分片键值决定了其在各个分片中的分布\n\n * 从 MongoDB 4.2 开始，除非您的分片 Key 字段是不可变的_id 字段，否则您可以更新 document 的分片键值。\n * 在 MongoDB 4.0 及更低版本中，文档的分片 Key 字段值是不可变的。\n\n分片 Key 索引：要对已填充的 collection 进行分片，该 collection 必须具有以分片 Key 开头的索引。分片一个空 collection 时，如果该 collection 还没有针对指定分片 Key 的适当索引，则 MongoDB 会创建支持索引。\n\n分片 Key 策略：分片 Key 的选择会影响分片集群的性能，效率和可伸缩性。分片 Key 及其后备索引的选择也会影响集群可以使用的分片策略。\n\nMongoDB 分区将数据分片。每个分块都有基于分片 Key 的上下限。\n\n为了在整个集群中的所有分片上实现块的均匀分布，均衡器在后台运行，并在各分片上迁移块。\n\n\n# 分片策略\n\nMongoDB 支持两种分片策略：Hash 分片和范围分片。\n\n\n# Hash 分片\n\nHash 分片策略会先计算分片 Key 字段值的哈希值；然后，根据分片键值为每个 chunk 分配一个范围。\n\n> 注意：使用哈希索引解析查询时，MongoDB 会自动计算哈希值，应用程序不需要计算哈希。\n\n\n\n尽管分片 Key 范围可能是“接近”的，但它们的哈希值不太可能在同一 chunk 上。基于 Hash 的数据分发有助于更均匀的数据分布，尤其是在分片 Key 单调更改的数据集中。\n\n但是，Hash 分片意味着对分片 Key 做范围查询时不太可能针对单个分片，从而导致更多的集群范围内的广播操作。\n\n\n# 范围分片\n\n范围分片根据分片 Key 值将数据划分为多个范围。然后，根据分片 Key 值为每个 chunk 分配一个范围。\n\n\n\n值比较近似的一系列分片 Key 更有可能驻留在同一 chunk 上。范围分片的效率取决于选择的分片 Key。分片 Key 考虑不周全会导致数据分布不均，这可能会削弱分片的某些优势或导致性能瓶颈。\n\n\n# 分片集群中的区域\n\n区域可以提高跨多个数据中心的分片集群的数据局部性。\n\n在分片集群中，可以基于分片 Key 创建分片数据区域。可以将每个区域与集群中的一个或多个分片关联。分片可以与任意数量的区域关联。在平衡的集群中，MongoDB 仅将区域覆盖的 chunk 迁移到与该区域关联的分片。\n\n每个区域覆盖一个或多个分片 Key 值范围。区域覆盖的每个范围始终包括其上下边界。\n\n\n\n在定义要覆盖的区域的新范围时，必须使用分片 Key 中包含的字段。如果使用复合分片 Key，则范围必须包含分片 Key 的前缀。\n\n选择分片 Key 时，应考虑将来可能使用的区域。\n\n\n# 参考资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课",normalizedContent:"# mongodb 分片\n\n\n# 分片集群简介\n\n当 mongodb 需要存储海量数据时，单节点不足以存储全量数据，且可能无法提供令人满意的吞吐量。所以，可以通过 mongodb 分片机制来支持水平扩展。\n\n\n# 分片集群特点\n\n对应用完全透明\n\n数据自动均衡\n\n动态扩容\n\n提供三种分片方式\n\n\n# 分片集群组件\n\nmongodb 分片集群含以下组件：\n\n * shard：每个分片包含分片数据的子集。每个分片都可以部署为副本集。\n * mongos：mongos 充当查询路由器，在客户端应用程序和分片集群之间提供接口。从 mongodb 4.4 开始，mongos 可以支持 hedged reads 以最大程度地减少延迟。\n * config servers：提供集群元数据存储和分片数据分布的映射。\n\n\n\n\n# 分片集群的分布\n\nmongodb 复制集以 collection 为单位，将数据分布在集群中的各个分片上。最多允许 1024 个分片。\n\nmongodb 复制集的分片之间数据不重复，只有当所有分片都正常时，才能完整工作。\n\nmongodb 数据库可以同时包含分片和未分片的集合的 collection。分片 collection 会分布在集群中各节点上。而未分片的 collection 存储在主节点上。每个数据库都有其自己的主节点。\n\n分片和未分片的 collection：\n\n\n\n\n# 路由节点 mongos\n\n要连接 mongodb 分片集群，必须连接到 mongos 路由器。这包括分片和未分片的 collection。客户端不应该连接到单个分片节点进行读写操作。\n\n连接 mongos 的方式和连接 mongod 相同，例如通过 mongo shell 或 mongodb 驱动程序。\n\n\n\n路由节点的作用：\n\n * 提供集群的单一入口\n * 转发应用端请求\n * 选择合适数据节点进行读写\n * 合并多个数据节点的返回\n\n一般，路由节点 mongos 建议至少 2 个。\n\n\n# 分片 key\n\nmongodb 使用分片 key 在各个分片之间分发 collection 的 document。分片 key 由 document 中的一个或多个字段组成。\n\n * 从 mongodb 4.4 开始，分片 collection 中的 document 可能缺少分片 key 字段。在跨分片分布文档时，缺少分片 key 字段将被视为具有空值，但在路由查询时则不会。\n\n * 在 mongodb 4.2 及更早版本中，分片 key 字段必须在每个 document 中存在一个分片 collection。\n\n在分片 collection 时选择分片 key。\n\n * 从 mongodb 4.4 开始，您可以通过在现有 key 中添加一个或多个后缀字段来优化 collection 的分片 key。\n * 在 mongodb 4.2 和更低版本中，无法在分片后更改分片 key 的选择。\n\ndocument 的分片键值决定了其在各个分片中的分布\n\n * 从 mongodb 4.2 开始，除非您的分片 key 字段是不可变的_id 字段，否则您可以更新 document 的分片键值。\n * 在 mongodb 4.0 及更低版本中，文档的分片 key 字段值是不可变的。\n\n分片 key 索引：要对已填充的 collection 进行分片，该 collection 必须具有以分片 key 开头的索引。分片一个空 collection 时，如果该 collection 还没有针对指定分片 key 的适当索引，则 mongodb 会创建支持索引。\n\n分片 key 策略：分片 key 的选择会影响分片集群的性能，效率和可伸缩性。分片 key 及其后备索引的选择也会影响集群可以使用的分片策略。\n\nmongodb 分区将数据分片。每个分块都有基于分片 key 的上下限。\n\n为了在整个集群中的所有分片上实现块的均匀分布，均衡器在后台运行，并在各分片上迁移块。\n\n\n# 分片策略\n\nmongodb 支持两种分片策略：hash 分片和范围分片。\n\n\n# hash 分片\n\nhash 分片策略会先计算分片 key 字段值的哈希值；然后，根据分片键值为每个 chunk 分配一个范围。\n\n> 注意：使用哈希索引解析查询时，mongodb 会自动计算哈希值，应用程序不需要计算哈希。\n\n\n\n尽管分片 key 范围可能是“接近”的，但它们的哈希值不太可能在同一 chunk 上。基于 hash 的数据分发有助于更均匀的数据分布，尤其是在分片 key 单调更改的数据集中。\n\n但是，hash 分片意味着对分片 key 做范围查询时不太可能针对单个分片，从而导致更多的集群范围内的广播操作。\n\n\n# 范围分片\n\n范围分片根据分片 key 值将数据划分为多个范围。然后，根据分片 key 值为每个 chunk 分配一个范围。\n\n\n\n值比较近似的一系列分片 key 更有可能驻留在同一 chunk 上。范围分片的效率取决于选择的分片 key。分片 key 考虑不周全会导致数据分布不均，这可能会削弱分片的某些优势或导致性能瓶颈。\n\n\n# 分片集群中的区域\n\n区域可以提高跨多个数据中心的分片集群的数据局部性。\n\n在分片集群中，可以基于分片 key 创建分片数据区域。可以将每个区域与集群中的一个或多个分片关联。分片可以与任意数量的区域关联。在平衡的集群中，mongodb 仅将区域覆盖的 chunk 迁移到与该区域关联的分片。\n\n每个区域覆盖一个或多个分片 key 值范围。区域覆盖的每个范围始终包括其上下边界。\n\n\n\n在定义要覆盖的区域的新范围时，必须使用分片 key 中包含的字段。如果使用复合分片 key，则范围必须包含分片 key 的前缀。\n\n选择分片 key 时，应考虑将来可能使用的区域。\n\n\n# 参考资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 运维",frontmatter:{title:"MongoDB 运维",date:"2020-09-09T20:47:14.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB","运维"],permalink:"/pages/5e3c30/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/20.MongoDB%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/04.文档数据库/01.MongoDB/20.MongoDB运维.md",key:"v-74c8571a",path:"/pages/5e3c30/",headers:[{level:2,title:"MongoDB 安装",slug:"mongodb-安装",normalizedTitle:"mongodb 安装",charIndex:17},{level:3,title:"Windows",slug:"windows",normalizedTitle:"windows",charIndex:32},{level:3,title:"Linux",slug:"linux",normalizedTitle:"linux",charIndex:358},{level:3,title:"设置用户名、密码",slug:"设置用户名、密码",normalizedTitle:"设置用户名、密码",charIndex:1427},{level:2,title:"备份和恢复",slug:"备份和恢复",normalizedTitle:"备份和恢复",charIndex:1794},{level:3,title:"数据备份",slug:"数据备份",normalizedTitle:"数据备份",charIndex:1804},{level:3,title:"数据恢复",slug:"数据恢复",normalizedTitle:"数据恢复",charIndex:3296},{level:2,title:"导入导出",slug:"导入导出",normalizedTitle:"导入导出",charIndex:4859},{level:3,title:"导入操作",slug:"导入操作",normalizedTitle:"导入操作",charIndex:4975},{level:3,title:"导出操作",slug:"导出操作",normalizedTitle:"导出操作",charIndex:6498},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7501}],headersStr:"MongoDB 安装 Windows Linux 设置用户名、密码 备份和恢复 数据备份 数据恢复 导入导出 导入操作 导出操作 参考资料",content:'# MongoDB 运维\n\n\n# MongoDB 安装\n\n\n# Windows\n\n（1）下载并解压到本地\n\n进入官网下载地址：官方下载地址 ，选择合适的版本下载。\n\n（2）创建数据目录\n\nMongoDB 将数据目录存储在 db 目录下。但是这个数据目录不会主动创建，我们在安装完成后需要创建它。\n\n例如：D:\\Tools\\Server\\mongodb\\mongodb-4.4.0\\data\\db\n\n（3）运行 MongoDB 服务\n\nmongod --dbpath D:\\Tools\\Server\\mongodb\\mongodb-4.4.0\\data\\db\n\n\n（4）客户端连接 MongoDB\n\n可以在命令窗口中运行 mongo.exe 命令即可连接上 MongoDB\n\n（5）配置 MongoDB 服务\n\n\n# Linux\n\n（1）使用安装包安装\n\n安装前我们需要安装各个 Linux 平台依赖包。\n\nRed Hat/CentOS：\n\nsudo yum install libcurl openssl\n\n\nUbuntu 18.04 LTS ("Bionic")/Debian 10 "Buster"：\n\nsudo apt-get install libcurl4 openssl\n\n\nUbuntu 16.04 LTS ("Xenial")/Debian 9 "Stretch"：\n\nsudo apt-get install libcurl3 openssl\n\n\n（2）创建数据目录\n\n默认情况下 MongoDB 启动后会初始化以下两个目录：\n\n * 数据存储目录：/var/lib/mongodb\n * 日志文件目录：/var/log/mongodb\n\n我们在启动前可以先创建这两个目录并设置当前用户有读写权限：\n\nsudo mkdir -p /var/lib/mongo\nsudo mkdir -p /var/log/mongodb\nsudo chown `whoami` /var/lib/mongo     # 设置权限\nsudo chown `whoami` /var/log/mongodb   # 设置权限\n\n\n（3）运行 MongoDB 服务\n\nmongod --dbpath /var/lib/mongo --logpath /var/log/mongodb/mongod.log --fork\n\n\n打开 /var/log/mongodb/mongod.log 文件看到以下信息，说明启动成功。\n\n# tail -10f /var/log/mongodb/mongod.log\n2020-07-09T12:20:17.391+0800 I  NETWORK  [listener] Listening on /tmp/mongodb-27017.sock\n2020-07-09T12:20:17.392+0800 I  NETWORK  [listener] Listening on 127.0.0.1\n2020-07-09T12:20:17.392+0800 I  NETWORK  [listener] waiting for connections on port 27017\n\n\n（4）客户端连接 MongoDB\n\ncd /usr/local/mongodb4/bin\n./mongo\n\n\n> Linux 安装脚本\n\n\n# 设置用户名、密码\n\n> use admin\nswitched to db admin\n> db.createUser({"user":"root","pwd":"root","roles":[{"role":"userAdminAnyDatabase","db":"admin"}]})\nSuccessfully added user: {\n        "user" : "root",\n        "roles" : [\n                {\n                        "role" : "userAdminAnyDatabase",\n                        "db" : "admin"\n                }\n        ]\n}\n>\n\n\n\n# 备份和恢复\n\n\n# 数据备份\n\n在 Mongodb 中，使用 mongodump 命令来备份 MongoDB 数据。该命令可以导出所有数据到指定目录中。\n\nmongodump 命令可以通过参数指定导出的数据量级转存的服务器。\n\nmongodump 命令语法如下：\n\nmongodump -h dbhost -d dbname -o dbdirectory\n\n\n * -h：MongDB 所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017\n\n * -d：需要备份的数据库实例，例如：test\n\n * -o：备份的数据存放位置，例如：c:\\data\\dump，当然该目录需要提前建立，在备份完成后，系统自动在 dump 目录下建立一个 test 目录，这个目录里面存放该数据库实例的备份数据。\n\nmongodump 命令可选参数列表如下所示：\n\n语法                                                  描述                    实例\nmongodump --host HOST_NAME --port PORT_NUMBER       该命令将备份所有 MongoDB 数据   mongodump --host runoob.com --port 27017\nmongodump --dbpath DB_PATH --out BACKUP_DIRECTORY                         mongodump --dbpath /data/db/ --out /data/backup/\nmongodump --collection COLLECTION --db DB_NAME      该命令将备份指定数据库的集合。       mongodump --collection mycol --db test\n\n【示例】备份全量数据\n\n$ mongodump -h 127.0.0.1 --port 27017 -o test2\n...\n2020-09-11T11:55:58.086+0800    done dumping test.company (18801 documents)\n2020-09-11T11:56:00.725+0800    [#############...........]  test.people  559101/1000000  (55.9%)\n2020-09-11T11:56:03.725+0800    [###################.....]  test.people  829496/1000000  (82.9%)\n2020-09-11T11:56:06.725+0800    [#####################...]  test.people  884614/1000000  (88.5%)\n2020-09-11T11:56:08.088+0800    [########################]  test.people  1000000/1000000  (100.0%)\n2020-09-11T11:56:08.350+0800    done dumping test.people (1000000 documents)\n\n\n【示例】备份指定数据库\n\nmongodump -h 127.0.0.1 --port 27017 -d admin -o test3\n\n\n\n# 数据恢复\n\nmongodb 使用 mongorestore 命令来恢复备份的数据。\n\nmongorestore 命令语法如下：\n\n> mongorestore -h <hostname><:port> -d dbname <path>\n\n\n * --host <:port>, -h <:port>：MongoDB 所在服务器地址，默认为： localhost:27017\n\n * --db , -d ：需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如 test2\n\n * --drop：恢复的时候，先删除当前数据，然后恢复备份的数据。就是说，恢复后，备份后添加修改的数据都会被删除，慎用哦！\n\n * <path>：mongorestore 最后的一个参数，设置备份数据所在位置，例如：c:\\data\\dump\\test。你不能同时指定 <path> 和 --dir 选项，--dir 也可以设置备份目录。\n\n * --dir：指定备份的目录。你不能同时指定 <path> 和 --dir 选项。\n\n【示例】\n\n$ mongorestore -h 127.0.0.1 --port 27017 -d test --dir test --drop\n...\n2020-09-11T11:46:16.053+0800    finished restoring test.tweets (966 documents, 0 failures)\n2020-09-11T11:46:18.256+0800    [###.....................]  test.people  164MB/1.03GB  (15.6%)\n2020-09-11T11:46:21.255+0800    [########................]  test.people  364MB/1.03GB  (34.6%)\n2020-09-11T11:46:24.256+0800    [############............]  test.people  558MB/1.03GB  (53.0%)\n2020-09-11T11:46:27.255+0800    [###############.........]  test.people  700MB/1.03GB  (66.5%)\n2020-09-11T11:46:30.257+0800    [###################.....]  test.people  846MB/1.03GB  (80.3%)\n2020-09-11T11:46:33.255+0800    [######################..]  test.people  990MB/1.03GB  (94.0%)\n2020-09-11T11:46:34.542+0800    [########################]  test.people  1.03GB/1.03GB  (100.0%)\n2020-09-11T11:46:34.543+0800    no indexes to restore\n2020-09-11T11:46:34.543+0800    finished restoring test.people (1000000 documents, 0 failures)\n2020-09-11T11:46:34.544+0800    1000966 document(s) restored successfully. 0 document(s) failed to restore.\n\n\n\n# 导入导出\n\nmongoimport 和 mongoexport 并不能可靠地保存所有的富文本 BSON 数据类型，因为 JSON 仅能代表一种 BSON 支持的子集类型。因此，数据用这些工具导出导入或许会丢失一些精确程度。\n\n\n# 导入操作\n\n在 MongoDB 中，使用 mongoimport 来导入数据。 默认情况下，mongoimport 会将数据导入到本地主机端口 27017 上的 MongoDB 实例中。要将数据导入在其他主机或端口上运行的 MongoDB 实例中，请通过包含 --host 和 --port 选项来指定主机名或端口。 使用 --drop 选项删除集合（如果已经存在）。 这样可以确保该集合仅包含您要导入的数据。\n\n语法格式：\n\nmongoimport -h IP --port 端口 -u 用户名 -p 密码 -d 数据库 -c 表名 --type 类型 --headerline --upsert --drop 文件名\n\n\n【示例】导入表数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c book --drop test/book.dat\n2020-09-11T10:53:56.359+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T10:53:56.372+0800    dropping: test.book\n2020-09-11T10:53:56.628+0800    431 document(s) imported successfully. 0 document(s) failed to import.\n\n\n【示例】从 json 文件中导入表数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c student --upsert test/student.json\n2020-09-11T11:02:55.907+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T11:02:56.068+0800    200 document(s) imported successfully. 0 document(s) failed to import.\n\n\n【示例】从 csv 文件中导入表数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c product --type csv --headerline test/product.csv\n2020-09-11T11:07:49.788+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T11:07:51.051+0800    11 document(s) imported successfully. 0 document(s) failed to import.\n\n\n【示例】导入部分表字段数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c product --type json --upsertFields name,price test/product.json\n2020-09-11T11:14:05.410+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T11:14:05.612+0800    11 document(s) imported successfully. 0 document(s) failed to import.\n\n\n\n# 导出操作\n\n语法格式：\n\nmongoexport -h <IP> --port <端口> -u <用户名> -p <密码> -d <数据库> -c <表名> -f <字段> -q <条件导出> --csv -o <文件名>\n\n\n * -f：导出指字段，以逗号分割，-f name,email,age 导出 name,email,age 这三个字段\n * -q：可以根查询条件导出，-q \'{ "uid" : "100" }\' 导出 uid 为 100 的数据\n * --csv：表示导出的文件格式为 csv 的，这个比较有用，因为大部分的关系型数据库都是支持 csv，在这里有共同点\n\n【示例】导出整张表\n\n$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product -o test/product.dat\n2020-09-11T10:44:23.161+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T10:44:23.177+0800    exported 11 records\n\n\n【示例】导出表到 json 文件\n\n$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product --type json -o test/product.json\n2020-09-11T10:49:52.735+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T10:49:52.750+0800    exported 11 records\n\n\n【示例】导出表中部分字段到 csv 文件\n\n$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product --type csv -f name,price -o test/product.csv\n2020-09-11T10:47:33.160+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11T10:47:33.176+0800    exported 11 records\n\n\n\n# 参考资料\n\n * MongoDB 官网\n * MongoDB Github\n * MongoDB 官方免费教程\n * MongoDB 教程',normalizedContent:'# mongodb 运维\n\n\n# mongodb 安装\n\n\n# windows\n\n（1）下载并解压到本地\n\n进入官网下载地址：官方下载地址 ，选择合适的版本下载。\n\n（2）创建数据目录\n\nmongodb 将数据目录存储在 db 目录下。但是这个数据目录不会主动创建，我们在安装完成后需要创建它。\n\n例如：d:\\tools\\server\\mongodb\\mongodb-4.4.0\\data\\db\n\n（3）运行 mongodb 服务\n\nmongod --dbpath d:\\tools\\server\\mongodb\\mongodb-4.4.0\\data\\db\n\n\n（4）客户端连接 mongodb\n\n可以在命令窗口中运行 mongo.exe 命令即可连接上 mongodb\n\n（5）配置 mongodb 服务\n\n\n# linux\n\n（1）使用安装包安装\n\n安装前我们需要安装各个 linux 平台依赖包。\n\nred hat/centos：\n\nsudo yum install libcurl openssl\n\n\nubuntu 18.04 lts ("bionic")/debian 10 "buster"：\n\nsudo apt-get install libcurl4 openssl\n\n\nubuntu 16.04 lts ("xenial")/debian 9 "stretch"：\n\nsudo apt-get install libcurl3 openssl\n\n\n（2）创建数据目录\n\n默认情况下 mongodb 启动后会初始化以下两个目录：\n\n * 数据存储目录：/var/lib/mongodb\n * 日志文件目录：/var/log/mongodb\n\n我们在启动前可以先创建这两个目录并设置当前用户有读写权限：\n\nsudo mkdir -p /var/lib/mongo\nsudo mkdir -p /var/log/mongodb\nsudo chown `whoami` /var/lib/mongo     # 设置权限\nsudo chown `whoami` /var/log/mongodb   # 设置权限\n\n\n（3）运行 mongodb 服务\n\nmongod --dbpath /var/lib/mongo --logpath /var/log/mongodb/mongod.log --fork\n\n\n打开 /var/log/mongodb/mongod.log 文件看到以下信息，说明启动成功。\n\n# tail -10f /var/log/mongodb/mongod.log\n2020-07-09t12:20:17.391+0800 i  network  [listener] listening on /tmp/mongodb-27017.sock\n2020-07-09t12:20:17.392+0800 i  network  [listener] listening on 127.0.0.1\n2020-07-09t12:20:17.392+0800 i  network  [listener] waiting for connections on port 27017\n\n\n（4）客户端连接 mongodb\n\ncd /usr/local/mongodb4/bin\n./mongo\n\n\n> linux 安装脚本\n\n\n# 设置用户名、密码\n\n> use admin\nswitched to db admin\n> db.createuser({"user":"root","pwd":"root","roles":[{"role":"useradminanydatabase","db":"admin"}]})\nsuccessfully added user: {\n        "user" : "root",\n        "roles" : [\n                {\n                        "role" : "useradminanydatabase",\n                        "db" : "admin"\n                }\n        ]\n}\n>\n\n\n\n# 备份和恢复\n\n\n# 数据备份\n\n在 mongodb 中，使用 mongodump 命令来备份 mongodb 数据。该命令可以导出所有数据到指定目录中。\n\nmongodump 命令可以通过参数指定导出的数据量级转存的服务器。\n\nmongodump 命令语法如下：\n\nmongodump -h dbhost -d dbname -o dbdirectory\n\n\n * -h：mongdb 所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017\n\n * -d：需要备份的数据库实例，例如：test\n\n * -o：备份的数据存放位置，例如：c:\\data\\dump，当然该目录需要提前建立，在备份完成后，系统自动在 dump 目录下建立一个 test 目录，这个目录里面存放该数据库实例的备份数据。\n\nmongodump 命令可选参数列表如下所示：\n\n语法                                                  描述                    实例\nmongodump --host host_name --port port_number       该命令将备份所有 mongodb 数据   mongodump --host runoob.com --port 27017\nmongodump --dbpath db_path --out backup_directory                         mongodump --dbpath /data/db/ --out /data/backup/\nmongodump --collection collection --db db_name      该命令将备份指定数据库的集合。       mongodump --collection mycol --db test\n\n【示例】备份全量数据\n\n$ mongodump -h 127.0.0.1 --port 27017 -o test2\n...\n2020-09-11t11:55:58.086+0800    done dumping test.company (18801 documents)\n2020-09-11t11:56:00.725+0800    [#############...........]  test.people  559101/1000000  (55.9%)\n2020-09-11t11:56:03.725+0800    [###################.....]  test.people  829496/1000000  (82.9%)\n2020-09-11t11:56:06.725+0800    [#####################...]  test.people  884614/1000000  (88.5%)\n2020-09-11t11:56:08.088+0800    [########################]  test.people  1000000/1000000  (100.0%)\n2020-09-11t11:56:08.350+0800    done dumping test.people (1000000 documents)\n\n\n【示例】备份指定数据库\n\nmongodump -h 127.0.0.1 --port 27017 -d admin -o test3\n\n\n\n# 数据恢复\n\nmongodb 使用 mongorestore 命令来恢复备份的数据。\n\nmongorestore 命令语法如下：\n\n> mongorestore -h <hostname><:port> -d dbname <path>\n\n\n * --host <:port>, -h <:port>：mongodb 所在服务器地址，默认为： localhost:27017\n\n * --db , -d ：需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如 test2\n\n * --drop：恢复的时候，先删除当前数据，然后恢复备份的数据。就是说，恢复后，备份后添加修改的数据都会被删除，慎用哦！\n\n * <path>：mongorestore 最后的一个参数，设置备份数据所在位置，例如：c:\\data\\dump\\test。你不能同时指定 <path> 和 --dir 选项，--dir 也可以设置备份目录。\n\n * --dir：指定备份的目录。你不能同时指定 <path> 和 --dir 选项。\n\n【示例】\n\n$ mongorestore -h 127.0.0.1 --port 27017 -d test --dir test --drop\n...\n2020-09-11t11:46:16.053+0800    finished restoring test.tweets (966 documents, 0 failures)\n2020-09-11t11:46:18.256+0800    [###.....................]  test.people  164mb/1.03gb  (15.6%)\n2020-09-11t11:46:21.255+0800    [########................]  test.people  364mb/1.03gb  (34.6%)\n2020-09-11t11:46:24.256+0800    [############............]  test.people  558mb/1.03gb  (53.0%)\n2020-09-11t11:46:27.255+0800    [###############.........]  test.people  700mb/1.03gb  (66.5%)\n2020-09-11t11:46:30.257+0800    [###################.....]  test.people  846mb/1.03gb  (80.3%)\n2020-09-11t11:46:33.255+0800    [######################..]  test.people  990mb/1.03gb  (94.0%)\n2020-09-11t11:46:34.542+0800    [########################]  test.people  1.03gb/1.03gb  (100.0%)\n2020-09-11t11:46:34.543+0800    no indexes to restore\n2020-09-11t11:46:34.543+0800    finished restoring test.people (1000000 documents, 0 failures)\n2020-09-11t11:46:34.544+0800    1000966 document(s) restored successfully. 0 document(s) failed to restore.\n\n\n\n# 导入导出\n\nmongoimport 和 mongoexport 并不能可靠地保存所有的富文本 bson 数据类型，因为 json 仅能代表一种 bson 支持的子集类型。因此，数据用这些工具导出导入或许会丢失一些精确程度。\n\n\n# 导入操作\n\n在 mongodb 中，使用 mongoimport 来导入数据。 默认情况下，mongoimport 会将数据导入到本地主机端口 27017 上的 mongodb 实例中。要将数据导入在其他主机或端口上运行的 mongodb 实例中，请通过包含 --host 和 --port 选项来指定主机名或端口。 使用 --drop 选项删除集合（如果已经存在）。 这样可以确保该集合仅包含您要导入的数据。\n\n语法格式：\n\nmongoimport -h ip --port 端口 -u 用户名 -p 密码 -d 数据库 -c 表名 --type 类型 --headerline --upsert --drop 文件名\n\n\n【示例】导入表数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c book --drop test/book.dat\n2020-09-11t10:53:56.359+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t10:53:56.372+0800    dropping: test.book\n2020-09-11t10:53:56.628+0800    431 document(s) imported successfully. 0 document(s) failed to import.\n\n\n【示例】从 json 文件中导入表数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c student --upsert test/student.json\n2020-09-11t11:02:55.907+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t11:02:56.068+0800    200 document(s) imported successfully. 0 document(s) failed to import.\n\n\n【示例】从 csv 文件中导入表数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c product --type csv --headerline test/product.csv\n2020-09-11t11:07:49.788+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t11:07:51.051+0800    11 document(s) imported successfully. 0 document(s) failed to import.\n\n\n【示例】导入部分表字段数据\n\n$ mongoimport -h 127.0.0.1 --port 27017 -d test -c product --type json --upsertfields name,price test/product.json\n2020-09-11t11:14:05.410+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t11:14:05.612+0800    11 document(s) imported successfully. 0 document(s) failed to import.\n\n\n\n# 导出操作\n\n语法格式：\n\nmongoexport -h <ip> --port <端口> -u <用户名> -p <密码> -d <数据库> -c <表名> -f <字段> -q <条件导出> --csv -o <文件名>\n\n\n * -f：导出指字段，以逗号分割，-f name,email,age 导出 name,email,age 这三个字段\n * -q：可以根查询条件导出，-q \'{ "uid" : "100" }\' 导出 uid 为 100 的数据\n * --csv：表示导出的文件格式为 csv 的，这个比较有用，因为大部分的关系型数据库都是支持 csv，在这里有共同点\n\n【示例】导出整张表\n\n$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product -o test/product.dat\n2020-09-11t10:44:23.161+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t10:44:23.177+0800    exported 11 records\n\n\n【示例】导出表到 json 文件\n\n$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product --type json -o test/product.json\n2020-09-11t10:49:52.735+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t10:49:52.750+0800    exported 11 records\n\n\n【示例】导出表中部分字段到 csv 文件\n\n$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product --type csv -f name,price -o test/product.csv\n2020-09-11t10:47:33.160+0800    connected to: mongodb://127.0.0.1:27017/\n2020-09-11t10:47:33.176+0800    exported 11 records\n\n\n\n# 参考资料\n\n * mongodb 官网\n * mongodb github\n * mongodb 官方免费教程\n * mongodb 教程',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"MongoDB 教程",frontmatter:{title:"MongoDB 教程",date:"2020-09-09T20:47:14.000Z",categories:["数据库","文档数据库","MongoDB"],tags:["数据库","文档数据库","MongoDB"],permalink:"/pages/b1a116/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/04.%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93/01.MongoDB/",relativePath:"12.数据库/04.文档数据库/01.MongoDB/README.md",key:"v-d98adcc4",path:"/pages/b1a116/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:284},{level:3,title:"MongoDB 应用指南",slug:"mongodb-应用指南",normalizedTitle:"mongodb 应用指南",charIndex:294},{level:3,title:"MongoDB 的 CRUD 操作",slug:"mongodb-的-crud-操作",normalizedTitle:"mongodb 的 crud 操作",charIndex:311},{level:3,title:"MongoDB 聚合操作",slug:"mongodb-聚合操作",normalizedTitle:"mongodb 聚合操作",charIndex:333},{level:3,title:"MongoDB 事务",slug:"mongodb-事务",normalizedTitle:"mongodb 事务",charIndex:350},{level:3,title:"MongoDB 建模",slug:"mongodb-建模",normalizedTitle:"mongodb 建模",charIndex:365},{level:3,title:"MongoDB 建模示例",slug:"mongodb-建模示例",normalizedTitle:"mongodb 建模示例",charIndex:380},{level:3,title:"MongoDB 索引",slug:"mongodb-索引",normalizedTitle:"mongodb 索引",charIndex:397},{level:3,title:"MongoDB 复制",slug:"mongodb-复制",normalizedTitle:"mongodb 复制",charIndex:412},{level:3,title:"MongoDB 分片",slug:"mongodb-分片",normalizedTitle:"mongodb 分片",charIndex:427},{level:3,title:"MongoDB 运维",slug:"mongodb-运维",normalizedTitle:"mongodb 运维",charIndex:442},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:457},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:634}],headersStr:"📖 内容 MongoDB 应用指南 MongoDB 的 CRUD 操作 MongoDB 聚合操作 MongoDB 事务 MongoDB 建模 MongoDB 建模示例 MongoDB 索引 MongoDB 复制 MongoDB 分片 MongoDB 运维 📚 资料 🚪 传送",content:"# MongoDB 教程\n\n> MongoDB 是一个基于文档的分布式数据库，由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\n> \n> MongoDB 是一个介于关系型数据库和非关系型数据库之间的产品。它是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。\n> \n> MongoDB 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n\n# 📖 内容\n\n\n# MongoDB 应用指南\n\n\n# MongoDB 的 CRUD 操作\n\n\n# MongoDB 聚合操作\n\n\n# MongoDB 事务\n\n\n# MongoDB 建模\n\n\n# MongoDB 建模示例\n\n\n# MongoDB 索引\n\n\n# MongoDB 复制\n\n\n# MongoDB 分片\n\n\n# MongoDB 运维\n\n\n# 📚 资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课\n * 数据\n   * mongodb-json-files\n * 文章\n   * Introduction to MongoDB\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# mongodb 教程\n\n> mongodb 是一个基于文档的分布式数据库，由 c++ 语言编写。旨在为 web 应用提供可扩展的高性能数据存储解决方案。\n> \n> mongodb 是一个介于关系型数据库和非关系型数据库之间的产品。它是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。\n> \n> mongodb 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n\n# 📖 内容\n\n\n# mongodb 应用指南\n\n\n# mongodb 的 crud 操作\n\n\n# mongodb 聚合操作\n\n\n# mongodb 事务\n\n\n# mongodb 建模\n\n\n# mongodb 建模示例\n\n\n# mongodb 索引\n\n\n# mongodb 复制\n\n\n# mongodb 分片\n\n\n# mongodb 运维\n\n\n# 📚 资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课\n * 数据\n   * mongodb-json-files\n * 文章\n   * introduction to mongodb\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 面试总结",frontmatter:{title:"Redis 面试总结",date:"2020-07-13T17:03:42.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","面试"],permalink:"/pages/451b73/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/01.Redis%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93.html",relativePath:"12.数据库/05.KV数据库/01.Redis/01.Redis面试总结.md",key:"v-cd3bd61e",path:"/pages/451b73/",headers:[{level:2,title:"Redis 数据类型",slug:"redis-数据类型",normalizedTitle:"redis 数据类型",charIndex:17},{level:2,title:"Redis 内存淘汰",slug:"redis-内存淘汰",normalizedTitle:"redis 内存淘汰",charIndex:460},{level:2,title:"Redis 持久化",slug:"redis-持久化",normalizedTitle:"redis 持久化",charIndex:1439},{level:2,title:"Redis 事务",slug:"redis-事务",normalizedTitle:"redis 事务",charIndex:2113},{level:2,title:"Redis 管道",slug:"redis-管道",normalizedTitle:"redis 管道",charIndex:2637},{level:2,title:"Redis 高并发",slug:"redis-高并发",normalizedTitle:"redis 高并发",charIndex:2894},{level:2,title:"Redis 复制",slug:"redis-复制",normalizedTitle:"redis 复制",charIndex:3785},{level:2,title:"Redis 哨兵",slug:"redis-哨兵",normalizedTitle:"redis 哨兵",charIndex:4419},{level:2,title:"Redis vs. Memcached",slug:"redis-vs-memcached",normalizedTitle:"redis vs. memcached",charIndex:4847},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5466}],headersStr:"Redis 数据类型 Redis 内存淘汰 Redis 持久化 Redis 事务 Redis 管道 Redis 高并发 Redis 复制 Redis 哨兵 Redis vs. Memcached 参考资料",content:"# Redis 面试总结\n\n\n# Redis 数据类型\n\n【问题】\n\n * Redis 有哪些数据类型？\n * Redis 的数据类型分别适用于什么样的场景？\n\n----------------------------------------\n\n【解答】\n\n> Redis 数据类型和应用\n> \n> 数据类型的特性和应用细节点较多，详情可以参考：Redis 数据类型\n\n（1）Redis 支持五种基本数据类型：\n\n * String：常用于 KV 缓存\n * Hash：存储结构化数据，如：产品信息、用户信息等。\n * List：存储列表，如：粉丝列表、文章评论列表等。可以通过 lrange 命令进行分页查询。\n * Set：存储去重列表，如：粉丝列表等。可以基于 set 玩儿交集、并集、差集的操作。例如：求两个人的共同好友列表。\n * Sorted Set：存储含评分的去重列表，如：各种排行榜。\n\n（2）除此以外，还有 Bitmaps、HyperLogLogs、GEO、Streams 等高级数据类型。\n\n\n# Redis 内存淘汰\n\n【问题】\n\n * Redis 有哪些内存淘汰策略？\n * 这些淘汰策略分别适用于什么场景？\n * Redis 有哪些删除失效 key 的方法？\n * 如何设置 Redis 中键的过期时间？\n * 如果让你实现一个 LRU 算法，怎么做？\n\n----------------------------------------\n\n【解答】\n\n（1）Redis 过期策略是：定期删除+惰性删除。\n\n * 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。\n * 主动方法（active way），定期从设置了失效时间的主键中选择一部分失效的主键删除。\n\n（2）Redis 内存淘汰策略：\n\n * noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。这是 Redis 默认的策略。\n * allkeys-lru - 在主键空间中，优先移除最近未使用的 key。\n * allkeys-random - 在主键空间中，随机移除某个 key。\n * volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。\n * volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。\n * volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。\n\n（3）如何选择内存淘汰策略：\n\n * 如果数据呈现幂等分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用 allkeys-lru。\n * 如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用 allkeys-random。\n * volatile-lru 策略和 volatile-random 策略适合我们将一个 Redis 实例既应用于缓存和又应用于持久化存储的时候，然而我们也可以通过使用两个 Redis 实例来达到相同的效果。\n * 将 key 设置过期时间实际上会消耗更多的内存，因此我们建议使用 allkeys-lru 策略从而更有效率的使用内存。\n\n（4）LRU 算法实现思路：可以继承 LinkedHashMap，并覆写 removeEldestEntry 方法来实现一个最简单的 LRUCache\n\n\n# Redis 持久化\n\n【问题】\n\n * Redis 有几种持久化方式？\n * Redis 的不同持久化方式的特性和原理是什么？\n * RDB 和 AOF 各有什么优缺点？分别适用于什么样的场景？\n * Redis 执行持久化时，可以处理请求吗？\n * AOF 有几种同步频率？\n\n----------------------------------------\n\n【解答】\n\n> Redis 持久化\n> \n> 详情可以参考：Redis 持久化\n\n（1）Redis 支持两种持久化方式：RDB 和 AOF。\n\n（2）RDB 即某一时刻的二进制数据快照。\n\nRedis 会周期性生成 RDB 文件。\n\n生成 RDB 流程：Redis fork 一个子进程，负责生成 RDB；生成 RDB 采用 Copy On Write 模式，此时，如果收到写请求，会在原副本上操作，不影响工作。\n\nRDB 只能恢复生成快照时刻的数据，之后的数据无法恢复。生成 RDB 的资源开销高昂。RDB 适合做冷备。\n\n（3）AOF 会将写命令不断追加到 AOF 文本日志末尾。\n\nAOF 丢数据比 RDB 少，但文件会比 RDB 文件大很多。\n\n一般，AOF 设置 appendfsync 同步频率为 everysec 即可。\n\n（4）RDB or AOF\n\n建议同时使用 RDB 和 AOF。用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。\n\n\n# Redis 事务\n\n【问题】\n\n * Redis 的并发竞争问题是什么？如何解决这个问题？\n * Redis 支持事务吗？\n * Redis 事务是严格意义的事务吗？Redis 为什么不支持回滚。\n * Redis 事务如何工作？\n * 了解 Redis 事务中的 CAS 行为吗？\n\n【解答】\n\n> Redis 的事务特性、原理\n> \n> 详情参考：Redis 应用指南之 事务\n\nRedis 提供的不是严格的事务，Redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。\n\nRedis 不支持回滚的理由：\n\n * Redis 命令只会因为错误的语法而失败，或是命令用在了错误类型的键上面。\n * 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。\n\nMULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。\n\nRedis 有天然解决这个并发竞争问题的类 CAS 乐观锁方案：每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。\n\n\n# Redis 管道\n\n【问题】\n\n * 除了事务，还有其他批量执行 Redis 命令的方式吗？\n\n【解答】\n\nRedis 是一种基于 C/S 模型以及请求/响应协议的 TCP 服务。Redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。使用管道发送命令时，Redis Server 会将部分请求放到缓存队列中（占用内存），执行完毕后一次性发送结果。如果需要发送大量的命令，会占用大量的内存，因此应该按照合理数量分批次的处理。\n\n\n# Redis 高并发\n\n【问题】\n\n * Redis 是单线程模型，为何吞吐量还很高？\n * Redis 的 IO 多路复用原理是什么？\n * Redis 集群如何分片和寻址？\n * Redis 集群如何扩展？\n * Redis 集群如何保证数据一致？\n * Redis 集群如何规划？你们公司的生产环境上如何部署 Redis 集群？\n\n----------------------------------------\n\n【解答】\n\n> Redis 集群\n> \n> 详情可以参考：Redis 集群\n\n（1）单线程\n\nRedis 为单进程单线程模式，采用队列模式将并发访问变为串行访问。Redis 单机吞吐量也很高，能达到几万 QPS。\n\nRedis 单线程模型，依然有很高的并发吞吐，原因在于：\n\n * Redis 读写都是内存操作。\n * Redis 基于非阻塞的 IO 多路复用机制，同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。\n * 单线程，避免了线程创建、销毁、上下文切换的开销，并且避免了资源竞争。\n\n（2）扩展并发吞吐量、存储容量\n\nRedis 的高性能（扩展并发吞吐量、存储容量）通过主从架构来实现。\n\nRedis 集群采用主从模型，提供复制和故障转移功能，来保证 Redis 集群的高可用。通常情况，一主多从模式已经可以满足大部分项目的需要。根据实际的并发量，可以通过增加节点来扩展并发吞吐。\n\n一主多从模式下，主节点负责写操作（单机几万 QPS），从节点负责查询操作（单机十万 QPS）。\n\n进一步，如果需要缓存大量数据，就需要分区（sharding）。Redis 集群通过划分虚拟 hash 槽来分片，每个主节点负责一定范围的 hash 槽。当需要扩展集群节点时，重新分配 hash 槽即可，redis-trib 会自动迁移变更 hash 槽中所属的 key。\n\n（3）Redis 集群数据一致性\n\nRedis 集群基于复制特性实现节点间的数据一致性。\n\n\n# Redis 复制\n\n【问题】\n\n * Redis 复制的工作原理？Redis 旧版复制和新版复制有何不同？\n * Redis 主从节点间如何复制数据？\n * Redis 的数据一致性是强一致性吗？\n\n----------------------------------------\n\n【解答】\n\n> Redis 复制\n> \n> 详情可以参考：Redis 复制\n\n（1）旧版复制基于 SYNC 命令实现。分为同步（sync）和命令传播（command propagate）两个操作。这种方式存在缺陷：不能高效处理断线重连后的复制情况。\n\n（2）新版复制基于 PSYNC 命令实现。同步操作分为了两块：\n\n * 完整重同步（full resychronization） 用于初次复制；\n * 部分重同步（partial resychronization） 用于断线后重复制。\n   * 主从服务器的复制偏移量（replication offset）\n   * 主服务器的复制积压缓冲区（replication backlog）\n   * 服务器的运行 ID\n\n（3）Redis 集群主从节点复制的工作流程：\n\n * 步骤 1. 设置主从服务器\n * 步骤 2. 主从服务器建立 TCP 连接。\n * 步骤 3. 发送 PING 检查通信状态。\n * 步骤 4. 身份验证。\n * 步骤 5. 发送端口信息。\n * 步骤 6. 同步。\n * 步骤 7. 命令传播。\n\n\n# Redis 哨兵\n\n【问题】\n\n * Redis 如何实现高可用？\n * Redis 哨兵的功能？\n * Redis 哨兵的原理？\n * Redis 哨兵如何选举 Leader？\n * Redis 如何实现故障转移？\n\n----------------------------------------\n\n【解答】\n\n> Redis 哨兵\n> \n> 详情可以参考：Redis 哨兵\n\n（1）Redis 的高可用是通过哨兵来实现（Raft 协议的 Redis 实现）。Sentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。\n\n由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器，以及这些主服务器的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。\n\n\n\n\n# Redis vs. Memcached\n\n【问题】\n\nRedis 和 Memcached 有什么区别？\n\n分布式缓存技术选型，选 Redis 还是 Memcached，为什么？\n\nRedis 和 Memcached 各自的线程模型是怎样的？\n\n为什么单线程的 Redis 性能却不输于多线程的 Memcached？\n\n【解答】\n\nRedis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。memcache 支持简单的数据类型，String。\n\nRedis 支持数据的备份，即 master-slave 模式的数据备份。\n\nRedis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存之中\n\nredis 的速度比 memcached 快很多\n\nMemcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用单线程的 IO 复用模型。\n\n\n\n如果想要更详细了解的话，可以查看慕课网上的这篇手记（非常推荐） ：《脚踏两只船的困惑 - Memcached 与 Redis》：www.imooc.com/article/23549\n\n终极策略： 使用 Redis 的 String 类型做的事，都可以用 Memcached 替换，以此换取更好的性能提升； 除此以外，优先考虑 Redis；\n\n\n# 参考资料\n\n * 面试中关于 Redis 的问题看这篇就够了\n * advanced-java",normalizedContent:"# redis 面试总结\n\n\n# redis 数据类型\n\n【问题】\n\n * redis 有哪些数据类型？\n * redis 的数据类型分别适用于什么样的场景？\n\n----------------------------------------\n\n【解答】\n\n> redis 数据类型和应用\n> \n> 数据类型的特性和应用细节点较多，详情可以参考：redis 数据类型\n\n（1）redis 支持五种基本数据类型：\n\n * string：常用于 kv 缓存\n * hash：存储结构化数据，如：产品信息、用户信息等。\n * list：存储列表，如：粉丝列表、文章评论列表等。可以通过 lrange 命令进行分页查询。\n * set：存储去重列表，如：粉丝列表等。可以基于 set 玩儿交集、并集、差集的操作。例如：求两个人的共同好友列表。\n * sorted set：存储含评分的去重列表，如：各种排行榜。\n\n（2）除此以外，还有 bitmaps、hyperloglogs、geo、streams 等高级数据类型。\n\n\n# redis 内存淘汰\n\n【问题】\n\n * redis 有哪些内存淘汰策略？\n * 这些淘汰策略分别适用于什么场景？\n * redis 有哪些删除失效 key 的方法？\n * 如何设置 redis 中键的过期时间？\n * 如果让你实现一个 lru 算法，怎么做？\n\n----------------------------------------\n\n【解答】\n\n（1）redis 过期策略是：定期删除+惰性删除。\n\n * 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。\n * 主动方法（active way），定期从设置了失效时间的主键中选择一部分失效的主键删除。\n\n（2）redis 内存淘汰策略：\n\n * noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。这是 redis 默认的策略。\n * allkeys-lru - 在主键空间中，优先移除最近未使用的 key。\n * allkeys-random - 在主键空间中，随机移除某个 key。\n * volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。\n * volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。\n * volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。\n\n（3）如何选择内存淘汰策略：\n\n * 如果数据呈现幂等分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用 allkeys-lru。\n * 如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用 allkeys-random。\n * volatile-lru 策略和 volatile-random 策略适合我们将一个 redis 实例既应用于缓存和又应用于持久化存储的时候，然而我们也可以通过使用两个 redis 实例来达到相同的效果。\n * 将 key 设置过期时间实际上会消耗更多的内存，因此我们建议使用 allkeys-lru 策略从而更有效率的使用内存。\n\n（4）lru 算法实现思路：可以继承 linkedhashmap，并覆写 removeeldestentry 方法来实现一个最简单的 lrucache\n\n\n# redis 持久化\n\n【问题】\n\n * redis 有几种持久化方式？\n * redis 的不同持久化方式的特性和原理是什么？\n * rdb 和 aof 各有什么优缺点？分别适用于什么样的场景？\n * redis 执行持久化时，可以处理请求吗？\n * aof 有几种同步频率？\n\n----------------------------------------\n\n【解答】\n\n> redis 持久化\n> \n> 详情可以参考：redis 持久化\n\n（1）redis 支持两种持久化方式：rdb 和 aof。\n\n（2）rdb 即某一时刻的二进制数据快照。\n\nredis 会周期性生成 rdb 文件。\n\n生成 rdb 流程：redis fork 一个子进程，负责生成 rdb；生成 rdb 采用 copy on write 模式，此时，如果收到写请求，会在原副本上操作，不影响工作。\n\nrdb 只能恢复生成快照时刻的数据，之后的数据无法恢复。生成 rdb 的资源开销高昂。rdb 适合做冷备。\n\n（3）aof 会将写命令不断追加到 aof 文本日志末尾。\n\naof 丢数据比 rdb 少，但文件会比 rdb 文件大很多。\n\n一般，aof 设置 appendfsync 同步频率为 everysec 即可。\n\n（4）rdb or aof\n\n建议同时使用 rdb 和 aof。用 aof 来保证数据不丢失，作为数据恢复的第一选择; 用 rdb 来做不同程度的冷备，在 aof 文件都丢失或损坏不可用的时候，还可以使用 rdb 来进行快速的数据恢复。\n\n\n# redis 事务\n\n【问题】\n\n * redis 的并发竞争问题是什么？如何解决这个问题？\n * redis 支持事务吗？\n * redis 事务是严格意义的事务吗？redis 为什么不支持回滚。\n * redis 事务如何工作？\n * 了解 redis 事务中的 cas 行为吗？\n\n【解答】\n\n> redis 的事务特性、原理\n> \n> 详情参考：redis 应用指南之 事务\n\nredis 提供的不是严格的事务，redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。\n\nredis 不支持回滚的理由：\n\n * redis 命令只会因为错误的语法而失败，或是命令用在了错误类型的键上面。\n * 因为不需要对回滚进行支持，所以 redis 的内部可以保持简单且快速。\n\nmulti 、 exec 、 discard 和 watch 是 redis 事务相关的命令。\n\nredis 有天然解决这个并发竞争问题的类 cas 乐观锁方案：每次要写之前，先判断一下当前这个 value 的时间戳是否比缓存里的 value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。\n\n\n# redis 管道\n\n【问题】\n\n * 除了事务，还有其他批量执行 redis 命令的方式吗？\n\n【解答】\n\nredis 是一种基于 c/s 模型以及请求/响应协议的 tcp 服务。redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。使用管道发送命令时，redis server 会将部分请求放到缓存队列中（占用内存），执行完毕后一次性发送结果。如果需要发送大量的命令，会占用大量的内存，因此应该按照合理数量分批次的处理。\n\n\n# redis 高并发\n\n【问题】\n\n * redis 是单线程模型，为何吞吐量还很高？\n * redis 的 io 多路复用原理是什么？\n * redis 集群如何分片和寻址？\n * redis 集群如何扩展？\n * redis 集群如何保证数据一致？\n * redis 集群如何规划？你们公司的生产环境上如何部署 redis 集群？\n\n----------------------------------------\n\n【解答】\n\n> redis 集群\n> \n> 详情可以参考：redis 集群\n\n（1）单线程\n\nredis 为单进程单线程模式，采用队列模式将并发访问变为串行访问。redis 单机吞吐量也很高，能达到几万 qps。\n\nredis 单线程模型，依然有很高的并发吞吐，原因在于：\n\n * redis 读写都是内存操作。\n * redis 基于非阻塞的 io 多路复用机制，同时监听多个 socket，将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。\n * 单线程，避免了线程创建、销毁、上下文切换的开销，并且避免了资源竞争。\n\n（2）扩展并发吞吐量、存储容量\n\nredis 的高性能（扩展并发吞吐量、存储容量）通过主从架构来实现。\n\nredis 集群采用主从模型，提供复制和故障转移功能，来保证 redis 集群的高可用。通常情况，一主多从模式已经可以满足大部分项目的需要。根据实际的并发量，可以通过增加节点来扩展并发吞吐。\n\n一主多从模式下，主节点负责写操作（单机几万 qps），从节点负责查询操作（单机十万 qps）。\n\n进一步，如果需要缓存大量数据，就需要分区（sharding）。redis 集群通过划分虚拟 hash 槽来分片，每个主节点负责一定范围的 hash 槽。当需要扩展集群节点时，重新分配 hash 槽即可，redis-trib 会自动迁移变更 hash 槽中所属的 key。\n\n（3）redis 集群数据一致性\n\nredis 集群基于复制特性实现节点间的数据一致性。\n\n\n# redis 复制\n\n【问题】\n\n * redis 复制的工作原理？redis 旧版复制和新版复制有何不同？\n * redis 主从节点间如何复制数据？\n * redis 的数据一致性是强一致性吗？\n\n----------------------------------------\n\n【解答】\n\n> redis 复制\n> \n> 详情可以参考：redis 复制\n\n（1）旧版复制基于 sync 命令实现。分为同步（sync）和命令传播（command propagate）两个操作。这种方式存在缺陷：不能高效处理断线重连后的复制情况。\n\n（2）新版复制基于 psync 命令实现。同步操作分为了两块：\n\n * 完整重同步（full resychronization） 用于初次复制；\n * 部分重同步（partial resychronization） 用于断线后重复制。\n   * 主从服务器的复制偏移量（replication offset）\n   * 主服务器的复制积压缓冲区（replication backlog）\n   * 服务器的运行 id\n\n（3）redis 集群主从节点复制的工作流程：\n\n * 步骤 1. 设置主从服务器\n * 步骤 2. 主从服务器建立 tcp 连接。\n * 步骤 3. 发送 ping 检查通信状态。\n * 步骤 4. 身份验证。\n * 步骤 5. 发送端口信息。\n * 步骤 6. 同步。\n * 步骤 7. 命令传播。\n\n\n# redis 哨兵\n\n【问题】\n\n * redis 如何实现高可用？\n * redis 哨兵的功能？\n * redis 哨兵的原理？\n * redis 哨兵如何选举 leader？\n * redis 如何实现故障转移？\n\n----------------------------------------\n\n【解答】\n\n> redis 哨兵\n> \n> 详情可以参考：redis 哨兵\n\n（1）redis 的高可用是通过哨兵来实现（raft 协议的 redis 实现）。sentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。\n\n由一个或多个 sentinel 实例组成的 sentinel 系统可以监视任意多个主服务器，以及这些主服务器的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。\n\n\n\n\n# redis vs. memcached\n\n【问题】\n\nredis 和 memcached 有什么区别？\n\n分布式缓存技术选型，选 redis 还是 memcached，为什么？\n\nredis 和 memcached 各自的线程模型是怎样的？\n\n为什么单线程的 redis 性能却不输于多线程的 memcached？\n\n【解答】\n\nredis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。memcache 支持简单的数据类型，string。\n\nredis 支持数据的备份，即 master-slave 模式的数据备份。\n\nredis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 memecache 把数据全部存在内存之中\n\nredis 的速度比 memcached 快很多\n\nmemcached 是多线程，非阻塞 io 复用的网络模型；redis 使用单线程的 io 复用模型。\n\n\n\n如果想要更详细了解的话，可以查看慕课网上的这篇手记（非常推荐） ：《脚踏两只船的困惑 - memcached 与 redis》：www.imooc.com/article/23549\n\n终极策略： 使用 redis 的 string 类型做的事，都可以用 memcached 替换，以此换取更好的性能提升； 除此以外，优先考虑 redis；\n\n\n# 参考资料\n\n * 面试中关于 redis 的问题看这篇就够了\n * advanced-java",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 应用指南",frontmatter:{title:"Redis 应用指南",date:"2020-01-30T21:48:57.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis"],permalink:"/pages/94e9d6/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/02.Redis%E5%BA%94%E7%94%A8%E6%8C%87%E5%8D%97.html",relativePath:"12.数据库/05.KV数据库/01.Redis/02.Redis应用指南.md",key:"v-2a8b7fae",path:"/pages/94e9d6/",headers:[{level:2,title:"一、Redis 简介",slug:"一、redis-简介",normalizedTitle:"一、redis 简介",charIndex:17},{level:3,title:"Redis 使用场景",slug:"redis-使用场景",normalizedTitle:"redis 使用场景",charIndex:138},{level:3,title:"Redis 的优势",slug:"redis-的优势",normalizedTitle:"redis 的优势",charIndex:495},{level:3,title:"Redis 与 Memcached",slug:"redis-与-memcached",normalizedTitle:"redis 与 memcached",charIndex:815},{level:3,title:"Redis 为什么快",slug:"redis-为什么快",normalizedTitle:"redis 为什么快",charIndex:1348},{level:2,title:"二、Redis 数据类型",slug:"二、redis-数据类型",normalizedTitle:"二、redis 数据类型",charIndex:1709},{level:2,title:"三、Redis 内存淘汰",slug:"三、redis-内存淘汰",normalizedTitle:"三、redis 内存淘汰",charIndex:1838},{level:3,title:"内存淘汰要点",slug:"内存淘汰要点",normalizedTitle:"内存淘汰要点",charIndex:1855},{level:3,title:"主键过期时间",slug:"主键过期时间",normalizedTitle:"主键过期时间",charIndex:2309},{level:3,title:"淘汰策略",slug:"淘汰策略",normalizedTitle:"淘汰策略",charIndex:183},{level:3,title:"如何选择淘汰策略",slug:"如何选择淘汰策略",normalizedTitle:"如何选择淘汰策略",charIndex:3112},{level:3,title:"内部实现",slug:"内部实现",normalizedTitle:"内部实现",charIndex:3440},{level:2,title:"四、Redis 持久化",slug:"四、redis-持久化",normalizedTitle:"四、redis 持久化",charIndex:3654},{level:2,title:"五、Redis 事件",slug:"五、redis-事件",normalizedTitle:"五、redis 事件",charIndex:3955},{level:3,title:"文件事件",slug:"文件事件",normalizedTitle:"文件事件",charIndex:4003},{level:3,title:"时间事件",slug:"时间事件",normalizedTitle:"时间事件",charIndex:4135},{level:3,title:"事件的调度与执行",slug:"事件的调度与执行",normalizedTitle:"事件的调度与执行",charIndex:4662},{level:2,title:"六、Redis 事务",slug:"六、redis-事务",normalizedTitle:"六、redis 事务",charIndex:5564},{level:3,title:"MULTI",slug:"multi",normalizedTitle:"multi",charIndex:646},{level:3,title:"EXEC",slug:"exec",normalizedTitle:"exec",charIndex:654},{level:3,title:"DISCARD",slug:"discard",normalizedTitle:"discard",charIndex:5663},{level:3,title:"WATCH",slug:"watch",normalizedTitle:"watch",charIndex:5673},{level:4,title:"取消 WATCH 的场景",slug:"取消-watch-的场景",normalizedTitle:"取消 watch 的场景",charIndex:6982},{level:4,title:"使用 WATCH 创建原子操作",slug:"使用-watch-创建原子操作",normalizedTitle:"使用 watch 创建原子操作",charIndex:7220},{level:3,title:"Rollback",slug:"rollback",normalizedTitle:"rollback",charIndex:7393},{level:2,title:"七、Redis 管道",slug:"七、redis-管道",normalizedTitle:"七、redis 管道",charIndex:7516},{level:2,title:"八、Redis 发布与订阅",slug:"八、redis-发布与订阅",normalizedTitle:"八、redis 发布与订阅",charIndex:8174},{level:2,title:"九、Redis 复制",slug:"九、redis-复制",normalizedTitle:"九、redis 复制",charIndex:8819},{level:3,title:"旧版复制",slug:"旧版复制",normalizedTitle:"旧版复制",charIndex:8906},{level:3,title:"新版复制",slug:"新版复制",normalizedTitle:"新版复制",charIndex:9152},{level:3,title:"部分重同步",slug:"部分重同步",normalizedTitle:"部分重同步",charIndex:9207},{level:3,title:"PSYNC 命令",slug:"psync-命令",normalizedTitle:"psync 命令",charIndex:9180},{level:3,title:"心跳检测",slug:"心跳检测",normalizedTitle:"心跳检测",charIndex:9673},{level:2,title:"十、Redis 哨兵",slug:"十、redis-哨兵",normalizedTitle:"十、redis 哨兵",charIndex:9816},{level:2,title:"十一、Redis 集群",slug:"十一、redis-集群",normalizedTitle:"十一、redis 集群",charIndex:9916},{level:2,title:"十二、Redis Client",slug:"十二、redis-client",normalizedTitle:"十二、redis client",charIndex:10376},{level:2,title:"扩展阅读",slug:"扩展阅读",normalizedTitle:"扩展阅读",charIndex:10645},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10696}],headersStr:"一、Redis 简介 Redis 使用场景 Redis 的优势 Redis 与 Memcached Redis 为什么快 二、Redis 数据类型 三、Redis 内存淘汰 内存淘汰要点 主键过期时间 淘汰策略 如何选择淘汰策略 内部实现 四、Redis 持久化 五、Redis 事件 文件事件 时间事件 事件的调度与执行 六、Redis 事务 MULTI EXEC DISCARD WATCH 取消 WATCH 的场景 使用 WATCH 创建原子操作 Rollback 七、Redis 管道 八、Redis 发布与订阅 九、Redis 复制 旧版复制 新版复制 部分重同步 PSYNC 命令 心跳检测 十、Redis 哨兵 十一、Redis 集群 十二、Redis Client 扩展阅读 参考资料",content:'# Redis 应用指南\n\n\n# 一、Redis 简介\n\n> Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。\n> \n> 键的类型只能为字符串，值支持的五种类型数据类型为：字符串、列表、集合、有序集合、散列表。\n\n\n# Redis 使用场景\n\n * 缓存 - 将热点数据放到内存中，设置内存的最大使用量以及过期淘汰策略来保证缓存的命中率。\n * 计数器 - Redis 这种内存数据库能支持计数器频繁的读写操作。\n * 应用限流 - 限制一个网站访问流量。\n * 消息队列 - 使用 List 数据类型，它是双向链表。\n * 查找表 - 使用 HASH 数据类型。\n * 交集运算 - 使用 SET 类型，例如求两个用户的共同好友。\n * 排行榜 - 使用 ZSET 数据类型。\n * 分布式 Session - 多个应用服务器的 Session 都存储到 Redis 中来保证 Session 的一致性。\n * 分布式锁 - 除了可以使用 SETNX 实现分布式锁之外，还可以使用官方提供的 RedLock 分布式锁实现。\n\n\n# Redis 的优势\n\n * 性能极高 – Redis 能读的速度是 110000 次/s,写的速度是 81000 次/s。\n * 丰富的数据类型 - 支持字符串、列表、集合、有序集合、散列表。\n * 原子 - Redis 的所有操作都是原子性的。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC 指令包起来。\n * 持久化 - Redis 支持数据的持久化。可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。\n * 备份 - Redis 支持数据的备份，即 master-slave 模式的数据备份。\n * 丰富的特性 - Redis 还支持发布订阅, 通知, key 过期等等特性。\n\n\n# Redis 与 Memcached\n\nRedis 与 Memcached 因为都可以用于缓存，所以常常被拿来做比较，二者主要有以下区别：\n\n数据类型\n\n * Memcached 仅支持字符串类型；\n * 而 Redis 支持五种不同种类的数据类型，使得它可以更灵活地解决问题。\n\n数据持久化\n\n * Memcached 不支持持久化；\n * Redis 支持两种持久化策略：RDB 快照和 AOF 日志。\n\n分布式\n\n * Memcached 不支持分布式，只能通过在客户端使用像一致性哈希这样的分布式算法来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。\n * Redis Cluster 实现了分布式的支持。\n\n内存管理机制\n\n * Memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。\n * 在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘。而 Memcached 的数据则会一直在内存中。\n\n\n# Redis 为什么快\n\nRedis 单机 QPS 能达到 100000。\n\nRedis 是单线程模型（Redis 6.0 已经支持多线程模型），为什么还能有这么高的并发？\n\n * Redis 完全基于内存操作。\n * Redis 数据结构简单。\n * 采用单线程，避免线程上下文切换和竞争。\n * 使用 I/O 多路复用模型（非阻塞 I/O）。\n\n> I/O 多路复用\n> \n> I/O 多路复用模型是利用 select、poll、epoll 可以同时监察多个流的 I/O 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 I/O 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。\n\n\n# 二、Redis 数据类型\n\nRedis 基本数据类型：STRING、HASH、LIST、SET、ZSET\n\nRedis 高级数据类型：BitMap、HyperLogLog、GEO\n\n> 💡 更详细的特性及原理说明请参考：Redis 数据类型和应用\n\n\n# 三、Redis 内存淘汰\n\n\n# 内存淘汰要点\n\n * 最大缓存 - Redis 允许通过 maxmemory 参数来设置内存最大值。\n\n * 失效时间 - 作为一种定期清理无效数据的重要机制，在 Redis 提供的诸多命令中，EXPIRE、EXPIREAT、PEXPIRE、PEXPIREAT 以及 SETEX 和 PSETEX 均可以用来设置一条键值对的失效时间。而一条键值对一旦被关联了失效时间就会在到期后自动删除（或者说变得无法访问更为准确）。\n\n * 淘汰策略 - 随着不断的向 Redis 中保存数据，当内存剩余空间无法满足添加的数据时，Redis 内就会施行数据淘汰策略，清除一部分内容然后保证新的数据可以保存到内存中。内存淘汰机制是为了更好的使用内存，用一定得 miss 来换取内存的利用率，保证 Redis 缓存中保存的都是热点数据。\n\n * 非精准的 LRU - 实际上 Redis 实现的 LRU 并不是可靠的 LRU，也就是名义上我们使用 LRU 算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。\n\n\n# 主键过期时间\n\nRedis 可以为每个键设置过期时间，当键过期时，会自动删除该键。\n\n对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。\n\n可以使用 EXPIRE 或 EXPIREAT 来为 key 设置过期时间。\n\n> 🔔 注意：当 EXPIRE 的时间如果设置的是负数，EXPIREAT 设置的时间戳是过期时间，将直接删除 key。\n\n示例：\n\nredis> SET mykey "Hello"\n"OK"\nredis> EXPIRE mykey 10\n(integer) 1\nredis> TTL mykey\n(integer) 10\nredis> SET mykey "Hello World"\n"OK"\nredis> TTL mykey\n(integer) -1\nredis>\n\n\n\n# 淘汰策略\n\n内存淘汰只是 Redis 提供的一个功能，为了更好地实现这个功能，必须为不同的应用场景提供不同的策略，内存淘汰策略讲的是为实现内存淘汰我们具体怎么做，要解决的问题包括淘汰键空间如何选择？在键空间中淘汰键如何选择？\n\nRedis 提供了下面几种内存淘汰策略供用户选：\n\n * noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。这是 Redis 默认的策略。\n * allkeys-lru - 在主键空间中，优先移除最近未使用的 key。\n * allkeys-random - 在主键空间中，随机移除某个 key。\n * volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。\n * volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。\n * volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。\n\n\n# 如何选择淘汰策略\n\n * 如果数据呈现幂等分布（存在热点数据，部分数据访问频率高，部分数据访问频率低），则使用 allkeys-lru。\n * 如果数据呈现平等分布（数据访问频率大致相同），则使用 allkeys-random。\n * 如果希望使用不同的 TTL 值向 Redis 提示哪些 key 更适合被淘汰，请使用 volatile-ttl。\n * volatile-lru 和 volatile-random 适合既应用于缓存和又应用于持久化存储的场景，然而我们也可以通过使用两个 Redis 实例来达到相同的效果。\n * 将 key 设置过期时间实际上会消耗更多的内存，因此建议使用 allkeys-lru 策略从而更有效率的使用内存。\n\n\n# 内部实现\n\nRedis 删除失效主键的方法主要有两种：\n\n * 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。\n * 主动方法（active way），周期性地从设置了失效时间的主键中选择一部分失效的主键删除。\n * 主动删除：当前已用内存超过 maxmemory 限定时，触发主动清理策略，该策略由启动参数的配置决定主键具体的失效时间全部都维护在 expires 这个字典表中。\n\n\n# 四、Redis 持久化\n\nRedis 是内存型数据库，为了保证数据在宕机后不会丢失，需要将内存中的数据持久化到硬盘上。\n\nRedis 支持两种持久化方式：RDB 和 AOF。\n\n * RDB - RDB 即快照方式，它将某个时间点的所有 Redis 数据保存到一个经过压缩的二进制文件（RDB 文件）中。\n * AOF - AOF(Append Only File) 是以文本日志形式将所有写命令追加到 AOF 文件的末尾，以此来记录数据的变化。当服务器重启的时候会重新载入和执行这些命令来恢复原始的数据。AOF 适合作为 热备。\n\n> 💡 更详细的特性及原理说明请参考：Redis 持久化\n\n\n# 五、Redis 事件\n\nRedis 服务器是一个事件驱动程序，服务器需要处理两类事件：\n\n * 文件事件（file event） - Redis 服务器通过套接字（Socket）与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。服务器与客户端（或其他的服务器）的通信会产生文件事件，而服务器通过监听并处理这些事件来完成一系列网络通信操作。\n * 时间事件（time event） - Redis 服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。\n\n\n# 文件事件\n\nRedis 基于 Reactor 模式开发了自己的网络时间处理器。\n\n * Redis 文件事件处理器使用 I/O 多路复用程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。\n * 当被监听的套接字准备好执行连接应答、读取、写入、关闭操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。\n\n虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字，文件事件处理器实现了高性能的网络通信模型。\n\n文件事件处理器有四个组成部分：套接字、I/O 多路复用程序、文件事件分派器、事件处理器。\n\n\n\n\n# 时间事件\n\n时间事件又分为：\n\n * 定时事件：是让一段程序在指定的时间之内执行一次；\n * 周期性事件：是让一段程序每隔指定时间就执行一次。\n\nRedis 将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，通过遍历整个链表查找出已到达的时间事件，并调用响应的事件处理器。\n\n\n# 事件的调度与执行\n\n服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。\n\n事件调度与执行由 aeProcessEvents 函数负责，伪代码如下：\n\ndef aeProcessEvents():\n\n    ## 获取到达时间离当前时间最接近的时间事件\n    time_event = aeSearchNearestTimer()\n\n    ## 计算最接近的时间事件距离到达还有多少毫秒\n    remaind_ms = time_event.when - unix_ts_now()\n\n    ## 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0\n    if remaind_ms < 0:\n        remaind_ms = 0\n\n    ## 根据 remaind_ms 的值，创建 timeval\n    timeval = create_timeval_with_ms(remaind_ms)\n\n    ## 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定\n    aeApiPoll(timeval)\n\n    ## 处理所有已产生的文件事件\n    procesFileEvents()\n\n    ## 处理所有已到达的时间事件\n    processTimeEvents()\n\n\n将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下：\n\ndef main():\n\n    ## 初始化服务器\n    init_server()\n\n    ## 一直处理事件，直到服务器关闭为止\n    while server_is_not_shutdown():\n        aeProcessEvents()\n\n    ## 服务器关闭，执行清理操作\n    clean_server()\n\n\n从事件处理的角度来看，服务器运行流程如下：\n\n\n\n\n# 六、Redis 事务\n\n> Redis 提供的不是严格的事务，Redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。\n\nMULTI 、 EXEC 、 DISCARD 和 WATCH 是 Redis 事务相关的命令。\n\n事务可以一次执行多个命令， 并且有以下两个重要的保证：\n\n * 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。\n * 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。\n\n\n# MULTI\n\nMULTI 命令用于开启一个事务，它总是返回 OK 。\n\nMULTI 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 EXEC 命令被调用时， 所有队列中的命令才会被执行。\n\n以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值：\n\n> MULTI\nOK\n> INCR foo\nQUEUED\n> INCR bar\nQUEUED\n> EXEC\n1) (integer) 1\n2) (integer) 1\n\n\n\n# EXEC\n\nEXEC 命令负责触发并执行事务中的所有命令。\n\n * 如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。\n * 另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。\n\n\n# DISCARD\n\n当执行 DISCARD 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出。\n\n示例：\n\n> SET foo 1\nOK\n> MULTI\nOK\n> INCR foo\nQUEUED\n> DISCARD\nOK\n> GET foo\n"1"\n\n\n\n# WATCH\n\nWATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。\n\n被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回 nil-reply 来表示事务已经失败。\n\nWATCH mykey\nval = GET mykey\nval = val + 1\nMULTI\nSET mykey $val\nEXEC\n\n\n使用上面的代码， 如果在 WATCH 执行之后， EXEC 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。\n\n这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。\n\nWATCH 使得 EXEC 命令需要有条件地执行：事务只能在所有被监视键都没有被修改的前提下执行，如果这个前提不能满足的话，事务就不会被执行。\n\nWATCH 命令可以被调用多次。对键的监视从 WATCH 执行之后开始生效，直到调用 EXEC 为止。\n\n用户还可以在单个 WATCH 命令中监视任意多个键，例如：\n\nredis> WATCH key1 key2 key3\nOK\n\n\n# 取消 WATCH 的场景\n\n当 EXEC 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。\n\n另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。\n\n使用无参数的 UNWATCH 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 UNWATCH 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。\n\n# 使用 WATCH 创建原子操作\n\nWATCH 可以用于创建 Redis 没有内置的原子操作。\n\n举个例子，以下代码实现了原创的 ZPOP 命令，它可以原子地弹出有序集合中分值（score）最小的元素：\n\nWATCH zset\nelement = ZRANGE zset 0 0\nMULTI\nZREM zset element\nEXEC\n\n\n\n# Rollback\n\nRedis 不支持回滚。Redis 不支持回滚的理由：\n\n * Redis 命令只会因为错误的语法而失败，或是命令用在了错误类型的键上面。\n * 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。\n\n\n# 七、Redis 管道\n\nRedis 是一种基于 C/S 模型以及请求/响应协议的 TCP 服务。Redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。\n\n在需要批量执行 Redis 命令时，如果一条一条执行，显然很低效。为了减少通信次数并降低延迟，可以使用 Redis 管道功能。Redis 的管道（pipeline）功能没有提供命令行支持，但是在各种语言版本的客户端中都有相应的实现。\n\n以 Jedis 为例：\n\nPipeline pipe = conn.pipelined();\npipe.multi();\npipe.hset("login:", token, user);\npipe.zadd("recent:", timestamp, token);\nif (item != null) {\n    pipe.zadd("viewed:" + token, timestamp, item);\n    pipe.zremrangeByRank("viewed:" + token, 0, -26);\n    pipe.zincrby("viewed:", -1, item);\n}\npipe.exec();\n\n\n> 🔔 注意：使用管道发送命令时，Redis Server 会将部分请求放到缓存队列中（占用内存），执行完毕后一次性发送结果。如果需要发送大量的命令，会占用大量的内存，因此应该按照合理数量分批次的处理。\n\n\n# 八、Redis 发布与订阅\n\nRedis 提供了 5 个发布与订阅命令：\n\n命令             描述\nSUBSCRIBE      SUBSCRIBE channel [channel ...]—订阅指定频道。\nUNSUBSCRIBE    UNSUBSCRIBE [channel [channel ...]]—取消订阅指定频道。\nPUBLISH        PUBLISH channel message—发送信息到指定的频道。\nPSUBSCRIBE     PSUBSCRIBE pattern [pattern ...]—订阅符合指定模式的频道。\nPUNSUBSCRIBE   PUNSUBSCRIBE [pattern [pattern ...]]—取消订阅符合指定模式的频道。\n\n订阅者订阅了频道之后，发布者向频道发送字符串消息会被所有订阅者接收到。\n\n某个客户端使用 SUBSCRIBE 订阅一个频道，其它客户端可以使用 PUBLISH 向这个频道发送消息。\n\n发布与订阅模式和观察者模式有以下不同：\n\n * 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，发布者与订阅者不知道对方的存在，它们之间通过频道进行通信。\n * 观察者模式是同步的，当事件触发时，主题会去调用观察者的方法；而发布与订阅模式是异步的；\n\n----------------------------------------\n\n分割线以下为 Redis 集群功能特性\n\n\n# 九、Redis 复制\n\n> 关系型数据库通常会使用一个主服务器向多个从服务器发送更新，并使用从服务器来处理所有读请求，Redis 也采用了同样的方式来实现复制特性。\n\n\n# 旧版复制\n\nRedis 2.8 版本以前的复制功能基于 SYNC 命令实现。\n\nRedis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作：\n\n * 同步（sync） - 用于将从服务器的数据库状态更新至主服务器当前的数据库状态。\n * 命令传播（command propagate） - 当主服务器的数据库状态被修改，导致主从数据库状态不一致时，让主从服务器的数据库重新回到一致状态。\n\n这种方式存在缺陷：不能高效处理断线重连后的复制情况。\n\n\n# 新版复制\n\nRedis 2.8 版本以后的复制功能基于 PSYNC 命令实现。PSYNC 命令具有完整重同步和部分重同步两种模式。\n\n * 完整重同步（full resychronization） - 用于初次复制。执行步骤与 SYNC 命令基本一致。\n * 部分重同步（partial resychronization） - 用于断线后重复制。如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只需接收并执行这些写命令，即可将主从服务器的数据库状态保持一致。\n\n\n# 部分重同步\n\n部分重同步有三个组成部分：\n\n * 主从服务器的复制偏移量（replication offset）\n * 主服务器的复制积压缓冲区（replication backlog）\n * 服务器的运行 ID\n\n\n# PSYNC 命令\n\n从服务器向要复制的主服务器发送 PSYNC <runid> <offset> 命令\n\n * 假如主从服务器的 master run id 相同，并且指定的偏移量（offset）在内存缓冲区中还有效，复制就会从上次中断的点开始继续。\n * 如果其中一个条件不满足，就会进行完全重新同步。\n\n\n# 心跳检测\n\n主服务器通过向从服务传播命令来更新从服务器状态，保持主从数据一致。\n\n从服务器通过向主服务器发送命令 REPLCONF ACK <replication_offset> 来进行心跳检测，以及命令丢失检测。\n\n> 💡 更详细的特性及原理说明请参考：Redis 复制\n\n\n# 十、Redis 哨兵\n\nSentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。\n\n> 💡 更详细的特性及原理说明请参考：Redis 哨兵\n\n\n# 十一、Redis 集群\n\n分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，也可以从多台机器里面获取数据，这种方法在解决某些问题时可以获得线性级别的性能提升。\n\n假设有 4 个 Reids 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，... 等等，有不同的方式来选择一个指定的键存储在哪个实例中。最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。\n\n主要有三种分片方式：\n\n * 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。\n * 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。\n * 服务器分片：Redis Cluster（官方的 Redis 集群解决方案）。\n\n\n# 十二、Redis Client\n\nRedis 社区中有多种编程语言的客户端，可以在这里查找合适的客户端：Redis 官方罗列的客户端清单\n\nredis 官方推荐的 Java Redis Client：\n\n * jedis - 最流行的 Redis Java 客户端\n * redisson - 额外提供了很多的分布式服务特性，如：分布式锁、分布式 Java 常用对象（BitSet、BlockingQueue、CountDownLatch 等）\n * lettuce - Spring Boot 2.0 默认 Redis 客户端\n\n\n# 扩展阅读\n\n> 💡 Redis 常用于分布式缓存，有关缓存的特性和原理请参考：缓存基本原理\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考\n * 资源汇总\n   * awesome-redis\n * Redis Client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * CRUG | Redisson PRO vs. Jedis: Which Is Faster? 翻译\n   * redis 分布锁 Redisson 性能测试',normalizedContent:'# redis 应用指南\n\n\n# 一、redis 简介\n\n> redis 是速度非常快的非关系型（nosql）内存键值数据库，可以存储键和五种不同类型的值之间的映射。\n> \n> 键的类型只能为字符串，值支持的五种类型数据类型为：字符串、列表、集合、有序集合、散列表。\n\n\n# redis 使用场景\n\n * 缓存 - 将热点数据放到内存中，设置内存的最大使用量以及过期淘汰策略来保证缓存的命中率。\n * 计数器 - redis 这种内存数据库能支持计数器频繁的读写操作。\n * 应用限流 - 限制一个网站访问流量。\n * 消息队列 - 使用 list 数据类型，它是双向链表。\n * 查找表 - 使用 hash 数据类型。\n * 交集运算 - 使用 set 类型，例如求两个用户的共同好友。\n * 排行榜 - 使用 zset 数据类型。\n * 分布式 session - 多个应用服务器的 session 都存储到 redis 中来保证 session 的一致性。\n * 分布式锁 - 除了可以使用 setnx 实现分布式锁之外，还可以使用官方提供的 redlock 分布式锁实现。\n\n\n# redis 的优势\n\n * 性能极高 – redis 能读的速度是 110000 次/s,写的速度是 81000 次/s。\n * 丰富的数据类型 - 支持字符串、列表、集合、有序集合、散列表。\n * 原子 - redis 的所有操作都是原子性的。单个操作是原子性的。多个操作也支持事务，即原子性，通过 multi 和 exec 指令包起来。\n * 持久化 - redis 支持数据的持久化。可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。\n * 备份 - redis 支持数据的备份，即 master-slave 模式的数据备份。\n * 丰富的特性 - redis 还支持发布订阅, 通知, key 过期等等特性。\n\n\n# redis 与 memcached\n\nredis 与 memcached 因为都可以用于缓存，所以常常被拿来做比较，二者主要有以下区别：\n\n数据类型\n\n * memcached 仅支持字符串类型；\n * 而 redis 支持五种不同种类的数据类型，使得它可以更灵活地解决问题。\n\n数据持久化\n\n * memcached 不支持持久化；\n * redis 支持两种持久化策略：rdb 快照和 aof 日志。\n\n分布式\n\n * memcached 不支持分布式，只能通过在客户端使用像一致性哈希这样的分布式算法来实现分布式存储，这种方式在存储和查询时都需要先在客户端计算一次数据所在的节点。\n * redis cluster 实现了分布式的支持。\n\n内存管理机制\n\n * memcached 将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高，例如块的大小为 128 bytes，只存储 100 bytes 的数据，那么剩下的 28 bytes 就浪费掉了。\n * 在 redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘。而 memcached 的数据则会一直在内存中。\n\n\n# redis 为什么快\n\nredis 单机 qps 能达到 100000。\n\nredis 是单线程模型（redis 6.0 已经支持多线程模型），为什么还能有这么高的并发？\n\n * redis 完全基于内存操作。\n * redis 数据结构简单。\n * 采用单线程，避免线程上下文切换和竞争。\n * 使用 i/o 多路复用模型（非阻塞 i/o）。\n\n> i/o 多路复用\n> \n> i/o 多路复用模型是利用 select、poll、epoll 可以同时监察多个流的 i/o 事件的能力，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有 i/o 事件时，就从阻塞态中唤醒，于是程序就会轮询一遍所有的流（epoll 是只轮询那些真正发出了事件的流），并且只依次顺序的处理就绪的流，这种做法就避免了大量的无用操作。\n\n\n# 二、redis 数据类型\n\nredis 基本数据类型：string、hash、list、set、zset\n\nredis 高级数据类型：bitmap、hyperloglog、geo\n\n> 💡 更详细的特性及原理说明请参考：redis 数据类型和应用\n\n\n# 三、redis 内存淘汰\n\n\n# 内存淘汰要点\n\n * 最大缓存 - redis 允许通过 maxmemory 参数来设置内存最大值。\n\n * 失效时间 - 作为一种定期清理无效数据的重要机制，在 redis 提供的诸多命令中，expire、expireat、pexpire、pexpireat 以及 setex 和 psetex 均可以用来设置一条键值对的失效时间。而一条键值对一旦被关联了失效时间就会在到期后自动删除（或者说变得无法访问更为准确）。\n\n * 淘汰策略 - 随着不断的向 redis 中保存数据，当内存剩余空间无法满足添加的数据时，redis 内就会施行数据淘汰策略，清除一部分内容然后保证新的数据可以保存到内存中。内存淘汰机制是为了更好的使用内存，用一定得 miss 来换取内存的利用率，保证 redis 缓存中保存的都是热点数据。\n\n * 非精准的 lru - 实际上 redis 实现的 lru 并不是可靠的 lru，也就是名义上我们使用 lru 算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。\n\n\n# 主键过期时间\n\nredis 可以为每个键设置过期时间，当键过期时，会自动删除该键。\n\n对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。\n\n可以使用 expire 或 expireat 来为 key 设置过期时间。\n\n> 🔔 注意：当 expire 的时间如果设置的是负数，expireat 设置的时间戳是过期时间，将直接删除 key。\n\n示例：\n\nredis> set mykey "hello"\n"ok"\nredis> expire mykey 10\n(integer) 1\nredis> ttl mykey\n(integer) 10\nredis> set mykey "hello world"\n"ok"\nredis> ttl mykey\n(integer) -1\nredis>\n\n\n\n# 淘汰策略\n\n内存淘汰只是 redis 提供的一个功能，为了更好地实现这个功能，必须为不同的应用场景提供不同的策略，内存淘汰策略讲的是为实现内存淘汰我们具体怎么做，要解决的问题包括淘汰键空间如何选择？在键空间中淘汰键如何选择？\n\nredis 提供了下面几种内存淘汰策略供用户选：\n\n * noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。这是 redis 默认的策略。\n * allkeys-lru - 在主键空间中，优先移除最近未使用的 key。\n * allkeys-random - 在主键空间中，随机移除某个 key。\n * volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。\n * volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。\n * volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。\n\n\n# 如何选择淘汰策略\n\n * 如果数据呈现幂等分布（存在热点数据，部分数据访问频率高，部分数据访问频率低），则使用 allkeys-lru。\n * 如果数据呈现平等分布（数据访问频率大致相同），则使用 allkeys-random。\n * 如果希望使用不同的 ttl 值向 redis 提示哪些 key 更适合被淘汰，请使用 volatile-ttl。\n * volatile-lru 和 volatile-random 适合既应用于缓存和又应用于持久化存储的场景，然而我们也可以通过使用两个 redis 实例来达到相同的效果。\n * 将 key 设置过期时间实际上会消耗更多的内存，因此建议使用 allkeys-lru 策略从而更有效率的使用内存。\n\n\n# 内部实现\n\nredis 删除失效主键的方法主要有两种：\n\n * 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。\n * 主动方法（active way），周期性地从设置了失效时间的主键中选择一部分失效的主键删除。\n * 主动删除：当前已用内存超过 maxmemory 限定时，触发主动清理策略，该策略由启动参数的配置决定主键具体的失效时间全部都维护在 expires 这个字典表中。\n\n\n# 四、redis 持久化\n\nredis 是内存型数据库，为了保证数据在宕机后不会丢失，需要将内存中的数据持久化到硬盘上。\n\nredis 支持两种持久化方式：rdb 和 aof。\n\n * rdb - rdb 即快照方式，它将某个时间点的所有 redis 数据保存到一个经过压缩的二进制文件（rdb 文件）中。\n * aof - aof(append only file) 是以文本日志形式将所有写命令追加到 aof 文件的末尾，以此来记录数据的变化。当服务器重启的时候会重新载入和执行这些命令来恢复原始的数据。aof 适合作为 热备。\n\n> 💡 更详细的特性及原理说明请参考：redis 持久化\n\n\n# 五、redis 事件\n\nredis 服务器是一个事件驱动程序，服务器需要处理两类事件：\n\n * 文件事件（file event） - redis 服务器通过套接字（socket）与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。服务器与客户端（或其他的服务器）的通信会产生文件事件，而服务器通过监听并处理这些事件来完成一系列网络通信操作。\n * 时间事件（time event） - redis 服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。\n\n\n# 文件事件\n\nredis 基于 reactor 模式开发了自己的网络时间处理器。\n\n * redis 文件事件处理器使用 i/o 多路复用程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。\n * 当被监听的套接字准备好执行连接应答、读取、写入、关闭操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。\n\n虽然文件事件处理器以单线程方式运行，但通过使用 i/o 多路复用程序来监听多个套接字，文件事件处理器实现了高性能的网络通信模型。\n\n文件事件处理器有四个组成部分：套接字、i/o 多路复用程序、文件事件分派器、事件处理器。\n\n\n\n\n# 时间事件\n\n时间事件又分为：\n\n * 定时事件：是让一段程序在指定的时间之内执行一次；\n * 周期性事件：是让一段程序每隔指定时间就执行一次。\n\nredis 将所有时间事件都放在一个无序链表中，每当时间事件执行器运行时，通过遍历整个链表查找出已到达的时间事件，并调用响应的事件处理器。\n\n\n# 事件的调度与执行\n\n服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。\n\n事件调度与执行由 aeprocessevents 函数负责，伪代码如下：\n\ndef aeprocessevents():\n\n    ## 获取到达时间离当前时间最接近的时间事件\n    time_event = aesearchnearesttimer()\n\n    ## 计算最接近的时间事件距离到达还有多少毫秒\n    remaind_ms = time_event.when - unix_ts_now()\n\n    ## 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0\n    if remaind_ms < 0:\n        remaind_ms = 0\n\n    ## 根据 remaind_ms 的值，创建 timeval\n    timeval = create_timeval_with_ms(remaind_ms)\n\n    ## 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定\n    aeapipoll(timeval)\n\n    ## 处理所有已产生的文件事件\n    procesfileevents()\n\n    ## 处理所有已到达的时间事件\n    processtimeevents()\n\n\n将 aeprocessevents 函数置于一个循环里面，加上初始化和清理函数，就构成了 redis 服务器的主函数，伪代码如下：\n\ndef main():\n\n    ## 初始化服务器\n    init_server()\n\n    ## 一直处理事件，直到服务器关闭为止\n    while server_is_not_shutdown():\n        aeprocessevents()\n\n    ## 服务器关闭，执行清理操作\n    clean_server()\n\n\n从事件处理的角度来看，服务器运行流程如下：\n\n\n\n\n# 六、redis 事务\n\n> redis 提供的不是严格的事务，redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。\n\nmulti 、 exec 、 discard 和 watch 是 redis 事务相关的命令。\n\n事务可以一次执行多个命令， 并且有以下两个重要的保证：\n\n * 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。\n * 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。\n\n\n# multi\n\nmulti 命令用于开启一个事务，它总是返回 ok 。\n\nmulti 执行之后， 客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 exec 命令被调用时， 所有队列中的命令才会被执行。\n\n以下是一个事务例子， 它原子地增加了 foo 和 bar 两个键的值：\n\n> multi\nok\n> incr foo\nqueued\n> incr bar\nqueued\n> exec\n1) (integer) 1\n2) (integer) 1\n\n\n\n# exec\n\nexec 命令负责触发并执行事务中的所有命令。\n\n * 如果客户端在使用 multi 开启了一个事务之后，却因为断线而没有成功执行 exec ，那么事务中的所有命令都不会被执行。\n * 另一方面，如果客户端成功在开启事务之后执行 exec ，那么事务中的所有命令都会被执行。\n\n\n# discard\n\n当执行 discard 命令时， 事务会被放弃， 事务队列会被清空， 并且客户端会从事务状态中退出。\n\n示例：\n\n> set foo 1\nok\n> multi\nok\n> incr foo\nqueued\n> discard\nok\n> get foo\n"1"\n\n\n\n# watch\n\nwatch 命令可以为 redis 事务提供 check-and-set （cas）行为。\n\n被 watch 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 exec 执行之前被修改了， 那么整个事务都会被取消， exec 返回 nil-reply 来表示事务已经失败。\n\nwatch mykey\nval = get mykey\nval = val + 1\nmulti\nset mykey $val\nexec\n\n\n使用上面的代码， 如果在 watch 执行之后， exec 执行之前， 有其他客户端修改了 mykey 的值， 那么当前客户端的事务就会失败。 程序需要做的， 就是不断重试这个操作， 直到没有发生碰撞为止。\n\n这种形式的锁被称作乐观锁， 它是一种非常强大的锁机制。 并且因为大多数情况下， 不同的客户端会访问不同的键， 碰撞的情况一般都很少， 所以通常并不需要进行重试。\n\nwatch 使得 exec 命令需要有条件地执行：事务只能在所有被监视键都没有被修改的前提下执行，如果这个前提不能满足的话，事务就不会被执行。\n\nwatch 命令可以被调用多次。对键的监视从 watch 执行之后开始生效，直到调用 exec 为止。\n\n用户还可以在单个 watch 命令中监视任意多个键，例如：\n\nredis> watch key1 key2 key3\nok\n\n\n# 取消 watch 的场景\n\n当 exec 被调用时， 不管事务是否成功执行， 对所有键的监视都会被取消。\n\n另外， 当客户端断开连接时， 该客户端对键的监视也会被取消。\n\n使用无参数的 unwatch 命令可以手动取消对所有键的监视。 对于一些需要改动多个键的事务， 有时候程序需要同时对多个键进行加锁， 然后检查这些键的当前值是否符合程序的要求。 当值达不到要求时， 就可以使用 unwatch 命令来取消目前对键的监视， 中途放弃这个事务， 并等待事务的下次尝试。\n\n# 使用 watch 创建原子操作\n\nwatch 可以用于创建 redis 没有内置的原子操作。\n\n举个例子，以下代码实现了原创的 zpop 命令，它可以原子地弹出有序集合中分值（score）最小的元素：\n\nwatch zset\nelement = zrange zset 0 0\nmulti\nzrem zset element\nexec\n\n\n\n# rollback\n\nredis 不支持回滚。redis 不支持回滚的理由：\n\n * redis 命令只会因为错误的语法而失败，或是命令用在了错误类型的键上面。\n * 因为不需要对回滚进行支持，所以 redis 的内部可以保持简单且快速。\n\n\n# 七、redis 管道\n\nredis 是一种基于 c/s 模型以及请求/响应协议的 tcp 服务。redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。\n\n在需要批量执行 redis 命令时，如果一条一条执行，显然很低效。为了减少通信次数并降低延迟，可以使用 redis 管道功能。redis 的管道（pipeline）功能没有提供命令行支持，但是在各种语言版本的客户端中都有相应的实现。\n\n以 jedis 为例：\n\npipeline pipe = conn.pipelined();\npipe.multi();\npipe.hset("login:", token, user);\npipe.zadd("recent:", timestamp, token);\nif (item != null) {\n    pipe.zadd("viewed:" + token, timestamp, item);\n    pipe.zremrangebyrank("viewed:" + token, 0, -26);\n    pipe.zincrby("viewed:", -1, item);\n}\npipe.exec();\n\n\n> 🔔 注意：使用管道发送命令时，redis server 会将部分请求放到缓存队列中（占用内存），执行完毕后一次性发送结果。如果需要发送大量的命令，会占用大量的内存，因此应该按照合理数量分批次的处理。\n\n\n# 八、redis 发布与订阅\n\nredis 提供了 5 个发布与订阅命令：\n\n命令             描述\nsubscribe      subscribe channel [channel ...]—订阅指定频道。\nunsubscribe    unsubscribe [channel [channel ...]]—取消订阅指定频道。\npublish        publish channel message—发送信息到指定的频道。\npsubscribe     psubscribe pattern [pattern ...]—订阅符合指定模式的频道。\npunsubscribe   punsubscribe [pattern [pattern ...]]—取消订阅符合指定模式的频道。\n\n订阅者订阅了频道之后，发布者向频道发送字符串消息会被所有订阅者接收到。\n\n某个客户端使用 subscribe 订阅一个频道，其它客户端可以使用 publish 向这个频道发送消息。\n\n发布与订阅模式和观察者模式有以下不同：\n\n * 观察者模式中，观察者和主题都知道对方的存在；而在发布与订阅模式中，发布者与订阅者不知道对方的存在，它们之间通过频道进行通信。\n * 观察者模式是同步的，当事件触发时，主题会去调用观察者的方法；而发布与订阅模式是异步的；\n\n----------------------------------------\n\n分割线以下为 redis 集群功能特性\n\n\n# 九、redis 复制\n\n> 关系型数据库通常会使用一个主服务器向多个从服务器发送更新，并使用从服务器来处理所有读请求，redis 也采用了同样的方式来实现复制特性。\n\n\n# 旧版复制\n\nredis 2.8 版本以前的复制功能基于 sync 命令实现。\n\nredis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作：\n\n * 同步（sync） - 用于将从服务器的数据库状态更新至主服务器当前的数据库状态。\n * 命令传播（command propagate） - 当主服务器的数据库状态被修改，导致主从数据库状态不一致时，让主从服务器的数据库重新回到一致状态。\n\n这种方式存在缺陷：不能高效处理断线重连后的复制情况。\n\n\n# 新版复制\n\nredis 2.8 版本以后的复制功能基于 psync 命令实现。psync 命令具有完整重同步和部分重同步两种模式。\n\n * 完整重同步（full resychronization） - 用于初次复制。执行步骤与 sync 命令基本一致。\n * 部分重同步（partial resychronization） - 用于断线后重复制。如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只需接收并执行这些写命令，即可将主从服务器的数据库状态保持一致。\n\n\n# 部分重同步\n\n部分重同步有三个组成部分：\n\n * 主从服务器的复制偏移量（replication offset）\n * 主服务器的复制积压缓冲区（replication backlog）\n * 服务器的运行 id\n\n\n# psync 命令\n\n从服务器向要复制的主服务器发送 psync <runid> <offset> 命令\n\n * 假如主从服务器的 master run id 相同，并且指定的偏移量（offset）在内存缓冲区中还有效，复制就会从上次中断的点开始继续。\n * 如果其中一个条件不满足，就会进行完全重新同步。\n\n\n# 心跳检测\n\n主服务器通过向从服务传播命令来更新从服务器状态，保持主从数据一致。\n\n从服务器通过向主服务器发送命令 replconf ack <replication_offset> 来进行心跳检测，以及命令丢失检测。\n\n> 💡 更详细的特性及原理说明请参考：redis 复制\n\n\n# 十、redis 哨兵\n\nsentinel（哨兵）可以监听主服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。\n\n> 💡 更详细的特性及原理说明请参考：redis 哨兵\n\n\n# 十一、redis 集群\n\n分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，也可以从多台机器里面获取数据，这种方法在解决某些问题时可以获得线性级别的性能提升。\n\n假设有 4 个 reids 实例 r0，r1，r2，r3，还有很多表示用户的键 user:1，user:2，... 等等，有不同的方式来选择一个指定的键存储在哪个实例中。最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 r0 中，用户 id 从 1001~2000 的存储到实例 r1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。还有一种方式是哈希分片，使用 crc32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。\n\n主要有三种分片方式：\n\n * 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。\n * 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。\n * 服务器分片：redis cluster（官方的 redis 集群解决方案）。\n\n\n# 十二、redis client\n\nredis 社区中有多种编程语言的客户端，可以在这里查找合适的客户端：redis 官方罗列的客户端清单\n\nredis 官方推荐的 java redis client：\n\n * jedis - 最流行的 redis java 客户端\n * redisson - 额外提供了很多的分布式服务特性，如：分布式锁、分布式 java 常用对象（bitset、blockingqueue、countdownlatch 等）\n * lettuce - spring boot 2.0 默认 redis 客户端\n\n\n# 扩展阅读\n\n> 💡 redis 常用于分布式缓存，有关缓存的特性和原理请参考：缓存基本原理\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考\n * 资源汇总\n   * awesome-redis\n * redis client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * crug | redisson pro vs. jedis: which is faster? 翻译\n   * redis 分布锁 redisson 性能测试',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 数据类型和应用",frontmatter:{title:"Redis 数据类型和应用",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","数据类型"],permalink:"/pages/ed757c/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/03.Redis%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E5%BA%94%E7%94%A8.html",relativePath:"12.数据库/05.KV数据库/01.Redis/03.Redis数据类型和应用.md",key:"v-a0623b42",path:"/pages/ed757c/",headers:[{level:2,title:"一、Redis 基本数据类型",slug:"一、redis-基本数据类型",normalizedTitle:"一、redis 基本数据类型",charIndex:113},{level:3,title:"STRING",slug:"string",normalizedTitle:"string",charIndex:157},{level:3,title:"HASH",slug:"hash",normalizedTitle:"hash",charIndex:495},{level:3,title:"LIST",slug:"list",normalizedTitle:"list",charIndex:243},{level:3,title:"SET",slug:"set",normalizedTitle:"set",charIndex:352},{level:3,title:"ZSET",slug:"zset",normalizedTitle:"zset",charIndex:596},{level:3,title:"通用命令",slug:"通用命令",normalizedTitle:"通用命令",charIndex:3993},{level:4,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:3238},{level:4,title:"键的过期时间",slug:"键的过期时间",normalizedTitle:"键的过期时间",charIndex:4860},{level:2,title:"二、Redis 高级数据类型",slug:"二、redis-高级数据类型",normalizedTitle:"二、redis 高级数据类型",charIndex:5580},{level:3,title:"BitMap",slug:"bitmap",normalizedTitle:"bitmap",charIndex:5599},{level:4,title:"BitMap 命令",slug:"bitmap-命令",normalizedTitle:"bitmap 命令",charIndex:5882},{level:4,title:"BitMap 示例",slug:"bitmap-示例",normalizedTitle:"bitmap 示例",charIndex:6057},{level:4,title:"BitMap 应用",slug:"bitmap-应用",normalizedTitle:"bitmap 应用",charIndex:6318},{level:3,title:"HyperLogLog",slug:"hyperloglog",normalizedTitle:"hyperloglog",charIndex:6761},{level:4,title:"HyperLogLog 命令",slug:"hyperloglog-命令",normalizedTitle:"hyperloglog 命令",charIndex:6875},{level:3,title:"GEO",slug:"geo",normalizedTitle:"geo",charIndex:7530},{level:4,title:"GEO 命令",slug:"geo-命令",normalizedTitle:"geo 命令",charIndex:7580},{level:2,title:"三、Redis 数据类型应用",slug:"三、redis-数据类型应用",normalizedTitle:"三、redis 数据类型应用",charIndex:7805},{level:3,title:"案例-最受欢迎文章",slug:"案例-最受欢迎文章",normalizedTitle:"案例-最受欢迎文章",charIndex:7824},{level:4,title:"对文章进行投票",slug:"对文章进行投票",normalizedTitle:"对文章进行投票",charIndex:7860},{level:4,title:"发布并获取文章",slug:"发布并获取文章",normalizedTitle:"发布并获取文章",charIndex:9134},{level:4,title:"对文章进行分组",slug:"对文章进行分组",normalizedTitle:"对文章进行分组",charIndex:11015},{level:3,title:"案例-管理令牌",slug:"案例-管理令牌",normalizedTitle:"案例-管理令牌",charIndex:12271},{level:4,title:"查询令牌",slug:"查询令牌",normalizedTitle:"查询令牌",charIndex:12376},{level:4,title:"更新令牌",slug:"更新令牌",normalizedTitle:"更新令牌",charIndex:12519},{level:4,title:"清理令牌",slug:"清理令牌",normalizedTitle:"清理令牌",charIndex:13293},{level:3,title:"案例-购物车",slug:"案例-购物车",normalizedTitle:"案例-购物车",charIndex:14906},{level:4,title:"在购物车中添加、删除商品",slug:"在购物车中添加、删除商品",normalizedTitle:"在购物车中添加、删除商品",charIndex:14968},{level:4,title:"清空购物车",slug:"清空购物车",normalizedTitle:"清空购物车",charIndex:15303},{level:3,title:"案例-页面缓存",slug:"案例-页面缓存",normalizedTitle:"案例-页面缓存",charIndex:16243},{level:3,title:"案例-数据行缓存",slug:"案例-数据行缓存",normalizedTitle:"案例-数据行缓存",charIndex:16981},{level:3,title:"案例-网页分析",slug:"案例-网页分析",normalizedTitle:"案例-网页分析",charIndex:18993},{level:3,title:"案例-记录日志",slug:"案例-记录日志",normalizedTitle:"案例-记录日志",charIndex:20077},{level:3,title:"案例-统计数据",slug:"案例-统计数据",normalizedTitle:"案例-统计数据",charIndex:20450},{level:3,title:"案例-查找 IP 所属地",slug:"案例-查找-ip-所属地",normalizedTitle:"案例-查找 ip 所属地",charIndex:21534},{level:4,title:"载入 IP 数据",slug:"载入-ip-数据",normalizedTitle:"载入 ip 数据",charIndex:21583},{level:4,title:"查找 IP 所属城市",slug:"查找-ip-所属城市",normalizedTitle:"查找 ip 所属城市",charIndex:24645},{level:3,title:"案例-服务的发现与配置",slug:"案例-服务的发现与配置",normalizedTitle:"案例-服务的发现与配置",charIndex:25228},{level:3,title:"案例-自动补全",slug:"案例-自动补全",normalizedTitle:"案例-自动补全",charIndex:25244},{level:3,title:"案例-广告定向",slug:"案例-广告定向",normalizedTitle:"案例-广告定向",charIndex:25594},{level:3,title:"案例-职位搜索",slug:"案例-职位搜索",normalizedTitle:"案例-职位搜索",charIndex:25606},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:27112}],headersStr:"一、Redis 基本数据类型 STRING HASH LIST SET ZSET 通用命令 排序 键的过期时间 二、Redis 高级数据类型 BitMap BitMap 命令 BitMap 示例 BitMap 应用 HyperLogLog HyperLogLog 命令 GEO GEO 命令 三、Redis 数据类型应用 案例-最受欢迎文章 对文章进行投票 发布并获取文章 对文章进行分组 案例-管理令牌 查询令牌 更新令牌 清理令牌 案例-购物车 在购物车中添加、删除商品 清空购物车 案例-页面缓存 案例-数据行缓存 案例-网页分析 案例-记录日志 案例-统计数据 案例-查找 IP 所属地 载入 IP 数据 查找 IP 所属城市 案例-服务的发现与配置 案例-自动补全 案例-广告定向 案例-职位搜索 参考资料",content:'# Redis 数据类型和应用\n\n> Redis 提供了多种数据类型，每种数据类型有丰富的命令支持。\n> \n> 使用 Redis ，不仅要了解其数据类型的特性，还需要根据业务场景，灵活的、高效的使用其数据类型来建模。\n\n\n# 一、Redis 基本数据类型\n\n\n\n数据类型     可以存储的值        操作\nSTRING   字符串、整数或者浮点数   对整个字符串或者字符串的其中一部分执行操作\n                       对整数和浮点数执行自增或者自减操作\nLIST     列表            从两端压入或者弹出元素\n                       读取单个或者多个元素\n                       进行修剪，只保留一个范围内的元素\nSET      无序集合          添加、获取、移除单个元素\n                       检查一个元素是否存在于集合中\n                       计算交集、并集、差集\n                       从集合里面随机获取元素\nHASH     包含键值对的无序散列表   添加、获取、移除单个键值对\n                       获取所有键值对\n                       检查某个键是否存在\nZSET     有序集合          添加、获取、删除元素\n                       根据分值范围或者成员来获取元素\n                       计算一个键的排名\n\n> What Redis data structures look like\n\n\n# STRING\n\n\n**适用场景：缓存、计数器、共享 Session**\n\n命令：\n\n命令     行为\nGET    获取存储在给定键中的值。\nSET    设置存储在给定键中的值。\nDEL    删除存储在给定键中的值（这个命令可以用于所有类型）。\nINCR   为键 key 储存的数字值加一\nDECR   为键 key 储存的数字值减一\n\n> 更多命令请参考：Redis String 类型命令\n\n示例：\n\n127.0.0.1:6379> set hello world\nOK\n127.0.0.1:6379> get hello\n"world"\n127.0.0.1:6379> del hello\n(integer) 1\n127.0.0.1:6379> get hello\n(nil)\n\n\n\n# HASH\n\n\n**适用场景：存储结构化数据**，如一个对象：用户信息、产品信息等。\n\n命令：\n\n命令        行为\nHSET      在散列里面关联起给定的键值对。\nHGET      获取指定散列键的值。\nHGETALL   获取散列包含的所有键值对。\nHDEL      如果给定键存在于散列里面，那么移除这个键。\n\n> 更多命令请参考：Redis Hash 类型命令\n\n示例：\n\n127.0.0.1:6379> hset hash-key sub-key1 value1\n(integer) 1\n127.0.0.1:6379> hset hash-key sub-key2 value2\n(integer) 1\n127.0.0.1:6379> hset hash-key sub-key1 value1\n(integer) 0\n127.0.0.1:6379> hset hash-key sub-key3 value2\n(integer) 0\n127.0.0.1:6379> hgetall hash-key\n1) "sub-key1"\n2) "value1"\n3) "sub-key2"\n4) "value2"\n127.0.0.1:6379> hdel hash-key sub-key2\n(integer) 1\n127.0.0.1:6379> hdel hash-key sub-key2\n(integer) 0\n127.0.0.1:6379> hget hash-key sub-key1\n"value1"\n127.0.0.1:6379> hgetall hash-key\n1) "sub-key1"\n2) "value1"\n\n\n\n# LIST\n\n\n**适用场景：用于存储列表型数据**。如：粉丝列表、商品列表等。\n\n命令：\n\n命令       行为\nLPUSH    将给定值推入列表的右端。\nRPUSH    将给定值推入列表的右端。\nLPOP     从列表的左端弹出一个值，并返回被弹出的值。\nRPOP     从列表的右端弹出一个值，并返回被弹出的值。\nLRANGE   获取列表在给定范围上的所有值。\nLINDEX   获取列表在给定位置上的单个元素。\nLREM     从列表的左端弹出一个值，并返回被弹出的值。\nLTRIM    只保留指定区间内的元素，删除其他元素。\n\n> 更多命令请参考：Redis List 类型命令\n\n示例：\n\n127.0.0.1:6379> rpush list-key item\n(integer) 1\n127.0.0.1:6379> rpush list-key item2\n(integer) 2\n127.0.0.1:6379> rpush list-key item\n(integer) 3\n127.0.0.1:6379> lrange list-key 0 -1\n1) "item"\n2) "item2"\n3) "item"\n127.0.0.1:6379> lindex list-key 1\n"item2"\n127.0.0.1:6379> lpop list-key\n"item"\n127.0.0.1:6379> lrange list-key 0 -1\n1) "item2"\n2) "item"\n\n\n\n# SET\n\n\n**适用场景：用于存储去重的列表型数据**。\n\n命令：\n\n命令          行为\nSADD        将给定元素添加到集合。\nSMEMBERS    返回集合包含的所有元素。\nSISMEMBER   检查给定元素是否存在于集合中。\nSREM        如果给定的元素存在于集合中，那么移除这个元素。\n\n> 更多命令请参考：Redis Set 类型命令\n\n示例：\n\n127.0.0.1:6379> sadd set-key item\n(integer) 1\n127.0.0.1:6379> sadd set-key item2\n(integer) 1\n127.0.0.1:6379> sadd set-key item3\n(integer) 1\n127.0.0.1:6379> sadd set-key item\n(integer) 0\n127.0.0.1:6379> smembers set-key\n1) "item"\n2) "item2"\n3) "item3"\n127.0.0.1:6379> sismember set-key item4\n(integer) 0\n127.0.0.1:6379> sismember set-key item\n(integer) 1\n127.0.0.1:6379> srem set-key item2\n(integer) 1\n127.0.0.1:6379> srem set-key item2\n(integer) 0\n127.0.0.1:6379> smembers set-key\n1) "item"\n2) "item3"\n\n\n\n# ZSET\n\n\n\n适用场景：由于可以设置 score，且不重复。适合用于存储各种排行数据，如：按评分排序的有序商品集合、按时间排序的有序文章集合。\n\n命令：\n\n命令              行为\nZADD            将一个带有给定分值的成员添加到有序集合里面。\nZRANGE          根据元素在有序排列中所处的位置，从有序集合里面获取多个元素。\nZRANGEBYSCORE   获取有序集合在给定分值范围内的所有元素。\nZREM            如果给定成员存在于有序集合，那么移除这个成员。\n\n> 更多命令请参考：Redis ZSet 类型命令\n\n示例：\n\n127.0.0.1:6379> zadd zset-key 728 member1\n(integer) 1\n127.0.0.1:6379> zadd zset-key 982 member0\n(integer) 1\n127.0.0.1:6379> zadd zset-key 982 member0\n(integer) 0\n\n127.0.0.1:6379> zrange zset-key 0 -1 withscores\n1) "member1"\n2) "728"\n3) "member0"\n4) "982"\n\n127.0.0.1:6379> zrangebyscore zset-key 0 800 withscores\n1) "member1"\n2) "728"\n\n127.0.0.1:6379> zrem zset-key member1\n(integer) 1\n127.0.0.1:6379> zrem zset-key member1\n(integer) 0\n127.0.0.1:6379> zrange zset-key 0 -1 withscores\n1) "member0"\n2) "982"\n\n\n\n# 通用命令\n\n# 排序\n\nRedis 的 SORT 命令可以对 LIST、SET、ZSET 进行排序。\n\n命令     描述\nSORT   SORT source-key [BY pattern] [LIMIT offset count] [GET\n       pattern [GET pattern ...]] [ASC | DESC] [ALPHA] [STORE\n       dest-key]—根据给定选项，对输入 LIST、SET、ZSET 进行排序，然后返回或存储排序的结果。\n\n示例：\n\n127.0.0.1:6379[15]> RPUSH \'sort-input\' 23 15 110 7\n(integer) 4\n127.0.0.1:6379[15]> SORT \'sort-input\'\n1) "7"\n2) "15"\n3) "23"\n4) "110"\n127.0.0.1:6379[15]> SORT \'sort-input\' alpha\n1) "110"\n2) "15"\n3) "23"\n4) "7"\n127.0.0.1:6379[15]> HSET \'d-7\' \'field\' 5\n(integer) 1\n127.0.0.1:6379[15]> HSET \'d-15\' \'field\' 1\n(integer) 1\n127.0.0.1:6379[15]> HSET \'d-23\' \'field\' 9\n(integer) 1\n127.0.0.1:6379[15]> HSET \'d-110\' \'field\' 3\n(integer) 1\n127.0.0.1:6379[15]> SORT \'sort-input\' by \'d-*->field\'\n1) "15"\n2) "110"\n3) "7"\n4) "23"\n127.0.0.1:6379[15]> SORT \'sort-input\' by \'d-*->field\' get \'d-*->field\'\n1) "1"\n2) "3"\n3) "5"\n4) "9"\n\n\n# 键的过期时间\n\nRedis 的 EXPIRE 命令可以指定一个键的过期时间，当达到过期时间后，Redis 会自动删除该键。\n\n命令          描述\nPERSIST     PERSIST key-name—移除键的过期时间\nTTL         TTL key-name—查看给定键距离过期还有多少秒\nEXPIRE      EXPIRE key-name seconds—让给定键在指定的秒数之后过期\nEXPIREAT    EXPIREAT key-name timestamp—将给定键的过期时间设置为给定的 UNIX 时间戳\nPTTL        PTTL key-name—查看给定键距离过期时间还有多少毫秒（这个命令在 Redis 2.6 或以上版本可用）\nPEXPIRE     PEXPIRE key-name milliseconds—让给定键在指定的毫秒数之后过期（这个命令在 Redis\n            2.6 或以上版本可用）\nPEXPIREAT   PEXPIREAT key-name timestamp-milliseconds—将一个毫秒级精度的 UNIX\n            时间戳设置为给定键的过期时间（这个命令在 Redis 2.6 或以上版本可用）\n\n示例：\n\n127.0.0.1:6379[15]> SET key value\nOK\n127.0.0.1:6379[15]> GET key\n"value"\n127.0.0.1:6379[15]> EXPIRE key 2\n(integer) 1\n127.0.0.1:6379[15]> GET key\n(nil)\n\n\n\n# 二、Redis 高级数据类型\n\n\n# BitMap\n\nBitMap 即位图。BitMap 不是一个真实的数据结构。而是 STRING 类型上的一组面向 bit 操作的集合。由于 STRING 是二进制安全的 blob，并且它们的最大长度是 512m，所以 BitMap 能最大设置 $$2^{32}$$ 个不同的 bit。\n\nBitmaps 的最大优点就是存储信息时可以节省大量的空间。例如在一个系统中，不同的用户被一个增长的用户 ID 表示。40 亿（$$2^{32}$$ = $$410241024*1024$$ ≈ 40 亿）用户只需要 512M 内存就能记住某种信息，例如用户是否登录过。\n\n# BitMap 命令\n\n * SETBIT - 对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。\n * GETBIT - 对 key 所储存的字符串值，获取指定偏移量上的位(bit)。\n * BITCOUNT - 计算给定字符串中，被设置为 1 的比特位的数量。\n * BITPOS\n * BITOP\n * BITFIELD\n\n# BitMap 示例\n\n# 对不存在的 key 或者不存在的 offset 进行 GETBIT， 返回 0\n\nredis> EXISTS bit\n(integer) 0\n\nredis> GETBIT bit 10086\n(integer) 0\n\n\n# 对已存在的 offset 进行 GETBIT\n\nredis> SETBIT bit 10086 1\n(integer) 0\n\nredis> GETBIT bit 10086\n(integer) 1\n\nredis> BITCOUNT bit\n(integer) 1\n\n\n# BitMap 应用\n\nBitmap 对于一些特定类型的计算非常有效。例如：使用 bitmap 实现用户上线次数统计。\n\n假设现在我们希望记录自己网站上的用户的上线频率，比如说，计算用户 A 上线了多少天，用户 B 上线了多少天，诸如此类，以此作为数据，从而决定让哪些用户参加 beta 测试等活动 —— 这个模式可以使用 SETBIT key offset value 和 [BITCOUNT key start] [end] 来实现。\n\n比如说，每当用户在某一天上线的时候，我们就使用 SETBIT key offset value ，以用户名作为 key，将那天所代表的网站的上线日作为 offset 参数，并将这个 offset 上的为设置为 1 。\n\n> 更详细的实现可以参考：\n> \n> 一看就懂系列之 详解 redis 的 bitmap 在亿级项目中的应用\n> \n> Fast, easy, realtime metrics using Redis bitmaps\n\n\n# HyperLogLog\n\nHyperLogLog 是用于计算唯一事物的概率数据结构（从技术上讲，这被称为估计集合的基数）。如果统计唯一项，项目越多，需要的内存就越多。因为需要记住过去已经看过的项，从而避免多次统计这些项。\n\n# HyperLogLog 命令\n\n * PFADD - 将任意数量的元素添加到指定的 HyperLogLog 里面。\n * PFCOUNT - 返回 HyperLogLog 包含的唯一元素的近似数量。\n * PFMERGE - 将多个 HyperLogLog 合并（merge）为一个 HyperLogLog ， 合并后的 HyperLogLog 的基数接近于所有输入 HyperLogLog 的可见集合（observed set）的并集。合并得出的 HyperLogLog 会被储存在 destkey 键里面， 如果该键并不存在， 那么命令在执行之前， 会先为该键创建一个空的 HyperLogLog 。\n\n示例：\n\nredis> PFADD  databases  "Redis"  "MongoDB"  "MySQL"\n(integer) 1\n\nredis> PFCOUNT  databases\n(integer) 3\n\nredis> PFADD  databases  "Redis"    # Redis 已经存在，不必对估计数量进行更新\n(integer) 0\n\nredis> PFCOUNT  databases    # 元素估计数量没有变化\n(integer) 3\n\nredis> PFADD  databases  "PostgreSQL"    # 添加一个不存在的元素\n(integer) 1\n\nredis> PFCOUNT  databases    # 估计数量增一\n4\n\n\n\n# GEO\n\n这个功能可以将用户给定的地理位置（经度和纬度）信息储存起来，并对这些信息进行操作。\n\n# GEO 命令\n\n * GEOADD - 将指定的地理空间位置（纬度、经度、名称）添加到指定的 key 中。\n * GEOPOS - 从 key 里返回所有给定位置元素的位置（经度和纬度）。\n * GEODIST - 返回两个给定位置之间的距离。\n * GEOHASH - 回一个或多个位置元素的标准 Geohash 值，它可以在http://geohash.org/使用。\n * GEORADIUS\n * GEORADIUSBYMEMBER\n\n\n# 三、Redis 数据类型应用\n\n\n# 案例-最受欢迎文章\n\n选出最受欢迎文章，需要支持对文章进行评分。\n\n# 对文章进行投票\n\n（1）使用 HASH 存储文章\n\n使用 HASH 类型存储文章信息。其中：key 是文章 ID；field 是文章的属性 key；value 是属性对应值。\n\n\n\n操作：\n\n * 存储文章信息 - 使用 HSET 或 HMGET 命令\n * 查询文章信息 - 使用 HGETALL 命令\n * 添加投票 - 使用 HINCRBY 命令\n\n（2）使用 ZSET 针对不同维度集合排序\n\n使用 ZSET 类型分别存储按照时间排序和按照评分排序的文章 ID 集合。\n\n\n\n操作：\n\n * 添加记录 - 使用 ZADD 命令\n * 添加分数 - 使用 ZINCRBY 命令\n * 取出多篇文章 - 使用 ZREVRANGE 命令\n\n（3）为了防止重复投票，使用 SET 类型记录每篇文章 ID 对应的投票集合。\n\n\n\n操作：\n\n * 添加投票者 - 使用 SADD 命令\n * 设置有效期 - 使用 EXPIRE 命令\n\n（4）假设 user:115423 给 article:100408 投票，分别需要高更新评分排序集合以及投票集合。\n\n\n\n当需要对一篇文章投票时，程序需要用 ZSCORE 命令检查记录文章发布时间的有序集合，判断文章的发布时间是否超过投票有效期（比如：一星期）。\n\n    public void articleVote(Jedis conn, String user, String article) {\n        // 计算文章的投票截止时间。\n        long cutoff = (System.currentTimeMillis() / 1000) - ONE_WEEK_IN_SECONDS;\n\n        // 检查是否还可以对文章进行投票\n        // （虽然使用散列也可以获取文章的发布时间，\n        // 但有序集合返回的文章发布时间为浮点数，\n        // 可以不进行转换直接使用）。\n        if (conn.zscore("time:", article) < cutoff) {\n            return;\n        }\n\n        // 从article:id标识符（identifier）里面取出文章的ID。\n        String articleId = article.substring(article.indexOf(\':\') + 1);\n\n        // 如果用户是第一次为这篇文章投票，那么增加这篇文章的投票数量和评分。\n        if (conn.sadd("voted:" + articleId, user) == 1) {\n            conn.zincrby("score:", VOTE_SCORE, article);\n            conn.hincrBy(article, "votes", 1);\n        }\n    }\n\n\n# 发布并获取文章\n\n发布文章：\n\n * 添加文章 - 使用 INCR 命令计算新的文章 ID，填充文章信息，然后用 HSET 命令或 HMSET 命令写入到 HASH 结构中。\n * 将文章作者 ID 添加到投票名单 - 使用 SADD 命令添加到代表投票名单的 SET 结构中。\n * 设置投票有效期 - 使用 EXPIRE 命令设置投票有效期。\n\n    public String postArticle(Jedis conn, String user, String title, String link) {\n        // 生成一个新的文章ID。\n        String articleId = String.valueOf(conn.incr("article:"));\n\n        String voted = "voted:" + articleId;\n        // 将发布文章的用户添加到文章的已投票用户名单里面，\n        conn.sadd(voted, user);\n        // 然后将这个名单的过期时间设置为一周（第3章将对过期时间作更详细的介绍）。\n        conn.expire(voted, ONE_WEEK_IN_SECONDS);\n\n        long now = System.currentTimeMillis() / 1000;\n        String article = "article:" + articleId;\n        // 将文章信息存储到一个散列里面。\n        HashMap<String, String> articleData = new HashMap<String, String>();\n        articleData.put("title", title);\n        articleData.put("link", link);\n        articleData.put("user", user);\n        articleData.put("now", String.valueOf(now));\n        articleData.put("votes", "1");\n        conn.hmset(article, articleData);\n\n        // 将文章添加到根据发布时间排序的有序集合和根据评分排序的有序集合里面。\n        conn.zadd("score:", now + VOTE_SCORE, article);\n        conn.zadd("time:", now, article);\n\n        return articleId;\n    }\n\n\n分页查询最受欢迎文章：\n\n使用 ZINTERSTORE 命令根据页码、每页记录数、排序号，根据评分值从大到小分页查出文章 ID 列表。\n\n    public List<Map<String, String>> getArticles(Jedis conn, int page, String order) {\n        // 设置获取文章的起始索引和结束索引。\n        int start = (page - 1) * ARTICLES_PER_PAGE;\n        int end = start + ARTICLES_PER_PAGE - 1;\n\n        // 获取多个文章ID。\n        Set<String> ids = conn.zrevrange(order, start, end);\n        List<Map<String, String>> articles = new ArrayList<>();\n        // 根据文章ID获取文章的详细信息。\n        for (String id : ids) {\n            Map<String, String> articleData = conn.hgetAll(id);\n            articleData.put("id", id);\n            articles.add(articleData);\n        }\n\n        return articles;\n    }\n\n\n# 对文章进行分组\n\n如果文章需要分组，功能需要分为两块：\n\n * 记录文章属于哪个群组\n * 负责取出群组里的文章\n\n将文章添加、删除群组：\n\n    public void addRemoveGroups(Jedis conn, String articleId, String[] toAdd, String[] toRemove) {\n        // 构建存储文章信息的键名。\n        String article = "article:" + articleId;\n        // 将文章添加到它所属的群组里面。\n        for (String group : toAdd) {\n            conn.sadd("group:" + group, article);\n        }\n        // 从群组里面移除文章。\n        for (String group : toRemove) {\n            conn.srem("group:" + group, article);\n        }\n    }\n\n\n取出群组里的文章：\n\n\n\n * 通过对存储群组文章的集合和存储文章评分的有序集合执行 ZINTERSTORE 命令，可以得到按照文章评分排序的群组文章。\n * 通过对存储群组文章的集合和存储文章发布时间的有序集合执行 ZINTERSTORE 命令，可以得到按照文章发布时间排序的群组文章。\n\n    public List<Map<String, String>> getGroupArticles(Jedis conn, String group, int page, String order) {\n        // 为每个群组的每种排列顺序都创建一个键。\n        String key = order + group;\n        // 检查是否有已缓存的排序结果，如果没有的话就现在进行排序。\n        if (!conn.exists(key)) {\n            // 根据评分或者发布时间，对群组文章进行排序。\n            ZParams params = new ZParams().aggregate(ZParams.Aggregate.MAX);\n            conn.zinterstore(key, params, "group:" + group, order);\n            // 让Redis在60秒钟之后自动删除这个有序集合。\n            conn.expire(key, 60);\n        }\n        // 调用之前定义的getArticles函数来进行分页并获取文章数据。\n        return getArticles(conn, page, key);\n    }\n\n\n\n# 案例-管理令牌\n\n网站一般会以 Cookie、Session、令牌这类信息存储用户身份信息。\n\n可以将 Cookie/Session/令牌 和用户的映射关系存储在 HASH 结构。\n\n下面以令牌来举例。\n\n# 查询令牌\n\n    public String checkToken(Jedis conn, String token) {\n        // 尝试获取并返回令牌对应的用户。\n        return conn.hget("login:", token);\n    }\n\n\n# 更新令牌\n\n * 用户每次访问页面，可以记录下令牌和当前时间戳的映射关系，存入一个 ZSET 结构中，以便分析用户是否活跃，继而可以周期性清理最老的令牌，统计当前在线用户数等行为。\n * 用户如果正在浏览商品，可以记录到用户最近浏览过的商品有序集合中（集合可以限定数量，超过数量进行裁剪），存入到一个 ZSET 结构中，以便分析用户最近可能感兴趣的商品，以便推荐商品。\n\n    public void updateToken(Jedis conn, String token, String user, String item) {\n        // 获取当前时间戳。\n        long timestamp = System.currentTimeMillis() / 1000;\n        // 维持令牌与已登录用户之间的映射。\n        conn.hset("login:", token, user);\n        // 记录令牌最后一次出现的时间。\n        conn.zadd("recent:", timestamp, token);\n        if (item != null) {\n            // 记录用户浏览过的商品。\n            conn.zadd("viewed:" + token, timestamp, item);\n            // 移除旧的记录，只保留用户最近浏览过的25个商品。\n            conn.zremrangeByRank("viewed:" + token, 0, -26);\n            conn.zincrby("viewed:", -1, item);\n        }\n    }\n\n\n# 清理令牌\n\n上一节提到，更新令牌时，将令牌和当前时间戳的映射关系，存入一个 ZSET 结构中。所以可以通过排序得知哪些令牌最老。如果没有清理操作，更新令牌占用的内存会不断膨胀，直到导致机器宕机。\n\n比如：最多允许存储 1000 万条令牌信息，周期性检查，一旦发现记录数超出 1000 万条，将 ZSET 从新到老排序，将超出 1000 万条的记录清除。\n\npublic static class CleanSessionsThread extends Thread {\n\n    private Jedis conn;\n\n    private int limit;\n\n    private volatile boolean quit;\n\n    public CleanSessionsThread(int limit) {\n        this.conn = new Jedis("localhost");\n        this.conn.select(15);\n        this.limit = limit;\n    }\n\n    public void quit() {\n        quit = true;\n    }\n\n    @Override\n    public void run() {\n        while (!quit) {\n            // 找出目前已有令牌的数量。\n            long size = conn.zcard("recent:");\n            // 令牌数量未超过限制，休眠并在之后重新检查。\n            if (size <= limit) {\n                try {\n                    sleep(1000);\n                } catch (InterruptedException ie) {\n                    Thread.currentThread().interrupt();\n                }\n                continue;\n            }\n\n            // 获取需要移除的令牌ID。\n            long endIndex = Math.min(size - limit, 100);\n            Set<String> tokenSet = conn.zrange("recent:", 0, endIndex - 1);\n            String[] tokens = tokenSet.toArray(new String[tokenSet.size()]);\n\n            // 为那些将要被删除的令牌构建键名。\n            ArrayList<String> sessionKeys = new ArrayList<String>();\n            for (String token : tokens) {\n                sessionKeys.add("viewed:" + token);\n            }\n\n            // 移除最旧的那些令牌。\n            conn.del(sessionKeys.toArray(new String[sessionKeys.size()]));\n            conn.hdel("login:", tokens);\n            conn.zrem("recent:", tokens);\n        }\n    }\n\n}\n\n\n\n# 案例-购物车\n\n可以使用 HASH 结构来实现购物车功能。\n\n每个用户的购物车，存储了商品 ID 和商品数量的映射。\n\n# 在购物车中添加、删除商品\n\n    public void addToCart(Jedis conn, String session, String item, int count) {\n        if (count <= 0) {\n            // 从购物车里面移除指定的商品。\n            conn.hdel("cart:" + session, item);\n        } else {\n            // 将指定的商品添加到购物车。\n            conn.hset("cart:" + session, item, String.valueOf(count));\n        }\n    }\n\n\n# 清空购物车\n\n在 清理令牌 的基础上，清空会话时，顺便将购物车缓存一并清理。\n\n   while (!quit) {\n        long size = conn.zcard("recent:");\n        if (size <= limit) {\n            try {\n                sleep(1000);\n            } catch (InterruptedException ie) {\n                Thread.currentThread().interrupt();\n            }\n            continue;\n        }\n\n        long endIndex = Math.min(size - limit, 100);\n        Set<String> sessionSet = conn.zrange("recent:", 0, endIndex - 1);\n        String[] sessions = sessionSet.toArray(new String[sessionSet.size()]);\n\n        ArrayList<String> sessionKeys = new ArrayList<String>();\n        for (String sess : sessions) {\n            sessionKeys.add("viewed:" + sess);\n            // 新增加的这行代码用于删除旧会话对应用户的购物车。\n            sessionKeys.add("cart:" + sess);\n        }\n\n        conn.del(sessionKeys.toArray(new String[sessionKeys.size()]));\n        conn.hdel("login:", sessions);\n        conn.zrem("recent:", sessions);\n    }\n\n\n\n# 案例-页面缓存\n\n大部分网页内容并不会经常改变，但是访问时，后台需要动态计算，这可能耗时较多，此时可以使用 STRING 结构存储页面缓存，\n\n    public String cacheRequest(Jedis conn, String request, Callback callback) {\n        // 对于不能被缓存的请求，直接调用回调函数。\n        if (!canCache(conn, request)) {\n            return callback != null ? callback.call(request) : null;\n        }\n\n        // 将请求转换成一个简单的字符串键，方便之后进行查找。\n        String pageKey = "cache:" + hashRequest(request);\n        // 尝试查找被缓存的页面。\n        String content = conn.get(pageKey);\n\n        if (content == null && callback != null) {\n            // 如果页面还没有被缓存，那么生成页面。\n            content = callback.call(request);\n            // 将新生成的页面放到缓存里面。\n            conn.setex(pageKey, 300, content);\n        }\n\n        // 返回页面。\n        return content;\n    }\n\n\n\n# 案例-数据行缓存\n\n电商网站可能会有促销、特卖、抽奖等活动，这些活动页面只需要从数据库中加载几行数据，如：用户信息、商品信息。\n\n可以使用 STRING 结构来缓存这些数据，使用 JSON 存储结构化的信息。\n\n此外，需要有两个 ZSET 结构来记录更新缓存的时机：\n\n * 第一个为调度有序集合；\n * 第二个为延时有序集合。\n\n记录缓存时机：\n\n    public void scheduleRowCache(Jedis conn, String rowId, int delay) {\n        // 先设置数据行的延迟值。\n        conn.zadd("delay:", delay, rowId);\n        // 立即缓存数据行。\n        conn.zadd("schedule:", System.currentTimeMillis() / 1000, rowId);\n    }\n\n\n定时更新数据行缓存：\n\npublic class CacheRowsThread extends Thread {\n\n    private Jedis conn;\n\n    private boolean quit;\n\n    public CacheRowsThread() {\n        this.conn = new Jedis("localhost");\n        this.conn.select(15);\n    }\n\n    public void quit() {\n        quit = true;\n    }\n\n    @Override\n    public void run() {\n        Gson gson = new Gson();\n        while (!quit) {\n            // 尝试获取下一个需要被缓存的数据行以及该行的调度时间戳，\n            // 命令会返回一个包含零个或一个元组（tuple）的列表。\n            Set<Tuple> range = conn.zrangeWithScores("schedule:", 0, 0);\n            Tuple next = range.size() > 0 ? range.iterator().next() : null;\n            long now = System.currentTimeMillis() / 1000;\n            if (next == null || next.getScore() > now) {\n                try {\n                    // 暂时没有行需要被缓存，休眠50毫秒后重试。\n                    sleep(50);\n                } catch (InterruptedException ie) {\n                    Thread.currentThread().interrupt();\n                }\n                continue;\n            }\n\n            String rowId = next.getElement();\n            // 获取下一次调度前的延迟时间。\n            double delay = conn.zscore("delay:", rowId);\n            if (delay <= 0) {\n                // 不必再缓存这个行，将它从缓存中移除。\n                conn.zrem("delay:", rowId);\n                conn.zrem("schedule:", rowId);\n                conn.del("inv:" + rowId);\n                continue;\n            }\n\n            // 读取数据行。\n            Inventory row = Inventory.get(rowId);\n            // 更新调度时间并设置缓存值。\n            conn.zadd("schedule:", now + delay, rowId);\n            conn.set("inv:" + rowId, gson.toJson(row));\n        }\n    }\n\n}\n\n\n\n# 案例-网页分析\n\n网站可以采集用户的访问、交互、购买行为，再分析用户习惯、喜好，从而判断市场行情和潜在商机等。\n\n那么，简单的，如何记录用户在一定时间内访问的商品页面呢？\n\n参考 更新令牌 代码示例，记录用户访问不同商品的浏览次数，并排序。\n\n判断页面是否需要缓存，根据评分判断商品页面是否热门：\n\n    public boolean canCache(Jedis conn, String request) {\n        try {\n            URL url = new URL(request);\n            HashMap<String, String> params = new HashMap<>();\n            if (url.getQuery() != null) {\n                for (String param : url.getQuery().split("&")) {\n                    String[] pair = param.split("=", 2);\n                    params.put(pair[0], pair.length == 2 ? pair[1] : null);\n                }\n            }\n\n            // 尝试从页面里面取出商品ID。\n            String itemId = extractItemId(params);\n            // 检查这个页面能否被缓存以及这个页面是否为商品页面。\n            if (itemId == null || isDynamic(params)) {\n                return false;\n            }\n            // 取得商品的浏览次数排名。\n            Long rank = conn.zrank("viewed:", itemId);\n            // 根据商品的浏览次数排名来判断是否需要缓存这个页面。\n            return rank != null && rank < 10000;\n        } catch (MalformedURLException mue) {\n            return false;\n        }\n    }\n\n\n\n# 案例-记录日志\n\n可用使用 LIST 结构存储日志数据。\n\n    public void logRecent(Jedis conn, String name, String message, String severity) {\n        String destination = "recent:" + name + \':\' + severity;\n        Pipeline pipe = conn.pipelined();\n        pipe.lpush(destination, TIMESTAMP.format(new Date()) + \' \' + message);\n        pipe.ltrim(destination, 0, 99);\n        pipe.sync();\n    }\n\n\n\n# 案例-统计数据\n\n更新计数器：\n\n    public static final int[] PRECISION = new int[] { 1, 5, 60, 300, 3600, 18000, 86400 };\n\n    public void updateCounter(Jedis conn, String name, int count, long now) {\n        Transaction trans = conn.multi();\n        for (int prec : PRECISION) {\n            long pnow = (now / prec) * prec;\n            String hash = String.valueOf(prec) + \':\' + name;\n            trans.zadd("known:", 0, hash);\n            trans.hincrBy("count:" + hash, String.valueOf(pnow), count);\n        }\n        trans.exec();\n    }\n\n\n查看计数器数据：\n\n    public List<Pair<Integer>> getCounter(\n        Jedis conn, String name, int precision) {\n        String hash = String.valueOf(precision) + \':\' + name;\n        Map<String, String> data = conn.hgetAll("count:" + hash);\n        List<Pair<Integer>> results = new ArrayList<>();\n        for (Map.Entry<String, String> entry : data.entrySet()) {\n            results.add(new Pair<>(\n                entry.getKey(),\n                Integer.parseInt(entry.getValue())));\n        }\n        Collections.sort(results);\n        return results;\n    }\n\n\n\n# 案例-查找 IP 所属地\n\nRedis 实现的 IP 所属地查找比关系型数据实现方式更快。\n\n# 载入 IP 数据\n\nIP 地址转为整数值：\n\n    public int ipToScore(String ipAddress) {\n        int score = 0;\n        for (String v : ipAddress.split("\\\\.")) {\n            score = score * 256 + Integer.parseInt(v, 10);\n        }\n        return score;\n    }\n\n\n创建 IP 地址与城市 ID 之间的映射：\n\n    public void importIpsToRedis(Jedis conn, File file) {\n        FileReader reader = null;\n        try {\n            // 载入 csv 文件数据\n            reader = new FileReader(file);\n            CSVFormat csvFormat = CSVFormat.DEFAULT.withRecordSeparator("\\n");\n            CSVParser csvParser = csvFormat.parse(reader);\n            int count = 0;\n            List<CSVRecord> records = csvParser.getRecords();\n            for (CSVRecord line : records) {\n                String startIp = line.get(0);\n                if (startIp.toLowerCase().indexOf(\'i\') != -1) {\n                    continue;\n                }\n                // 将 IP 地址转为整数值\n                int score = 0;\n                if (startIp.indexOf(\'.\') != -1) {\n                    score = ipToScore(startIp);\n                } else {\n                    try {\n                        score = Integer.parseInt(startIp, 10);\n                    } catch (NumberFormatException nfe) {\n                        // 略过文件的第一行以及格式不正确的条目\n                        continue;\n                    }\n                }\n\n                // 构建唯一的城市 ID\n                String cityId = line.get(2) + \'_\' + count;\n                // 将城市 ID 及其对应的 IP 地址整数值添加到 ZSET\n                conn.zadd("ip2cityid:", score, cityId);\n                count++;\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        } finally {\n            try {\n                reader.close();\n            } catch (Exception e) {\n                // ignore\n            }\n        }\n    }\n\n\n存储城市信息：\n\n    public void importCitiesToRedis(Jedis conn, File file) {\n        Gson gson = new Gson();\n        FileReader reader = null;\n        try {\n            // 加载 csv 信息\n            reader = new FileReader(file);\n            CSVFormat csvFormat = CSVFormat.DEFAULT.withRecordSeparator("\\n");\n            CSVParser parser = new CSVParser(reader, csvFormat);\n            // String[] line;\n            List<CSVRecord> records = parser.getRecords();\n            for (CSVRecord record : records) {\n\n                if (record.size() < 4 || !Character.isDigit(record.get(0).charAt(0))) {\n                    continue;\n                }\n\n                // 将城市地理信息转为 json 结构，存入 HASH 结构中\n                String cityId = record.get(0);\n                String country = record.get(1);\n                String region = record.get(2);\n                String city = record.get(3);\n                String json = gson.toJson(new String[] { city, region, country });\n                conn.hset("cityid2city:", cityId, json);\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        } finally {\n            try {\n                reader.close();\n            } catch (Exception e) {\n                // ignore\n            }\n        }\n    }\n\n\n# 查找 IP 所属城市\n\n操作步骤：\n\n 1. 将要查找的 IP 地址转为整数值；\n 2. 查找所有分值小于等于要查找的 IP 地址的地址，取出其中最大分值的那个记录；\n 3. 用找到的记录所对应的城市 ID 去检索城市信息。\n\n    public String[] findCityByIp(Jedis conn, String ipAddress) {\n        int score = ipToScore(ipAddress);\n        Set<String> results = conn.zrevrangeByScore("ip2cityid:", score, 0, 0, 1);\n        if (results.size() == 0) {\n            return null;\n        }\n\n        String cityId = results.iterator().next();\n        cityId = cityId.substring(0, cityId.indexOf(\'_\'));\n        return new Gson().fromJson(conn.hget("cityid2city:", cityId), String[].class);\n    }\n\n\n\n# 案例-服务的发现与配置\n\n\n# 案例-自动补全\n\n需求：根据用户输入，自动补全信息，如：联系人、商品名等。\n\n * 典型场景一：社交网站后台记录用户最近联系过的 100 个好友，当用户查找好友时，根据输入的关键字自动补全姓名。\n * 典型场景二：电商网站后台记录用户最近浏览过的 10 件商品，当用户查找商品是，根据输入的关键字自动补全商品名称。\n\n数据模型：使用 Redis 的 LIST 类型存储最近联系人列表。\n\n构建自动补全列表通常有以下操作：\n\n * 如果指定联系人已经存在于最近联系人列表里，那么从列表里移除他。对应 LREM 命令。\n * 将指定联系人添加到最近联系人列表的最前面。对应 LPUSH 命令。\n * 添加操作完成后，如果联系人列表中的数量超过 100 个，进行裁剪操作。对应 LTRIM 命令。\n\n\n# 案例-广告定向\n\n\n# 案例-职位搜索\n\n需求：在一个招聘网站上，求职者有自己的技能清单；用人公司的职位有必要的技能清单。用人公司需要查询满足自己职位要求的求职者；求职者需要查询自己可以投递简历的职位。\n\n关键数据模型：使用 SET 类型存储求职者的技能列表，使用 SET 类型存储职位的技能列表。\n\n关键操作：使用 SDIFF 命令对比两个 SET 的差异，返回 empty 表示匹配要求。\n\nredis cli 示例：\n\n# -----------------------------------------------------------\n# Redis 职位搜索数据模型示例\n# -----------------------------------------------------------\n\n# （1）职位技能表：使用 set 存储\n# job:001 职位添加 4 种技能\nSADD job:001 skill:001\nSADD job:001 skill:002\nSADD job:001 skill:003\nSADD job:001 skill:004\n\n# job:002 职位添加 3 种技能\nSADD job:002 skill:001\nSADD job:002 skill:002\nSADD job:002 skill:003\n\n# job:003 职位添加 2 种技能\nSADD job:003 skill:001\nSADD job:003 skill:003\n\n# 查看\nSMEMBERS job:001\nSMEMBERS job:002\nSMEMBERS job:003\n\n# （2）求职者技能表：使用 set 存储\nSADD interviewee:001 skill:001\nSADD interviewee:001 skill:003\n\nSADD interviewee:002 skill:001\nSADD interviewee:002 skill:002\nSADD interviewee:002 skill:003\nSADD interviewee:002 skill:004\nSADD interviewee:002 skill:005\n\n# 查看\nSMEMBERS interviewee:001\nSMEMBERS interviewee:002\n\n# （3）求职者遍历查找自己符合要求的职位（返回结果为 empty 表示要求的技能全部命中）\n# 比较职位技能清单和求职者技能清单的差异\nSDIFF job:001 interviewee:001\nSDIFF job:002 interviewee:001\nSDIFF job:003 interviewee:001\n\nSDIFF job:001 interviewee:002\nSDIFF job:002 interviewee:002\nSDIFF job:003 interviewee:002\n\n# （4）用人公司根据遍历查找符合自己职位要求的求职者（返回结果为 empty 表示要求的技能全部命中）\n# 比较职位技能清单和求职者技能清单的差异\nSDIFF interviewee:001 job:001\nSDIFF interviewee:002 job:001\n\nSDIFF interviewee:001 job:002\nSDIFF interviewee:002 job:002\n\nSDIFF interviewee:001 job:003\nSDIFF interviewee:002 job:003\n\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考\n * 文章\n   * 一看就懂系列之 详解 redis 的 bitmap 在亿级项目中的应用\n   * Fast, easy, realtime metrics using Redis bitmaps',normalizedContent:'# redis 数据类型和应用\n\n> redis 提供了多种数据类型，每种数据类型有丰富的命令支持。\n> \n> 使用 redis ，不仅要了解其数据类型的特性，还需要根据业务场景，灵活的、高效的使用其数据类型来建模。\n\n\n# 一、redis 基本数据类型\n\n\n\n数据类型     可以存储的值        操作\nstring   字符串、整数或者浮点数   对整个字符串或者字符串的其中一部分执行操作\n                       对整数和浮点数执行自增或者自减操作\nlist     列表            从两端压入或者弹出元素\n                       读取单个或者多个元素\n                       进行修剪，只保留一个范围内的元素\nset      无序集合          添加、获取、移除单个元素\n                       检查一个元素是否存在于集合中\n                       计算交集、并集、差集\n                       从集合里面随机获取元素\nhash     包含键值对的无序散列表   添加、获取、移除单个键值对\n                       获取所有键值对\n                       检查某个键是否存在\nzset     有序集合          添加、获取、删除元素\n                       根据分值范围或者成员来获取元素\n                       计算一个键的排名\n\n> what redis data structures look like\n\n\n# string\n\n\n**适用场景：缓存、计数器、共享 session**\n\n命令：\n\n命令     行为\nget    获取存储在给定键中的值。\nset    设置存储在给定键中的值。\ndel    删除存储在给定键中的值（这个命令可以用于所有类型）。\nincr   为键 key 储存的数字值加一\ndecr   为键 key 储存的数字值减一\n\n> 更多命令请参考：redis string 类型命令\n\n示例：\n\n127.0.0.1:6379> set hello world\nok\n127.0.0.1:6379> get hello\n"world"\n127.0.0.1:6379> del hello\n(integer) 1\n127.0.0.1:6379> get hello\n(nil)\n\n\n\n# hash\n\n\n**适用场景：存储结构化数据**，如一个对象：用户信息、产品信息等。\n\n命令：\n\n命令        行为\nhset      在散列里面关联起给定的键值对。\nhget      获取指定散列键的值。\nhgetall   获取散列包含的所有键值对。\nhdel      如果给定键存在于散列里面，那么移除这个键。\n\n> 更多命令请参考：redis hash 类型命令\n\n示例：\n\n127.0.0.1:6379> hset hash-key sub-key1 value1\n(integer) 1\n127.0.0.1:6379> hset hash-key sub-key2 value2\n(integer) 1\n127.0.0.1:6379> hset hash-key sub-key1 value1\n(integer) 0\n127.0.0.1:6379> hset hash-key sub-key3 value2\n(integer) 0\n127.0.0.1:6379> hgetall hash-key\n1) "sub-key1"\n2) "value1"\n3) "sub-key2"\n4) "value2"\n127.0.0.1:6379> hdel hash-key sub-key2\n(integer) 1\n127.0.0.1:6379> hdel hash-key sub-key2\n(integer) 0\n127.0.0.1:6379> hget hash-key sub-key1\n"value1"\n127.0.0.1:6379> hgetall hash-key\n1) "sub-key1"\n2) "value1"\n\n\n\n# list\n\n\n**适用场景：用于存储列表型数据**。如：粉丝列表、商品列表等。\n\n命令：\n\n命令       行为\nlpush    将给定值推入列表的右端。\nrpush    将给定值推入列表的右端。\nlpop     从列表的左端弹出一个值，并返回被弹出的值。\nrpop     从列表的右端弹出一个值，并返回被弹出的值。\nlrange   获取列表在给定范围上的所有值。\nlindex   获取列表在给定位置上的单个元素。\nlrem     从列表的左端弹出一个值，并返回被弹出的值。\nltrim    只保留指定区间内的元素，删除其他元素。\n\n> 更多命令请参考：redis list 类型命令\n\n示例：\n\n127.0.0.1:6379> rpush list-key item\n(integer) 1\n127.0.0.1:6379> rpush list-key item2\n(integer) 2\n127.0.0.1:6379> rpush list-key item\n(integer) 3\n127.0.0.1:6379> lrange list-key 0 -1\n1) "item"\n2) "item2"\n3) "item"\n127.0.0.1:6379> lindex list-key 1\n"item2"\n127.0.0.1:6379> lpop list-key\n"item"\n127.0.0.1:6379> lrange list-key 0 -1\n1) "item2"\n2) "item"\n\n\n\n# set\n\n\n**适用场景：用于存储去重的列表型数据**。\n\n命令：\n\n命令          行为\nsadd        将给定元素添加到集合。\nsmembers    返回集合包含的所有元素。\nsismember   检查给定元素是否存在于集合中。\nsrem        如果给定的元素存在于集合中，那么移除这个元素。\n\n> 更多命令请参考：redis set 类型命令\n\n示例：\n\n127.0.0.1:6379> sadd set-key item\n(integer) 1\n127.0.0.1:6379> sadd set-key item2\n(integer) 1\n127.0.0.1:6379> sadd set-key item3\n(integer) 1\n127.0.0.1:6379> sadd set-key item\n(integer) 0\n127.0.0.1:6379> smembers set-key\n1) "item"\n2) "item2"\n3) "item3"\n127.0.0.1:6379> sismember set-key item4\n(integer) 0\n127.0.0.1:6379> sismember set-key item\n(integer) 1\n127.0.0.1:6379> srem set-key item2\n(integer) 1\n127.0.0.1:6379> srem set-key item2\n(integer) 0\n127.0.0.1:6379> smembers set-key\n1) "item"\n2) "item3"\n\n\n\n# zset\n\n\n\n适用场景：由于可以设置 score，且不重复。适合用于存储各种排行数据，如：按评分排序的有序商品集合、按时间排序的有序文章集合。\n\n命令：\n\n命令              行为\nzadd            将一个带有给定分值的成员添加到有序集合里面。\nzrange          根据元素在有序排列中所处的位置，从有序集合里面获取多个元素。\nzrangebyscore   获取有序集合在给定分值范围内的所有元素。\nzrem            如果给定成员存在于有序集合，那么移除这个成员。\n\n> 更多命令请参考：redis zset 类型命令\n\n示例：\n\n127.0.0.1:6379> zadd zset-key 728 member1\n(integer) 1\n127.0.0.1:6379> zadd zset-key 982 member0\n(integer) 1\n127.0.0.1:6379> zadd zset-key 982 member0\n(integer) 0\n\n127.0.0.1:6379> zrange zset-key 0 -1 withscores\n1) "member1"\n2) "728"\n3) "member0"\n4) "982"\n\n127.0.0.1:6379> zrangebyscore zset-key 0 800 withscores\n1) "member1"\n2) "728"\n\n127.0.0.1:6379> zrem zset-key member1\n(integer) 1\n127.0.0.1:6379> zrem zset-key member1\n(integer) 0\n127.0.0.1:6379> zrange zset-key 0 -1 withscores\n1) "member0"\n2) "982"\n\n\n\n# 通用命令\n\n# 排序\n\nredis 的 sort 命令可以对 list、set、zset 进行排序。\n\n命令     描述\nsort   sort source-key [by pattern] [limit offset count] [get\n       pattern [get pattern ...]] [asc | desc] [alpha] [store\n       dest-key]—根据给定选项，对输入 list、set、zset 进行排序，然后返回或存储排序的结果。\n\n示例：\n\n127.0.0.1:6379[15]> rpush \'sort-input\' 23 15 110 7\n(integer) 4\n127.0.0.1:6379[15]> sort \'sort-input\'\n1) "7"\n2) "15"\n3) "23"\n4) "110"\n127.0.0.1:6379[15]> sort \'sort-input\' alpha\n1) "110"\n2) "15"\n3) "23"\n4) "7"\n127.0.0.1:6379[15]> hset \'d-7\' \'field\' 5\n(integer) 1\n127.0.0.1:6379[15]> hset \'d-15\' \'field\' 1\n(integer) 1\n127.0.0.1:6379[15]> hset \'d-23\' \'field\' 9\n(integer) 1\n127.0.0.1:6379[15]> hset \'d-110\' \'field\' 3\n(integer) 1\n127.0.0.1:6379[15]> sort \'sort-input\' by \'d-*->field\'\n1) "15"\n2) "110"\n3) "7"\n4) "23"\n127.0.0.1:6379[15]> sort \'sort-input\' by \'d-*->field\' get \'d-*->field\'\n1) "1"\n2) "3"\n3) "5"\n4) "9"\n\n\n# 键的过期时间\n\nredis 的 expire 命令可以指定一个键的过期时间，当达到过期时间后，redis 会自动删除该键。\n\n命令          描述\npersist     persist key-name—移除键的过期时间\nttl         ttl key-name—查看给定键距离过期还有多少秒\nexpire      expire key-name seconds—让给定键在指定的秒数之后过期\nexpireat    expireat key-name timestamp—将给定键的过期时间设置为给定的 unix 时间戳\npttl        pttl key-name—查看给定键距离过期时间还有多少毫秒（这个命令在 redis 2.6 或以上版本可用）\npexpire     pexpire key-name milliseconds—让给定键在指定的毫秒数之后过期（这个命令在 redis\n            2.6 或以上版本可用）\npexpireat   pexpireat key-name timestamp-milliseconds—将一个毫秒级精度的 unix\n            时间戳设置为给定键的过期时间（这个命令在 redis 2.6 或以上版本可用）\n\n示例：\n\n127.0.0.1:6379[15]> set key value\nok\n127.0.0.1:6379[15]> get key\n"value"\n127.0.0.1:6379[15]> expire key 2\n(integer) 1\n127.0.0.1:6379[15]> get key\n(nil)\n\n\n\n# 二、redis 高级数据类型\n\n\n# bitmap\n\nbitmap 即位图。bitmap 不是一个真实的数据结构。而是 string 类型上的一组面向 bit 操作的集合。由于 string 是二进制安全的 blob，并且它们的最大长度是 512m，所以 bitmap 能最大设置 $$2^{32}$$ 个不同的 bit。\n\nbitmaps 的最大优点就是存储信息时可以节省大量的空间。例如在一个系统中，不同的用户被一个增长的用户 id 表示。40 亿（$$2^{32}$$ = $$410241024*1024$$ ≈ 40 亿）用户只需要 512m 内存就能记住某种信息，例如用户是否登录过。\n\n# bitmap 命令\n\n * setbit - 对 key 所储存的字符串值，设置或清除指定偏移量上的位(bit)。\n * getbit - 对 key 所储存的字符串值，获取指定偏移量上的位(bit)。\n * bitcount - 计算给定字符串中，被设置为 1 的比特位的数量。\n * bitpos\n * bitop\n * bitfield\n\n# bitmap 示例\n\n# 对不存在的 key 或者不存在的 offset 进行 getbit， 返回 0\n\nredis> exists bit\n(integer) 0\n\nredis> getbit bit 10086\n(integer) 0\n\n\n# 对已存在的 offset 进行 getbit\n\nredis> setbit bit 10086 1\n(integer) 0\n\nredis> getbit bit 10086\n(integer) 1\n\nredis> bitcount bit\n(integer) 1\n\n\n# bitmap 应用\n\nbitmap 对于一些特定类型的计算非常有效。例如：使用 bitmap 实现用户上线次数统计。\n\n假设现在我们希望记录自己网站上的用户的上线频率，比如说，计算用户 a 上线了多少天，用户 b 上线了多少天，诸如此类，以此作为数据，从而决定让哪些用户参加 beta 测试等活动 —— 这个模式可以使用 setbit key offset value 和 [bitcount key start] [end] 来实现。\n\n比如说，每当用户在某一天上线的时候，我们就使用 setbit key offset value ，以用户名作为 key，将那天所代表的网站的上线日作为 offset 参数，并将这个 offset 上的为设置为 1 。\n\n> 更详细的实现可以参考：\n> \n> 一看就懂系列之 详解 redis 的 bitmap 在亿级项目中的应用\n> \n> fast, easy, realtime metrics using redis bitmaps\n\n\n# hyperloglog\n\nhyperloglog 是用于计算唯一事物的概率数据结构（从技术上讲，这被称为估计集合的基数）。如果统计唯一项，项目越多，需要的内存就越多。因为需要记住过去已经看过的项，从而避免多次统计这些项。\n\n# hyperloglog 命令\n\n * pfadd - 将任意数量的元素添加到指定的 hyperloglog 里面。\n * pfcount - 返回 hyperloglog 包含的唯一元素的近似数量。\n * pfmerge - 将多个 hyperloglog 合并（merge）为一个 hyperloglog ， 合并后的 hyperloglog 的基数接近于所有输入 hyperloglog 的可见集合（observed set）的并集。合并得出的 hyperloglog 会被储存在 destkey 键里面， 如果该键并不存在， 那么命令在执行之前， 会先为该键创建一个空的 hyperloglog 。\n\n示例：\n\nredis> pfadd  databases  "redis"  "mongodb"  "mysql"\n(integer) 1\n\nredis> pfcount  databases\n(integer) 3\n\nredis> pfadd  databases  "redis"    # redis 已经存在，不必对估计数量进行更新\n(integer) 0\n\nredis> pfcount  databases    # 元素估计数量没有变化\n(integer) 3\n\nredis> pfadd  databases  "postgresql"    # 添加一个不存在的元素\n(integer) 1\n\nredis> pfcount  databases    # 估计数量增一\n4\n\n\n\n# geo\n\n这个功能可以将用户给定的地理位置（经度和纬度）信息储存起来，并对这些信息进行操作。\n\n# geo 命令\n\n * geoadd - 将指定的地理空间位置（纬度、经度、名称）添加到指定的 key 中。\n * geopos - 从 key 里返回所有给定位置元素的位置（经度和纬度）。\n * geodist - 返回两个给定位置之间的距离。\n * geohash - 回一个或多个位置元素的标准 geohash 值，它可以在http://geohash.org/使用。\n * georadius\n * georadiusbymember\n\n\n# 三、redis 数据类型应用\n\n\n# 案例-最受欢迎文章\n\n选出最受欢迎文章，需要支持对文章进行评分。\n\n# 对文章进行投票\n\n（1）使用 hash 存储文章\n\n使用 hash 类型存储文章信息。其中：key 是文章 id；field 是文章的属性 key；value 是属性对应值。\n\n\n\n操作：\n\n * 存储文章信息 - 使用 hset 或 hmget 命令\n * 查询文章信息 - 使用 hgetall 命令\n * 添加投票 - 使用 hincrby 命令\n\n（2）使用 zset 针对不同维度集合排序\n\n使用 zset 类型分别存储按照时间排序和按照评分排序的文章 id 集合。\n\n\n\n操作：\n\n * 添加记录 - 使用 zadd 命令\n * 添加分数 - 使用 zincrby 命令\n * 取出多篇文章 - 使用 zrevrange 命令\n\n（3）为了防止重复投票，使用 set 类型记录每篇文章 id 对应的投票集合。\n\n\n\n操作：\n\n * 添加投票者 - 使用 sadd 命令\n * 设置有效期 - 使用 expire 命令\n\n（4）假设 user:115423 给 article:100408 投票，分别需要高更新评分排序集合以及投票集合。\n\n\n\n当需要对一篇文章投票时，程序需要用 zscore 命令检查记录文章发布时间的有序集合，判断文章的发布时间是否超过投票有效期（比如：一星期）。\n\n    public void articlevote(jedis conn, string user, string article) {\n        // 计算文章的投票截止时间。\n        long cutoff = (system.currenttimemillis() / 1000) - one_week_in_seconds;\n\n        // 检查是否还可以对文章进行投票\n        // （虽然使用散列也可以获取文章的发布时间，\n        // 但有序集合返回的文章发布时间为浮点数，\n        // 可以不进行转换直接使用）。\n        if (conn.zscore("time:", article) < cutoff) {\n            return;\n        }\n\n        // 从article:id标识符（identifier）里面取出文章的id。\n        string articleid = article.substring(article.indexof(\':\') + 1);\n\n        // 如果用户是第一次为这篇文章投票，那么增加这篇文章的投票数量和评分。\n        if (conn.sadd("voted:" + articleid, user) == 1) {\n            conn.zincrby("score:", vote_score, article);\n            conn.hincrby(article, "votes", 1);\n        }\n    }\n\n\n# 发布并获取文章\n\n发布文章：\n\n * 添加文章 - 使用 incr 命令计算新的文章 id，填充文章信息，然后用 hset 命令或 hmset 命令写入到 hash 结构中。\n * 将文章作者 id 添加到投票名单 - 使用 sadd 命令添加到代表投票名单的 set 结构中。\n * 设置投票有效期 - 使用 expire 命令设置投票有效期。\n\n    public string postarticle(jedis conn, string user, string title, string link) {\n        // 生成一个新的文章id。\n        string articleid = string.valueof(conn.incr("article:"));\n\n        string voted = "voted:" + articleid;\n        // 将发布文章的用户添加到文章的已投票用户名单里面，\n        conn.sadd(voted, user);\n        // 然后将这个名单的过期时间设置为一周（第3章将对过期时间作更详细的介绍）。\n        conn.expire(voted, one_week_in_seconds);\n\n        long now = system.currenttimemillis() / 1000;\n        string article = "article:" + articleid;\n        // 将文章信息存储到一个散列里面。\n        hashmap<string, string> articledata = new hashmap<string, string>();\n        articledata.put("title", title);\n        articledata.put("link", link);\n        articledata.put("user", user);\n        articledata.put("now", string.valueof(now));\n        articledata.put("votes", "1");\n        conn.hmset(article, articledata);\n\n        // 将文章添加到根据发布时间排序的有序集合和根据评分排序的有序集合里面。\n        conn.zadd("score:", now + vote_score, article);\n        conn.zadd("time:", now, article);\n\n        return articleid;\n    }\n\n\n分页查询最受欢迎文章：\n\n使用 zinterstore 命令根据页码、每页记录数、排序号，根据评分值从大到小分页查出文章 id 列表。\n\n    public list<map<string, string>> getarticles(jedis conn, int page, string order) {\n        // 设置获取文章的起始索引和结束索引。\n        int start = (page - 1) * articles_per_page;\n        int end = start + articles_per_page - 1;\n\n        // 获取多个文章id。\n        set<string> ids = conn.zrevrange(order, start, end);\n        list<map<string, string>> articles = new arraylist<>();\n        // 根据文章id获取文章的详细信息。\n        for (string id : ids) {\n            map<string, string> articledata = conn.hgetall(id);\n            articledata.put("id", id);\n            articles.add(articledata);\n        }\n\n        return articles;\n    }\n\n\n# 对文章进行分组\n\n如果文章需要分组，功能需要分为两块：\n\n * 记录文章属于哪个群组\n * 负责取出群组里的文章\n\n将文章添加、删除群组：\n\n    public void addremovegroups(jedis conn, string articleid, string[] toadd, string[] toremove) {\n        // 构建存储文章信息的键名。\n        string article = "article:" + articleid;\n        // 将文章添加到它所属的群组里面。\n        for (string group : toadd) {\n            conn.sadd("group:" + group, article);\n        }\n        // 从群组里面移除文章。\n        for (string group : toremove) {\n            conn.srem("group:" + group, article);\n        }\n    }\n\n\n取出群组里的文章：\n\n\n\n * 通过对存储群组文章的集合和存储文章评分的有序集合执行 zinterstore 命令，可以得到按照文章评分排序的群组文章。\n * 通过对存储群组文章的集合和存储文章发布时间的有序集合执行 zinterstore 命令，可以得到按照文章发布时间排序的群组文章。\n\n    public list<map<string, string>> getgrouparticles(jedis conn, string group, int page, string order) {\n        // 为每个群组的每种排列顺序都创建一个键。\n        string key = order + group;\n        // 检查是否有已缓存的排序结果，如果没有的话就现在进行排序。\n        if (!conn.exists(key)) {\n            // 根据评分或者发布时间，对群组文章进行排序。\n            zparams params = new zparams().aggregate(zparams.aggregate.max);\n            conn.zinterstore(key, params, "group:" + group, order);\n            // 让redis在60秒钟之后自动删除这个有序集合。\n            conn.expire(key, 60);\n        }\n        // 调用之前定义的getarticles函数来进行分页并获取文章数据。\n        return getarticles(conn, page, key);\n    }\n\n\n\n# 案例-管理令牌\n\n网站一般会以 cookie、session、令牌这类信息存储用户身份信息。\n\n可以将 cookie/session/令牌 和用户的映射关系存储在 hash 结构。\n\n下面以令牌来举例。\n\n# 查询令牌\n\n    public string checktoken(jedis conn, string token) {\n        // 尝试获取并返回令牌对应的用户。\n        return conn.hget("login:", token);\n    }\n\n\n# 更新令牌\n\n * 用户每次访问页面，可以记录下令牌和当前时间戳的映射关系，存入一个 zset 结构中，以便分析用户是否活跃，继而可以周期性清理最老的令牌，统计当前在线用户数等行为。\n * 用户如果正在浏览商品，可以记录到用户最近浏览过的商品有序集合中（集合可以限定数量，超过数量进行裁剪），存入到一个 zset 结构中，以便分析用户最近可能感兴趣的商品，以便推荐商品。\n\n    public void updatetoken(jedis conn, string token, string user, string item) {\n        // 获取当前时间戳。\n        long timestamp = system.currenttimemillis() / 1000;\n        // 维持令牌与已登录用户之间的映射。\n        conn.hset("login:", token, user);\n        // 记录令牌最后一次出现的时间。\n        conn.zadd("recent:", timestamp, token);\n        if (item != null) {\n            // 记录用户浏览过的商品。\n            conn.zadd("viewed:" + token, timestamp, item);\n            // 移除旧的记录，只保留用户最近浏览过的25个商品。\n            conn.zremrangebyrank("viewed:" + token, 0, -26);\n            conn.zincrby("viewed:", -1, item);\n        }\n    }\n\n\n# 清理令牌\n\n上一节提到，更新令牌时，将令牌和当前时间戳的映射关系，存入一个 zset 结构中。所以可以通过排序得知哪些令牌最老。如果没有清理操作，更新令牌占用的内存会不断膨胀，直到导致机器宕机。\n\n比如：最多允许存储 1000 万条令牌信息，周期性检查，一旦发现记录数超出 1000 万条，将 zset 从新到老排序，将超出 1000 万条的记录清除。\n\npublic static class cleansessionsthread extends thread {\n\n    private jedis conn;\n\n    private int limit;\n\n    private volatile boolean quit;\n\n    public cleansessionsthread(int limit) {\n        this.conn = new jedis("localhost");\n        this.conn.select(15);\n        this.limit = limit;\n    }\n\n    public void quit() {\n        quit = true;\n    }\n\n    @override\n    public void run() {\n        while (!quit) {\n            // 找出目前已有令牌的数量。\n            long size = conn.zcard("recent:");\n            // 令牌数量未超过限制，休眠并在之后重新检查。\n            if (size <= limit) {\n                try {\n                    sleep(1000);\n                } catch (interruptedexception ie) {\n                    thread.currentthread().interrupt();\n                }\n                continue;\n            }\n\n            // 获取需要移除的令牌id。\n            long endindex = math.min(size - limit, 100);\n            set<string> tokenset = conn.zrange("recent:", 0, endindex - 1);\n            string[] tokens = tokenset.toarray(new string[tokenset.size()]);\n\n            // 为那些将要被删除的令牌构建键名。\n            arraylist<string> sessionkeys = new arraylist<string>();\n            for (string token : tokens) {\n                sessionkeys.add("viewed:" + token);\n            }\n\n            // 移除最旧的那些令牌。\n            conn.del(sessionkeys.toarray(new string[sessionkeys.size()]));\n            conn.hdel("login:", tokens);\n            conn.zrem("recent:", tokens);\n        }\n    }\n\n}\n\n\n\n# 案例-购物车\n\n可以使用 hash 结构来实现购物车功能。\n\n每个用户的购物车，存储了商品 id 和商品数量的映射。\n\n# 在购物车中添加、删除商品\n\n    public void addtocart(jedis conn, string session, string item, int count) {\n        if (count <= 0) {\n            // 从购物车里面移除指定的商品。\n            conn.hdel("cart:" + session, item);\n        } else {\n            // 将指定的商品添加到购物车。\n            conn.hset("cart:" + session, item, string.valueof(count));\n        }\n    }\n\n\n# 清空购物车\n\n在 清理令牌 的基础上，清空会话时，顺便将购物车缓存一并清理。\n\n   while (!quit) {\n        long size = conn.zcard("recent:");\n        if (size <= limit) {\n            try {\n                sleep(1000);\n            } catch (interruptedexception ie) {\n                thread.currentthread().interrupt();\n            }\n            continue;\n        }\n\n        long endindex = math.min(size - limit, 100);\n        set<string> sessionset = conn.zrange("recent:", 0, endindex - 1);\n        string[] sessions = sessionset.toarray(new string[sessionset.size()]);\n\n        arraylist<string> sessionkeys = new arraylist<string>();\n        for (string sess : sessions) {\n            sessionkeys.add("viewed:" + sess);\n            // 新增加的这行代码用于删除旧会话对应用户的购物车。\n            sessionkeys.add("cart:" + sess);\n        }\n\n        conn.del(sessionkeys.toarray(new string[sessionkeys.size()]));\n        conn.hdel("login:", sessions);\n        conn.zrem("recent:", sessions);\n    }\n\n\n\n# 案例-页面缓存\n\n大部分网页内容并不会经常改变，但是访问时，后台需要动态计算，这可能耗时较多，此时可以使用 string 结构存储页面缓存，\n\n    public string cacherequest(jedis conn, string request, callback callback) {\n        // 对于不能被缓存的请求，直接调用回调函数。\n        if (!cancache(conn, request)) {\n            return callback != null ? callback.call(request) : null;\n        }\n\n        // 将请求转换成一个简单的字符串键，方便之后进行查找。\n        string pagekey = "cache:" + hashrequest(request);\n        // 尝试查找被缓存的页面。\n        string content = conn.get(pagekey);\n\n        if (content == null && callback != null) {\n            // 如果页面还没有被缓存，那么生成页面。\n            content = callback.call(request);\n            // 将新生成的页面放到缓存里面。\n            conn.setex(pagekey, 300, content);\n        }\n\n        // 返回页面。\n        return content;\n    }\n\n\n\n# 案例-数据行缓存\n\n电商网站可能会有促销、特卖、抽奖等活动，这些活动页面只需要从数据库中加载几行数据，如：用户信息、商品信息。\n\n可以使用 string 结构来缓存这些数据，使用 json 存储结构化的信息。\n\n此外，需要有两个 zset 结构来记录更新缓存的时机：\n\n * 第一个为调度有序集合；\n * 第二个为延时有序集合。\n\n记录缓存时机：\n\n    public void schedulerowcache(jedis conn, string rowid, int delay) {\n        // 先设置数据行的延迟值。\n        conn.zadd("delay:", delay, rowid);\n        // 立即缓存数据行。\n        conn.zadd("schedule:", system.currenttimemillis() / 1000, rowid);\n    }\n\n\n定时更新数据行缓存：\n\npublic class cacherowsthread extends thread {\n\n    private jedis conn;\n\n    private boolean quit;\n\n    public cacherowsthread() {\n        this.conn = new jedis("localhost");\n        this.conn.select(15);\n    }\n\n    public void quit() {\n        quit = true;\n    }\n\n    @override\n    public void run() {\n        gson gson = new gson();\n        while (!quit) {\n            // 尝试获取下一个需要被缓存的数据行以及该行的调度时间戳，\n            // 命令会返回一个包含零个或一个元组（tuple）的列表。\n            set<tuple> range = conn.zrangewithscores("schedule:", 0, 0);\n            tuple next = range.size() > 0 ? range.iterator().next() : null;\n            long now = system.currenttimemillis() / 1000;\n            if (next == null || next.getscore() > now) {\n                try {\n                    // 暂时没有行需要被缓存，休眠50毫秒后重试。\n                    sleep(50);\n                } catch (interruptedexception ie) {\n                    thread.currentthread().interrupt();\n                }\n                continue;\n            }\n\n            string rowid = next.getelement();\n            // 获取下一次调度前的延迟时间。\n            double delay = conn.zscore("delay:", rowid);\n            if (delay <= 0) {\n                // 不必再缓存这个行，将它从缓存中移除。\n                conn.zrem("delay:", rowid);\n                conn.zrem("schedule:", rowid);\n                conn.del("inv:" + rowid);\n                continue;\n            }\n\n            // 读取数据行。\n            inventory row = inventory.get(rowid);\n            // 更新调度时间并设置缓存值。\n            conn.zadd("schedule:", now + delay, rowid);\n            conn.set("inv:" + rowid, gson.tojson(row));\n        }\n    }\n\n}\n\n\n\n# 案例-网页分析\n\n网站可以采集用户的访问、交互、购买行为，再分析用户习惯、喜好，从而判断市场行情和潜在商机等。\n\n那么，简单的，如何记录用户在一定时间内访问的商品页面呢？\n\n参考 更新令牌 代码示例，记录用户访问不同商品的浏览次数，并排序。\n\n判断页面是否需要缓存，根据评分判断商品页面是否热门：\n\n    public boolean cancache(jedis conn, string request) {\n        try {\n            url url = new url(request);\n            hashmap<string, string> params = new hashmap<>();\n            if (url.getquery() != null) {\n                for (string param : url.getquery().split("&")) {\n                    string[] pair = param.split("=", 2);\n                    params.put(pair[0], pair.length == 2 ? pair[1] : null);\n                }\n            }\n\n            // 尝试从页面里面取出商品id。\n            string itemid = extractitemid(params);\n            // 检查这个页面能否被缓存以及这个页面是否为商品页面。\n            if (itemid == null || isdynamic(params)) {\n                return false;\n            }\n            // 取得商品的浏览次数排名。\n            long rank = conn.zrank("viewed:", itemid);\n            // 根据商品的浏览次数排名来判断是否需要缓存这个页面。\n            return rank != null && rank < 10000;\n        } catch (malformedurlexception mue) {\n            return false;\n        }\n    }\n\n\n\n# 案例-记录日志\n\n可用使用 list 结构存储日志数据。\n\n    public void logrecent(jedis conn, string name, string message, string severity) {\n        string destination = "recent:" + name + \':\' + severity;\n        pipeline pipe = conn.pipelined();\n        pipe.lpush(destination, timestamp.format(new date()) + \' \' + message);\n        pipe.ltrim(destination, 0, 99);\n        pipe.sync();\n    }\n\n\n\n# 案例-统计数据\n\n更新计数器：\n\n    public static final int[] precision = new int[] { 1, 5, 60, 300, 3600, 18000, 86400 };\n\n    public void updatecounter(jedis conn, string name, int count, long now) {\n        transaction trans = conn.multi();\n        for (int prec : precision) {\n            long pnow = (now / prec) * prec;\n            string hash = string.valueof(prec) + \':\' + name;\n            trans.zadd("known:", 0, hash);\n            trans.hincrby("count:" + hash, string.valueof(pnow), count);\n        }\n        trans.exec();\n    }\n\n\n查看计数器数据：\n\n    public list<pair<integer>> getcounter(\n        jedis conn, string name, int precision) {\n        string hash = string.valueof(precision) + \':\' + name;\n        map<string, string> data = conn.hgetall("count:" + hash);\n        list<pair<integer>> results = new arraylist<>();\n        for (map.entry<string, string> entry : data.entryset()) {\n            results.add(new pair<>(\n                entry.getkey(),\n                integer.parseint(entry.getvalue())));\n        }\n        collections.sort(results);\n        return results;\n    }\n\n\n\n# 案例-查找 ip 所属地\n\nredis 实现的 ip 所属地查找比关系型数据实现方式更快。\n\n# 载入 ip 数据\n\nip 地址转为整数值：\n\n    public int iptoscore(string ipaddress) {\n        int score = 0;\n        for (string v : ipaddress.split("\\\\.")) {\n            score = score * 256 + integer.parseint(v, 10);\n        }\n        return score;\n    }\n\n\n创建 ip 地址与城市 id 之间的映射：\n\n    public void importipstoredis(jedis conn, file file) {\n        filereader reader = null;\n        try {\n            // 载入 csv 文件数据\n            reader = new filereader(file);\n            csvformat csvformat = csvformat.default.withrecordseparator("\\n");\n            csvparser csvparser = csvformat.parse(reader);\n            int count = 0;\n            list<csvrecord> records = csvparser.getrecords();\n            for (csvrecord line : records) {\n                string startip = line.get(0);\n                if (startip.tolowercase().indexof(\'i\') != -1) {\n                    continue;\n                }\n                // 将 ip 地址转为整数值\n                int score = 0;\n                if (startip.indexof(\'.\') != -1) {\n                    score = iptoscore(startip);\n                } else {\n                    try {\n                        score = integer.parseint(startip, 10);\n                    } catch (numberformatexception nfe) {\n                        // 略过文件的第一行以及格式不正确的条目\n                        continue;\n                    }\n                }\n\n                // 构建唯一的城市 id\n                string cityid = line.get(2) + \'_\' + count;\n                // 将城市 id 及其对应的 ip 地址整数值添加到 zset\n                conn.zadd("ip2cityid:", score, cityid);\n                count++;\n            }\n        } catch (exception e) {\n            throw new runtimeexception(e);\n        } finally {\n            try {\n                reader.close();\n            } catch (exception e) {\n                // ignore\n            }\n        }\n    }\n\n\n存储城市信息：\n\n    public void importcitiestoredis(jedis conn, file file) {\n        gson gson = new gson();\n        filereader reader = null;\n        try {\n            // 加载 csv 信息\n            reader = new filereader(file);\n            csvformat csvformat = csvformat.default.withrecordseparator("\\n");\n            csvparser parser = new csvparser(reader, csvformat);\n            // string[] line;\n            list<csvrecord> records = parser.getrecords();\n            for (csvrecord record : records) {\n\n                if (record.size() < 4 || !character.isdigit(record.get(0).charat(0))) {\n                    continue;\n                }\n\n                // 将城市地理信息转为 json 结构，存入 hash 结构中\n                string cityid = record.get(0);\n                string country = record.get(1);\n                string region = record.get(2);\n                string city = record.get(3);\n                string json = gson.tojson(new string[] { city, region, country });\n                conn.hset("cityid2city:", cityid, json);\n            }\n        } catch (exception e) {\n            throw new runtimeexception(e);\n        } finally {\n            try {\n                reader.close();\n            } catch (exception e) {\n                // ignore\n            }\n        }\n    }\n\n\n# 查找 ip 所属城市\n\n操作步骤：\n\n 1. 将要查找的 ip 地址转为整数值；\n 2. 查找所有分值小于等于要查找的 ip 地址的地址，取出其中最大分值的那个记录；\n 3. 用找到的记录所对应的城市 id 去检索城市信息。\n\n    public string[] findcitybyip(jedis conn, string ipaddress) {\n        int score = iptoscore(ipaddress);\n        set<string> results = conn.zrevrangebyscore("ip2cityid:", score, 0, 0, 1);\n        if (results.size() == 0) {\n            return null;\n        }\n\n        string cityid = results.iterator().next();\n        cityid = cityid.substring(0, cityid.indexof(\'_\'));\n        return new gson().fromjson(conn.hget("cityid2city:", cityid), string[].class);\n    }\n\n\n\n# 案例-服务的发现与配置\n\n\n# 案例-自动补全\n\n需求：根据用户输入，自动补全信息，如：联系人、商品名等。\n\n * 典型场景一：社交网站后台记录用户最近联系过的 100 个好友，当用户查找好友时，根据输入的关键字自动补全姓名。\n * 典型场景二：电商网站后台记录用户最近浏览过的 10 件商品，当用户查找商品是，根据输入的关键字自动补全商品名称。\n\n数据模型：使用 redis 的 list 类型存储最近联系人列表。\n\n构建自动补全列表通常有以下操作：\n\n * 如果指定联系人已经存在于最近联系人列表里，那么从列表里移除他。对应 lrem 命令。\n * 将指定联系人添加到最近联系人列表的最前面。对应 lpush 命令。\n * 添加操作完成后，如果联系人列表中的数量超过 100 个，进行裁剪操作。对应 ltrim 命令。\n\n\n# 案例-广告定向\n\n\n# 案例-职位搜索\n\n需求：在一个招聘网站上，求职者有自己的技能清单；用人公司的职位有必要的技能清单。用人公司需要查询满足自己职位要求的求职者；求职者需要查询自己可以投递简历的职位。\n\n关键数据模型：使用 set 类型存储求职者的技能列表，使用 set 类型存储职位的技能列表。\n\n关键操作：使用 sdiff 命令对比两个 set 的差异，返回 empty 表示匹配要求。\n\nredis cli 示例：\n\n# -----------------------------------------------------------\n# redis 职位搜索数据模型示例\n# -----------------------------------------------------------\n\n# （1）职位技能表：使用 set 存储\n# job:001 职位添加 4 种技能\nsadd job:001 skill:001\nsadd job:001 skill:002\nsadd job:001 skill:003\nsadd job:001 skill:004\n\n# job:002 职位添加 3 种技能\nsadd job:002 skill:001\nsadd job:002 skill:002\nsadd job:002 skill:003\n\n# job:003 职位添加 2 种技能\nsadd job:003 skill:001\nsadd job:003 skill:003\n\n# 查看\nsmembers job:001\nsmembers job:002\nsmembers job:003\n\n# （2）求职者技能表：使用 set 存储\nsadd interviewee:001 skill:001\nsadd interviewee:001 skill:003\n\nsadd interviewee:002 skill:001\nsadd interviewee:002 skill:002\nsadd interviewee:002 skill:003\nsadd interviewee:002 skill:004\nsadd interviewee:002 skill:005\n\n# 查看\nsmembers interviewee:001\nsmembers interviewee:002\n\n# （3）求职者遍历查找自己符合要求的职位（返回结果为 empty 表示要求的技能全部命中）\n# 比较职位技能清单和求职者技能清单的差异\nsdiff job:001 interviewee:001\nsdiff job:002 interviewee:001\nsdiff job:003 interviewee:001\n\nsdiff job:001 interviewee:002\nsdiff job:002 interviewee:002\nsdiff job:003 interviewee:002\n\n# （4）用人公司根据遍历查找符合自己职位要求的求职者（返回结果为 empty 表示要求的技能全部命中）\n# 比较职位技能清单和求职者技能清单的差异\nsdiff interviewee:001 job:001\nsdiff interviewee:002 job:001\n\nsdiff interviewee:001 job:002\nsdiff interviewee:002 job:002\n\nsdiff interviewee:001 job:003\nsdiff interviewee:002 job:003\n\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考\n * 文章\n   * 一看就懂系列之 详解 redis 的 bitmap 在亿级项目中的应用\n   * fast, easy, realtime metrics using redis bitmaps',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 持久化",frontmatter:{title:"Redis 持久化",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","持久化"],permalink:"/pages/4de901/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/04.Redis%E6%8C%81%E4%B9%85%E5%8C%96.html",relativePath:"12.数据库/05.KV数据库/01.Redis/04.Redis持久化.md",key:"v-80c82fb0",path:"/pages/4de901/",headers:[{level:2,title:"一、RDB",slug:"一、rdb",normalizedTitle:"一、rdb",charIndex:493},{level:3,title:"RDB 简介",slug:"rdb-简介",normalizedTitle:"rdb 简介",charIndex:503},{level:4,title:"RDB 的优点",slug:"rdb-的优点",normalizedTitle:"rdb 的优点",charIndex:721},{level:4,title:"RDB 的缺点",slug:"rdb-的缺点",normalizedTitle:"rdb 的缺点",charIndex:951},{level:3,title:"RDB 的创建",slug:"rdb-的创建",normalizedTitle:"rdb 的创建",charIndex:1358},{level:4,title:"自动间隔保存",slug:"自动间隔保存",normalizedTitle:"自动间隔保存",charIndex:1625},{level:3,title:"RDB 的载入",slug:"rdb-的载入",normalizedTitle:"rdb 的载入",charIndex:1950},{level:3,title:"RDB 的文件结构",slug:"rdb-的文件结构",normalizedTitle:"rdb 的文件结构",charIndex:2173},{level:3,title:"RDB 的配置",slug:"rdb-的配置",normalizedTitle:"rdb 的配置",charIndex:2329},{level:2,title:"二、AOF",slug:"二、aof",normalizedTitle:"二、aof",charIndex:2773},{level:3,title:"AOF 简介",slug:"aof-简介",normalizedTitle:"aof 简介",charIndex:2783},{level:4,title:"AOF 的优点",slug:"aof-的优点",normalizedTitle:"aof 的优点",charIndex:3e3},{level:4,title:"AOF 的缺点",slug:"aof-的缺点",normalizedTitle:"aof 的缺点",charIndex:3773},{level:3,title:"AOF 的创建",slug:"aof-的创建",normalizedTitle:"aof 的创建",charIndex:4011},{level:3,title:"AOF 的载入",slug:"aof-的载入",normalizedTitle:"aof 的载入",charIndex:4613},{level:3,title:"AOF 的重写",slug:"aof-的重写",normalizedTitle:"aof 的重写",charIndex:4886},{level:4,title:"AOF 后台重写",slug:"aof-后台重写",normalizedTitle:"aof 后台重写",charIndex:5227},{level:3,title:"AOF 的配置",slug:"aof-的配置",normalizedTitle:"aof 的配置",charIndex:5884},{level:2,title:"三、RDB 和 AOF",slug:"三、rdb-和-aof",normalizedTitle:"三、rdb 和 aof",charIndex:6556},{level:3,title:"如何选择持久化",slug:"如何选择持久化",normalizedTitle:"如何选择持久化",charIndex:6655},{level:3,title:"RDB 切换为 AOF",slug:"rdb-切换为-aof",normalizedTitle:"rdb 切换为 aof",charIndex:6886},{level:3,title:"AOF 和 RDB 的相互作用",slug:"aof-和-rdb-的相互作用",normalizedTitle:"aof 和 rdb 的相互作用",charIndex:7350},{level:2,title:"四、Redis 备份",slug:"四、redis-备份",normalizedTitle:"四、redis 备份",charIndex:7567},{level:3,title:"备份过程",slug:"备份过程",normalizedTitle:"备份过程",charIndex:7626},{level:3,title:"容灾备份",slug:"容灾备份",normalizedTitle:"容灾备份",charIndex:7864},{level:2,title:"五、要点总结",slug:"五、要点总结",normalizedTitle:"五、要点总结",charIndex:7970},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7983}],headersStr:"一、RDB RDB 简介 RDB 的优点 RDB 的缺点 RDB 的创建 自动间隔保存 RDB 的载入 RDB 的文件结构 RDB 的配置 二、AOF AOF 简介 AOF 的优点 AOF 的缺点 AOF 的创建 AOF 的载入 AOF 的重写 AOF 后台重写 AOF 的配置 三、RDB 和 AOF 如何选择持久化 RDB 切换为 AOF AOF 和 RDB 的相互作用 四、Redis 备份 备份过程 容灾备份 五、要点总结 参考资料",content:"# Redis 持久化\n\n> Redis 支持持久化，即把数据存储到硬盘中。\n> \n> Redis 提供了两种持久化方式：\n> \n>  * RDB 快照（snapshot） - 将存在于某一时刻的所有数据都写入到硬盘中。\n>  * 只追加文件（append-only file，AOF） - 它会在执行写命令时，将被执行的写命令复制到硬盘中。\n> \n> 这两种持久化方式既可以同时使用，也可以单独使用。\n> \n> 将内存中的数据存储到硬盘的一个主要原因是为了在之后重用数据，或者是为了防止系统故障而将数据备份到一个远程位置。另外，存储在 Redis 里面的数据有可能是经过长时间计算得出的，或者有程序正在使用 Redis 存储的数据进行计算，所以用户会希望自己可以将这些数据存储起来以便之后使用，这样就不必重新计算了。\n> \n> Redis 提供了两种持久方式：RDB 和 AOF。你可以同时开启两种持久化方式。在这种情况下, 当 redis 重启的时候会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整。\n\n\n# 一、RDB\n\n\n# RDB 简介\n\nRDB 即快照方式，它将某个时间点的所有 Redis 数据保存到一个经过压缩的二进制文件（RDB 文件）中。\n\n创建 RDB 后，用户可以对 RDB 进行备份，可以将 RDB 复制到其他服务器从而创建具有相同数据的服务器副本，还可以在重启服务器时使用。一句话来说：RDB 适合作为 冷备。\n\nRDB 既可以手动执行，也可以根据服务器配置选项定期执行。该功能可以将某个时间点的数据库状态保存到一个 RDB 文件中。\n\n# RDB 的优点\n\n * RDB 文件非常紧凑，适合作为冷备。比如你可以在每个小时报保存一下过去 24 小时内的数据，同时每天保存过去 30 天的数据，这样即使出了问题你也可以根据需求恢复到不同版本的数据集。\n * 快照在保存 RDB 文件时父进程唯一需要做的就是 fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，所以快照持久化方式可以最大化 Redis 的性能。\n * 恢复大数据集时，RDB 比 AOF 更快。\n\n# RDB 的缺点\n\n * 如果系统发生故障，将会丢失最后一次创建快照之后的数据。如果你希望在 Redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么 快照不适合你。虽然你可以配置不同的 save 时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作)，是 Redis 要完整的保存整个数据集是一个比较繁重的工作，你通常会每隔 5 分钟或者更久做一次完整的保存，万一在 Redis 意外宕机，你可能会丢失几分钟的数据。\n * 如果数据量很大，保存快照的时间会很长。快照需要经常 fork 子进程来保存数据集到硬盘上。当数据集比较大的时候，fork 的过程是非常耗时的，可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。如果数据集巨大并且 CPU 性能不是很好的情况下，这种情况会持续 1 秒。AOF 也需要 fork，但是你可以调节重写日志文件的频率来提高数据集的耐久度。\n\n\n# RDB 的创建\n\n有两个 Redis 命令可以用于生成 RDB 文件：SAVE 和 BGSAVE。\n\n * SAVE 命令会阻塞 Redis 服务器进程，直到 RDB 创建完成为止，在阻塞期间，服务器不能响应任何命令请求。\n * BGSAVE 命令会派生出（fork）一个子进程，然后由子进程负责创建 RDB 文件，服务器进程（父进程）继续处理命令请求。\n\n> 🔔 注意：BGSAVE 命令执行期间，SAVE、BGSAVE、BGREWRITEAOF 三个命令会被拒绝，以免与当前的 BGSAVE 操作产生竞态条件，降低性能。\n\n# 自动间隔保存\n\nRedis 允许用户通过设置服务器配置的 save 选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令。\n\n用户可以通过 save 选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行 BGSAVE 命令。\n\n举例来说，redis.conf 中设置了如下配置：\n\nsave 900 1       -- 900 秒内，至少对数据库进行了 1 次修改\nsave 300 10      -- 300 秒内，至少对数据库进行了 10 次修改\nsave 60 10000    -- 60 秒内，至少对数据库进行了 10000 次修改\n\n\n只要满足以上任意条件，Redis 服务就会执行 BGSAVE 命令。\n\n\n# RDB 的载入\n\nRDB 文件的载入工作是在服务器启动时自动执行的，Redis 并没有专门用于载入 RDB 文件的命令。\n\n服务器载入 RDB 文件期间，会一直处于阻塞状态，直到载入完成为止。\n\n> 🔔 注意：因为 AOF 通常更新频率比 RDB 高，所以丢失数据相对更少。基于这个原因，Redis 有以下默认行为：\n> \n>  * 只有在关闭 AOF 功能的情况下，才会使用 RDB 还原数据，否则优先使用 AOF 文件来还原数据。\n\n\n# RDB 的文件结构\n\nRDB 文件是一个经过压缩的二进制文件，由多个部分组成。\n\n对于不同类型（STRING、HASH、LIST、SET、SORTED SET）的键值对，RDB 文件会使用不同的方式来保存它们。\n\n\n\nRedis 本身提供了一个 RDB 文件检查工具 redis-check-dump。\n\n\n# RDB 的配置\n\nRedis RDB 默认配置如下：\n\nsave 900 1\nsave 300 10\nsave 60 10000\nstop-writes-on-bgsave-error yes\nrdbcompression yes\nrdbchecksum yes\ndbfilename dump.rdb\ndir ./\n\n\nRedis 的配置文件 redis.conf 中与 RDB 有关的选项：\n\n * save - Redis 会根据 save 选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令。\n\n * stop-writes-on-bgsave-error - 当 BGSAVE 命令出现错误时停止写 RDB 文件\n\n * rdbcompression - RDB 文件开启压缩功能。\n\n * rdbchecksum - 对 RDB 文件进行校验。\n\n * dbfilename - RDB 文件名。\n\n * dir - RDB 文件和 AOF 文件的存储路径。\n\n\n# 二、AOF\n\n\n# AOF 简介\n\nAOF(Append Only File) 是以 文本日志形式 将 所有写命令以 Redis 命令请求协议格式追加到 AOF 文件的末尾，以此来记录数据的变化。当服务器重启时，会重新载入和执行 AOF 文件中的命令，就可以恢复原始的数据。AOF 适合作为 热备。\n\nAOF 可以通过 appendonly yes 配置选项来开启。\n\n命令请求会先保存到 AOF 缓冲区中，之后再定期写入并同步到 AOF 文件。\n\n# AOF 的优点\n\n * 如果系统发生故障，AOF 丢失数据比 RDB 少。你可以使用不同的 fsync 策略：无 fsync；每秒 fsync；每次写的时候 fsync。使用默认的每秒 fsync 策略，Redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求)，一旦出现故障，你最多丢失 1 秒的数据。\n * AOF 文件可修复 - AOF 文件是一个只进行追加的日志文件，所以不需要写入 seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也也可使用 redis-check-aof 工具修复这些问题。\n * AOF 文件可压缩。Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写：重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。\n * AOF 文件可读 - AOF 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 Redis 命令的格式保存。因此 AOF 文件的内容非常容易被人读懂，对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单。举个例子，如果你不小心执行了 FLUSHALL 命令，但只要 AOF 文件未被重写，那么只要停止服务器，移除 AOF 文件末尾的 FLUSHALL 命令，并重启 Redis ，就可以将数据集恢复到 FLUSHALL 执行之前的状态。\n\n# AOF 的缺点\n\n * AOF 文件体积一般比 RDB 大 - 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。\n * 恢复大数据集时，AOF 比 RDB 慢。 - 根据所使用的 fsync 策略，AOF 的速度可能会慢于快照。在一般情况下，每秒 fsync 的性能依然非常高，而关闭 fsync 可以让 AOF 的速度和快照一样快，即使在高负荷之下也是如此。不过在处理巨大的写入载入时，快照可以提供更有保证的最大延迟时间（latency）。\n\n\n# AOF 的创建\n\nRedis 命令请求会先保存到 AOF 缓冲区，再定期写入并同步到 AOF 文件。\n\nAOF 的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤。\n\n * 命令追加 - 当 Redis 服务器开启 AOF 功能时，服务器在执行完一个写命令后，会以 Redis 命令协议格式将被执行的写命令追加到 AOF 缓冲区的末尾。\n * 文件写入和文件同步 - Redis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复。而时间事件则负责执行定时运行的函数。因为服务器在处理文件事件时可能会执行写命令，这些写命令会被追加到 AOF 缓冲区，服务器每次结束事件循环前，都会根据 appendfsync 选项来判断 AOF 缓冲区内容是否需要写入和同步到 AOF 文件中。\n\nappendfsync 不同选项决定了不同的持久化行为：\n\n * always - 将缓冲区所有内容写入并同步到 AOF 文件。\n * everysec - 将缓冲区所有内容写入到 AOF 文件，如果上次同步 AOF 文件的时间距离现在超过一秒钟，那么再次对 AOF 文件进行同步，这个同步操作是有一个线程专门负责执行的。\n * no - 将缓冲区所有内容写入到 AOF 文件，但并不对 AOF 文件进行同步，何时同步由操作系统决定。\n\n\n# AOF 的载入\n\n因为 AOF 文件中包含了重建数据库所需的所有写命令，所以服务器只要载入并执行一遍 AOF 文件中保存的写命令，就可以还原服务器关闭前的数据库状态。\n\nAOF 载入过程如下：\n\n 1. 服务器启动载入程序。\n 2. 创建一个伪客户端。因为 Redis 命令只能在客户端上下文中执行，所以需要创建一个伪客户端来载入、执行 AOF 文件中记录的命令。\n 3. 从 AOF 文件中分析并读取一条写命令。\n 4. 使用伪客户端执行写命令。\n 5. 循环执行步骤 3、4，直到所有写命令都被处理完毕为止。\n 6. 载入完毕。\n\n\n\n\n# AOF 的重写\n\n随着 Redis 不断运行，AOF 的体积也会不断增长，这将导致两个问题：\n\n * AOF 耗尽磁盘可用空间。\n * Redis 重启后需要执行 AOF 文件记录的所有写命令来还原数据集，如果 AOF 过大，则还原操作执行的时间就会非常长。\n\n为了解决 AOF 体积膨胀问题，Redis 提供了 AOF 重写功能，来对 AOF 文件进行压缩。AOF 重写可以产生一个新的 AOF 文件，这个新的 AOF 文件和原来的 AOF 文件所保存的数据库状态一致，但体积更小。\n\nAOF 重写并非读取和分析现有 AOF 文件的内容，而是直接从数据库中读取当前的数据库状态。即依次读取数据库中的每个键值对，然后用一条命令去记录该键值对，以此代替之前可能存在冗余的命令。\n\n# AOF 后台重写\n\n作为一种辅助性功能，显然 Redis 并不想在 AOF 重写时阻塞 Redis 服务接收其他命令。因此，Redis 决定通过 BGREWRITEAOF 命令创建一个子进程，然后由子进程负责对 AOF 文件进行重写，这与 BGSAVE 原理类似。\n\n * 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区。当 AOF 重写子进程开始工作后，Redis 每执行完一个写命令，会同时将这个命令发送给 AOF 缓冲区和 AOF 重写缓冲区。\n * 由于彼此不是在同一个进程中工作，AOF 重写不影响 AOF 写入和同步。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新旧两个 AOF 文件所保存的数据库状态一致。\n * 最后，服务器用新的 AOF 文件替换就的 AOF 文件，以此来完成 AOF 重写操作。\n\n\n\n可以通过设置 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size，使得 Redis 在满足条件时，自动执行 BGREWRITEAOF。\n\n假设配置如下：\n\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n\n表明，当 AOF 大于 64MB，且 AOF 体积比上一次重写后的体积大了至少 100% 时，执行 BGREWRITEAOF。\n\n\n# AOF 的配置\n\nAOF 的默认配置：\n\nappendonly no\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n\nAOF 持久化通过在 redis.conf 中的 appendonly yes 配置选项来开启。\n\n * appendonly - 开启 AOF 功能。\n * appendfilename - AOF 文件名。\n * appendfsync - 用于设置同步频率，它有以下可选项：\n   * always - 每个 Redis 写命令都要同步写入硬盘。这样做会严重降低 Redis 的速度。\n   * everysec - 每秒执行一次同步，显示地将多个写命令同步到硬盘。为了兼顾数据安全和写入性能，推荐使用 appendfsync everysec 选项。Redis 每秒同步一次 AOF 文件时的性能和不使用任何持久化特性时的性能相差无几。\n   * no - 让操作系统来决定应该何时进行同步。\n * no-appendfsync-on-rewrite - AOF 重写时不支持追加命令。\n * auto-aof-rewrite-percentage - AOF 重写百分比。\n * auto-aof-rewrite-min-size - AOF 重写文件的最小大小。\n * dir - RDB 文件和 AOF 文件的存储路径。\n\n\n# 三、RDB 和 AOF\n\n> 当 Redis 启动时， 如果 RDB 和 AOF 功能都开启了，那么程序会优先使用 AOF 文件来恢复数据集，因为 AOF 文件所保存的数据通常是最完整的。\n\n\n# 如何选择持久化\n\n * 如果不关心数据丢失，可以不持久化。\n * 如果可以承受数分钟以内的数据丢失，可以只使用 RDB。\n * 如果不能承受数分钟以内的数据丢失，可以同时使用 RDB 和 AOF。\n\n有很多用户都只使用 AOF 持久化， 但并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份，并且快照恢复数据集的速度也要比 AOF 恢复的速度要快，除此之外，使用快照还可以避免之前提到的 AOF 程序的 bug 。\n\n\n# RDB 切换为 AOF\n\n在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换为 AOF ：\n\n * 为最新的 dump.rdb 文件创建一个备份。\n * 将备份放到一个安全的地方。\n * 执行以下两条命令:\n * redis-cli config set appendonly yes\n * redis-cli config set save\n * 确保写命令会被正确地追加到 AOF 文件的末尾。\n * 执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。\n\n执行的第二条命令用于关闭快照功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用快照和 AOF 这两种持久化功能。\n\n> 🔔 重要：别忘了在 redis.conf 中打开 AOF 功能！否则的话，服务器重启之后，之前通过 CONFIG SET 设置的配置就会被遗忘，程序会按原来的配置来启动服务器。\n\n\n# AOF 和 RDB 的相互作用\n\nBGSAVE 和 BGREWRITEAOF 命令不可以同时执行。这是为了避免两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。\n\n如果 BGSAVE 正在执行，并且用户显示地调用 BGREWRITEAOF 命令，那么服务器将向用户回复一个 OK 状态，并告知用户，BGREWRITEAOF 已经被预定执行。一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。\n\n\n# 四、Redis 备份\n\n应该确保 Redis 数据有完整的备份。\n\n备份 Redis 数据建议采用 RDB。\n\n\n# 备份过程\n\n 1. 创建一个定期任务（cron job），每小时将一个 RDB 文件备份到一个文件夹，并且每天将一个 RDB 文件备份到另一个文件夹。\n 2. 确保快照的备份都带有相应的日期和时间信息，每次执行定期任务脚本时，使用 find 命令来删除过期的快照：比如说，你可以保留最近 48 小时内的每小时快照，还可以保留最近一两个月的每日快照。\n 3. 至少每天一次，将 RDB 备份到你的数据中心之外，或者至少是备份到你运行 Redis 服务器的物理机器之外。\n\n\n# 容灾备份\n\nRedis 的容灾备份基本上就是对数据进行备份，并将这些备份传送到多个不同的外部数据中心。\n\n容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时，仍然让数据处于安全状态。\n\n\n# 五、要点总结\n\n\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考",normalizedContent:"# redis 持久化\n\n> redis 支持持久化，即把数据存储到硬盘中。\n> \n> redis 提供了两种持久化方式：\n> \n>  * rdb 快照（snapshot） - 将存在于某一时刻的所有数据都写入到硬盘中。\n>  * 只追加文件（append-only file，aof） - 它会在执行写命令时，将被执行的写命令复制到硬盘中。\n> \n> 这两种持久化方式既可以同时使用，也可以单独使用。\n> \n> 将内存中的数据存储到硬盘的一个主要原因是为了在之后重用数据，或者是为了防止系统故障而将数据备份到一个远程位置。另外，存储在 redis 里面的数据有可能是经过长时间计算得出的，或者有程序正在使用 redis 存储的数据进行计算，所以用户会希望自己可以将这些数据存储起来以便之后使用，这样就不必重新计算了。\n> \n> redis 提供了两种持久方式：rdb 和 aof。你可以同时开启两种持久化方式。在这种情况下, 当 redis 重启的时候会优先载入 aof 文件来恢复原始的数据，因为在通常情况下 aof 文件保存的数据集要比 rdb 文件保存的数据集要完整。\n\n\n# 一、rdb\n\n\n# rdb 简介\n\nrdb 即快照方式，它将某个时间点的所有 redis 数据保存到一个经过压缩的二进制文件（rdb 文件）中。\n\n创建 rdb 后，用户可以对 rdb 进行备份，可以将 rdb 复制到其他服务器从而创建具有相同数据的服务器副本，还可以在重启服务器时使用。一句话来说：rdb 适合作为 冷备。\n\nrdb 既可以手动执行，也可以根据服务器配置选项定期执行。该功能可以将某个时间点的数据库状态保存到一个 rdb 文件中。\n\n# rdb 的优点\n\n * rdb 文件非常紧凑，适合作为冷备。比如你可以在每个小时报保存一下过去 24 小时内的数据，同时每天保存过去 30 天的数据，这样即使出了问题你也可以根据需求恢复到不同版本的数据集。\n * 快照在保存 rdb 文件时父进程唯一需要做的就是 fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 io 操作，所以快照持久化方式可以最大化 redis 的性能。\n * 恢复大数据集时，rdb 比 aof 更快。\n\n# rdb 的缺点\n\n * 如果系统发生故障，将会丢失最后一次创建快照之后的数据。如果你希望在 redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么 快照不适合你。虽然你可以配置不同的 save 时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作)，是 redis 要完整的保存整个数据集是一个比较繁重的工作，你通常会每隔 5 分钟或者更久做一次完整的保存，万一在 redis 意外宕机，你可能会丢失几分钟的数据。\n * 如果数据量很大，保存快照的时间会很长。快照需要经常 fork 子进程来保存数据集到硬盘上。当数据集比较大的时候，fork 的过程是非常耗时的，可能会导致 redis 在一些毫秒级内不能响应客户端的请求。如果数据集巨大并且 cpu 性能不是很好的情况下，这种情况会持续 1 秒。aof 也需要 fork，但是你可以调节重写日志文件的频率来提高数据集的耐久度。\n\n\n# rdb 的创建\n\n有两个 redis 命令可以用于生成 rdb 文件：save 和 bgsave。\n\n * save 命令会阻塞 redis 服务器进程，直到 rdb 创建完成为止，在阻塞期间，服务器不能响应任何命令请求。\n * bgsave 命令会派生出（fork）一个子进程，然后由子进程负责创建 rdb 文件，服务器进程（父进程）继续处理命令请求。\n\n> 🔔 注意：bgsave 命令执行期间，save、bgsave、bgrewriteaof 三个命令会被拒绝，以免与当前的 bgsave 操作产生竞态条件，降低性能。\n\n# 自动间隔保存\n\nredis 允许用户通过设置服务器配置的 save 选项，让服务器每隔一段时间自动执行一次 bgsave 命令。\n\n用户可以通过 save 选项设置多个保存条件，但只要其中任意一个条件被满足，服务器就会执行 bgsave 命令。\n\n举例来说，redis.conf 中设置了如下配置：\n\nsave 900 1       -- 900 秒内，至少对数据库进行了 1 次修改\nsave 300 10      -- 300 秒内，至少对数据库进行了 10 次修改\nsave 60 10000    -- 60 秒内，至少对数据库进行了 10000 次修改\n\n\n只要满足以上任意条件，redis 服务就会执行 bgsave 命令。\n\n\n# rdb 的载入\n\nrdb 文件的载入工作是在服务器启动时自动执行的，redis 并没有专门用于载入 rdb 文件的命令。\n\n服务器载入 rdb 文件期间，会一直处于阻塞状态，直到载入完成为止。\n\n> 🔔 注意：因为 aof 通常更新频率比 rdb 高，所以丢失数据相对更少。基于这个原因，redis 有以下默认行为：\n> \n>  * 只有在关闭 aof 功能的情况下，才会使用 rdb 还原数据，否则优先使用 aof 文件来还原数据。\n\n\n# rdb 的文件结构\n\nrdb 文件是一个经过压缩的二进制文件，由多个部分组成。\n\n对于不同类型（string、hash、list、set、sorted set）的键值对，rdb 文件会使用不同的方式来保存它们。\n\n\n\nredis 本身提供了一个 rdb 文件检查工具 redis-check-dump。\n\n\n# rdb 的配置\n\nredis rdb 默认配置如下：\n\nsave 900 1\nsave 300 10\nsave 60 10000\nstop-writes-on-bgsave-error yes\nrdbcompression yes\nrdbchecksum yes\ndbfilename dump.rdb\ndir ./\n\n\nredis 的配置文件 redis.conf 中与 rdb 有关的选项：\n\n * save - redis 会根据 save 选项，让服务器每隔一段时间自动执行一次 bgsave 命令。\n\n * stop-writes-on-bgsave-error - 当 bgsave 命令出现错误时停止写 rdb 文件\n\n * rdbcompression - rdb 文件开启压缩功能。\n\n * rdbchecksum - 对 rdb 文件进行校验。\n\n * dbfilename - rdb 文件名。\n\n * dir - rdb 文件和 aof 文件的存储路径。\n\n\n# 二、aof\n\n\n# aof 简介\n\naof(append only file) 是以 文本日志形式 将 所有写命令以 redis 命令请求协议格式追加到 aof 文件的末尾，以此来记录数据的变化。当服务器重启时，会重新载入和执行 aof 文件中的命令，就可以恢复原始的数据。aof 适合作为 热备。\n\naof 可以通过 appendonly yes 配置选项来开启。\n\n命令请求会先保存到 aof 缓冲区中，之后再定期写入并同步到 aof 文件。\n\n# aof 的优点\n\n * 如果系统发生故障，aof 丢失数据比 rdb 少。你可以使用不同的 fsync 策略：无 fsync；每秒 fsync；每次写的时候 fsync。使用默认的每秒 fsync 策略，redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求)，一旦出现故障，你最多丢失 1 秒的数据。\n * aof 文件可修复 - aof 文件是一个只进行追加的日志文件，所以不需要写入 seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也也可使用 redis-check-aof 工具修复这些问题。\n * aof 文件可压缩。redis 可以在 aof 文件体积变得过大时，自动地在后台对 aof 进行重写：重写后的新 aof 文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为 redis 在创建新 aof 文件的过程中，会继续将命令追加到现有的 aof 文件里面，即使重写过程中发生停机，现有的 aof 文件也不会丢失。而一旦新 aof 文件创建完毕，redis 就会从旧 aof 文件切换到新 aof 文件，并开始对新 aof 文件进行追加操作。\n * aof 文件可读 - aof 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 redis 命令的格式保存。因此 aof 文件的内容非常容易被人读懂，对文件进行分析（parse）也很轻松。 导出（export） aof 文件也非常简单。举个例子，如果你不小心执行了 flushall 命令，但只要 aof 文件未被重写，那么只要停止服务器，移除 aof 文件末尾的 flushall 命令，并重启 redis ，就可以将数据集恢复到 flushall 执行之前的状态。\n\n# aof 的缺点\n\n * aof 文件体积一般比 rdb 大 - 对于相同的数据集来说，aof 文件的体积通常要大于 rdb 文件的体积。\n * 恢复大数据集时，aof 比 rdb 慢。 - 根据所使用的 fsync 策略，aof 的速度可能会慢于快照。在一般情况下，每秒 fsync 的性能依然非常高，而关闭 fsync 可以让 aof 的速度和快照一样快，即使在高负荷之下也是如此。不过在处理巨大的写入载入时，快照可以提供更有保证的最大延迟时间（latency）。\n\n\n# aof 的创建\n\nredis 命令请求会先保存到 aof 缓冲区，再定期写入并同步到 aof 文件。\n\naof 的实现可以分为命令追加（append）、文件写入、文件同步（sync）三个步骤。\n\n * 命令追加 - 当 redis 服务器开启 aof 功能时，服务器在执行完一个写命令后，会以 redis 命令协议格式将被执行的写命令追加到 aof 缓冲区的末尾。\n * 文件写入和文件同步 - redis 的服务器进程就是一个事件循环，这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复。而时间事件则负责执行定时运行的函数。因为服务器在处理文件事件时可能会执行写命令，这些写命令会被追加到 aof 缓冲区，服务器每次结束事件循环前，都会根据 appendfsync 选项来判断 aof 缓冲区内容是否需要写入和同步到 aof 文件中。\n\nappendfsync 不同选项决定了不同的持久化行为：\n\n * always - 将缓冲区所有内容写入并同步到 aof 文件。\n * everysec - 将缓冲区所有内容写入到 aof 文件，如果上次同步 aof 文件的时间距离现在超过一秒钟，那么再次对 aof 文件进行同步，这个同步操作是有一个线程专门负责执行的。\n * no - 将缓冲区所有内容写入到 aof 文件，但并不对 aof 文件进行同步，何时同步由操作系统决定。\n\n\n# aof 的载入\n\n因为 aof 文件中包含了重建数据库所需的所有写命令，所以服务器只要载入并执行一遍 aof 文件中保存的写命令，就可以还原服务器关闭前的数据库状态。\n\naof 载入过程如下：\n\n 1. 服务器启动载入程序。\n 2. 创建一个伪客户端。因为 redis 命令只能在客户端上下文中执行，所以需要创建一个伪客户端来载入、执行 aof 文件中记录的命令。\n 3. 从 aof 文件中分析并读取一条写命令。\n 4. 使用伪客户端执行写命令。\n 5. 循环执行步骤 3、4，直到所有写命令都被处理完毕为止。\n 6. 载入完毕。\n\n\n\n\n# aof 的重写\n\n随着 redis 不断运行，aof 的体积也会不断增长，这将导致两个问题：\n\n * aof 耗尽磁盘可用空间。\n * redis 重启后需要执行 aof 文件记录的所有写命令来还原数据集，如果 aof 过大，则还原操作执行的时间就会非常长。\n\n为了解决 aof 体积膨胀问题，redis 提供了 aof 重写功能，来对 aof 文件进行压缩。aof 重写可以产生一个新的 aof 文件，这个新的 aof 文件和原来的 aof 文件所保存的数据库状态一致，但体积更小。\n\naof 重写并非读取和分析现有 aof 文件的内容，而是直接从数据库中读取当前的数据库状态。即依次读取数据库中的每个键值对，然后用一条命令去记录该键值对，以此代替之前可能存在冗余的命令。\n\n# aof 后台重写\n\n作为一种辅助性功能，显然 redis 并不想在 aof 重写时阻塞 redis 服务接收其他命令。因此，redis 决定通过 bgrewriteaof 命令创建一个子进程，然后由子进程负责对 aof 文件进行重写，这与 bgsave 原理类似。\n\n * 在执行 bgrewriteaof 命令时，redis 服务器会维护一个 aof 重写缓冲区。当 aof 重写子进程开始工作后，redis 每执行完一个写命令，会同时将这个命令发送给 aof 缓冲区和 aof 重写缓冲区。\n * 由于彼此不是在同一个进程中工作，aof 重写不影响 aof 写入和同步。当子进程完成创建新 aof 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 aof 文件的末尾，使得新旧两个 aof 文件所保存的数据库状态一致。\n * 最后，服务器用新的 aof 文件替换就的 aof 文件，以此来完成 aof 重写操作。\n\n\n\n可以通过设置 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size，使得 redis 在满足条件时，自动执行 bgrewriteaof。\n\n假设配置如下：\n\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n\n表明，当 aof 大于 64mb，且 aof 体积比上一次重写后的体积大了至少 100% 时，执行 bgrewriteaof。\n\n\n# aof 的配置\n\naof 的默认配置：\n\nappendonly no\nappendfsync everysec\nno-appendfsync-on-rewrite no\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n\naof 持久化通过在 redis.conf 中的 appendonly yes 配置选项来开启。\n\n * appendonly - 开启 aof 功能。\n * appendfilename - aof 文件名。\n * appendfsync - 用于设置同步频率，它有以下可选项：\n   * always - 每个 redis 写命令都要同步写入硬盘。这样做会严重降低 redis 的速度。\n   * everysec - 每秒执行一次同步，显示地将多个写命令同步到硬盘。为了兼顾数据安全和写入性能，推荐使用 appendfsync everysec 选项。redis 每秒同步一次 aof 文件时的性能和不使用任何持久化特性时的性能相差无几。\n   * no - 让操作系统来决定应该何时进行同步。\n * no-appendfsync-on-rewrite - aof 重写时不支持追加命令。\n * auto-aof-rewrite-percentage - aof 重写百分比。\n * auto-aof-rewrite-min-size - aof 重写文件的最小大小。\n * dir - rdb 文件和 aof 文件的存储路径。\n\n\n# 三、rdb 和 aof\n\n> 当 redis 启动时， 如果 rdb 和 aof 功能都开启了，那么程序会优先使用 aof 文件来恢复数据集，因为 aof 文件所保存的数据通常是最完整的。\n\n\n# 如何选择持久化\n\n * 如果不关心数据丢失，可以不持久化。\n * 如果可以承受数分钟以内的数据丢失，可以只使用 rdb。\n * 如果不能承受数分钟以内的数据丢失，可以同时使用 rdb 和 aof。\n\n有很多用户都只使用 aof 持久化， 但并不推荐这种方式： 因为定时生成 rdb 快照（snapshot）非常便于进行数据库备份，并且快照恢复数据集的速度也要比 aof 恢复的速度要快，除此之外，使用快照还可以避免之前提到的 aof 程序的 bug 。\n\n\n# rdb 切换为 aof\n\n在 redis 2.2 或以上版本，可以在不重启的情况下，从 rdb 切换为 aof ：\n\n * 为最新的 dump.rdb 文件创建一个备份。\n * 将备份放到一个安全的地方。\n * 执行以下两条命令:\n * redis-cli config set appendonly yes\n * redis-cli config set save\n * 确保写命令会被正确地追加到 aof 文件的末尾。\n * 执行的第一条命令开启了 aof 功能： redis 会阻塞直到初始 aof 文件创建完成为止， 之后 redis 会继续处理命令请求， 并开始将写入命令追加到 aof 文件末尾。\n\n执行的第二条命令用于关闭快照功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用快照和 aof 这两种持久化功能。\n\n> 🔔 重要：别忘了在 redis.conf 中打开 aof 功能！否则的话，服务器重启之后，之前通过 config set 设置的配置就会被遗忘，程序会按原来的配置来启动服务器。\n\n\n# aof 和 rdb 的相互作用\n\nbgsave 和 bgrewriteaof 命令不可以同时执行。这是为了避免两个 redis 后台进程同时对磁盘进行大量的 i/o 操作。\n\n如果 bgsave 正在执行，并且用户显示地调用 bgrewriteaof 命令，那么服务器将向用户回复一个 ok 状态，并告知用户，bgrewriteaof 已经被预定执行。一旦 bgsave 执行完毕， bgrewriteaof 就会正式开始。\n\n\n# 四、redis 备份\n\n应该确保 redis 数据有完整的备份。\n\n备份 redis 数据建议采用 rdb。\n\n\n# 备份过程\n\n 1. 创建一个定期任务（cron job），每小时将一个 rdb 文件备份到一个文件夹，并且每天将一个 rdb 文件备份到另一个文件夹。\n 2. 确保快照的备份都带有相应的日期和时间信息，每次执行定期任务脚本时，使用 find 命令来删除过期的快照：比如说，你可以保留最近 48 小时内的每小时快照，还可以保留最近一两个月的每日快照。\n 3. 至少每天一次，将 rdb 备份到你的数据中心之外，或者至少是备份到你运行 redis 服务器的物理机器之外。\n\n\n# 容灾备份\n\nredis 的容灾备份基本上就是对数据进行备份，并将这些备份传送到多个不同的外部数据中心。\n\n容灾备份可以在 redis 运行并产生快照的主数据中心发生严重的问题时，仍然让数据处于安全状态。\n\n\n# 五、要点总结\n\n\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 复制",frontmatter:{title:"Redis 复制",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","复制"],permalink:"/pages/379cd8/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/05.Redis%E5%A4%8D%E5%88%B6.html",relativePath:"12.数据库/05.KV数据库/01.Redis/05.Redis复制.md",key:"v-26fbb292",path:"/pages/379cd8/",headers:[{level:2,title:"一、复制简介",slug:"一、复制简介",normalizedTitle:"一、复制简介",charIndex:190},{level:2,title:"二、旧版复制",slug:"二、旧版复制",normalizedTitle:"二、旧版复制",charIndex:698},{level:3,title:"同步",slug:"同步",normalizedTitle:"同步",charIndex:172},{level:3,title:"命令传播",slug:"命令传播",normalizedTitle:"命令传播",charIndex:758},{level:3,title:"旧版复制的缺陷",slug:"旧版复制的缺陷",normalizedTitle:"旧版复制的缺陷",charIndex:1265},{level:2,title:"三、新版复制",slug:"三、新版复制",normalizedTitle:"三、新版复制",charIndex:1650},{level:3,title:"部分重同步",slug:"部分重同步",normalizedTitle:"部分重同步",charIndex:169},{level:4,title:"复制偏移量",slug:"复制偏移量",normalizedTitle:"复制偏移量",charIndex:1954},{level:4,title:"复制积压缓冲区",slug:"复制积压缓冲区",normalizedTitle:"复制积压缓冲区",charIndex:1988},{level:4,title:"服务器的运行 ID",slug:"服务器的运行-id",normalizedTitle:"服务器的运行 id",charIndex:2020},{level:3,title:"PSYNC 命令",slug:"psync-命令",normalizedTitle:"psync 命令",charIndex:1684},{level:2,title:"四、心跳检测",slug:"四、心跳检测",normalizedTitle:"四、心跳检测",charIndex:3306},{level:3,title:"检测主从连接状态",slug:"检测主从连接状态",normalizedTitle:"检测主从连接状态",charIndex:3514},{level:3,title:"辅助实现 min-slaves 选项",slug:"辅助实现-min-slaves-选项",normalizedTitle:"辅助实现 min-slaves 选项",charIndex:3479},{level:3,title:"检测命令丢失",slug:"检测命令丢失",normalizedTitle:"检测命令丢失",charIndex:3502},{level:2,title:"五、复制的流程",slug:"五、复制的流程",normalizedTitle:"五、复制的流程",charIndex:4112},{level:3,title:"步骤 1. 设置主从服务器",slug:"步骤-1-设置主从服务器",normalizedTitle:"步骤 1. 设置主从服务器",charIndex:4204},{level:3,title:"步骤 2. 主从服务器建立 TCP 连接。",slug:"步骤-2-主从服务器建立-tcp-连接。",normalizedTitle:"步骤 2. 主从服务器建立 tcp 连接。",charIndex:4439},{level:3,title:"步骤 3. 发送 PING 检查通信状态。",slug:"步骤-3-发送-ping-检查通信状态。",normalizedTitle:"步骤 3. 发送 ping 检查通信状态。",charIndex:4465},{level:3,title:"步骤 4. 身份验证。",slug:"步骤-4-身份验证。",normalizedTitle:"步骤 4. 身份验证。",charIndex:4491},{level:3,title:"步骤 5. 发送端口信息。",slug:"步骤-5-发送端口信息。",normalizedTitle:"步骤 5. 发送端口信息。",charIndex:4868},{level:3,title:"步骤 6. 同步。",slug:"步骤-6-同步。",normalizedTitle:"步骤 6. 同步。",charIndex:4952},{level:3,title:"步骤 7. 命令传播。",slug:"步骤-7-命令传播。",normalizedTitle:"步骤 7. 命令传播。",charIndex:4980},{level:2,title:"六、复制的配置项",slug:"六、复制的配置项",normalizedTitle:"六、复制的配置项",charIndex:5129},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:5878}],headersStr:"一、复制简介 二、旧版复制 同步 命令传播 旧版复制的缺陷 三、新版复制 部分重同步 复制偏移量 复制积压缓冲区 服务器的运行 ID PSYNC 命令 四、心跳检测 检测主从连接状态 辅助实现 min-slaves 选项 检测命令丢失 五、复制的流程 步骤 1. 设置主从服务器 步骤 2. 主从服务器建立 TCP 连接。 步骤 3. 发送 PING 检查通信状态。 步骤 4. 身份验证。 步骤 5. 发送端口信息。 步骤 6. 同步。 步骤 7. 命令传播。 六、复制的配置项 参考资料",content:"# Redis 复制\n\n> 在 Redis 中，可以通过执行 SLAVEOF 命令或设置 slaveof 选项，让一个服务器去复制（replicate）另一个服务器，其中，后者叫主服务器（master），前者叫从服务器（slave）。\n> \n> Redis 2.8 以前的复制不能高效处理断线后重复制的情况，而 Redis 2.8 新添的部分重同步可以解决这个问题。\n\n\n\n\n# 一、复制简介\n\nRedis 通过 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。\n\n一个主服务器可以有多个从服务器。不仅主服务器可以有从服务器，从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个主从链。\n\n一个从服务器只能有一个主服务器，并且不支持主主复制。\n\n可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。\n\n在使用 Redis 复制功能时的设置中，强烈建议在 master 和在 slave 中启用持久化。当不启用时，例如由于非常慢的磁盘性能而导致的延迟问题，应该配置实例来避免重置后自动重启。\n\n从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。\n\n * 只读模式由 redis.conf 文件中的 slave-read-only 选项控制， 也可以通过 CONFIG SET parameter value 命令来开启或关闭这个模式。\n * 只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。\n\n\n# 二、旧版复制\n\n> Redis 2.8 版本以前实现方式：SYNC 命令\n\nRedis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作：\n\n * 同步（sync） - 用于将从服务器的数据库状态更新至主服务器当前的数据库状态。\n * 命令传播（command propagate） - 当主服务器的数据库状态被修改，导致主从数据库状态不一致时，让主从服务器的数据库重新回到一致状态。\n\n\n# 同步\n\nSYNC 命令的执行步骤：\n\n 1. 从服务器向主服务器发送 SYNC 命令。\n 2. 收到 SYNC 命令的主服务器执行 BGSAVE 命令，在后台生成一个 RDB 文件，并使用一个缓冲区记录从现在开始执行的所有写命令。\n 3. 主服务器执行 BGSAVE 完毕后，主服务器会将生成的 RDB 文件发送给从服务器。从服务器接收并载入 RDB 文件，更新自己的数据库状态。\n 4. 主服务器将记录在缓冲区中的所有写命令发送给从服务器，从服务器执行这些写命令，更新自己的数据库状态。\n\n\n\n\n# 命令传播\n\n同步操作完成后，主从数据库的数据库状态将达到一致。每当主服务器执行客户端发送的写命令时，主从数据库状态不再一致。需要将写命令发送给从服务器执行，使得二者的数据库状态重新达到一致。\n\n\n# 旧版复制的缺陷\n\n从服务器对主服务器的复制存在两种情况：\n\n * 初次复制 - 从服务器以前没有复制过将要复制的主服务器。\n * 断线后重复制 - 处于命令传播阶段的主从服务器因为网络原因而中断了复制，当从服务器通过自动重连重新连上了主服务器后，继续复制主服务器。\n\n对于初次复制，旧版复制功能可用很好完成任务；但是对于断线后重复制，由于每次任然需要生成 RDB 并传输，效率很低。\n\n> 🔔 注意：SYNC 命令是一个非常耗费资源的操作。\n> \n>  * 主服务器执行 BGSAVE 命令生成 RDB 文件，这个操作会耗费主服务器大量的 CPU、内存和磁盘 I/O 资源。\n>  * 主服务器传输 RDB 文件给从服务器，这个操作会耗费主从服务器大量的网络资源，并对主服务器响应时延产生影响。\n>  * 从服务器载入 RDB 文件期间，会阻塞其他命令请求。\n\n\n# 三、新版复制\n\n> Redis 2.8 版本以后的新实现方式：使用 PSYNC 命令替代 SYNC 命令。\n\nPSYNC 命令具有完整重同步和部分重同步两种模式：\n\n * 完整重同步（full resychronization） - 用于初次复制。执行步骤与 SYNC 命令基本一致。\n * 部分重同步（partial resychronization） - 用于断线后重复制。如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只需接收并执行这些写命令，即可将主从服务器的数据库状态保持一致。\n\n\n# 部分重同步\n\n部分重同步功能实现由三个部分构成：\n\n * 主从服务器的复制偏移量（replication offset）\n * 主服务器的复制积压缓冲区（replication backlog）\n * 服务器的运行 ID\n\n# 复制偏移量\n\n主服务器和从服务器会分别维护一个复制偏移量。\n\n * 如果主从服务器的复制偏移量相同，则说明二者的数据库状态一致；\n * 反之，则说明二者的数据库状态不一致。\n\n\n\n# 复制积压缓冲区\n\n复制积压缓冲区是主服务器维护的一个固定长度的先进先出（FIFO）队列，默认大小为 1MB。\n\n复制积压缓冲区会保存一部分最近传播的写命令，并且复制积压缓冲区会为队列中的每个字节记录相应的复制偏移量。\n\n当从服务器断线重连主服务时，从服务器会通过 PSYNC 命令将自己的复制偏移量 offset 发送给主服务器，主服务器会根据这个复制偏移量来决定对从服务器执行何种同步操作。\n\n * 如果 offset 之后的数据仍然在复制积压缓冲区，则主服务器对从服务器执行部分重同步操作。\n * 反之，则主服务器对从服务器执行完整重同步操作。\n\n> 🔔 注意：合理调整复制积压缓冲区的大小\n> \n>  * Redis 复制积压缓冲区默认大小为 1MB。\n> \n>  * 复制积压缓冲区的最小大小可以根据公式 second * write_size_per_second 估算。\n\n# 服务器的运行 ID\n\n * 每个 Redis 服务器，都有运行 ID，用于唯一识别身份。\n * 运行 ID 在服务器启动时自动生成，由 40 个随机的十六进制字符组成。例如：132e358005e29741f8d7b0a42d666aace286edda\n\n从服务器对主服务器进行初次复制时，主服务器会将自己的运行 ID 传送给从服务器，从服务器会将这个运行 ID 保存下来。\n\n当从服务器断线重连一个主服务器时，从服务器会发送之前保存的运行 ID：\n\n * 如果保存的运行 ID 和当前主服务器的运行 ID 一致，则说明从服务器断线之前连接的就是这个主服务器，主服务器可以继续尝试执行部分重同步操作；\n * 反之，若运行 ID 不一致，则说明从服务器断线之前连接的不是这个主服务器，主服务器将对从服务器执行完整重同步操作。\n\n\n# PSYNC 命令\n\n了解了部分重同步的实现，PSYNC 的实现就很容易理解了，它的基本工作原理大致如下：\n\n当从服务接收到 SLAVEOF 命令时，先判断从服务器以前是否执行过复制操作。\n\n * 如果没有复制过任何主服务器，向要复制的主服务器发送 PSYNC ? -1 命令，主动请求进行完整重同步。\n * 反之，向要复制的主服务器发送 PSYNC <runid> <offset> 命令。\n   * runid 是上一次复制的主服务器的运行 ID。\n   * offset 是复制偏移量。\n\n接收到 PSYNC <runid> <offset> 命令的主服务会进行分析：\n\n * 假如主从服务器的 master run id 相同，并且指定的偏移量（offset）在内存缓冲区中还有效，复制就会从上次中断的点开始继续。\n * 如果其中一个条件不满足，就会进行完全重新同步（在 2.8 版本之前就是直接进行完全重新同步）。\n\n\n\n\n# 四、心跳检测\n\n在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令：\n\nREPLCONF ACK <replication_offset>\n\n\n其中，replication_offset 是从服务器当前的复制偏移量。\n\n发送 REPLCONF ACK 命令对于主从服务器有三个作用：\n\n * 检测主从服务器的网络连接状态。\n * 辅助实现 min-slaves 选项。\n * 检测命令丢失。\n\n\n# 检测主从连接状态\n\n可以通过发送和接收 REPLCONF ACK 命令来检查主从服务器之间的网络连接是否正常：如果主服务器超过一秒没有收到从服务器发来的 REPLCONF ACK 命令，那么主服务器就知道主从服务器之间的连接出现问题了。\n\n可以通过向主服务器发送 INFO replication 命令，在列出的从服务器列表的 lag 一栏中，可以看到从服务器向主服务器发送 REPLCONF ACK 命令已经过去多少秒。\n\n\n# 辅助实现 min-slaves 选项\n\nRedis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。\n\n【示例】min-slaves 配置项\n\nmin-slaves-to-write 3\nmin-slaves-max-lag 10\n\n\n以上配置表示：从服务器小于 3 个，或三个从服务器的延迟（lag）都大于等于 10 秒时，主服务器将拒绝执行写命令。\n\n\n# 检测命令丢失\n\n如果因为网络故障，主服务传播给从服务器的写命令丢失，那么从服务器定时向主服务器发送 REPLCONF ACK 命令时，主服务器将发觉从服务器的复制偏移量少于自己的。然后，主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区中找到从服务器缺少的数据，并将这些数据重新发送给从服务器。\n\n\n# 五、复制的流程\n\n通过向从服务器发送如下 SLAVEOF 命令，可以让一个从服务器去复制一个主服务器。\n\nSLAVEOF <master_ip> <master_port>\n\n\n\n# 步骤 1. 设置主从服务器\n\n配置一个从服务器非常简单， 只要在配置文件中增加以下的这一行就可以了：\n\nslaveof 127.0.0.1 6379\n\n\n当然， 你需要将代码中的 127.0.0.1 和 6379 替换成你的主服务器的 IP 和端口号。\n\n另外一种方法是调用 SLAVEOF host port 命令， 输入主服务器的 IP 和端口， 然后同步就会开始：\n\n127.0.0.1:6379> SLAVEOF 127.0.0.1 10086\nOK\n\n\n\n# 步骤 2. 主从服务器建立 TCP 连接。\n\n\n# 步骤 3. 发送 PING 检查通信状态。\n\n\n# 步骤 4. 身份验证。\n\n如果主服务器没有设置 requirepass ，从服务器没有设置 masterauth，则不进行身份验证；反之，则需要进行身份验证。如果身份验证失败，则放弃执行复制工作。\n\n如果主服务器通过 requirepass 选项设置了密码， 那么为了让从服务器的同步操作可以顺利进行， 我们也必须为从服务器进行相应的身份验证设置。\n\n对于一个正在运行的服务器， 可以使用客户端输入以下命令：\n\nconfig set masterauth <password>\n\n\n要永久地设置这个密码， 那么可以将它加入到配置文件中：\n\nmasterauth <password>\n\n\n另外还有几个选项， 它们和主服务器执行部分重同步时所使用的复制流缓冲区有关， 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。\n\n\n# 步骤 5. 发送端口信息。\n\n从服务器执行 REPLCONF listening-port <port-number> ，向主服务器发送从服务器的监听端口号。\n\n\n# 步骤 6. 同步。\n\n前文已介绍，此处不赘述。\n\n\n# 步骤 7. 命令传播。\n\n在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务发送命令：\n\nREPLCONF ACK <replication_coffset>\n\n\n命令的作用：\n\n * 检测主从服务器的网络连接状态。\n * 辅助实现 min-slave 选项。\n * 检测命令丢失。\n\n\n# 六、复制的配置项\n\n从 Redis 2.8 开始， 为了保证数据的安全性， 可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。\n\n不过， 因为 Redis 使用异步复制， 所以主服务器发送的写数据并不一定会被从服务器接收到， 因此， 数据丢失的可能性仍然是存在的。\n\n以下是这个特性的运作原理：\n\n * 从服务器以每秒一次的频率 PING 主服务器一次， 并报告复制流的处理情况。\n * 主服务器会记录各个从服务器最后一次向它发送 PING 的时间。\n * 用户可以通过配置， 指定网络延迟的最大值 min-slaves-max-lag ， 以及执行写操作所需的至少从服务器数量 min-slaves-to-write 。\n\n如果至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag秒， 那么主服务器就会执行客户端请求的写操作。\n\n你可以将这个特性看作 CAP 理论中的 C 的条件放宽版本： 尽管不能保证写操作的持久性， 但起码丢失数据的窗口会被严格限制在指定的秒数中。\n\n另一方面， 如果条件达不到 min-slaves-to-write 和 min-slaves-max-lag 所指定的条件， 那么写操作就不会被执行， 主服务器会向请求执行写操作的客户端返回一个错误。\n\n以下是这个特性的两个选项和它们所需的参数：\n\n * min-slaves-to-write <number of slaves>\n * min-slaves-max-lag <number of seconds>\n\n详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考",normalizedContent:"# redis 复制\n\n> 在 redis 中，可以通过执行 slaveof 命令或设置 slaveof 选项，让一个服务器去复制（replicate）另一个服务器，其中，后者叫主服务器（master），前者叫从服务器（slave）。\n> \n> redis 2.8 以前的复制不能高效处理断线后重复制的情况，而 redis 2.8 新添的部分重同步可以解决这个问题。\n\n\n\n\n# 一、复制简介\n\nredis 通过 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。\n\n一个主服务器可以有多个从服务器。不仅主服务器可以有从服务器，从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个主从链。\n\n一个从服务器只能有一个主服务器，并且不支持主主复制。\n\n可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。\n\n在使用 redis 复制功能时的设置中，强烈建议在 master 和在 slave 中启用持久化。当不启用时，例如由于非常慢的磁盘性能而导致的延迟问题，应该配置实例来避免重置后自动重启。\n\n从 redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。\n\n * 只读模式由 redis.conf 文件中的 slave-read-only 选项控制， 也可以通过 config set parameter value 命令来开启或关闭这个模式。\n * 只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。\n\n\n# 二、旧版复制\n\n> redis 2.8 版本以前实现方式：sync 命令\n\nredis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作：\n\n * 同步（sync） - 用于将从服务器的数据库状态更新至主服务器当前的数据库状态。\n * 命令传播（command propagate） - 当主服务器的数据库状态被修改，导致主从数据库状态不一致时，让主从服务器的数据库重新回到一致状态。\n\n\n# 同步\n\nsync 命令的执行步骤：\n\n 1. 从服务器向主服务器发送 sync 命令。\n 2. 收到 sync 命令的主服务器执行 bgsave 命令，在后台生成一个 rdb 文件，并使用一个缓冲区记录从现在开始执行的所有写命令。\n 3. 主服务器执行 bgsave 完毕后，主服务器会将生成的 rdb 文件发送给从服务器。从服务器接收并载入 rdb 文件，更新自己的数据库状态。\n 4. 主服务器将记录在缓冲区中的所有写命令发送给从服务器，从服务器执行这些写命令，更新自己的数据库状态。\n\n\n\n\n# 命令传播\n\n同步操作完成后，主从数据库的数据库状态将达到一致。每当主服务器执行客户端发送的写命令时，主从数据库状态不再一致。需要将写命令发送给从服务器执行，使得二者的数据库状态重新达到一致。\n\n\n# 旧版复制的缺陷\n\n从服务器对主服务器的复制存在两种情况：\n\n * 初次复制 - 从服务器以前没有复制过将要复制的主服务器。\n * 断线后重复制 - 处于命令传播阶段的主从服务器因为网络原因而中断了复制，当从服务器通过自动重连重新连上了主服务器后，继续复制主服务器。\n\n对于初次复制，旧版复制功能可用很好完成任务；但是对于断线后重复制，由于每次任然需要生成 rdb 并传输，效率很低。\n\n> 🔔 注意：sync 命令是一个非常耗费资源的操作。\n> \n>  * 主服务器执行 bgsave 命令生成 rdb 文件，这个操作会耗费主服务器大量的 cpu、内存和磁盘 i/o 资源。\n>  * 主服务器传输 rdb 文件给从服务器，这个操作会耗费主从服务器大量的网络资源，并对主服务器响应时延产生影响。\n>  * 从服务器载入 rdb 文件期间，会阻塞其他命令请求。\n\n\n# 三、新版复制\n\n> redis 2.8 版本以后的新实现方式：使用 psync 命令替代 sync 命令。\n\npsync 命令具有完整重同步和部分重同步两种模式：\n\n * 完整重同步（full resychronization） - 用于初次复制。执行步骤与 sync 命令基本一致。\n * 部分重同步（partial resychronization） - 用于断线后重复制。如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只需接收并执行这些写命令，即可将主从服务器的数据库状态保持一致。\n\n\n# 部分重同步\n\n部分重同步功能实现由三个部分构成：\n\n * 主从服务器的复制偏移量（replication offset）\n * 主服务器的复制积压缓冲区（replication backlog）\n * 服务器的运行 id\n\n# 复制偏移量\n\n主服务器和从服务器会分别维护一个复制偏移量。\n\n * 如果主从服务器的复制偏移量相同，则说明二者的数据库状态一致；\n * 反之，则说明二者的数据库状态不一致。\n\n\n\n# 复制积压缓冲区\n\n复制积压缓冲区是主服务器维护的一个固定长度的先进先出（fifo）队列，默认大小为 1mb。\n\n复制积压缓冲区会保存一部分最近传播的写命令，并且复制积压缓冲区会为队列中的每个字节记录相应的复制偏移量。\n\n当从服务器断线重连主服务时，从服务器会通过 psync 命令将自己的复制偏移量 offset 发送给主服务器，主服务器会根据这个复制偏移量来决定对从服务器执行何种同步操作。\n\n * 如果 offset 之后的数据仍然在复制积压缓冲区，则主服务器对从服务器执行部分重同步操作。\n * 反之，则主服务器对从服务器执行完整重同步操作。\n\n> 🔔 注意：合理调整复制积压缓冲区的大小\n> \n>  * redis 复制积压缓冲区默认大小为 1mb。\n> \n>  * 复制积压缓冲区的最小大小可以根据公式 second * write_size_per_second 估算。\n\n# 服务器的运行 id\n\n * 每个 redis 服务器，都有运行 id，用于唯一识别身份。\n * 运行 id 在服务器启动时自动生成，由 40 个随机的十六进制字符组成。例如：132e358005e29741f8d7b0a42d666aace286edda\n\n从服务器对主服务器进行初次复制时，主服务器会将自己的运行 id 传送给从服务器，从服务器会将这个运行 id 保存下来。\n\n当从服务器断线重连一个主服务器时，从服务器会发送之前保存的运行 id：\n\n * 如果保存的运行 id 和当前主服务器的运行 id 一致，则说明从服务器断线之前连接的就是这个主服务器，主服务器可以继续尝试执行部分重同步操作；\n * 反之，若运行 id 不一致，则说明从服务器断线之前连接的不是这个主服务器，主服务器将对从服务器执行完整重同步操作。\n\n\n# psync 命令\n\n了解了部分重同步的实现，psync 的实现就很容易理解了，它的基本工作原理大致如下：\n\n当从服务接收到 slaveof 命令时，先判断从服务器以前是否执行过复制操作。\n\n * 如果没有复制过任何主服务器，向要复制的主服务器发送 psync ? -1 命令，主动请求进行完整重同步。\n * 反之，向要复制的主服务器发送 psync <runid> <offset> 命令。\n   * runid 是上一次复制的主服务器的运行 id。\n   * offset 是复制偏移量。\n\n接收到 psync <runid> <offset> 命令的主服务会进行分析：\n\n * 假如主从服务器的 master run id 相同，并且指定的偏移量（offset）在内存缓冲区中还有效，复制就会从上次中断的点开始继续。\n * 如果其中一个条件不满足，就会进行完全重新同步（在 2.8 版本之前就是直接进行完全重新同步）。\n\n\n\n\n# 四、心跳检测\n\n在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令：\n\nreplconf ack <replication_offset>\n\n\n其中，replication_offset 是从服务器当前的复制偏移量。\n\n发送 replconf ack 命令对于主从服务器有三个作用：\n\n * 检测主从服务器的网络连接状态。\n * 辅助实现 min-slaves 选项。\n * 检测命令丢失。\n\n\n# 检测主从连接状态\n\n可以通过发送和接收 replconf ack 命令来检查主从服务器之间的网络连接是否正常：如果主服务器超过一秒没有收到从服务器发来的 replconf ack 命令，那么主服务器就知道主从服务器之间的连接出现问题了。\n\n可以通过向主服务器发送 info replication 命令，在列出的从服务器列表的 lag 一栏中，可以看到从服务器向主服务器发送 replconf ack 命令已经过去多少秒。\n\n\n# 辅助实现 min-slaves 选项\n\nredis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。\n\n【示例】min-slaves 配置项\n\nmin-slaves-to-write 3\nmin-slaves-max-lag 10\n\n\n以上配置表示：从服务器小于 3 个，或三个从服务器的延迟（lag）都大于等于 10 秒时，主服务器将拒绝执行写命令。\n\n\n# 检测命令丢失\n\n如果因为网络故障，主服务传播给从服务器的写命令丢失，那么从服务器定时向主服务器发送 replconf ack 命令时，主服务器将发觉从服务器的复制偏移量少于自己的。然后，主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区中找到从服务器缺少的数据，并将这些数据重新发送给从服务器。\n\n\n# 五、复制的流程\n\n通过向从服务器发送如下 slaveof 命令，可以让一个从服务器去复制一个主服务器。\n\nslaveof <master_ip> <master_port>\n\n\n\n# 步骤 1. 设置主从服务器\n\n配置一个从服务器非常简单， 只要在配置文件中增加以下的这一行就可以了：\n\nslaveof 127.0.0.1 6379\n\n\n当然， 你需要将代码中的 127.0.0.1 和 6379 替换成你的主服务器的 ip 和端口号。\n\n另外一种方法是调用 slaveof host port 命令， 输入主服务器的 ip 和端口， 然后同步就会开始：\n\n127.0.0.1:6379> slaveof 127.0.0.1 10086\nok\n\n\n\n# 步骤 2. 主从服务器建立 tcp 连接。\n\n\n# 步骤 3. 发送 ping 检查通信状态。\n\n\n# 步骤 4. 身份验证。\n\n如果主服务器没有设置 requirepass ，从服务器没有设置 masterauth，则不进行身份验证；反之，则需要进行身份验证。如果身份验证失败，则放弃执行复制工作。\n\n如果主服务器通过 requirepass 选项设置了密码， 那么为了让从服务器的同步操作可以顺利进行， 我们也必须为从服务器进行相应的身份验证设置。\n\n对于一个正在运行的服务器， 可以使用客户端输入以下命令：\n\nconfig set masterauth <password>\n\n\n要永久地设置这个密码， 那么可以将它加入到配置文件中：\n\nmasterauth <password>\n\n\n另外还有几个选项， 它们和主服务器执行部分重同步时所使用的复制流缓冲区有关， 详细的信息可以参考 redis 源码中附带的 redis.conf 示例文件。\n\n\n# 步骤 5. 发送端口信息。\n\n从服务器执行 replconf listening-port <port-number> ，向主服务器发送从服务器的监听端口号。\n\n\n# 步骤 6. 同步。\n\n前文已介绍，此处不赘述。\n\n\n# 步骤 7. 命令传播。\n\n在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务发送命令：\n\nreplconf ack <replication_coffset>\n\n\n命令的作用：\n\n * 检测主从服务器的网络连接状态。\n * 辅助实现 min-slave 选项。\n * 检测命令丢失。\n\n\n# 六、复制的配置项\n\n从 redis 2.8 开始， 为了保证数据的安全性， 可以通过配置， 让主服务器只在有至少 n 个当前已连接从服务器的情况下， 才执行写命令。\n\n不过， 因为 redis 使用异步复制， 所以主服务器发送的写数据并不一定会被从服务器接收到， 因此， 数据丢失的可能性仍然是存在的。\n\n以下是这个特性的运作原理：\n\n * 从服务器以每秒一次的频率 ping 主服务器一次， 并报告复制流的处理情况。\n * 主服务器会记录各个从服务器最后一次向它发送 ping 的时间。\n * 用户可以通过配置， 指定网络延迟的最大值 min-slaves-max-lag ， 以及执行写操作所需的至少从服务器数量 min-slaves-to-write 。\n\n如果至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag秒， 那么主服务器就会执行客户端请求的写操作。\n\n你可以将这个特性看作 cap 理论中的 c 的条件放宽版本： 尽管不能保证写操作的持久性， 但起码丢失数据的窗口会被严格限制在指定的秒数中。\n\n另一方面， 如果条件达不到 min-slaves-to-write 和 min-slaves-max-lag 所指定的条件， 那么写操作就不会被执行， 主服务器会向请求执行写操作的客户端返回一个错误。\n\n以下是这个特性的两个选项和它们所需的参数：\n\n * min-slaves-to-write <number of slaves>\n * min-slaves-max-lag <number of seconds>\n\n详细的信息可以参考 redis 源码中附带的 redis.conf 示例文件。\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 哨兵",frontmatter:{title:"Redis 哨兵",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","哨兵"],permalink:"/pages/615afe/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/06.Redis%E5%93%A8%E5%85%B5.html",relativePath:"12.数据库/05.KV数据库/01.Redis/06.Redis哨兵.md",key:"v-18f5009d",path:"/pages/615afe/",headers:[{level:2,title:"一、哨兵简介",slug:"一、哨兵简介",normalizedTitle:"一、哨兵简介",charIndex:107},{level:2,title:"二、启动哨兵",slug:"二、启动哨兵",normalizedTitle:"二、启动哨兵",charIndex:711},{level:2,title:"三、监控",slug:"三、监控",normalizedTitle:"三、监控",charIndex:1312},{level:3,title:"检测服务器状态",slug:"检测服务器状态",normalizedTitle:"检测服务器状态",charIndex:1321},{level:3,title:"获取服务器信息",slug:"获取服务器信息",normalizedTitle:"获取服务器信息",charIndex:1813},{level:2,title:"四、通知",slug:"四、通知",normalizedTitle:"四、通知",charIndex:2153},{level:3,title:"向服务器发送消息",slug:"向服务器发送消息",normalizedTitle:"向服务器发送消息",charIndex:2265},{level:3,title:"接收服务器的消息",slug:"接收服务器的消息",normalizedTitle:"接收服务器的消息",charIndex:2479},{level:2,title:"五、选举 Leader",slug:"五、选举-leader",normalizedTitle:"五、选举 leader",charIndex:2648},{level:2,title:"六、故障转移",slug:"六、故障转移",normalizedTitle:"六、故障转移",charIndex:3808},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:4486}],headersStr:"一、哨兵简介 二、启动哨兵 三、监控 检测服务器状态 获取服务器信息 四、通知 向服务器发送消息 接收服务器的消息 五、选举 Leader 六、故障转移 参考资料",content:'# Redis 哨兵\n\n> Redis 哨兵（Sentinel）是 Redis 的高可用性（Hight Availability）解决方案。\n> \n> Redis 哨兵是 Raft 算法 的具体实现。\n\n\n\n\n# 一、哨兵简介\n\nRedis 哨兵（Sentinel）是 Redis 的高可用性（Hight Availability）解决方案：由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器，以及这些主服务器的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。\n\n\n\nSentinel 的主要功能如下：\n\n * 监控（Monitoring） - Sentinel 不断检查主从服务器是否正常在工作。\n * 通知（Notification） - Sentinel 可以通过一个 api 来通知系统管理员或者另外的应用程序，被监控的 Redis 实例有一些问题。\n * 自动故障转移（Automatic Failover） - 如果一个主服务器下线，Sentinel 会开始自动故障转移：把一个从节点提升为主节点，并重新配置其他的从节点使用新的主节点，使用 Redis 服务的应用程序在连接的时候也被通知新的地址。\n * 配置提供者（Configuration provider） - Sentinel 给客户端的服务发现提供来源：对于一个给定的服务，客户端连接到 Sentinels 来寻找当前主节点的地址。当故障转移发生的时候，Sentinel 将报告新的地址。\n\n\n# 二、启动哨兵\n\n启动一个 Sentinel 可以使用下面任意一条命令，两条命令效果完全相同。\n\nredis-sentinel /path/to/sentinel.conf\nredis-server /path/to/sentinel.conf --sentinel\n\n\n当一个 Sentinel 启动时，它需要执行以下步骤：\n\n 1. 初始化服务器。\n 2. 使用 Sentinel 专用代码。\n 3. 初始化 Sentinel 状态。\n 4. 初始化 Sentinel 的主服务器列表。\n 5. 创建连向被监视的主服务器的网络连接。\n\nSentinel 本质上是一个运行在特殊状模式下的 Redis 服务器。\n\nSentinel 模式下 Redis 服务器只支持 PING、SENTINEL、INFO、SUBSCRIBE、UNSUBSCRIBE、PSUBSCRIBE、PUNSUBSCRIBE 七个命令。\n\n创建连向被监视的主服务器的网络连接，Sentinel 将成为主服务器的客户端，它可以向主服务器发送命令，并从命令回复中获取相关的信息。对于每个被 Sentinel 监视的主服务器，Sentinel 会创建两个连向主服务器的异步网络：\n\n * 命令连接：专门用于向主服务器发送命令，并接受命令回复。\n * 订阅连接：专门用于订阅主服务器的 __sentinel__:hello 频道。\n\n\n# 三、监控\n\n\n# 检测服务器状态\n\n> Sentinel 向 Redis 服务器发送 PING 命令，检查其状态。\n\n默认情况下，每个 Sentinel 节点会以 每秒一次 的频率对 Redis 节点和 其它 的 Sentinel 节点发送 PING 命令，并通过节点的 回复 来判断节点是否在线。\n\n * 主观下线：主观下线 适用于所有 主节点 和 从节点。如果在 down-after-milliseconds 毫秒内，Sentinel 没有收到 目标节点 的有效回复，则会判定 该节点 为 主观下线。\n * 客观下线：客观下线 只适用于 主节点。当 Sentinel 将一个主服务器判断为主管下线后，为了确认这个主服务器是否真的下线，会向同样监视这一主服务器的其他 Sentinel 询问，看它们是否也认为主服务器已经下线。当足够数量的 Sentinel 认为主服务器已下线，就判定其为客观下线，并对其执行故障转移操作。\n   * Sentinel 节点通过 sentinel is-master-down-by-addr 命令，向其它 Sentinel 节点询问对该节点的 状态判断。\n\n\n# 获取服务器信息\n\n> Sentinel 向主服务器发送 INFO 命令，获取主服务器及它的从服务器信息。\n\n * 获取主服务器信息 - Sentinel 默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送 INFO 命令，并通过分析 INFO 命令的回复来获取主服务器的当前信息。\n   * 主服务自身信息：包括 run_id 域记录的服务器运行 ID，以及 role 域记录的服务器角色\n   * 主服务的从服务器信息：包括 IP 地址和端口号\n * 获取从服务器信息 - 当 Sentinel 发现主服务器有新的从服务器出现时，Sentinel 除了会为这个新的从服务器创建相应的实例结构之外，Sentinel 还会创建连接到从服务器的命令连接和订阅连接。\n\n\n# 四、通知\n\n对于每个与 Sentinel 连接的服务器，Sentinel 既会向服务器的 __sentinel__:hello 频道发送消息，也会订阅服务器的 __sentinel__:hello 频道的消息。\n\n\n\n\n# 向服务器发送消息\n\n在默认情况下，Sentinel 会以每两秒一次的频率，通过命令向所有被监视的主服务器和从服务器发送以下格式的命令。\n\nPUBLISH __sentinel__:hello "<s_ip>,<s_port>,<s_runid>,<s_epoch>,<m_name>,<m_ip>,<m_port>,<m_epoch>"\n\n\n这条命令向服务器的 __sentinel__:hello 频道发送一条消息。\n\n\n# 接收服务器的消息\n\n当 Sentinel 与一个主服务器或从服务器建立起订阅连接后，Sentinel 就会通过订阅连接，向服务器发送以下命令：SUBSCRIBE __sentinel__:hello。\n\nSentinel 对 __sentinel__:hello 频道的订阅会一直持续到 Sentinel 与服务器断开连接为止。\n\n\n# 五、选举 Leader\n\n> Redis Sentinel 系统选举 Leader 的算法是 Raft 的实现。\n> \n> Raft 是一种共识性算法，想了解其原理，可以参考 深入剖析共识性算法 Raft。\n\n当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个 Sentinel 会进行协商，选举出一个领头的 Sentinel，并由领头 Sentinel 对下线主服务器执行故障转移操作。\n\n所有在线 Sentinel 都有资格被选为 Leader。\n\n每个 Sentinel 节点都需要 定期执行 以下任务：\n\n（1）每个 Sentinel 以 每秒钟 一次的频率，向它所知的 主服务器、从服务器 以及其他 Sentinel 实例 发送一个 PING 命令。\n\n\n\n（2）如果一个 实例（instance）距离 最后一次 有效回复 PING 命令的时间超过 down-after-milliseconds 所指定的值，那么这个实例会被 Sentinel 标记为 主观下线。\n\n\n\n（3）如果一个 主服务器 被标记为 主观下线，那么正在 监视 这个 主服务器 的所有 Sentinel 节点，要以 每秒一次 的频率确认 主服务器 的确进入了 主观下线 状态。\n\n\n\n（4）如果一个 主服务器 被标记为 主观下线，并且有 足够数量 的 Sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围 内同意这一判断，那么这个 主服务器 被标记为 客观下线。\n\n\n\n（5）在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率，向它已知的所有 主服务器 和 从服务器 发送 INFO 命令。当一个 主服务器 被 Sentinel 标记为 客观下线 时，Sentinel 向 下线主服务器 的所有 从服务器 发送 INFO 命令的频率，会从 10 秒一次改为 每秒一次。\n\n\n\n（6）Sentinel 和其他 Sentinel 协商 主节点 的状态，如果 主节点 处于 SDOWN 状态，则投票自动选出新的 主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。\n\n\n\n（7）当没有足够数量的 Sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 Sentinel 的 PING 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。\n\n\n\n> 注意：一个有效的 PING 回复可以是：+PONG、-LOADING 或者 -MASTERDOWN。如果 服务器 返回除以上三种回复之外的其他回复，又或者在 指定时间 内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复 无效（non-valid）。\n\n\n# 六、故障转移\n\n在选举产生出 Sentinel Leader 后，Sentinel Leader 将对已下线的主服务器执行故障转移操作。操作含以下三个步骤：\n\n（一）选出新的主服务器\n\n故障转移第一步，是 Sentinel Leader 在已下线主服务属下的所有从服务器中，挑选一个状态良好、数据完整的从服务器。然后，向这个从服务器发送 SLAVEOF no one 命令，将其转换为主服务器。\n\nSentinel Leader 如何选出新的主服务器：\n\n * 删除列表中所有处于下线或断线状态的从服务器。\n * 删除列表中所有最近五秒没有回复过 Sentinel Leader 的 INFO 命令的从服务器。\n * 删除所有与已下线主服务器连接断开超过 down-after-milliseconds * 10 毫秒的从服务器（down-after-milliseconds 指定了判断主服务器下线所需的时间）。\n * 之后， Sentinel Leader 先选出优先级最高的从服务器；如果优先级一样高，再选择复制偏移量最大的从服务器；如果结果还不唯一，则选出运行 ID 最小的从服务器。\n\n（二）修改从服务器的复制目标\n\n选出新的主服务器后，Sentinel Leader 会向所有从服务器发送 SLAVEOF 命令，让它们去复制新的主服务器。\n\n（三）将旧的主服务器变为从服务器\n\nSentinel Leader 将旧的主服务器标记为从服务器。当旧的主服务器重新上线，Sentinel 会向它发送 SLAVEOF 命令，让其成为从服务器。\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考\n * 文章\n   * 渐进式解析 Redis 源码 - 哨兵 sentinel\n   * 深入剖析 Redis 系列(二) - Redis 哨兵模式与高可用集群',normalizedContent:'# redis 哨兵\n\n> redis 哨兵（sentinel）是 redis 的高可用性（hight availability）解决方案。\n> \n> redis 哨兵是 raft 算法 的具体实现。\n\n\n\n\n# 一、哨兵简介\n\nredis 哨兵（sentinel）是 redis 的高可用性（hight availability）解决方案：由一个或多个 sentinel 实例组成的 sentinel 系统可以监视任意多个主服务器，以及这些主服务器的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。\n\n\n\nsentinel 的主要功能如下：\n\n * 监控（monitoring） - sentinel 不断检查主从服务器是否正常在工作。\n * 通知（notification） - sentinel 可以通过一个 api 来通知系统管理员或者另外的应用程序，被监控的 redis 实例有一些问题。\n * 自动故障转移（automatic failover） - 如果一个主服务器下线，sentinel 会开始自动故障转移：把一个从节点提升为主节点，并重新配置其他的从节点使用新的主节点，使用 redis 服务的应用程序在连接的时候也被通知新的地址。\n * 配置提供者（configuration provider） - sentinel 给客户端的服务发现提供来源：对于一个给定的服务，客户端连接到 sentinels 来寻找当前主节点的地址。当故障转移发生的时候，sentinel 将报告新的地址。\n\n\n# 二、启动哨兵\n\n启动一个 sentinel 可以使用下面任意一条命令，两条命令效果完全相同。\n\nredis-sentinel /path/to/sentinel.conf\nredis-server /path/to/sentinel.conf --sentinel\n\n\n当一个 sentinel 启动时，它需要执行以下步骤：\n\n 1. 初始化服务器。\n 2. 使用 sentinel 专用代码。\n 3. 初始化 sentinel 状态。\n 4. 初始化 sentinel 的主服务器列表。\n 5. 创建连向被监视的主服务器的网络连接。\n\nsentinel 本质上是一个运行在特殊状模式下的 redis 服务器。\n\nsentinel 模式下 redis 服务器只支持 ping、sentinel、info、subscribe、unsubscribe、psubscribe、punsubscribe 七个命令。\n\n创建连向被监视的主服务器的网络连接，sentinel 将成为主服务器的客户端，它可以向主服务器发送命令，并从命令回复中获取相关的信息。对于每个被 sentinel 监视的主服务器，sentinel 会创建两个连向主服务器的异步网络：\n\n * 命令连接：专门用于向主服务器发送命令，并接受命令回复。\n * 订阅连接：专门用于订阅主服务器的 __sentinel__:hello 频道。\n\n\n# 三、监控\n\n\n# 检测服务器状态\n\n> sentinel 向 redis 服务器发送 ping 命令，检查其状态。\n\n默认情况下，每个 sentinel 节点会以 每秒一次 的频率对 redis 节点和 其它 的 sentinel 节点发送 ping 命令，并通过节点的 回复 来判断节点是否在线。\n\n * 主观下线：主观下线 适用于所有 主节点 和 从节点。如果在 down-after-milliseconds 毫秒内，sentinel 没有收到 目标节点 的有效回复，则会判定 该节点 为 主观下线。\n * 客观下线：客观下线 只适用于 主节点。当 sentinel 将一个主服务器判断为主管下线后，为了确认这个主服务器是否真的下线，会向同样监视这一主服务器的其他 sentinel 询问，看它们是否也认为主服务器已经下线。当足够数量的 sentinel 认为主服务器已下线，就判定其为客观下线，并对其执行故障转移操作。\n   * sentinel 节点通过 sentinel is-master-down-by-addr 命令，向其它 sentinel 节点询问对该节点的 状态判断。\n\n\n# 获取服务器信息\n\n> sentinel 向主服务器发送 info 命令，获取主服务器及它的从服务器信息。\n\n * 获取主服务器信息 - sentinel 默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送 info 命令，并通过分析 info 命令的回复来获取主服务器的当前信息。\n   * 主服务自身信息：包括 run_id 域记录的服务器运行 id，以及 role 域记录的服务器角色\n   * 主服务的从服务器信息：包括 ip 地址和端口号\n * 获取从服务器信息 - 当 sentinel 发现主服务器有新的从服务器出现时，sentinel 除了会为这个新的从服务器创建相应的实例结构之外，sentinel 还会创建连接到从服务器的命令连接和订阅连接。\n\n\n# 四、通知\n\n对于每个与 sentinel 连接的服务器，sentinel 既会向服务器的 __sentinel__:hello 频道发送消息，也会订阅服务器的 __sentinel__:hello 频道的消息。\n\n\n\n\n# 向服务器发送消息\n\n在默认情况下，sentinel 会以每两秒一次的频率，通过命令向所有被监视的主服务器和从服务器发送以下格式的命令。\n\npublish __sentinel__:hello "<s_ip>,<s_port>,<s_runid>,<s_epoch>,<m_name>,<m_ip>,<m_port>,<m_epoch>"\n\n\n这条命令向服务器的 __sentinel__:hello 频道发送一条消息。\n\n\n# 接收服务器的消息\n\n当 sentinel 与一个主服务器或从服务器建立起订阅连接后，sentinel 就会通过订阅连接，向服务器发送以下命令：subscribe __sentinel__:hello。\n\nsentinel 对 __sentinel__:hello 频道的订阅会一直持续到 sentinel 与服务器断开连接为止。\n\n\n# 五、选举 leader\n\n> redis sentinel 系统选举 leader 的算法是 raft 的实现。\n> \n> raft 是一种共识性算法，想了解其原理，可以参考 深入剖析共识性算法 raft。\n\n当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个 sentinel 会进行协商，选举出一个领头的 sentinel，并由领头 sentinel 对下线主服务器执行故障转移操作。\n\n所有在线 sentinel 都有资格被选为 leader。\n\n每个 sentinel 节点都需要 定期执行 以下任务：\n\n（1）每个 sentinel 以 每秒钟 一次的频率，向它所知的 主服务器、从服务器 以及其他 sentinel 实例 发送一个 ping 命令。\n\n\n\n（2）如果一个 实例（instance）距离 最后一次 有效回复 ping 命令的时间超过 down-after-milliseconds 所指定的值，那么这个实例会被 sentinel 标记为 主观下线。\n\n\n\n（3）如果一个 主服务器 被标记为 主观下线，那么正在 监视 这个 主服务器 的所有 sentinel 节点，要以 每秒一次 的频率确认 主服务器 的确进入了 主观下线 状态。\n\n\n\n（4）如果一个 主服务器 被标记为 主观下线，并且有 足够数量 的 sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围 内同意这一判断，那么这个 主服务器 被标记为 客观下线。\n\n\n\n（5）在一般情况下， 每个 sentinel 会以每 10 秒一次的频率，向它已知的所有 主服务器 和 从服务器 发送 info 命令。当一个 主服务器 被 sentinel 标记为 客观下线 时，sentinel 向 下线主服务器 的所有 从服务器 发送 info 命令的频率，会从 10 秒一次改为 每秒一次。\n\n\n\n（6）sentinel 和其他 sentinel 协商 主节点 的状态，如果 主节点 处于 sdown 状态，则投票自动选出新的 主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。\n\n\n\n（7）当没有足够数量的 sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 sentinel 的 ping 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。\n\n\n\n> 注意：一个有效的 ping 回复可以是：+pong、-loading 或者 -masterdown。如果 服务器 返回除以上三种回复之外的其他回复，又或者在 指定时间 内没有回复 ping 命令， 那么 sentinel 认为服务器返回的回复 无效（non-valid）。\n\n\n# 六、故障转移\n\n在选举产生出 sentinel leader 后，sentinel leader 将对已下线的主服务器执行故障转移操作。操作含以下三个步骤：\n\n（一）选出新的主服务器\n\n故障转移第一步，是 sentinel leader 在已下线主服务属下的所有从服务器中，挑选一个状态良好、数据完整的从服务器。然后，向这个从服务器发送 slaveof no one 命令，将其转换为主服务器。\n\nsentinel leader 如何选出新的主服务器：\n\n * 删除列表中所有处于下线或断线状态的从服务器。\n * 删除列表中所有最近五秒没有回复过 sentinel leader 的 info 命令的从服务器。\n * 删除所有与已下线主服务器连接断开超过 down-after-milliseconds * 10 毫秒的从服务器（down-after-milliseconds 指定了判断主服务器下线所需的时间）。\n * 之后， sentinel leader 先选出优先级最高的从服务器；如果优先级一样高，再选择复制偏移量最大的从服务器；如果结果还不唯一，则选出运行 id 最小的从服务器。\n\n（二）修改从服务器的复制目标\n\n选出新的主服务器后，sentinel leader 会向所有从服务器发送 slaveof 命令，让它们去复制新的主服务器。\n\n（三）将旧的主服务器变为从服务器\n\nsentinel leader 将旧的主服务器标记为从服务器。当旧的主服务器重新上线，sentinel 会向它发送 slaveof 命令，让其成为从服务器。\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考\n * 文章\n   * 渐进式解析 redis 源码 - 哨兵 sentinel\n   * 深入剖析 redis 系列(二) - redis 哨兵模式与高可用集群',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 集群",frontmatter:{title:"Redis 集群",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","集群"],permalink:"/pages/77dfbe/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/07.Redis%E9%9B%86%E7%BE%A4.html",relativePath:"12.数据库/05.KV数据库/01.Redis/07.Redis集群.md",key:"v-5b5c3224",path:"/pages/77dfbe/",headers:[{level:2,title:"1. Redis Cluster 分区",slug:"_1-redis-cluster-分区",normalizedTitle:"1. redis cluster 分区",charIndex:320},{level:3,title:"1.1. 集群节点",slug:"_1-1-集群节点",normalizedTitle:"1.1. 集群节点",charIndex:344},{level:3,title:"1.2. 分配 Hash 槽",slug:"_1-2-分配-hash-槽",normalizedTitle:"1.2. 分配 hash 槽",charIndex:639},{level:3,title:"1.3. 寻址",slug:"_1-3-寻址",normalizedTitle:"1.3. 寻址",charIndex:1104},{level:4,title:"1.3.1. 计算键属于哪个槽",slug:"_1-3-1-计算键属于哪个槽",normalizedTitle:"1.3.1. 计算键属于哪个槽",charIndex:1274},{level:4,title:"1.3.2. MOVED 错误",slug:"_1-3-2-moved-错误",normalizedTitle:"1.3.2. moved 错误",charIndex:1470},{level:3,title:"1.4. 重新分区",slug:"_1-4-重新分区",normalizedTitle:"1.4. 重新分区",charIndex:1631},{level:3,title:"1.5. ASK 错误",slug:"_1-5-ask-错误",normalizedTitle:"1.5. ask 错误",charIndex:1888},{level:2,title:"2. Redis Cluster 故障转移",slug:"_2-redis-cluster-故障转移",normalizedTitle:"2. redis cluster 故障转移",charIndex:2129},{level:3,title:"2.1. 复制",slug:"_2-1-复制",normalizedTitle:"2.1. 复制",charIndex:2155},{level:3,title:"2.2. 故障检测",slug:"_2-2-故障检测",normalizedTitle:"2.2. 故障检测",charIndex:2192},{level:3,title:"2.3. 故障转移",slug:"_2-3-故障转移",normalizedTitle:"2.3. 故障转移",charIndex:2332},{level:3,title:"2.4. 选举新的主节点",slug:"_2-4-选举新的主节点",normalizedTitle:"2.4. 选举新的主节点",charIndex:2502},{level:2,title:"3. Redis Cluster 通信",slug:"_3-redis-cluster-通信",normalizedTitle:"3. redis cluster 通信",charIndex:2549},{level:2,title:"4. Redis Cluster 应用",slug:"_4-redis-cluster-应用",normalizedTitle:"4. redis cluster 应用",charIndex:2998},{level:3,title:"4.1. 集群功能限制",slug:"_4-1-集群功能限制",normalizedTitle:"4.1. 集群功能限制",charIndex:3022},{level:3,title:"4.2. 集群规模限制",slug:"_4-2-集群规模限制",normalizedTitle:"4.2. 集群规模限制",charIndex:3464},{level:3,title:"4.3. 集群配置",slug:"_4-3-集群配置",normalizedTitle:"4.3. 集群配置",charIndex:3887},{level:2,title:"5. 其他 Redis 集群方案",slug:"_5-其他-redis-集群方案",normalizedTitle:"5. 其他 redis 集群方案",charIndex:4968},{level:3,title:"5.1. 客户端分区方案",slug:"_5-1-客户端分区方案",normalizedTitle:"5.1. 客户端分区方案",charIndex:5088},{level:3,title:"5.2. 代理分区方案",slug:"_5-2-代理分区方案",normalizedTitle:"5.2. 代理分区方案",charIndex:5523},{level:4,title:"5.2.1. Twemproxy",slug:"_5-2-1-twemproxy",normalizedTitle:"5.2.1. twemproxy",charIndex:5718},{level:4,title:"5.2.2. Codis",slug:"_5-2-2-codis",normalizedTitle:"5.2.2. codis",charIndex:6004},{level:3,title:"5.3. 查询路由方案",slug:"_5-3-查询路由方案",normalizedTitle:"5.3. 查询路由方案",charIndex:6322},{level:2,title:"6. 参考资料",slug:"_6-参考资料",normalizedTitle:"6. 参考资料",charIndex:6766}],headersStr:"1. Redis Cluster 分区 1.1. 集群节点 1.2. 分配 Hash 槽 1.3. 寻址 1.3.1. 计算键属于哪个槽 1.3.2. MOVED 错误 1.4. 重新分区 1.5. ASK 错误 2. Redis Cluster 故障转移 2.1. 复制 2.2. 故障检测 2.3. 故障转移 2.4. 选举新的主节点 3. Redis Cluster 通信 4. Redis Cluster 应用 4.1. 集群功能限制 4.2. 集群规模限制 4.3. 集群配置 5. 其他 Redis 集群方案 5.1. 客户端分区方案 5.2. 代理分区方案 5.2.1. Twemproxy 5.2.2. Codis 5.3. 查询路由方案 6. 参考资料",content:"# Redis 集群\n\n> Redis 集群（Redis Cluster） 是 Redis 官方提供的分布式数据库方案。\n> \n> 既然是分布式，自然具备分布式系统的基本特性：可扩展、高可用、一致性。\n> \n>  * Redis 集群通过划分 hash 槽来分区，进行数据分享。\n>  * Redis 集群采用主从模型，提供复制和故障转移功能，来保证 Redis 集群的高可用。\n>  * 根据 CAP 理论，Consistency、Availability、Partition tolerance 三者不可兼得，而 Redis 集群的选择是 AP。Redis 集群节点间采用异步通信方式，不保证强一致性，尽力达到最终一致性。\n\n\n\n\n# 1. Redis Cluster 分区\n\n\n# 1.1. 集群节点\n\nRedis 集群由多个节点组成，节点刚启动时，彼此是相互独立的。节点通过握手（ CLUSTER MEET 命令）来将其他节点添加到自己所处的集群中。\n\n向一个节点发送 CLUSTER MEET 命令，可以让当前节点与指定 IP、PORT 的节点进行握手，握手成功时，当前节点会将指定节点加入所在集群。\n\n集群节点保存键值对以及过期时间的方式与单机 Redis 服务完全相同。\n\nRedis 集群节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。\n\n\n# 1.2. 分配 Hash 槽\n\n分布式存储需要解决的首要问题是把 整个数据集 按照 分区规则 映射到 多个节点 的问题，即把 数据集 划分到 多个节点 上，每个节点负责 整体数据 的一个 子集。\n\nRedis 集群通过划分 hash 槽来将数据分区。Redis 集群通过分区的方式来保存数据库的键值对：集群的整个数据库被分为 16384 个哈希槽（slot），数据库中的每个键都属于这 16384 个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态。\n\n通过向节点发送 CLUSTER ADDSLOTS 命令，可以将一个或多个槽指派给节点负责。\n\n> CLUSTER ADDSLOTS 1 2 3\nOK\n\n\n集群中的每个节点负责一部分哈希槽，比如集群中有３个节点，则：\n\n * 节点Ａ存储的哈希槽范围是：0 – 5500\n * 节点Ｂ存储的哈希槽范围是：5501 – 11000\n * 节点Ｃ存储的哈希槽范围是：11001 – 16384\n\n\n# 1.3. 寻址\n\n当客户端向节点发送与数据库键有关的命令时，接受命令的节点会计算出命令要处理的数据库属于哪个槽，并检查这个槽是否指派给了自己：\n\n * 如果键所在的槽正好指派给了当前节点，那么当前节点直接执行命令。\n * 如果键所在的槽没有指派给当前节点，那么节点会向客户端返回一个 MOVED 错误，指引客户端重定向至正确的节点。\n\n# 1.3.1. 计算键属于哪个槽\n\n决定一个 key 应该分配到那个槽的算法是：计算该 key 的 CRC16 结果再模 16834。\n\nHASH_SLOT = CRC16(KEY) mod 16384\n\n\n当节点计算出 key 所属的槽为 i 之后，节点会根据以下条件判断槽是否由自己负责：\n\nclusterState.slots[i] == clusterState.myself\n\n\n# 1.3.2. MOVED 错误\n\n当节点发现键所在的槽并非自己负责处理的时候，节点就会向客户端返回一个 MOVED 错误，指引客户端转向正在负责槽的节点。\n\nMOVED 错误的格式为：\n\nMOVED <slot> <ip>:<port>\n\n\n> 个人理解：MOVED 这种操作有点类似 HTTP 协议中的重定向。\n\n\n# 1.4. 重新分区\n\nRedis 集群的重新分区操作可以将任意数量的已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点。\n\n重新分区操作可以在线进行，在重新分区的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。\n\nRedis 集群的重新分区操作由 Redis 集群管理软件 redis-trib 负责执行的，redis-trib 通过向源节点和目标节点发送命令来进行重新分区操作。\n\n重新分区的实现原理如下图所示：\n\n\n\n\n# 1.5. ASK 错误\n\nASK 错误与 MOVED 的区别在于：ASK 错误只是两个节点在迁移槽的过程中使用的一种临时措施，在客户端收到关于槽 X 的 ASK 错误之后，客户端只会在接下来的一次命令请求中将关于槽 X 的命令请求发送至 ASK 错误所指示的节点，但这种转向不会对客户端今后发送关于槽 X 的命令请求产生任何影响，客户端仍然会将关于槽 X 的命令请求发送至目前负责处理槽 X 的节点，除非 ASK 错误再次出现。\n\n判断 ASK 错误的过程如下图所示：\n\n\n\n\n# 2. Redis Cluster 故障转移\n\n\n# 2.1. 复制\n\nRedis 复制机制可以参考：Redis 复制\n\n\n# 2.2. 故障检测\n\n集群中每个节点都会定期向集群中的其他节点发送 PING 消息，以此来检测对方是否在线。\n\n节点的状态信息可以分为：\n\n * 在线状态；\n\n * 下线状态（FAIL）;\n\n * 疑似下线状态（PFAIL），即在规定的时间内，没有应答 PING 消息；\n\n\n# 2.3. 故障转移\n\n 1. 下线主节点的所有从节点中，会有一个从节点被选中。\n 2. 被选中的从节点会执行 SLAVEOF no one 命令，成为新的主节点。\n 3. 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。\n 4. 新的主节点向集群广播一条 PONG 消息，告知其他节点这个从节点已变成主节点。\n\n\n# 2.4. 选举新的主节点\n\nRedis 集群选举新的主节点流程基于共识算法：Raft\n\n\n# 3. Redis Cluster 通信\n\n集群中的节点通过发送和接收消息来进行通信。\n\nRedis 集群节点发送的消息主要有以下五种：\n\n * MEET - 请求接收方加入发送方所在的集群。\n * PING - 集群中每个节点每隔一段时间（默认为一秒）从已知节点列表中随机选出五个节点，然后对这五个节点中最久没联系的节点发送 PING 消息，以此检测被选中的节点是否在线。\n * PONG - 当接收方收到发送方发来的 MEET 消息或 PING 消息时，会返回一条 PONG 消息作为应答。\n * FAIL - 当一个主节点 A 判断另一个主节点 B 已经进入 FAIL 状态时，节点 A 会向集群广播一条关于节点 B 的 FAIL 消息，所有收到这条消息的节点都会立即将节点 B 标记为已下线。\n * PUBLISH - 当节点收到一个 PUBLISH 命令时，节点会执行这个命令，并向集群广播一条 PUBLISH 消息，所有接受到这条消息的节点都会执行相同的 PUBLISH 命令。\n\n\n# 4. Redis Cluster 应用\n\n\n# 4.1. 集群功能限制\n\nRedis 集群相对 单机，存在一些功能限制，需要 开发人员 提前了解，在使用时做好规避。\n\n * key 批量操作 支持有限：类似 mset、mget 操作，目前只支持对具有相同 slot 值的 key 执行 批量操作。对于 映射为不同 slot 值的 key 由于执行 mget、mget 等操作可能存在于多个节点上，因此不被支持。\n\n * key 事务操作 支持有限：只支持 多 key 在 同一节点上 的 事务操作，当多个 key 分布在 不同 的节点上时 无法 使用事务功能。\n\n * key 作为 数据分区 的最小粒度，不能将一个 大的键值 对象如 hash、list 等映射到 不同的节点。\n\n * 不支持 多数据库空间：单机 下的 Redis 可以支持 16 个数据库（db0 ~ db15），集群模式 下只能使用 一个 数据库空间，即 db0。\n\n * 复制结构 只支持一层：从节点 只能复制 主节点，不支持 嵌套树状复制 结构。\n\n\n# 4.2. 集群规模限制\n\nRedis Cluster 的优点是易于使用。分区、主从复制、弹性扩容这些功能都可以做到自动化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 Redis 集群，并且对于应用来说，近乎于是透明的。\n\n所以，Redis Cluster 非常适合构建中小规模 Redis 集群，这里的中小规模指的是，大概几个到几十个节点这样规模的 Redis 集群。\n\n但是 Redis Cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设计。\n\nRedis 的每个节点上，都保存了所有槽和节点的映射关系表，客户端可以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。那么，这个映射关系表是如何更新的呢？Redis Cluster 采用了一种去中心化的流言 (Gossip) 协议来传播集群配置的变化。\n\nGossip 协议的优点是去中心化；缺点是传播速度慢，并且是集群规模越大，传播的越慢。\n\n\n# 4.3. 集群配置\n\n我们后面会部署一个 Redis 集群作为例子，在那之前，先介绍一下集群在 redis.conf 中的参数。\n\n * cluster-enabled <yes/no> - 如果配置”yes”则开启集群功能，此 redis 实例作为集群的一个节点，否则，它是一个普通的单一的 redis 实例。\n * cluster-config-file <filename> - 注意：虽然此配置的名字叫“集群配置文件”，但是此配置文件不能人工编辑，它是集群节点自动维护的文件，主要用于记录集群中有哪些节点、他们的状态以及一些持久化参数等，方便在重启时恢复这些状态。通常是在收到请求之后这个文件就会被更新。\n * cluster-node-timeout <milliseconds> - 这是集群中的节点能够失联的最大时间，超过这个时间，该节点就会被认为故障。如果主节点超过这个时间还是不可达，则用它的从节点将启动故障迁移，升级成主节点。注意，任何一个节点在这个时间之内如果还是没有连上大部分的主节点，则此节点将停止接收任何请求。\n * cluster-slave-validity-factor <factor> - 如果设置成０，则无论从节点与主节点失联多久，从节点都会尝试升级成主节点。如果设置成正数，则 cluster-node-timeout 乘以 cluster-slave-validity-factor 得到的时间，是从节点与主节点失联后，此从节点数据有效的最长时间，超过这个时间，从节点不会启动故障迁移。假设 cluster-node-timeout=5，cluster-slave-validity-factor=10，则如果从节点跟主节点失联超过 50 秒，此从节点不能成为主节点。注意，如果此参数配置为非 0，将可能出现由于某主节点失联却没有从节点能顶上的情况，从而导致集群不能正常工作，在这种情况下，只有等到原来的主节点重新回归到集群，集群才恢复运作。\n * cluster-migration-barrier <count> - 主节点需要的最小从节点数，只有达到这个数，主节点失败时，它从节点才会进行迁移。更详细介绍可以看本教程后面关于副本迁移到部分。\n * cluster-require-full-coverage <yes/no> - 在部分 key 所在的节点不可用时，如果此参数设置为”yes”(默认值), 则整个集群停止接受操作；如果此参数设置为”no”，则集群依然为可达节点上的 key 提供读操作。\n\n\n# 5. 其他 Redis 集群方案\n\nRedis Cluster 不太适合用于大规模集群，所以，如果要构建超大 Redis 集群，需要选择替代方案。一般有三种方案类型：\n\n * 客户端分区方案\n * 代理分区方案\n * 查询路由方案\n\n\n# 5.1. 客户端分区方案\n\n客户端 就已经决定数据会被 存储 到哪个 Redis 节点或者从哪个 Redis 节点 读取数据。其主要思想是采用 哈希算法 将 Redis 数据的 key 进行散列，通过 hash 函数，特定的 key会 映射 到特定的 Redis 节点上。\n\n客户端分区方案 的代表为 Redis Sharding，Redis Sharding 是 Redis Cluster 出来之前，业界普遍使用的 Redis 多实例集群 方法。Java 的 Redis 客户端驱动库 Jedis，支持 Redis Sharding 功能，即 ShardedJedis 以及 结合缓存池 的 ShardedJedisPool。\n\n * 优点：不使用 第三方中间件，分区逻辑 可控，配置 简单，节点之间无关联，容易 线性扩展，灵活性强。\n\n * 缺点：客户端 无法 动态增删 服务节点，客户端需要自行维护 分发逻辑，客户端之间 无连接共享，会造成 连接浪费。\n\n\n# 5.2. 代理分区方案\n\n客户端 发送请求到一个 代理组件，代理 解析 客户端 的数据，并将请求转发至正确的节点，最后将结果回复给客户端。\n\n * 优点：简化 客户端 的分布式逻辑，客户端 透明接入，切换成本低，代理的 转发 和 存储 分离。\n * 缺点：多了一层 代理层，加重了 架构部署复杂度 和 性能损耗。\n\n代理分区 主流实现的有方案有 Twemproxy 和 Codis。\n\n# 5.2.1. Twemproxy\n\nTwemproxy 也叫 nutcraker，是 Twitter 开源的一个 Redis 和 Memcache 的 中间代理服务器 程序。\n\nTwemproxy 作为 代理，可接受来自多个程序的访问，按照 路由规则，转发给后台的各个 Redis 服务器，再原路返回。Twemproxy 存在 单点故障 问题，需要结合 Lvs 和 Keepalived 做 高可用方案。\n\n * 优点：应用范围广，稳定性较高，中间代理层 高可用。\n * 缺点：无法平滑地 水平扩容/缩容，无 可视化管理界面，运维不友好，出现故障，不能 自动转移。\n\n# 5.2.2. Codis\n\nCodis 是一个 分布式 Redis 解决方案，对于上层应用来说，连接 Codis-Proxy 和直接连接 原生的 Redis-Server 没有的区别。Codis 底层会 处理请求的转发，不停机的进行 数据迁移 等工作。Codis 采用了无状态的 代理层，对于 客户端 来说，一切都是透明的。\n\n * 优点：实现了上层 Proxy 和底层 Redis 的 高可用，数据分区 和 自动平衡，提供 命令行接口 和 RESTful API，提供 监控 和 管理 界面，可以动态 添加 和 删除 Redis 节点。\n\n * 缺点：部署架构 和 配置 复杂，不支持 跨机房 和 多租户，不支持 鉴权管理。\n\n\n# 5.3. 查询路由方案\n\n客户端随机地 请求任意一个 Redis 实例，然后由 Redis 将请求 转发 给 正确 的 Redis 节点。Redis Cluster 实现了一种 混合形式 的 查询路由，但并不是 直接 将请求从一个 Redis 节点 转发 到另一个 Redis 节点，而是在 客户端 的帮助下直接 重定向（ redirected）到正确的 Redis 节点。\n\n * 优点：去中心化，数据按照 槽 存储分布在多个 Redis 实例上，可以平滑的进行节点 扩容/缩容，支持 高可用 和 自动故障转移，运维成本低。\n\n * 缺点：重度依赖 Redis-trib 工具，缺乏 监控管理，需要依赖 Smart Client (维护连接，缓存路由表，MultiOp 和 Pipeline 支持)。Failover 节点的 检测过慢，不如有 中心节点 的集群及时（如 ZooKeeper）。Gossip 消息采用广播方式，集群规模越大，开销越大。无法根据统计区分 冷热数据。\n\n\n# 6. 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 中间件\n   * Twemproxy\n   * Codis\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * 后端存储实战课\n * 文章\n   * Redis 集群教程\n   * Redis 集群的原理和搭建\n   * 深入剖析 Redis 系列(三) - Redis 集群模式搭建与原理详解",normalizedContent:"# redis 集群\n\n> redis 集群（redis cluster） 是 redis 官方提供的分布式数据库方案。\n> \n> 既然是分布式，自然具备分布式系统的基本特性：可扩展、高可用、一致性。\n> \n>  * redis 集群通过划分 hash 槽来分区，进行数据分享。\n>  * redis 集群采用主从模型，提供复制和故障转移功能，来保证 redis 集群的高可用。\n>  * 根据 cap 理论，consistency、availability、partition tolerance 三者不可兼得，而 redis 集群的选择是 ap。redis 集群节点间采用异步通信方式，不保证强一致性，尽力达到最终一致性。\n\n\n\n\n# 1. redis cluster 分区\n\n\n# 1.1. 集群节点\n\nredis 集群由多个节点组成，节点刚启动时，彼此是相互独立的。节点通过握手（ cluster meet 命令）来将其他节点添加到自己所处的集群中。\n\n向一个节点发送 cluster meet 命令，可以让当前节点与指定 ip、port 的节点进行握手，握手成功时，当前节点会将指定节点加入所在集群。\n\n集群节点保存键值对以及过期时间的方式与单机 redis 服务完全相同。\n\nredis 集群节点分为主节点（master）和从节点（slave），其中主节点用于处理槽，而从节点则用于复制某个主节点，并在被复制的主节点下线时，代替下线主节点继续处理命令请求。\n\n\n# 1.2. 分配 hash 槽\n\n分布式存储需要解决的首要问题是把 整个数据集 按照 分区规则 映射到 多个节点 的问题，即把 数据集 划分到 多个节点 上，每个节点负责 整体数据 的一个 子集。\n\nredis 集群通过划分 hash 槽来将数据分区。redis 集群通过分区的方式来保存数据库的键值对：集群的整个数据库被分为 16384 个哈希槽（slot），数据库中的每个键都属于这 16384 个槽的其中一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态。\n\n通过向节点发送 cluster addslots 命令，可以将一个或多个槽指派给节点负责。\n\n> cluster addslots 1 2 3\nok\n\n\n集群中的每个节点负责一部分哈希槽，比如集群中有３个节点，则：\n\n * 节点ａ存储的哈希槽范围是：0 – 5500\n * 节点ｂ存储的哈希槽范围是：5501 – 11000\n * 节点ｃ存储的哈希槽范围是：11001 – 16384\n\n\n# 1.3. 寻址\n\n当客户端向节点发送与数据库键有关的命令时，接受命令的节点会计算出命令要处理的数据库属于哪个槽，并检查这个槽是否指派给了自己：\n\n * 如果键所在的槽正好指派给了当前节点，那么当前节点直接执行命令。\n * 如果键所在的槽没有指派给当前节点，那么节点会向客户端返回一个 moved 错误，指引客户端重定向至正确的节点。\n\n# 1.3.1. 计算键属于哪个槽\n\n决定一个 key 应该分配到那个槽的算法是：计算该 key 的 crc16 结果再模 16834。\n\nhash_slot = crc16(key) mod 16384\n\n\n当节点计算出 key 所属的槽为 i 之后，节点会根据以下条件判断槽是否由自己负责：\n\nclusterstate.slots[i] == clusterstate.myself\n\n\n# 1.3.2. moved 错误\n\n当节点发现键所在的槽并非自己负责处理的时候，节点就会向客户端返回一个 moved 错误，指引客户端转向正在负责槽的节点。\n\nmoved 错误的格式为：\n\nmoved <slot> <ip>:<port>\n\n\n> 个人理解：moved 这种操作有点类似 http 协议中的重定向。\n\n\n# 1.4. 重新分区\n\nredis 集群的重新分区操作可以将任意数量的已经指派给某个节点（源节点）的槽改为指派给另一个节点（目标节点），并且相关槽所属的键值对也会从源节点被移动到目标节点。\n\n重新分区操作可以在线进行，在重新分区的过程中，集群不需要下线，并且源节点和目标节点都可以继续处理命令请求。\n\nredis 集群的重新分区操作由 redis 集群管理软件 redis-trib 负责执行的，redis-trib 通过向源节点和目标节点发送命令来进行重新分区操作。\n\n重新分区的实现原理如下图所示：\n\n\n\n\n# 1.5. ask 错误\n\nask 错误与 moved 的区别在于：ask 错误只是两个节点在迁移槽的过程中使用的一种临时措施，在客户端收到关于槽 x 的 ask 错误之后，客户端只会在接下来的一次命令请求中将关于槽 x 的命令请求发送至 ask 错误所指示的节点，但这种转向不会对客户端今后发送关于槽 x 的命令请求产生任何影响，客户端仍然会将关于槽 x 的命令请求发送至目前负责处理槽 x 的节点，除非 ask 错误再次出现。\n\n判断 ask 错误的过程如下图所示：\n\n\n\n\n# 2. redis cluster 故障转移\n\n\n# 2.1. 复制\n\nredis 复制机制可以参考：redis 复制\n\n\n# 2.2. 故障检测\n\n集群中每个节点都会定期向集群中的其他节点发送 ping 消息，以此来检测对方是否在线。\n\n节点的状态信息可以分为：\n\n * 在线状态；\n\n * 下线状态（fail）;\n\n * 疑似下线状态（pfail），即在规定的时间内，没有应答 ping 消息；\n\n\n# 2.3. 故障转移\n\n 1. 下线主节点的所有从节点中，会有一个从节点被选中。\n 2. 被选中的从节点会执行 slaveof no one 命令，成为新的主节点。\n 3. 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己。\n 4. 新的主节点向集群广播一条 pong 消息，告知其他节点这个从节点已变成主节点。\n\n\n# 2.4. 选举新的主节点\n\nredis 集群选举新的主节点流程基于共识算法：raft\n\n\n# 3. redis cluster 通信\n\n集群中的节点通过发送和接收消息来进行通信。\n\nredis 集群节点发送的消息主要有以下五种：\n\n * meet - 请求接收方加入发送方所在的集群。\n * ping - 集群中每个节点每隔一段时间（默认为一秒）从已知节点列表中随机选出五个节点，然后对这五个节点中最久没联系的节点发送 ping 消息，以此检测被选中的节点是否在线。\n * pong - 当接收方收到发送方发来的 meet 消息或 ping 消息时，会返回一条 pong 消息作为应答。\n * fail - 当一个主节点 a 判断另一个主节点 b 已经进入 fail 状态时，节点 a 会向集群广播一条关于节点 b 的 fail 消息，所有收到这条消息的节点都会立即将节点 b 标记为已下线。\n * publish - 当节点收到一个 publish 命令时，节点会执行这个命令，并向集群广播一条 publish 消息，所有接受到这条消息的节点都会执行相同的 publish 命令。\n\n\n# 4. redis cluster 应用\n\n\n# 4.1. 集群功能限制\n\nredis 集群相对 单机，存在一些功能限制，需要 开发人员 提前了解，在使用时做好规避。\n\n * key 批量操作 支持有限：类似 mset、mget 操作，目前只支持对具有相同 slot 值的 key 执行 批量操作。对于 映射为不同 slot 值的 key 由于执行 mget、mget 等操作可能存在于多个节点上，因此不被支持。\n\n * key 事务操作 支持有限：只支持 多 key 在 同一节点上 的 事务操作，当多个 key 分布在 不同 的节点上时 无法 使用事务功能。\n\n * key 作为 数据分区 的最小粒度，不能将一个 大的键值 对象如 hash、list 等映射到 不同的节点。\n\n * 不支持 多数据库空间：单机 下的 redis 可以支持 16 个数据库（db0 ~ db15），集群模式 下只能使用 一个 数据库空间，即 db0。\n\n * 复制结构 只支持一层：从节点 只能复制 主节点，不支持 嵌套树状复制 结构。\n\n\n# 4.2. 集群规模限制\n\nredis cluster 的优点是易于使用。分区、主从复制、弹性扩容这些功能都可以做到自动化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 redis 集群，并且对于应用来说，近乎于是透明的。\n\n所以，redis cluster 非常适合构建中小规模 redis 集群，这里的中小规模指的是，大概几个到几十个节点这样规模的 redis 集群。\n\n但是 redis cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设计。\n\nredis 的每个节点上，都保存了所有槽和节点的映射关系表，客户端可以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。那么，这个映射关系表是如何更新的呢？redis cluster 采用了一种去中心化的流言 (gossip) 协议来传播集群配置的变化。\n\ngossip 协议的优点是去中心化；缺点是传播速度慢，并且是集群规模越大，传播的越慢。\n\n\n# 4.3. 集群配置\n\n我们后面会部署一个 redis 集群作为例子，在那之前，先介绍一下集群在 redis.conf 中的参数。\n\n * cluster-enabled <yes/no> - 如果配置”yes”则开启集群功能，此 redis 实例作为集群的一个节点，否则，它是一个普通的单一的 redis 实例。\n * cluster-config-file <filename> - 注意：虽然此配置的名字叫“集群配置文件”，但是此配置文件不能人工编辑，它是集群节点自动维护的文件，主要用于记录集群中有哪些节点、他们的状态以及一些持久化参数等，方便在重启时恢复这些状态。通常是在收到请求之后这个文件就会被更新。\n * cluster-node-timeout <milliseconds> - 这是集群中的节点能够失联的最大时间，超过这个时间，该节点就会被认为故障。如果主节点超过这个时间还是不可达，则用它的从节点将启动故障迁移，升级成主节点。注意，任何一个节点在这个时间之内如果还是没有连上大部分的主节点，则此节点将停止接收任何请求。\n * cluster-slave-validity-factor <factor> - 如果设置成０，则无论从节点与主节点失联多久，从节点都会尝试升级成主节点。如果设置成正数，则 cluster-node-timeout 乘以 cluster-slave-validity-factor 得到的时间，是从节点与主节点失联后，此从节点数据有效的最长时间，超过这个时间，从节点不会启动故障迁移。假设 cluster-node-timeout=5，cluster-slave-validity-factor=10，则如果从节点跟主节点失联超过 50 秒，此从节点不能成为主节点。注意，如果此参数配置为非 0，将可能出现由于某主节点失联却没有从节点能顶上的情况，从而导致集群不能正常工作，在这种情况下，只有等到原来的主节点重新回归到集群，集群才恢复运作。\n * cluster-migration-barrier <count> - 主节点需要的最小从节点数，只有达到这个数，主节点失败时，它从节点才会进行迁移。更详细介绍可以看本教程后面关于副本迁移到部分。\n * cluster-require-full-coverage <yes/no> - 在部分 key 所在的节点不可用时，如果此参数设置为”yes”(默认值), 则整个集群停止接受操作；如果此参数设置为”no”，则集群依然为可达节点上的 key 提供读操作。\n\n\n# 5. 其他 redis 集群方案\n\nredis cluster 不太适合用于大规模集群，所以，如果要构建超大 redis 集群，需要选择替代方案。一般有三种方案类型：\n\n * 客户端分区方案\n * 代理分区方案\n * 查询路由方案\n\n\n# 5.1. 客户端分区方案\n\n客户端 就已经决定数据会被 存储 到哪个 redis 节点或者从哪个 redis 节点 读取数据。其主要思想是采用 哈希算法 将 redis 数据的 key 进行散列，通过 hash 函数，特定的 key会 映射 到特定的 redis 节点上。\n\n客户端分区方案 的代表为 redis sharding，redis sharding 是 redis cluster 出来之前，业界普遍使用的 redis 多实例集群 方法。java 的 redis 客户端驱动库 jedis，支持 redis sharding 功能，即 shardedjedis 以及 结合缓存池 的 shardedjedispool。\n\n * 优点：不使用 第三方中间件，分区逻辑 可控，配置 简单，节点之间无关联，容易 线性扩展，灵活性强。\n\n * 缺点：客户端 无法 动态增删 服务节点，客户端需要自行维护 分发逻辑，客户端之间 无连接共享，会造成 连接浪费。\n\n\n# 5.2. 代理分区方案\n\n客户端 发送请求到一个 代理组件，代理 解析 客户端 的数据，并将请求转发至正确的节点，最后将结果回复给客户端。\n\n * 优点：简化 客户端 的分布式逻辑，客户端 透明接入，切换成本低，代理的 转发 和 存储 分离。\n * 缺点：多了一层 代理层，加重了 架构部署复杂度 和 性能损耗。\n\n代理分区 主流实现的有方案有 twemproxy 和 codis。\n\n# 5.2.1. twemproxy\n\ntwemproxy 也叫 nutcraker，是 twitter 开源的一个 redis 和 memcache 的 中间代理服务器 程序。\n\ntwemproxy 作为 代理，可接受来自多个程序的访问，按照 路由规则，转发给后台的各个 redis 服务器，再原路返回。twemproxy 存在 单点故障 问题，需要结合 lvs 和 keepalived 做 高可用方案。\n\n * 优点：应用范围广，稳定性较高，中间代理层 高可用。\n * 缺点：无法平滑地 水平扩容/缩容，无 可视化管理界面，运维不友好，出现故障，不能 自动转移。\n\n# 5.2.2. codis\n\ncodis 是一个 分布式 redis 解决方案，对于上层应用来说，连接 codis-proxy 和直接连接 原生的 redis-server 没有的区别。codis 底层会 处理请求的转发，不停机的进行 数据迁移 等工作。codis 采用了无状态的 代理层，对于 客户端 来说，一切都是透明的。\n\n * 优点：实现了上层 proxy 和底层 redis 的 高可用，数据分区 和 自动平衡，提供 命令行接口 和 restful api，提供 监控 和 管理 界面，可以动态 添加 和 删除 redis 节点。\n\n * 缺点：部署架构 和 配置 复杂，不支持 跨机房 和 多租户，不支持 鉴权管理。\n\n\n# 5.3. 查询路由方案\n\n客户端随机地 请求任意一个 redis 实例，然后由 redis 将请求 转发 给 正确 的 redis 节点。redis cluster 实现了一种 混合形式 的 查询路由，但并不是 直接 将请求从一个 redis 节点 转发 到另一个 redis 节点，而是在 客户端 的帮助下直接 重定向（ redirected）到正确的 redis 节点。\n\n * 优点：去中心化，数据按照 槽 存储分布在多个 redis 实例上，可以平滑的进行节点 扩容/缩容，支持 高可用 和 自动故障转移，运维成本低。\n\n * 缺点：重度依赖 redis-trib 工具，缺乏 监控管理，需要依赖 smart client (维护连接，缓存路由表，multiop 和 pipeline 支持)。failover 节点的 检测过慢，不如有 中心节点 的集群及时（如 zookeeper）。gossip 消息采用广播方式，集群规模越大，开销越大。无法根据统计区分 冷热数据。\n\n\n# 6. 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 中间件\n   * twemproxy\n   * codis\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * 后端存储实战课\n * 文章\n   * redis 集群教程\n   * redis 集群的原理和搭建\n   * 深入剖析 redis 系列(三) - redis 集群模式搭建与原理详解",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 实战",frontmatter:{title:"Redis 实战",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis"],permalink:"/pages/1fc9c4/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/08.Redis%E5%AE%9E%E6%88%98.html",relativePath:"12.数据库/05.KV数据库/01.Redis/08.Redis实战.md",key:"v-2ccee58b",path:"/pages/1fc9c4/",headers:[{level:2,title:"一、应用场景",slug:"一、应用场景",normalizedTitle:"一、应用场景",charIndex:15},{level:3,title:"缓存",slug:"缓存",normalizedTitle:"缓存",charIndex:58},{level:3,title:"BitMap 和 BloomFilter",slug:"bitmap-和-bloomfilter",normalizedTitle:"bitmap 和 bloomfilter",charIndex:175},{level:3,title:"分布式锁",slug:"分布式锁",normalizedTitle:"分布式锁",charIndex:384},{level:2,title:"二、技巧",slug:"二、技巧",normalizedTitle:"二、技巧",charIndex:597},{level:3,title:"keys 和 scan",slug:"keys-和-scan",normalizedTitle:"keys 和 scan",charIndex:638},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1023}],headersStr:"一、应用场景 缓存 BitMap 和 BloomFilter 分布式锁 二、技巧 keys 和 scan 参考资料",content:"# Redis 实战\n\n\n# 一、应用场景\n\nRedis 可以应用于很多场景，这里列举几个经典的应用场景。\n\n\n# 缓存\n\n缓存是 Redis 最常见的应用场景。\n\nRedis 有多种数据类型，以及丰富的操作命令，并且有着高性能、高可用的特性，非常适合用于分布式缓存。\n\n> 缓存应用的基本原理，请参考 缓存基本原理 第四 ~ 第六节内容。\n\n\n# BitMap 和 BloomFilter\n\nRedis 除了 5 种基本数据类型外，还支持 BitMap 和 BloomFilter（即布隆过滤器，可以通过 Redis Module 支持）。\n\nBitMap 和 BloomFilter 都可以用于解决缓存穿透问题。要点在于：过滤一些不可能存在的数据。\n\n> 什么是缓存穿透，可以参考：缓存基本原理\n\n小数据量可以用 BitMap，大数据量可以用布隆过滤器。\n\n\n# 分布式锁\n\n使用 Redis 作为分布式锁，基本要点如下：\n\n * 互斥性 - 使用 setnx 抢占锁。\n * 避免永远不释放锁 - 使用 expire 加一个过期时间，避免一直不释放锁，导致阻塞。\n * 原子性 - setnx 和 expire 必须合并为一个原子指令，避免 setnx 后，机器崩溃，没来得及设置 expire，从而导致锁永不释放。\n\n> 更多分布式锁的实现方式及细节，请参考：分布式锁基本原理\n\n\n# 二、技巧\n\n根据 Redis 的特性，在实际应用中，存在一些应用小技巧。\n\n\n# keys 和 scan\n\n使用 keys 指令可以扫出指定模式的 key 列表。\n\n如果这个 redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？\n\n首先，Redis 是单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。\n\n这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。\n\n不过，增量式迭代命令也不是没有缺点的： 举个例子， 使用 SMEMBERS 命令可以返回集合键当前包含的所有元素， 但是对于 SCAN 这类增量式迭代命令来说， 因为在对键进行增量式迭代的过程中， 键可能会被修改， 所以增量式迭代命令只能对被返回的元素提供有限的保证 。\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考\n * 文章\n   * 《我们一起进大厂》系列- Redis 基础",normalizedContent:"# redis 实战\n\n\n# 一、应用场景\n\nredis 可以应用于很多场景，这里列举几个经典的应用场景。\n\n\n# 缓存\n\n缓存是 redis 最常见的应用场景。\n\nredis 有多种数据类型，以及丰富的操作命令，并且有着高性能、高可用的特性，非常适合用于分布式缓存。\n\n> 缓存应用的基本原理，请参考 缓存基本原理 第四 ~ 第六节内容。\n\n\n# bitmap 和 bloomfilter\n\nredis 除了 5 种基本数据类型外，还支持 bitmap 和 bloomfilter（即布隆过滤器，可以通过 redis module 支持）。\n\nbitmap 和 bloomfilter 都可以用于解决缓存穿透问题。要点在于：过滤一些不可能存在的数据。\n\n> 什么是缓存穿透，可以参考：缓存基本原理\n\n小数据量可以用 bitmap，大数据量可以用布隆过滤器。\n\n\n# 分布式锁\n\n使用 redis 作为分布式锁，基本要点如下：\n\n * 互斥性 - 使用 setnx 抢占锁。\n * 避免永远不释放锁 - 使用 expire 加一个过期时间，避免一直不释放锁，导致阻塞。\n * 原子性 - setnx 和 expire 必须合并为一个原子指令，避免 setnx 后，机器崩溃，没来得及设置 expire，从而导致锁永不释放。\n\n> 更多分布式锁的实现方式及细节，请参考：分布式锁基本原理\n\n\n# 二、技巧\n\n根据 redis 的特性，在实际应用中，存在一些应用小技巧。\n\n\n# keys 和 scan\n\n使用 keys 指令可以扫出指定模式的 key 列表。\n\n如果这个 redis 正在给线上的业务提供服务，那使用 keys 指令会有什么问题？\n\n首先，redis 是单线程的。keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。\n\n这个时候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 keys 指令长。\n\n不过，增量式迭代命令也不是没有缺点的： 举个例子， 使用 smembers 命令可以返回集合键当前包含的所有元素， 但是对于 scan 这类增量式迭代命令来说， 因为在对键进行增量式迭代的过程中， 键可能会被修改， 所以增量式迭代命令只能对被返回的元素提供有限的保证 。\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考\n * 文章\n   * 《我们一起进大厂》系列- redis 基础",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 运维",frontmatter:{title:"Redis 运维",date:"2020-06-24T10:45:38.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis","运维"],permalink:"/pages/537098/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/20.Redis%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/05.KV数据库/01.Redis/20.Redis运维.md",key:"v-5ed748af",path:"/pages/537098/",headers:[{level:2,title:"一、Redis 安装",slug:"一、redis-安装",normalizedTitle:"一、redis 安装",charIndex:90},{level:3,title:"Window 下安装",slug:"window-下安装",normalizedTitle:"window 下安装",charIndex:105},{level:3,title:"Linux 下安装",slug:"linux-下安装",normalizedTitle:"linux 下安装",charIndex:521},{level:3,title:"Ubuntu 下安装",slug:"ubuntu-下安装",normalizedTitle:"ubuntu 下安装",charIndex:926},{level:3,title:"开机启动",slug:"开机启动",normalizedTitle:"开机启动",charIndex:1028},{level:3,title:"开放防火墙端口",slug:"开放防火墙端口",normalizedTitle:"开放防火墙端口",charIndex:1116},{level:3,title:"Redis 安装脚本",slug:"redis-安装脚本",normalizedTitle:"redis 安装脚本",charIndex:1262},{level:2,title:"二、Redis 单机使用和配置",slug:"二、redis-单机使用和配置",normalizedTitle:"二、redis 单机使用和配置",charIndex:1797},{level:3,title:"启动 Redis",slug:"启动-redis",normalizedTitle:"启动 redis",charIndex:806},{level:3,title:"Redis 常见配置",slug:"redis-常见配置",normalizedTitle:"redis 常见配置",charIndex:2110},{level:3,title:"设为守护进程",slug:"设为守护进程",normalizedTitle:"设为守护进程",charIndex:2475},{level:4,title:"远程访问",slug:"远程访问",normalizedTitle:"远程访问",charIndex:2547},{level:4,title:"设置密码",slug:"设置密码",normalizedTitle:"设置密码",charIndex:2623},{level:4,title:"配置参数表",slug:"配置参数表",normalizedTitle:"配置参数表",charIndex:2708},{level:3,title:"压力测试",slug:"压力测试",normalizedTitle:"压力测试",charIndex:8470},{level:2,title:"三、Redis 集群使用和配置",slug:"三、redis-集群使用和配置",normalizedTitle:"三、redis 集群使用和配置",charIndex:8900},{level:3,title:"集群规划",slug:"集群规划",normalizedTitle:"集群规划",charIndex:8940},{level:3,title:"部署集群",slug:"部署集群",normalizedTitle:"部署集群",charIndex:9380},{level:3,title:"部署哨兵",slug:"部署哨兵",normalizedTitle:"部署哨兵",charIndex:14391},{level:3,title:"扩容",slug:"扩容",normalizedTitle:"扩容",charIndex:15521},{level:2,title:"四、Redis 命令",slug:"四、redis-命令",normalizedTitle:"四、redis 命令",charIndex:20733},{level:3,title:"通用命令",slug:"通用命令",normalizedTitle:"通用命令",charIndex:20748},{level:3,title:"集群命令",slug:"集群命令",normalizedTitle:"集群命令",charIndex:20874},{level:4,title:"重新分片",slug:"重新分片",normalizedTitle:"重新分片",charIndex:21940},{level:2,title:"五、客户端",slug:"五、客户端",normalizedTitle:"五、客户端",charIndex:22085},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:22121}],headersStr:"一、Redis 安装 Window 下安装 Linux 下安装 Ubuntu 下安装 开机启动 开放防火墙端口 Redis 安装脚本 二、Redis 单机使用和配置 启动 Redis Redis 常见配置 设为守护进程 远程访问 设置密码 配置参数表 压力测试 三、Redis 集群使用和配置 集群规划 部署集群 部署哨兵 扩容 四、Redis 命令 通用命令 集群命令 重新分片 五、客户端 参考资料",content:'# Redis 运维\n\n> Redis 是一个高性能的 key-value 数据库。\n> \n> SET 操作每秒钟 110000 次；GET 操作每秒钟 81000 次。\n\n\n# 一、Redis 安装\n\n\n# Window 下安装\n\n下载地址：https://github.com/MSOpenTech/redis/releases。\n\nRedis 支持 32 位和 64 位。这个需要根据你系统平台的实际情况选择，这里我们下载 Redis-x64-xxx.zip压缩包到 C 盘，解压后，将文件夹重新命名为 redis。\n\n打开一个 cmd 窗口 使用 cd 命令切换目录到 C:\\redis 运行 redis-server.exe redis.windows.conf 。\n\n如果想方便的话，可以把 redis 的路径加到系统的环境变量里，这样就省得再输路径了，后面的那个 redis.windows.conf 可以省略，如果省略，会启用默认的。\n\n这时候另启一个 cmd 窗口，原来的不要关闭，不然就无法访问服务端了。\n\n切换到 redis 目录下运行 redis-cli.exe -h 127.0.0.1 -p 6379 。\n\n\n# Linux 下安装\n\n下载地址： http://redis.io/download，下载最新文档版本。\n\n下载、解压、编译 Redis\n\nwget http://download.redis.io/releases/redis-5.0.4.tar.gz\ntar xzf redis-5.0.4.tar.gz\ncd redis-5.0.4\nmake\n\n\n为了编译 Redis 源码，你需要 gcc-c++和 tcl。如果你的系统是 CentOS，可以直接执行命令：yum install -y gcc-c++ tcl 来安装。\n\n进入到解压后的 src 目录，通过如下命令启动 Redis:\n\nsrc/redis-server\n\n\n您可以使用内置的客户端与 Redis 进行交互:\n\n$ src/redis-cli\nredis> set foo bar\nOK\nredis> get foo\n"bar"\n\n\n\n# Ubuntu 下安装\n\n在 Ubuntu 系统安装 Redis 可以使用以下命令:\n\nsudo apt-get update\nsudo apt-get install redis-server\n\n\n\n# 开机启动\n\n * 开机启动配置：echo "/usr/local/bin/redis-server /etc/redis.conf" >> /etc/rc.local\n\n\n# 开放防火墙端口\n\n * 添加规则：iptables -I INPUT -p tcp -m tcp --dport 6379 -j ACCEPT\n * 保存规则：service iptables save\n * 重启 iptables：service iptables restart\n\n\n# Redis 安装脚本\n\n> CentOS7 环境安装脚本：软件运维配置脚本集合\n\n安装说明\n\n * 采用编译方式安装 Redis, 并将其注册为 systemd 服务\n * 安装路径为：/usr/local/redis\n * 默认下载安装 5.0.4 版本，端口号为：6379，密码为空\n\n使用方法\n\n * 默认安装 - 执行以下任意命令即可：\n\ncurl -o- https://gitee.com/turnon/linux-tutorial/raw/master/codes/linux/soft/redis-install.sh | bash\nwget -qO- https://gitee.com/turnon/linux-tutorial/raw/master/codes/linux/soft/redis-install.sh | bash\n\n\n * 自定义安装 - 下载脚本到本地，并按照以下格式执行：\n\nsh redis-install.sh [version] [port] [password]\n\n\n参数说明：\n\n * version - redis 版本号\n * port - redis 服务端口号\n * password - 访问密码\n\n\n# 二、Redis 单机使用和配置\n\n\n# 启动 Redis\n\n启动 redis 服务\n\ncd /usr/local/redis/src\n./redis-server\n\n\n启动 redis 客户端\n\ncd /usr/local/redis/src\n./redis-cli\n\n\n查看 redis 是否启动\n\nredis-cli\n\n\n以上命令将打开以下终端：\n\nredis 127.0.0.1:6379>\n\n\n127.0.0.1 是本机 IP ，6379 是 redis 服务端口。现在我们输入 PING 命令。\n\nredis 127.0.0.1:6379> ping\nPONG\n\n\n以上说明我们已经成功启动了 redis。\n\n\n# Redis 常见配置\n\n> Redis 默认的配置文件是根目录下的 redis.conf 文件。\n> \n> 如果需要指定特定文件作为配置文件，需要使用命令： ./redis-server -c xxx.conf\n> \n> 每次修改配置后，需要重启才能生效。\n> \n> Redis 官方默认配置：\n> \n>  * 自描述文档 redis.conf for Redis 2.8\n>  * 自描述文档 redis.conf for Redis 2.6.\n>  * 自描述文档 redis.conf for Redis 2.4.\n> \n> 自 Redis2.6 起就可以直接通过命令行传递 Redis 配置参数。这种方法可以用于测试。自 Redis2.6 起就可以直接通过命令行传递 Redis 配置参数。这种方法可以用于测试。\n\n\n# 设为守护进程\n\nRedis 默认以非守护进程方式启动，而通常我们会将 Redis 设为守护进程启动方式，配置：daemonize yes\n\n# 远程访问\n\nRedis 默认绑定 127.0.0.1，这样就只能本机才能访问，若要 Redis 允许远程访问，需要配置：bind 0.0.0.0\n\n# 设置密码\n\nRedis 默认访问不需要密码，如果需要设置密码，需要如下配置：\n\n * protected-mode yes\n * requirepass <密码>\n\n# 配置参数表\n\n配置项                                                         说明\ndaemonize no                                                Redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程（Windows\n                                                            不支持守护线程的配置为 no ）\npidfile /var/run/redis.pid                                  当 Redis 以守护进程方式运行时，Redis 默认会把 pid 写入 /var/run/redis.pid\n                                                            文件，可以通过 pidfile 指定\nport 6379                                                   指定 Redis 监听端口，默认端口为 6379，作者在自己的一篇博文中解释了为什么选用 6379 作为默认端口，因为\n                                                            6379 在手机按键上 MERZ 对应的号码，而 MERZ 取自意大利歌女 Alessia Merz 的名字\nbind 127.0.0.1                                              绑定的主机地址\ntimeout 300                                                 当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能\nloglevel notice                                             指定日志记录级别，Redis 总共支持四个级别：debug、verbose、notice、warning，默认为\n                                                            notice\nlogfile stdout                                              日志记录方式，默认为标准输出，如果配置 Redis\n                                                            为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 /dev/null\ndatabases 16                                                设置数据库的数量，默认数据库为 0，可以使用 SELECT 命令在连接上指定数据库 id\nsave <seconds> <changes> Redis 默认配置文件中提供了三个条件：save 900      指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合\n1、save 300 10、save 60 10000 分别表示 900 秒（15 分钟）内有 1 个更改，300\n秒（5 分钟）内有 10 个更改以及 60 秒内有 10000 个更改。\nrdbcompression yes                                          指定存储至本地数据库时是否压缩数据，默认为 yes，Redis 采用 LZF 压缩，如果为了节省 CPU\n                                                            时间，可以关闭该选项，但会导致数据库文件变的巨大\ndbfilename dump.rdb                                         指定本地数据库文件名，默认值为 dump.rdb\ndir ./                                                      指定本地数据库存放目录\nslaveof <masterip> <masterport>                             设置当本机为 slav 服务时，设置 master 服务的 IP 地址及端口，在 Redis 启动时，它会自动从\n                                                            master 进行数据同步\nmasterauth <master-password>                                当 master 服务设置了密码保护时，slav 服务连接 master 的密码\nrequirepass foobared                                        设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需要通过 AUTH <password>\n                                                            命令提供密码，默认关闭\nmaxclients 128                                              设置同一时间最大客户端连接数，默认无限制，Redis 可以同时打开的客户端连接数为 Redis\n                                                            进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis\n                                                            会关闭新的连接并向客户端返回 max number of clients reached 错误信息\nmaxmemory <bytes>                                           指定 Redis 最大内存限制，Redis 在启动时会把数据加载到内存中，达到最大内存后，Redis\n                                                            会先尝试清除已到期或即将到期的 Key，当此方法处理\n                                                            后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis 新的 vm 机制，会把 Key\n                                                            存放内存，Value 会存放在 swap 区\nappendonly no                                               指定是否在每次更新操作后进行日志记录，Redis\n                                                            在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis\n                                                            本身同步数据文件是按上面 save 条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为 no\nappendfilename appendonly.aof                               指定更新日志文件名，默认为 appendonly.aof\nappendfsync everysec                                        指定更新日志条件，共有 3\n                                                            个可选值：no：表示等操作系统进行数据缓存同步到磁盘（快）always：表示每次更新操作后手动调用 fsync()\n                                                            将数据写到磁盘（慢，安全）everysec：表示每秒同步一次（折中，默认值）\nvm-enabled no                                               指定是否启用虚拟内存机制，默认值为 no，简单的介绍一下，VM 机制将数据分页存放，由 Redis\n                                                            将访问量较少的页即冷数据 swap 到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析 Redis\n                                                            的 VM 机制）\nvm-swap-file /tmp/redis.swap                                虚拟内存文件路径，默认值为 /tmp/redis.swap，不可多个 Redis 实例共享\nvm-max-memory 0                                             将所有大于 vm-max-memory 的数据存入虚拟内存，无论 vm-max-memory\n                                                            设置多小，所有索引数据都是内存存储的(Redis 的索引数据 就是 keys)，也就是说，当 vm-max-memory\n                                                            设置为 0 的时候，其实是所有 value 都存在于磁盘。默认值为 0\nvm-page-size 32                                             Redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上面，但一个 page\n                                                            上不能被多个对象共享，vm-page-size 是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page\n                                                            大小最好设置为 32 或者 64bytes；如果存储很大大对象，则可以使用更大的 page，如果不确定，就使用默认值\nvm-pages 134217728                                          设置 swap 文件中的 page 数量，由于页表（一种表示页面空闲或使用的\n                                                            bitmap）是在放在内存中的，，在磁盘上每 8 个 pages 将消耗 1byte 的内存。\nvm-max-threads 4                                            设置访问 swap 文件的线程数,最好不要超过机器的核数,如果设置为 0,那么所有对 swap\n                                                            文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为 4\nglueoutputbuf yes                                           设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启\nhash-max-zipmap-entries 64 hash-max-zipmap-value 512        指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法\nactiverehashing yes                                         指定是否激活重置哈希，默认为开启（后面在介绍 Redis 的哈希算法时具体介绍）\ninclude /path/to/local.conf                                 指定包含其它的配置文件，可以在同一主机上多个 Redis\n                                                            实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件\n\n\n# 压力测试\n\n> 参考官方文档：How fast is Redis?\n\nRedis 自带了一个性能测试工具：redis-benchmark\n\n（1）基本测试\n\nredis-benchmark -q -n 100000\n\n\n * -q 表示静默（quiet）执行\n * -n 100000 请求 10 万次\n\n（2）测试指定读写指令\n\n$ redis-benchmark -t set,lpush -n 100000 -q\nSET: 74239.05 requests per second\nLPUSH: 79239.30 requests per second\n\n\n（3）测试 pipeline 模式下指定读写指令\n\nredis-benchmark -n 1000000 -t set,get -P 16 -q\nSET: 403063.28 requests per second\nGET: 508388.41 requests per second\n\n\n\n# 三、Redis 集群使用和配置\n\nRedis 3.0 后支持集群模式。\n\n\n# 集群规划\n\nRedis 集群一般由 多个节点 组成，节点数量至少为 6 个，才能保证组成 完整高可用 的集群。\n\n\n\n理想情况当然是所有节点各自在不同的机器上，首先于资源，本人在部署 Redis 集群时，只得到 3 台服务器。所以，我计划每台服务器部署 2 个 Redis 节点。\n\n【示例】最简高可用 Redis 集群规划\n\n机器配置：16G 内存 + 8 核 CPU + 1T 磁盘\n\nRedis 进程分配 10 G 内存。一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。\n\n集群拓扑：三主三从；三哨兵，每个哨兵监听所有主节点。\n\n估算性能：\n\n * 容量：三主，占用 30 G 内存，所以最大存储容量为 30 G。假设每条数据记录平均 大小为 10 K，则最大能存储 300 万条数据。\n * 吞吐量：单机一般 TPS/QPS 为 五万到八万左右。假设为五万，那么三主三从架构理论上能达到 TPS 15 万，QPS 30 万。\n\n\n# 部署集群\n\n> Redis 集群节点的安装与单节点服务相同，差异仅在于部署方式。\n> \n> 注意：为了演示方便，本示例将所有 Redis 集群节点都部署在一台机器上，实际生产环境中，基本都会将节点部署在不同机器上。要求更高的，可能还要考虑多机房部署。\n\n（1）创建节点目录\n\n我个人偏好将软件放在 /opt 目录下，在我的机器中，Redis 都安装在 /usr/local/redis 目录下。所以，下面的命令和配置都假设 Redis 安装目录为 /usr/local/redis 。\n\n确保机器上已经安装了 Redis 后，执行以下命令，创建 Redis 集群节点实例目录：\n\nsudo mkdir -p /usr/local/redis/conf/7001\nsudo mkdir -p /usr/local/redis/conf/7002\nsudo mkdir -p /usr/local/redis/conf/7003\nsudo mkdir -p /usr/local/redis/conf/7004\nsudo mkdir -p /usr/local/redis/conf/7005\nsudo mkdir -p /usr/local/redis/conf/7006\n\n\n（2）配置集群节点\n\n每个实例目录下，新建 redis.conf 配置文件。\n\n实例配置模板以 7001 节点为例（其他节点，完全替换配置中的端口号 7001 即可），如下：\n\n# 端口号\nport 7001\n# 绑定的主机端口（0.0.0.0 表示允许远程访问）\nbind 0.0.0.0\n# 以守护进程方式启动\ndaemonize yes\n\n# 开启集群模式\ncluster-enabled yes\n# 集群的配置，配置文件首次启动自动生成\ncluster-config-file /usr/local/redis/conf/7001/7001.conf\n# 请求超时时间，设置 10 秒\ncluster-node-timeout 10000\n\n# 开启 AOF 持久化\nappendonly yes\n# 数据存放目录\ndir /usr/local/redis/conf/7001\n# 进程文件\npidfile /usr/local/redis/conf/7001/7001.pid\n# 日志文件\nlogfile /usr/local/redis/conf/7001/7001.log\n\n\n（3）批量启动 Redis 节点\n\nRedis 的 utils/create-cluster 目录下自带了一个名为 create-cluster 的脚本工具，可以利用它来新建、启动、停止、重启 Redis 节点。\n\n脚本中有几个关键参数：\n\n * PORT=30000 - 初始端口号\n * TIMEOUT=2000 - 超时时间\n * NODES=6 - 节点数\n * REPLICAS=1 - 备份数\n\n脚本中的每个命令项会根据初始端口号，以及设置的节点数，遍历的去执行操作。\n\n由于前面的规划中，节点端口是从 7001 ~ 7006，所以需要将 PORT 变量设为 7000。\n\n脚本中启动每个 Redis 节点是通过指定命令行参数来配置属性。所以，我们需要改一下：\n\nPORT=7000\nTIMEOUT=2000\nNODES=6\nENDPORT=$((PORT+NODES))\n\n# ...\n\nif [ "$1" == "start" ]\nthen\n    while [ $((PORT < ENDPORT)) != "0" ]; do\n        PORT=$((PORT+1))\n        echo "Starting $PORT"\n        /usr/local/redis/src/redis-server /usr/local/redis/conf/${PORT}/redis.conf\n    done\n    exit 0\nfi\n\n\n好了，在每台服务器上，都执行 ./create-cluster start 来启动节点。\n\n然后，通过 ps 命令来确认 Redis 进程是否已经工作：\n\n# root @ dbClusterDev01 in /usr/local/redis/conf [11:07:55]\n$ ps -ef | grep redis\nroot      4604     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7001 [cluster]\nroot      4609     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7002 [cluster]\nroot      4614     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7003 [cluster]\nroot      4619     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7004 [cluster]\nroot      4624     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7005 [cluster]\nroot      4629     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7006 [cluster]\n\n\n（4）启动集群\n\n通过 redis-cli --cluster create 命令可以自动配置集群，如下：\n\n./redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.2:7003 127.0.0.2:7004 127.0.0.3:7005 127.0.0.3:7006 --cluster-replicas 1\n\n\nredis-cluster 会根据设置的节点数和副本数自动分片（分配 Hash 虚拟槽 slot），如果满意，输入 yes ，直接开始分片。\n\n>>> Performing hash slots allocation on 6 nodes...\nMaster[0] -> Slots 0 - 5460\nMaster[1] -> Slots 5461 - 10922\nMaster[2] -> Slots 10923 - 16383\nAdding replica 127.0.0.2:7004 to 127.0.0.1:7001\nAdding replica 127.0.0.3:7006 to 127.0.0.2:7003\nAdding replica 127.0.0.1:7002 to 127.0.0.3:7005\nM: b721235997deb6b9a7a2be690b5b9663db8057c6 127.0.0.1:7001\n   slots:[0-5460] (5461 slots) master\nS: bda9b7036df0bbefe601bda4ce45d3787a2e9bd9 127.0.0.1:7002\n   replicates 3623fff69b5243ed18c02a2fbb6f53069b0f1505\nM: 91523c0391a044da6cc9f53bb965aabe89502187 127.0.0.2:7003\n   slots:[5461-10922] (5462 slots) master\nS: 9d899cbe49dead7b8c4f769920cdb75714a441ae 127.0.0.2:7004\n   replicates b721235997deb6b9a7a2be690b5b9663db8057c6\nM: 3623fff69b5243ed18c02a2fbb6f53069b0f1505 127.0.0.3:7005\n   slots:[10923-16383] (5461 slots) master\nS: a2869dc153ea4977ca790b76483574a5d56cb40e 127.0.0.3:7006\n   replicates 91523c0391a044da6cc9f53bb965aabe89502187\nCan I set the above configuration? (type \'yes\' to accept): yes\n>>> Nodes configuration updated\n>>> Assign a different config epoch to each node\n>>> Sending CLUSTER MEET messages to join the cluster\nWaiting for the cluster to join\n....\n>>> Performing Cluster Check (using node 127.0.0.1:7001)\nM: b721235997deb6b9a7a2be690b5b9663db8057c6 127.0.0.1:7001\n   slots:[0-5460] (5461 slots) master\n   1 additional replica(s)\nS: a2869dc153ea4977ca790b76483574a5d56cb40e 127.0.0.1:7006\n   slots: (0 slots) slave\n   replicates 91523c0391a044da6cc9f53bb965aabe89502187\nM: 91523c0391a044da6cc9f53bb965aabe89502187 127.0.0.1:7003\n   slots:[5461-10922] (5462 slots) master\n   1 additional replica(s)\nM: 3623fff69b5243ed18c02a2fbb6f53069b0f1505 127.0.0.1:7005\n   slots:[10923-16383] (5461 slots) master\n   1 additional replica(s)\nS: 9d899cbe49dead7b8c4f769920cdb75714a441ae 127.0.0.1:7004\n   slots: (0 slots) slave\n   replicates b721235997deb6b9a7a2be690b5b9663db8057c6\nS: bda9b7036df0bbefe601bda4ce45d3787a2e9bd9 127.0.0.1:7002\n   slots: (0 slots) slave\n   replicates 3623fff69b5243ed18c02a2fbb6f53069b0f1505\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n\n\n（5）日常维护操作\n\n * 关闭集群 - ./create-cluster stop\n * 检查集群是否健康（指定任意节点即可）：./redis-cli --cluster check <ip:port>\n * 尝试修复集群节点：./redis-cli --cluster fix <ip:port>\n\n\n# 部署哨兵\n\nredis-cluster 实现了 Redis 的分片、复制。\n\n但 redis-cluster 没有解决故障转移问题，一旦任意分片的 Master 节点宕机、网络不通，就会导致 redis-cluster 的集群不能工作。为了解决高可用的问题，Redis 提供了 Redis 哨兵来监控 Redis 节点状态，并且会在 Master 宕机时，发起选举，将这个 Master 的一个 Slave 节点选举为 Master。\n\n（1）创建节点目录\n\n我个人偏好将软件放在 /opt 目录下，在我的机器中，Redis 都安装在 /usr/local/redis 目录下。所以，下面的命令和配置都假设 Redis 安装目录为 /usr/local/redis 。\n\n确保机器上已经安装了 Redis 后，执行以下命令，创建 Redis 集群节点实例目录：\n\nsudo mkdir -p /usr/local/redis/conf/27001\nsudo mkdir -p /usr/local/redis/conf/27002\nsudo mkdir -p /usr/local/redis/conf/27003\n\n\n（2）配置集群节点\n\n每个实例目录下，新建 redis.conf 配置文件。\n\n实例配置模板以 7001 节点为例（其他节点，完全替换配置中的端口号 7001 即可），如下：\n\nport 27001\ndaemonize yes\nsentinel monitor redis-master 172.22.6.3 7001 2\nsentinel down-after-milliseconds redis-master 5000\nsentinel failover-timeout redis-master 900000\nsentinel parallel-syncs redis-master 1\n#sentinel auth-pass redis-master 123456\nlogfile /usr/local/redis/conf/27001/27001.log\n\n\n（3）批量启动哨兵节点\n\n/opt/redis/src/redis-sentinel /usr/local/redis/conf/27001/sentinel.conf\n/opt/redis/src/redis-sentinel /usr/local/redis/conf/27002/sentinel.conf\n/opt/redis/src/redis-sentinel /usr/local/redis/conf/27003/sentinel.conf\n\n\n\n# 扩容\n\n（1）查看信息\n\n进入任意节点\n\n./redis-cli -h 172.22.6.3 -p 7001\n\n\ncluster info 查看集群节点状态\n\n172.22.6.3:7001> cluster nodes\nf158bf70bb2767cac271ce4efcfc14ba0b7ca98b 172.22.6.3:7006@17006 slave e7aa182e756b76ec85b471797db9b66e4b2da725 0 1594528179000 6 connected\nf348e67648460c7a800120d69b4977bf2e4524cb 172.22.6.3:7001@17001 myself,master - 0 1594528179000 1 connected 0-5460\n52601e2d4af0e64b83f4cc6d20e8316d0ac38b99 172.22.6.3:7004@17004 slave 4802fafe897160c46392c6e569d6f5e466cca696 0 1594528178000 4 connected\nc6c6a68674ae8aac3c6ec792c8af4dc1228c6c31 172.22.6.3:7005@17005 slave f348e67648460c7a800120d69b4977bf2e4524cb 0 1594528179852 5 connected\ne7aa182e756b76ec85b471797db9b66e4b2da725 172.22.6.3:7002@17002 master - 0 1594528178000 2 connected 5461-10922\n4802fafe897160c46392c6e569d6f5e466cca696 172.22.6.3:7003@17003 master - 0 1594528178000 3 connected 10923-16383\n\n\ncluster info 查看集群信息\n\n172.22.6.3:7001> cluster info\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:6\ncluster_size:3\ncluster_current_epoch:6\ncluster_my_epoch:1\ncluster_stats_messages_ping_sent:3406\ncluster_stats_messages_pong_sent:3569\ncluster_stats_messages_publish_sent:5035\ncluster_stats_messages_sent:12010\ncluster_stats_messages_ping_received:3564\ncluster_stats_messages_pong_received:3406\ncluster_stats_messages_meet_received:5\ncluster_stats_messages_publish_received:5033\ncluster_stats_messages_received:12008\n\n\n（2）添加节点到集群\n\n将已启动的节点实例添加到集群中\n\nredis-cli --cluster add-node 127.0.0.1:7007 127.0.0.1:7008\n\n\n添加主节点\n\n添加一组主节点\n\n./redis-cli --cluster add-node 172.22.6.3:7007 172.22.6.3:7001\n./redis-cli --cluster add-node 172.22.6.3:7008 172.22.6.3:7001\n./redis-cli --cluster add-node 172.22.6.3:7009 172.22.6.3:7001\n\n\n查看节点状态\n\n172.22.6.3:7001> cluster nodes\nf158bf70bb2767cac271ce4efcfc14ba0b7ca98b 172.22.6.3:7006@17006 slave e7aa182e756b76ec85b471797db9b66e4b2da725 0 1594529342575 6 connected\nf348e67648460c7a800120d69b4977bf2e4524cb 172.22.6.3:7001@17001 myself,master - 0 1594529340000 1 connected 0-5460\n55cacf121662833a4a19dbeb4a5df712cfedf77f 172.22.6.3:7009@17009 master - 0 1594529342000 0 connected\nc6c6a68674ae8aac3c6ec792c8af4dc1228c6c31 172.22.6.3:7005@17005 slave f348e67648460c7a800120d69b4977bf2e4524cb 0 1594529341573 5 connected\n4802fafe897160c46392c6e569d6f5e466cca696 172.22.6.3:7003@17003 master - 0 1594529343577 3 connected 10923-16383\ne7aa182e756b76ec85b471797db9b66e4b2da725 172.22.6.3:7002@17002 master - 0 1594529342000 2 connected 5461-10922\ne5ba78fe629115977a74fbbe1478caf8868d6d55 172.22.6.3:7007@17007 master - 0 1594529341000 0 connected\n52601e2d4af0e64b83f4cc6d20e8316d0ac38b99 172.22.6.3:7004@17004 slave 4802fafe897160c46392c6e569d6f5e466cca696 0 1594529340000 4 connected\n79d4fffc2cec210556c3b4c44e63ab506e87eda3 172.22.6.3:7008@17008 master - 0 1594529340000 7 connected\n\n\n可以发现，新加入的三个主节点，还没有分配哈希槽，所以，暂时还无法访问。\n\n添加从节点\n\n--slave：设置该参数，则新节点以 slave 的角色加入集群\n--master-id：这个参数需要设置了--slave 才能生效，--master-id 用来指定新节点的 master 节点。如果不设置该参数，则会随机为节点选择 master 节点。\n\n语法\n\nredis-cli --cluster add-node  新节点IP地址：端口    存在节点IP：端口 --cluster-slave （从节点） --cluster-master-id （master节点的ID）\nredis-cli --cluster add-node   10.42.141.119:6379  10.42.166.105:6379  --cluster-slave   --cluster-master-id  dfa238fff8a7a49230cff7eb74f573f5645c8ec5\n\n\n示例\n\n./redis-cli --cluster add-node 172.22.6.3:7010 172.22.6.3:7007 --cluster-slave\n./redis-cli --cluster add-node 172.22.6.3:7011 172.22.6.3:7008 --cluster-slave\n./redis-cli --cluster add-node 172.22.6.3:7012 172.22.6.3:7009 --cluster-slave\n\n\n查看状态\n\n172.22.6.3:7001> cluster nodes\nef5c1b9ce4cc795dc12b2c1e8736a572647b4c3e 172.22.6.3:7011@17011 slave 79d4fffc2cec210556c3b4c44e63ab506e87eda3 0 1594529492043 7 connected\nf158bf70bb2767cac271ce4efcfc14ba0b7ca98b 172.22.6.3:7006@17006 slave e7aa182e756b76ec85b471797db9b66e4b2da725 0 1594529491943 6 connected\nf348e67648460c7a800120d69b4977bf2e4524cb 172.22.6.3:7001@17001 myself,master - 0 1594529488000 1 connected 0-5460\n5140d1129ed850df59c51cf818c4eb74545d9959 172.22.6.3:7010@17010 slave e5ba78fe629115977a74fbbe1478caf8868d6d55 0 1594529488000 0 connected\n55cacf121662833a4a19dbeb4a5df712cfedf77f 172.22.6.3:7009@17009 master - 0 1594529488000 8 connected\nc6c6a68674ae8aac3c6ec792c8af4dc1228c6c31 172.22.6.3:7005@17005 slave f348e67648460c7a800120d69b4977bf2e4524cb 0 1594529490000 5 connected\n4802fafe897160c46392c6e569d6f5e466cca696 172.22.6.3:7003@17003 master - 0 1594529489939 3 connected 10923-16383\ne7aa182e756b76ec85b471797db9b66e4b2da725 172.22.6.3:7002@17002 master - 0 1594529491000 2 connected 5461-10922\ne5ba78fe629115977a74fbbe1478caf8868d6d55 172.22.6.3:7007@17007 master - 0 1594529490942 0 connected\n52601e2d4af0e64b83f4cc6d20e8316d0ac38b99 172.22.6.3:7004@17004 slave 4802fafe897160c46392c6e569d6f5e466cca696 0 1594529491000 4 connected\n02e9f57b5b45c350dc57acf1c8efa8db136db7b7 172.22.6.3:7012@17012 master - 0 1594529489000 0 connected\n79d4fffc2cec210556c3b4c44e63ab506e87eda3 172.22.6.3:7008@17008 master - 0 1594529489000 7 connected\n\n\n分配哈希槽\n\n执行 ./redis-cli --cluster rebalance 172.22.6.3:7001 --cluster-threshold 1 --cluster-use-empty-masters\n\n参数说明：\n\nrebalance：表明让 Redis 自动根据节点数进行均衡哈希槽分配。\n\n--cluster-use-empty-masters：表明\n\n\n\n执行结束后，查看状态：\n\n\n\n\n# 四、Redis 命令\n\n\n# 通用命令\n\n> 命令详细用法，请参考 Redis 命令官方文档\n> \n> 搬迁两张 cheat sheet 图，原址：https://www.cheatography.com/tasjaevan/cheat-sheets/redis/\n\n\n\n\n\n\n# 集群命令\n\n * 集群\n   * cluster info - 打印集群的信息\n   * cluster nodes - 列出集群当前已知的所有节点（ node），以及这些节点的相关信息。\n * 节点\n   * cluster meet <ip> <port> - 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。\n   * cluster forget <node_id> - 从集群中移除 node_id 指定的节点。\n   * cluster replicate <node_id> - 将当前节点设置为 node_id 指定的节点的从节点。\n   * cluster saveconfig - 将节点的配置文件保存到硬盘里面。\n * 槽(slot)\n   * cluster addslots <slot> [slot ...] - 将一个或多个槽（ slot）指派（ assign）给当前节点。\n   * cluster delslots <slot> [slot ...] - 移除一个或多个槽对当前节点的指派。\n   * cluster flushslots - 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。\n   * cluster setslot <slot> node <node_id> - 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽>，然后再进行指派。\n   * cluster setslot <slot> migrating <node_id> - 将本节点的槽 slot 迁移到 node_id 指定的节点中。\n   * cluster setslot <slot> importing <node_id> - 从 node_id 指定的节点中导入槽 slot 到本节点。\n   * cluster setslot <slot> stable - 取消对槽 slot 的导入（ import）或者迁移（ migrate）。\n * 键\n   * cluster keyslot <key> - 计算键 key 应该被放置在哪个槽上。\n   * cluster countkeysinslot <slot> - 返回槽 slot 目前包含的键值对数量。\n   * cluster getkeysinslot <slot> <count> - 返回 count 个 slot 槽中的键。\n\n# 重新分片\n\n添加节点：./redis-cli --cluster add-node 192.168.1.136:7007 192.168.1.136:7001 --cluster-slave\n\nredis-cli --cluster reshard 172.22.6.3 7001\n\n\n# 五、客户端\n\n推荐使用 RedisDesktopManager\n\n\n# 参考资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考\n * 文章\n   * 深入剖析 Redis 系列(三) - Redis 集群模式搭建与原理详解',normalizedContent:'# redis 运维\n\n> redis 是一个高性能的 key-value 数据库。\n> \n> set 操作每秒钟 110000 次；get 操作每秒钟 81000 次。\n\n\n# 一、redis 安装\n\n\n# window 下安装\n\n下载地址：https://github.com/msopentech/redis/releases。\n\nredis 支持 32 位和 64 位。这个需要根据你系统平台的实际情况选择，这里我们下载 redis-x64-xxx.zip压缩包到 c 盘，解压后，将文件夹重新命名为 redis。\n\n打开一个 cmd 窗口 使用 cd 命令切换目录到 c:\\redis 运行 redis-server.exe redis.windows.conf 。\n\n如果想方便的话，可以把 redis 的路径加到系统的环境变量里，这样就省得再输路径了，后面的那个 redis.windows.conf 可以省略，如果省略，会启用默认的。\n\n这时候另启一个 cmd 窗口，原来的不要关闭，不然就无法访问服务端了。\n\n切换到 redis 目录下运行 redis-cli.exe -h 127.0.0.1 -p 6379 。\n\n\n# linux 下安装\n\n下载地址： http://redis.io/download，下载最新文档版本。\n\n下载、解压、编译 redis\n\nwget http://download.redis.io/releases/redis-5.0.4.tar.gz\ntar xzf redis-5.0.4.tar.gz\ncd redis-5.0.4\nmake\n\n\n为了编译 redis 源码，你需要 gcc-c++和 tcl。如果你的系统是 centos，可以直接执行命令：yum install -y gcc-c++ tcl 来安装。\n\n进入到解压后的 src 目录，通过如下命令启动 redis:\n\nsrc/redis-server\n\n\n您可以使用内置的客户端与 redis 进行交互:\n\n$ src/redis-cli\nredis> set foo bar\nok\nredis> get foo\n"bar"\n\n\n\n# ubuntu 下安装\n\n在 ubuntu 系统安装 redis 可以使用以下命令:\n\nsudo apt-get update\nsudo apt-get install redis-server\n\n\n\n# 开机启动\n\n * 开机启动配置：echo "/usr/local/bin/redis-server /etc/redis.conf" >> /etc/rc.local\n\n\n# 开放防火墙端口\n\n * 添加规则：iptables -i input -p tcp -m tcp --dport 6379 -j accept\n * 保存规则：service iptables save\n * 重启 iptables：service iptables restart\n\n\n# redis 安装脚本\n\n> centos7 环境安装脚本：软件运维配置脚本集合\n\n安装说明\n\n * 采用编译方式安装 redis, 并将其注册为 systemd 服务\n * 安装路径为：/usr/local/redis\n * 默认下载安装 5.0.4 版本，端口号为：6379，密码为空\n\n使用方法\n\n * 默认安装 - 执行以下任意命令即可：\n\ncurl -o- https://gitee.com/turnon/linux-tutorial/raw/master/codes/linux/soft/redis-install.sh | bash\nwget -qo- https://gitee.com/turnon/linux-tutorial/raw/master/codes/linux/soft/redis-install.sh | bash\n\n\n * 自定义安装 - 下载脚本到本地，并按照以下格式执行：\n\nsh redis-install.sh [version] [port] [password]\n\n\n参数说明：\n\n * version - redis 版本号\n * port - redis 服务端口号\n * password - 访问密码\n\n\n# 二、redis 单机使用和配置\n\n\n# 启动 redis\n\n启动 redis 服务\n\ncd /usr/local/redis/src\n./redis-server\n\n\n启动 redis 客户端\n\ncd /usr/local/redis/src\n./redis-cli\n\n\n查看 redis 是否启动\n\nredis-cli\n\n\n以上命令将打开以下终端：\n\nredis 127.0.0.1:6379>\n\n\n127.0.0.1 是本机 ip ，6379 是 redis 服务端口。现在我们输入 ping 命令。\n\nredis 127.0.0.1:6379> ping\npong\n\n\n以上说明我们已经成功启动了 redis。\n\n\n# redis 常见配置\n\n> redis 默认的配置文件是根目录下的 redis.conf 文件。\n> \n> 如果需要指定特定文件作为配置文件，需要使用命令： ./redis-server -c xxx.conf\n> \n> 每次修改配置后，需要重启才能生效。\n> \n> redis 官方默认配置：\n> \n>  * 自描述文档 redis.conf for redis 2.8\n>  * 自描述文档 redis.conf for redis 2.6.\n>  * 自描述文档 redis.conf for redis 2.4.\n> \n> 自 redis2.6 起就可以直接通过命令行传递 redis 配置参数。这种方法可以用于测试。自 redis2.6 起就可以直接通过命令行传递 redis 配置参数。这种方法可以用于测试。\n\n\n# 设为守护进程\n\nredis 默认以非守护进程方式启动，而通常我们会将 redis 设为守护进程启动方式，配置：daemonize yes\n\n# 远程访问\n\nredis 默认绑定 127.0.0.1，这样就只能本机才能访问，若要 redis 允许远程访问，需要配置：bind 0.0.0.0\n\n# 设置密码\n\nredis 默认访问不需要密码，如果需要设置密码，需要如下配置：\n\n * protected-mode yes\n * requirepass <密码>\n\n# 配置参数表\n\n配置项                                                         说明\ndaemonize no                                                redis 默认不是以守护进程的方式运行，可以通过该配置项修改，使用 yes 启用守护进程（windows\n                                                            不支持守护线程的配置为 no ）\npidfile /var/run/redis.pid                                  当 redis 以守护进程方式运行时，redis 默认会把 pid 写入 /var/run/redis.pid\n                                                            文件，可以通过 pidfile 指定\nport 6379                                                   指定 redis 监听端口，默认端口为 6379，作者在自己的一篇博文中解释了为什么选用 6379 作为默认端口，因为\n                                                            6379 在手机按键上 merz 对应的号码，而 merz 取自意大利歌女 alessia merz 的名字\nbind 127.0.0.1                                              绑定的主机地址\ntimeout 300                                                 当客户端闲置多长时间后关闭连接，如果指定为 0，表示关闭该功能\nloglevel notice                                             指定日志记录级别，redis 总共支持四个级别：debug、verbose、notice、warning，默认为\n                                                            notice\nlogfile stdout                                              日志记录方式，默认为标准输出，如果配置 redis\n                                                            为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给 /dev/null\ndatabases 16                                                设置数据库的数量，默认数据库为 0，可以使用 select 命令在连接上指定数据库 id\nsave <seconds> <changes> redis 默认配置文件中提供了三个条件：save 900      指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合\n1、save 300 10、save 60 10000 分别表示 900 秒（15 分钟）内有 1 个更改，300\n秒（5 分钟）内有 10 个更改以及 60 秒内有 10000 个更改。\nrdbcompression yes                                          指定存储至本地数据库时是否压缩数据，默认为 yes，redis 采用 lzf 压缩，如果为了节省 cpu\n                                                            时间，可以关闭该选项，但会导致数据库文件变的巨大\ndbfilename dump.rdb                                         指定本地数据库文件名，默认值为 dump.rdb\ndir ./                                                      指定本地数据库存放目录\nslaveof <masterip> <masterport>                             设置当本机为 slav 服务时，设置 master 服务的 ip 地址及端口，在 redis 启动时，它会自动从\n                                                            master 进行数据同步\nmasterauth <master-password>                                当 master 服务设置了密码保护时，slav 服务连接 master 的密码\nrequirepass foobared                                        设置 redis 连接密码，如果配置了连接密码，客户端在连接 redis 时需要通过 auth <password>\n                                                            命令提供密码，默认关闭\nmaxclients 128                                              设置同一时间最大客户端连接数，默认无限制，redis 可以同时打开的客户端连接数为 redis\n                                                            进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，redis\n                                                            会关闭新的连接并向客户端返回 max number of clients reached 错误信息\nmaxmemory <bytes>                                           指定 redis 最大内存限制，redis 在启动时会把数据加载到内存中，达到最大内存后，redis\n                                                            会先尝试清除已到期或即将到期的 key，当此方法处理\n                                                            后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。redis 新的 vm 机制，会把 key\n                                                            存放内存，value 会存放在 swap 区\nappendonly no                                               指定是否在每次更新操作后进行日志记录，redis\n                                                            在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis\n                                                            本身同步数据文件是按上面 save 条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为 no\nappendfilename appendonly.aof                               指定更新日志文件名，默认为 appendonly.aof\nappendfsync everysec                                        指定更新日志条件，共有 3\n                                                            个可选值：no：表示等操作系统进行数据缓存同步到磁盘（快）always：表示每次更新操作后手动调用 fsync()\n                                                            将数据写到磁盘（慢，安全）everysec：表示每秒同步一次（折中，默认值）\nvm-enabled no                                               指定是否启用虚拟内存机制，默认值为 no，简单的介绍一下，vm 机制将数据分页存放，由 redis\n                                                            将访问量较少的页即冷数据 swap 到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析 redis\n                                                            的 vm 机制）\nvm-swap-file /tmp/redis.swap                                虚拟内存文件路径，默认值为 /tmp/redis.swap，不可多个 redis 实例共享\nvm-max-memory 0                                             将所有大于 vm-max-memory 的数据存入虚拟内存，无论 vm-max-memory\n                                                            设置多小，所有索引数据都是内存存储的(redis 的索引数据 就是 keys)，也就是说，当 vm-max-memory\n                                                            设置为 0 的时候，其实是所有 value 都存在于磁盘。默认值为 0\nvm-page-size 32                                             redis swap 文件分成了很多的 page，一个对象可以保存在多个 page 上面，但一个 page\n                                                            上不能被多个对象共享，vm-page-size 是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page\n                                                            大小最好设置为 32 或者 64bytes；如果存储很大大对象，则可以使用更大的 page，如果不确定，就使用默认值\nvm-pages 134217728                                          设置 swap 文件中的 page 数量，由于页表（一种表示页面空闲或使用的\n                                                            bitmap）是在放在内存中的，，在磁盘上每 8 个 pages 将消耗 1byte 的内存。\nvm-max-threads 4                                            设置访问 swap 文件的线程数,最好不要超过机器的核数,如果设置为 0,那么所有对 swap\n                                                            文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为 4\nglueoutputbuf yes                                           设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启\nhash-max-zipmap-entries 64 hash-max-zipmap-value 512        指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法\nactiverehashing yes                                         指定是否激活重置哈希，默认为开启（后面在介绍 redis 的哈希算法时具体介绍）\ninclude /path/to/local.conf                                 指定包含其它的配置文件，可以在同一主机上多个 redis\n                                                            实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件\n\n\n# 压力测试\n\n> 参考官方文档：how fast is redis?\n\nredis 自带了一个性能测试工具：redis-benchmark\n\n（1）基本测试\n\nredis-benchmark -q -n 100000\n\n\n * -q 表示静默（quiet）执行\n * -n 100000 请求 10 万次\n\n（2）测试指定读写指令\n\n$ redis-benchmark -t set,lpush -n 100000 -q\nset: 74239.05 requests per second\nlpush: 79239.30 requests per second\n\n\n（3）测试 pipeline 模式下指定读写指令\n\nredis-benchmark -n 1000000 -t set,get -p 16 -q\nset: 403063.28 requests per second\nget: 508388.41 requests per second\n\n\n\n# 三、redis 集群使用和配置\n\nredis 3.0 后支持集群模式。\n\n\n# 集群规划\n\nredis 集群一般由 多个节点 组成，节点数量至少为 6 个，才能保证组成 完整高可用 的集群。\n\n\n\n理想情况当然是所有节点各自在不同的机器上，首先于资源，本人在部署 redis 集群时，只得到 3 台服务器。所以，我计划每台服务器部署 2 个 redis 节点。\n\n【示例】最简高可用 redis 集群规划\n\n机器配置：16g 内存 + 8 核 cpu + 1t 磁盘\n\nredis 进程分配 10 g 内存。一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。\n\n集群拓扑：三主三从；三哨兵，每个哨兵监听所有主节点。\n\n估算性能：\n\n * 容量：三主，占用 30 g 内存，所以最大存储容量为 30 g。假设每条数据记录平均 大小为 10 k，则最大能存储 300 万条数据。\n * 吞吐量：单机一般 tps/qps 为 五万到八万左右。假设为五万，那么三主三从架构理论上能达到 tps 15 万，qps 30 万。\n\n\n# 部署集群\n\n> redis 集群节点的安装与单节点服务相同，差异仅在于部署方式。\n> \n> 注意：为了演示方便，本示例将所有 redis 集群节点都部署在一台机器上，实际生产环境中，基本都会将节点部署在不同机器上。要求更高的，可能还要考虑多机房部署。\n\n（1）创建节点目录\n\n我个人偏好将软件放在 /opt 目录下，在我的机器中，redis 都安装在 /usr/local/redis 目录下。所以，下面的命令和配置都假设 redis 安装目录为 /usr/local/redis 。\n\n确保机器上已经安装了 redis 后，执行以下命令，创建 redis 集群节点实例目录：\n\nsudo mkdir -p /usr/local/redis/conf/7001\nsudo mkdir -p /usr/local/redis/conf/7002\nsudo mkdir -p /usr/local/redis/conf/7003\nsudo mkdir -p /usr/local/redis/conf/7004\nsudo mkdir -p /usr/local/redis/conf/7005\nsudo mkdir -p /usr/local/redis/conf/7006\n\n\n（2）配置集群节点\n\n每个实例目录下，新建 redis.conf 配置文件。\n\n实例配置模板以 7001 节点为例（其他节点，完全替换配置中的端口号 7001 即可），如下：\n\n# 端口号\nport 7001\n# 绑定的主机端口（0.0.0.0 表示允许远程访问）\nbind 0.0.0.0\n# 以守护进程方式启动\ndaemonize yes\n\n# 开启集群模式\ncluster-enabled yes\n# 集群的配置，配置文件首次启动自动生成\ncluster-config-file /usr/local/redis/conf/7001/7001.conf\n# 请求超时时间，设置 10 秒\ncluster-node-timeout 10000\n\n# 开启 aof 持久化\nappendonly yes\n# 数据存放目录\ndir /usr/local/redis/conf/7001\n# 进程文件\npidfile /usr/local/redis/conf/7001/7001.pid\n# 日志文件\nlogfile /usr/local/redis/conf/7001/7001.log\n\n\n（3）批量启动 redis 节点\n\nredis 的 utils/create-cluster 目录下自带了一个名为 create-cluster 的脚本工具，可以利用它来新建、启动、停止、重启 redis 节点。\n\n脚本中有几个关键参数：\n\n * port=30000 - 初始端口号\n * timeout=2000 - 超时时间\n * nodes=6 - 节点数\n * replicas=1 - 备份数\n\n脚本中的每个命令项会根据初始端口号，以及设置的节点数，遍历的去执行操作。\n\n由于前面的规划中，节点端口是从 7001 ~ 7006，所以需要将 port 变量设为 7000。\n\n脚本中启动每个 redis 节点是通过指定命令行参数来配置属性。所以，我们需要改一下：\n\nport=7000\ntimeout=2000\nnodes=6\nendport=$((port+nodes))\n\n# ...\n\nif [ "$1" == "start" ]\nthen\n    while [ $((port < endport)) != "0" ]; do\n        port=$((port+1))\n        echo "starting $port"\n        /usr/local/redis/src/redis-server /usr/local/redis/conf/${port}/redis.conf\n    done\n    exit 0\nfi\n\n\n好了，在每台服务器上，都执行 ./create-cluster start 来启动节点。\n\n然后，通过 ps 命令来确认 redis 进程是否已经工作：\n\n# root @ dbclusterdev01 in /usr/local/redis/conf [11:07:55]\n$ ps -ef | grep redis\nroot      4604     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7001 [cluster]\nroot      4609     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7002 [cluster]\nroot      4614     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7003 [cluster]\nroot      4619     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7004 [cluster]\nroot      4624     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7005 [cluster]\nroot      4629     1  0 11:07 ?        00:00:00 /opt/redis/src/redis-server 0.0.0.0:7006 [cluster]\n\n\n（4）启动集群\n\n通过 redis-cli --cluster create 命令可以自动配置集群，如下：\n\n./redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.2:7003 127.0.0.2:7004 127.0.0.3:7005 127.0.0.3:7006 --cluster-replicas 1\n\n\nredis-cluster 会根据设置的节点数和副本数自动分片（分配 hash 虚拟槽 slot），如果满意，输入 yes ，直接开始分片。\n\n>>> performing hash slots allocation on 6 nodes...\nmaster[0] -> slots 0 - 5460\nmaster[1] -> slots 5461 - 10922\nmaster[2] -> slots 10923 - 16383\nadding replica 127.0.0.2:7004 to 127.0.0.1:7001\nadding replica 127.0.0.3:7006 to 127.0.0.2:7003\nadding replica 127.0.0.1:7002 to 127.0.0.3:7005\nm: b721235997deb6b9a7a2be690b5b9663db8057c6 127.0.0.1:7001\n   slots:[0-5460] (5461 slots) master\ns: bda9b7036df0bbefe601bda4ce45d3787a2e9bd9 127.0.0.1:7002\n   replicates 3623fff69b5243ed18c02a2fbb6f53069b0f1505\nm: 91523c0391a044da6cc9f53bb965aabe89502187 127.0.0.2:7003\n   slots:[5461-10922] (5462 slots) master\ns: 9d899cbe49dead7b8c4f769920cdb75714a441ae 127.0.0.2:7004\n   replicates b721235997deb6b9a7a2be690b5b9663db8057c6\nm: 3623fff69b5243ed18c02a2fbb6f53069b0f1505 127.0.0.3:7005\n   slots:[10923-16383] (5461 slots) master\ns: a2869dc153ea4977ca790b76483574a5d56cb40e 127.0.0.3:7006\n   replicates 91523c0391a044da6cc9f53bb965aabe89502187\ncan i set the above configuration? (type \'yes\' to accept): yes\n>>> nodes configuration updated\n>>> assign a different config epoch to each node\n>>> sending cluster meet messages to join the cluster\nwaiting for the cluster to join\n....\n>>> performing cluster check (using node 127.0.0.1:7001)\nm: b721235997deb6b9a7a2be690b5b9663db8057c6 127.0.0.1:7001\n   slots:[0-5460] (5461 slots) master\n   1 additional replica(s)\ns: a2869dc153ea4977ca790b76483574a5d56cb40e 127.0.0.1:7006\n   slots: (0 slots) slave\n   replicates 91523c0391a044da6cc9f53bb965aabe89502187\nm: 91523c0391a044da6cc9f53bb965aabe89502187 127.0.0.1:7003\n   slots:[5461-10922] (5462 slots) master\n   1 additional replica(s)\nm: 3623fff69b5243ed18c02a2fbb6f53069b0f1505 127.0.0.1:7005\n   slots:[10923-16383] (5461 slots) master\n   1 additional replica(s)\ns: 9d899cbe49dead7b8c4f769920cdb75714a441ae 127.0.0.1:7004\n   slots: (0 slots) slave\n   replicates b721235997deb6b9a7a2be690b5b9663db8057c6\ns: bda9b7036df0bbefe601bda4ce45d3787a2e9bd9 127.0.0.1:7002\n   slots: (0 slots) slave\n   replicates 3623fff69b5243ed18c02a2fbb6f53069b0f1505\n[ok] all nodes agree about slots configuration.\n>>> check for open slots...\n>>> check slots coverage...\n[ok] all 16384 slots covered.\n\n\n（5）日常维护操作\n\n * 关闭集群 - ./create-cluster stop\n * 检查集群是否健康（指定任意节点即可）：./redis-cli --cluster check <ip:port>\n * 尝试修复集群节点：./redis-cli --cluster fix <ip:port>\n\n\n# 部署哨兵\n\nredis-cluster 实现了 redis 的分片、复制。\n\n但 redis-cluster 没有解决故障转移问题，一旦任意分片的 master 节点宕机、网络不通，就会导致 redis-cluster 的集群不能工作。为了解决高可用的问题，redis 提供了 redis 哨兵来监控 redis 节点状态，并且会在 master 宕机时，发起选举，将这个 master 的一个 slave 节点选举为 master。\n\n（1）创建节点目录\n\n我个人偏好将软件放在 /opt 目录下，在我的机器中，redis 都安装在 /usr/local/redis 目录下。所以，下面的命令和配置都假设 redis 安装目录为 /usr/local/redis 。\n\n确保机器上已经安装了 redis 后，执行以下命令，创建 redis 集群节点实例目录：\n\nsudo mkdir -p /usr/local/redis/conf/27001\nsudo mkdir -p /usr/local/redis/conf/27002\nsudo mkdir -p /usr/local/redis/conf/27003\n\n\n（2）配置集群节点\n\n每个实例目录下，新建 redis.conf 配置文件。\n\n实例配置模板以 7001 节点为例（其他节点，完全替换配置中的端口号 7001 即可），如下：\n\nport 27001\ndaemonize yes\nsentinel monitor redis-master 172.22.6.3 7001 2\nsentinel down-after-milliseconds redis-master 5000\nsentinel failover-timeout redis-master 900000\nsentinel parallel-syncs redis-master 1\n#sentinel auth-pass redis-master 123456\nlogfile /usr/local/redis/conf/27001/27001.log\n\n\n（3）批量启动哨兵节点\n\n/opt/redis/src/redis-sentinel /usr/local/redis/conf/27001/sentinel.conf\n/opt/redis/src/redis-sentinel /usr/local/redis/conf/27002/sentinel.conf\n/opt/redis/src/redis-sentinel /usr/local/redis/conf/27003/sentinel.conf\n\n\n\n# 扩容\n\n（1）查看信息\n\n进入任意节点\n\n./redis-cli -h 172.22.6.3 -p 7001\n\n\ncluster info 查看集群节点状态\n\n172.22.6.3:7001> cluster nodes\nf158bf70bb2767cac271ce4efcfc14ba0b7ca98b 172.22.6.3:7006@17006 slave e7aa182e756b76ec85b471797db9b66e4b2da725 0 1594528179000 6 connected\nf348e67648460c7a800120d69b4977bf2e4524cb 172.22.6.3:7001@17001 myself,master - 0 1594528179000 1 connected 0-5460\n52601e2d4af0e64b83f4cc6d20e8316d0ac38b99 172.22.6.3:7004@17004 slave 4802fafe897160c46392c6e569d6f5e466cca696 0 1594528178000 4 connected\nc6c6a68674ae8aac3c6ec792c8af4dc1228c6c31 172.22.6.3:7005@17005 slave f348e67648460c7a800120d69b4977bf2e4524cb 0 1594528179852 5 connected\ne7aa182e756b76ec85b471797db9b66e4b2da725 172.22.6.3:7002@17002 master - 0 1594528178000 2 connected 5461-10922\n4802fafe897160c46392c6e569d6f5e466cca696 172.22.6.3:7003@17003 master - 0 1594528178000 3 connected 10923-16383\n\n\ncluster info 查看集群信息\n\n172.22.6.3:7001> cluster info\ncluster_state:ok\ncluster_slots_assigned:16384\ncluster_slots_ok:16384\ncluster_slots_pfail:0\ncluster_slots_fail:0\ncluster_known_nodes:6\ncluster_size:3\ncluster_current_epoch:6\ncluster_my_epoch:1\ncluster_stats_messages_ping_sent:3406\ncluster_stats_messages_pong_sent:3569\ncluster_stats_messages_publish_sent:5035\ncluster_stats_messages_sent:12010\ncluster_stats_messages_ping_received:3564\ncluster_stats_messages_pong_received:3406\ncluster_stats_messages_meet_received:5\ncluster_stats_messages_publish_received:5033\ncluster_stats_messages_received:12008\n\n\n（2）添加节点到集群\n\n将已启动的节点实例添加到集群中\n\nredis-cli --cluster add-node 127.0.0.1:7007 127.0.0.1:7008\n\n\n添加主节点\n\n添加一组主节点\n\n./redis-cli --cluster add-node 172.22.6.3:7007 172.22.6.3:7001\n./redis-cli --cluster add-node 172.22.6.3:7008 172.22.6.3:7001\n./redis-cli --cluster add-node 172.22.6.3:7009 172.22.6.3:7001\n\n\n查看节点状态\n\n172.22.6.3:7001> cluster nodes\nf158bf70bb2767cac271ce4efcfc14ba0b7ca98b 172.22.6.3:7006@17006 slave e7aa182e756b76ec85b471797db9b66e4b2da725 0 1594529342575 6 connected\nf348e67648460c7a800120d69b4977bf2e4524cb 172.22.6.3:7001@17001 myself,master - 0 1594529340000 1 connected 0-5460\n55cacf121662833a4a19dbeb4a5df712cfedf77f 172.22.6.3:7009@17009 master - 0 1594529342000 0 connected\nc6c6a68674ae8aac3c6ec792c8af4dc1228c6c31 172.22.6.3:7005@17005 slave f348e67648460c7a800120d69b4977bf2e4524cb 0 1594529341573 5 connected\n4802fafe897160c46392c6e569d6f5e466cca696 172.22.6.3:7003@17003 master - 0 1594529343577 3 connected 10923-16383\ne7aa182e756b76ec85b471797db9b66e4b2da725 172.22.6.3:7002@17002 master - 0 1594529342000 2 connected 5461-10922\ne5ba78fe629115977a74fbbe1478caf8868d6d55 172.22.6.3:7007@17007 master - 0 1594529341000 0 connected\n52601e2d4af0e64b83f4cc6d20e8316d0ac38b99 172.22.6.3:7004@17004 slave 4802fafe897160c46392c6e569d6f5e466cca696 0 1594529340000 4 connected\n79d4fffc2cec210556c3b4c44e63ab506e87eda3 172.22.6.3:7008@17008 master - 0 1594529340000 7 connected\n\n\n可以发现，新加入的三个主节点，还没有分配哈希槽，所以，暂时还无法访问。\n\n添加从节点\n\n--slave：设置该参数，则新节点以 slave 的角色加入集群\n--master-id：这个参数需要设置了--slave 才能生效，--master-id 用来指定新节点的 master 节点。如果不设置该参数，则会随机为节点选择 master 节点。\n\n语法\n\nredis-cli --cluster add-node  新节点ip地址：端口    存在节点ip：端口 --cluster-slave （从节点） --cluster-master-id （master节点的id）\nredis-cli --cluster add-node   10.42.141.119:6379  10.42.166.105:6379  --cluster-slave   --cluster-master-id  dfa238fff8a7a49230cff7eb74f573f5645c8ec5\n\n\n示例\n\n./redis-cli --cluster add-node 172.22.6.3:7010 172.22.6.3:7007 --cluster-slave\n./redis-cli --cluster add-node 172.22.6.3:7011 172.22.6.3:7008 --cluster-slave\n./redis-cli --cluster add-node 172.22.6.3:7012 172.22.6.3:7009 --cluster-slave\n\n\n查看状态\n\n172.22.6.3:7001> cluster nodes\nef5c1b9ce4cc795dc12b2c1e8736a572647b4c3e 172.22.6.3:7011@17011 slave 79d4fffc2cec210556c3b4c44e63ab506e87eda3 0 1594529492043 7 connected\nf158bf70bb2767cac271ce4efcfc14ba0b7ca98b 172.22.6.3:7006@17006 slave e7aa182e756b76ec85b471797db9b66e4b2da725 0 1594529491943 6 connected\nf348e67648460c7a800120d69b4977bf2e4524cb 172.22.6.3:7001@17001 myself,master - 0 1594529488000 1 connected 0-5460\n5140d1129ed850df59c51cf818c4eb74545d9959 172.22.6.3:7010@17010 slave e5ba78fe629115977a74fbbe1478caf8868d6d55 0 1594529488000 0 connected\n55cacf121662833a4a19dbeb4a5df712cfedf77f 172.22.6.3:7009@17009 master - 0 1594529488000 8 connected\nc6c6a68674ae8aac3c6ec792c8af4dc1228c6c31 172.22.6.3:7005@17005 slave f348e67648460c7a800120d69b4977bf2e4524cb 0 1594529490000 5 connected\n4802fafe897160c46392c6e569d6f5e466cca696 172.22.6.3:7003@17003 master - 0 1594529489939 3 connected 10923-16383\ne7aa182e756b76ec85b471797db9b66e4b2da725 172.22.6.3:7002@17002 master - 0 1594529491000 2 connected 5461-10922\ne5ba78fe629115977a74fbbe1478caf8868d6d55 172.22.6.3:7007@17007 master - 0 1594529490942 0 connected\n52601e2d4af0e64b83f4cc6d20e8316d0ac38b99 172.22.6.3:7004@17004 slave 4802fafe897160c46392c6e569d6f5e466cca696 0 1594529491000 4 connected\n02e9f57b5b45c350dc57acf1c8efa8db136db7b7 172.22.6.3:7012@17012 master - 0 1594529489000 0 connected\n79d4fffc2cec210556c3b4c44e63ab506e87eda3 172.22.6.3:7008@17008 master - 0 1594529489000 7 connected\n\n\n分配哈希槽\n\n执行 ./redis-cli --cluster rebalance 172.22.6.3:7001 --cluster-threshold 1 --cluster-use-empty-masters\n\n参数说明：\n\nrebalance：表明让 redis 自动根据节点数进行均衡哈希槽分配。\n\n--cluster-use-empty-masters：表明\n\n\n\n执行结束后，查看状态：\n\n\n\n\n# 四、redis 命令\n\n\n# 通用命令\n\n> 命令详细用法，请参考 redis 命令官方文档\n> \n> 搬迁两张 cheat sheet 图，原址：https://www.cheatography.com/tasjaevan/cheat-sheets/redis/\n\n\n\n\n\n\n# 集群命令\n\n * 集群\n   * cluster info - 打印集群的信息\n   * cluster nodes - 列出集群当前已知的所有节点（ node），以及这些节点的相关信息。\n * 节点\n   * cluster meet <ip> <port> - 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。\n   * cluster forget <node_id> - 从集群中移除 node_id 指定的节点。\n   * cluster replicate <node_id> - 将当前节点设置为 node_id 指定的节点的从节点。\n   * cluster saveconfig - 将节点的配置文件保存到硬盘里面。\n * 槽(slot)\n   * cluster addslots <slot> [slot ...] - 将一个或多个槽（ slot）指派（ assign）给当前节点。\n   * cluster delslots <slot> [slot ...] - 移除一个或多个槽对当前节点的指派。\n   * cluster flushslots - 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。\n   * cluster setslot <slot> node <node_id> - 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽>，然后再进行指派。\n   * cluster setslot <slot> migrating <node_id> - 将本节点的槽 slot 迁移到 node_id 指定的节点中。\n   * cluster setslot <slot> importing <node_id> - 从 node_id 指定的节点中导入槽 slot 到本节点。\n   * cluster setslot <slot> stable - 取消对槽 slot 的导入（ import）或者迁移（ migrate）。\n * 键\n   * cluster keyslot <key> - 计算键 key 应该被放置在哪个槽上。\n   * cluster countkeysinslot <slot> - 返回槽 slot 目前包含的键值对数量。\n   * cluster getkeysinslot <slot> <count> - 返回 count 个 slot 槽中的键。\n\n# 重新分片\n\n添加节点：./redis-cli --cluster add-node 192.168.1.136:7007 192.168.1.136:7001 --cluster-slave\n\nredis-cli --cluster reshard 172.22.6.3 7001\n\n\n# 五、客户端\n\n推荐使用 redisdesktopmanager\n\n\n# 参考资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考\n * 文章\n   * 深入剖析 redis 系列(三) - redis 集群模式搭建与原理详解',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Redis 教程",frontmatter:{title:"Redis 教程",date:"2020-02-10T14:27:39.000Z",categories:["数据库","KV数据库","Redis"],tags:["数据库","KV数据库","Redis"],permalink:"/pages/fe3808/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/05.KV%E6%95%B0%E6%8D%AE%E5%BA%93/01.Redis/",relativePath:"12.数据库/05.KV数据库/01.Redis/README.md",key:"v-671a08c2",path:"/pages/fe3808/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:143},{level:3,title:"Redis 面试总结 💯",slug:"redis-面试总结-💯",normalizedTitle:"redis 面试总结 💯",charIndex:153},{level:3,title:"Redis 应用指南 ⚡",slug:"redis-应用指南-⚡",normalizedTitle:"redis 应用指南 ⚡",charIndex:171},{level:3,title:"Redis 数据类型和应用",slug:"redis-数据类型和应用",normalizedTitle:"redis 数据类型和应用",charIndex:217},{level:3,title:"Redis 持久化",slug:"redis-持久化",normalizedTitle:"redis 持久化",charIndex:293},{level:3,title:"Redis 复制",slug:"redis-复制",normalizedTitle:"redis 复制",charIndex:348},{level:3,title:"Redis 哨兵",slug:"redis-哨兵",normalizedTitle:"redis 哨兵",charIndex:402},{level:3,title:"Redis 集群",slug:"redis-集群",normalizedTitle:"redis 集群",charIndex:540},{level:3,title:"Redis 实战",slug:"redis-实战",normalizedTitle:"redis 实战",charIndex:621},{level:3,title:"Redis 运维 🔨",slug:"redis-运维-🔨",normalizedTitle:"redis 运维 🔨",charIndex:655},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:691},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:1123}],headersStr:"📖 内容 Redis 面试总结 💯 Redis 应用指南 ⚡ Redis 数据类型和应用 Redis 持久化 Redis 复制 Redis 哨兵 Redis 集群 Redis 实战 Redis 运维 🔨 📚 资料 🚪 传送",content:"# Redis 教程\n\n> Redis 最典型的应用场景是作为分布式缓存。\n> \n> 学习 Redis，有必要深入理解缓存的原理，以及 Redis 作为一种缓存方案，在系统应用中的定位。\n> \n> 参考：缓存基本原理，有助于理解缓存的特性、原理，使用缓存常见的问题及解决方案。\n\n\n# 📖 内容\n\n\n# Redis 面试总结 💯\n\n\n# Redis 应用指南 ⚡\n\n> 关键词：内存淘汰、事件、事务、管道、发布与订阅\n\n\n\n\n# Redis 数据类型和应用\n\n> 关键词：STRING、HASH、LIST、SET、ZSET、BitMap、HyperLogLog、Geo\n\n\n\n\n# Redis 持久化\n\n> 关键词：RDB、AOF、SAVE、BGSAVE、appendfsync\n\n\n\n\n# Redis 复制\n\n> 关键词：SLAVEOF、SYNC、PSYNC、REPLCONF ACK\n\n\n\n\n# Redis 哨兵\n\n> Redis 哨兵（Sentinel）是 Redis 的高可用性（Hight Availability）解决方案。\n> \n> Redis 哨兵是 Raft 算法 的具体实现。\n> \n> 关键词：Sentinel、PING、INFO、Raft\n\n\n\n\n# Redis 集群\n\n> 关键词：CLUSTER MEET、Hash slot、MOVED、ASK、SLAVEOF no one、redis-trib\n\n\n\n\n# Redis 实战\n\n> 关键词：缓存、分布式锁、布隆过滤器\n\n\n# Redis 运维 🔨\n\n> 关键词：安装、命令、集群、客户端\n\n\n# 📚 资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 教程\n   * Redis 命令参考\n * 文章\n   * Introduction to Redis\n   * 《我们一起进大厂》系列- Redis 基础\n * 源码\n   * 《Redis 实战》配套 Python 源码\n * 资源汇总\n   * awesome-redis\n * Redis Client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * CRUG | Redisson PRO vs. Jedis: Which Is Faster? 翻译\n   * redis 分布锁 Redisson 性能测试\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# redis 教程\n\n> redis 最典型的应用场景是作为分布式缓存。\n> \n> 学习 redis，有必要深入理解缓存的原理，以及 redis 作为一种缓存方案，在系统应用中的定位。\n> \n> 参考：缓存基本原理，有助于理解缓存的特性、原理，使用缓存常见的问题及解决方案。\n\n\n# 📖 内容\n\n\n# redis 面试总结 💯\n\n\n# redis 应用指南 ⚡\n\n> 关键词：内存淘汰、事件、事务、管道、发布与订阅\n\n\n\n\n# redis 数据类型和应用\n\n> 关键词：string、hash、list、set、zset、bitmap、hyperloglog、geo\n\n\n\n\n# redis 持久化\n\n> 关键词：rdb、aof、save、bgsave、appendfsync\n\n\n\n\n# redis 复制\n\n> 关键词：slaveof、sync、psync、replconf ack\n\n\n\n\n# redis 哨兵\n\n> redis 哨兵（sentinel）是 redis 的高可用性（hight availability）解决方案。\n> \n> redis 哨兵是 raft 算法 的具体实现。\n> \n> 关键词：sentinel、ping、info、raft\n\n\n\n\n# redis 集群\n\n> 关键词：cluster meet、hash slot、moved、ask、slaveof no one、redis-trib\n\n\n\n\n# redis 实战\n\n> 关键词：缓存、分布式锁、布隆过滤器\n\n\n# redis 运维 🔨\n\n> 关键词：安装、命令、集群、客户端\n\n\n# 📚 资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 教程\n   * redis 命令参考\n * 文章\n   * introduction to redis\n   * 《我们一起进大厂》系列- redis 基础\n * 源码\n   * 《redis 实战》配套 python 源码\n * 资源汇总\n   * awesome-redis\n * redis client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * crug | redisson pro vs. jedis: which is faster? 翻译\n   * redis 分布锁 redisson 性能测试\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Hbase",frontmatter:{title:"Hbase",date:"2020-02-10T14:27:39.000Z",categories:["数据库","列式数据库"],tags:["数据库","列式数据库","Hbase"],permalink:"/pages/7ab03c/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/06.%E5%88%97%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/01.Hbase.html",relativePath:"12.数据库/06.列式数据库/01.Hbase.md",key:"v-dbc2e0c0",path:"/pages/7ab03c/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:12},{level:2,title:"基础",slug:"基础",normalizedTitle:"基础",charIndex:32},{level:2,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:206},{level:3,title:"数据模型",slug:"数据模型",normalizedTitle:"数据模型",charIndex:213},{level:3,title:"HBase 架构",slug:"hbase-架构",normalizedTitle:"hbase 架构",charIndex:498},{level:4,title:"Regin",slug:"regin",normalizedTitle:"regin",charIndex:766},{level:4,title:"Master 服务器",slug:"master-服务器",normalizedTitle:"master 服务器",charIndex:873},{level:4,title:"Regin 服务器",slug:"regin-服务器",normalizedTitle:"regin 服务器",charIndex:1059},{level:4,title:"ZooKeeper",slug:"zookeeper",normalizedTitle:"zookeeper",charIndex:744},{level:2,title:"HBase 和 RDBMS",slug:"hbase-和-rdbms",normalizedTitle:"hbase 和 rdbms",charIndex:1442},{level:2,title:"API",slug:"api",normalizedTitle:"api",charIndex:1739},{level:2,title:"附录",slug:"附录",normalizedTitle:"附录",charIndex:1774},{level:3,title:"命令行",slug:"命令行",normalizedTitle:"命令行",charIndex:1781},{level:2,title:"更多内容",slug:"更多内容",normalizedTitle:"更多内容",charIndex:1816},{level:3,title:"扩展阅读",slug:"扩展阅读",normalizedTitle:"扩展阅读",charIndex:1825},{level:3,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1859},{level:4,title:"官方",slug:"官方",normalizedTitle:"官方",charIndex:1867},{level:4,title:"文章",slug:"文章",normalizedTitle:"文章",charIndex:1930}],headersStr:"简介 基础 原理 数据模型 HBase 架构 Regin Master 服务器 Regin 服务器 ZooKeeper HBase 和 RDBMS API 附录 命令行 更多内容 扩展阅读 参考资料 官方 文章",content:"# HBase\n\n\n# 简介\n\nHBase 是建立在 HDFS 基础上的面向列的分布式数据库。\n\n * HBase 参考了谷歌的 BigTable 建模，实现的编程语言为 Java。\n * 它是 Hadoop 项目的子项目，运行于 HDFS 文件系统之上。\n\nHBase 适用场景：实时地随机访问超大数据集。\n\n在 CAP 理论中，HBase 属于 CP 类型的系统。\n\n\n# 基础\n\nHBase 维护\n\n\n# 原理\n\n\n# 数据模型\n\nHBase 是一个面向列的数据库，在表中它由行排序。\n\nHBase 表模型结构为：\n\n * 表（table）是行的集合。\n * 行（row）是列族的集合。\n * 列族（column family）是列的集合。\n * 列（row）是键值对的集合。\n\n\n\nHBase 表的单元格（cell）由行和列的坐标交叉决定，是有版本的。默认情况下，版本号是自动分配的，为 HBase 插入单元格时的时间戳。单元格的内容是未解释的字节数组。\n\n行的键也是未解释的字节数组，所以理论上，任何数据都可以通过序列化表示成字符串或二进制，从而存为 HBase 的键值。\n\n\n\n\n# HBase 架构\n\n\n\n和 HDFS、YARN 一样，HBase 也采用 master / slave 架构：\n\n * HBase 有一个 master 节点。master 节点负责将区域（region）分配给 region 节点；恢复 region 节点的故障。\n * HBase 有多个 region 节点。region 节点负责零个或多个区域（region）的管理并相应客户端的读写请求。region 节点还负责区域的划分并通知 master 节点有了新的子区域。\n\nHBase 依赖 ZooKeeper 来实现故障恢复。\n\n# Regin\n\nHBase 表按行键范围水平自动划分为区域（region）。每个区域由表中行的子集构成。每个区域由它所属的表、它所含的第一行及最后一行来表示。\n\n区域只不过是表被拆分，并分布在区域服务器。\n\n\n\n# Master 服务器\n\n区域分配、DDL(create、delete)操作由 HBase master 服务器处理。\n\n * master 服务器负责协调 region 服务器\n   * 协助区域启动，出现故障恢复或负载均衡情况时，重新分配 region 服务器\n   * 监控集群中的所有 region 服务器\n * 支持 DDL 接口（创建、删除、更新表）\n\n\n\n# Regin 服务器\n\n区域服务器运行在 HDFS 数据节点上，具有以下组件\n\n * WAL - Write Ahead Log 是 HDFS 上的文件。WAL 存储尚未持久存储到永久存储的新数据，它用于在发生故障时进行恢复。\n\n * BlockCache - 是读缓存。它将频繁读取的数据存储在内存中。至少最近使用的数据在完整时被逐出。\n\n * MemStore - 是写缓存。它存储尚未写入磁盘的新数据。在写入磁盘之前对其进行排序。每个区域每个列族有一个 MemStore。\n\n * Hfiles - 将行存储为磁盘上的排序键值对。\n\n\n\n# ZooKeeper\n\nHBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。Zookeeper 维护哪些服务器是活动的和可用的，并提供服务器故障通知。集群至少应该有 3 个节点。\n\n\n\n\n# HBase 和 RDBMS\n\nHBASE                           RDBMS\nHBase 无模式，它不具有固定列模式的概念;仅定义列族。   RDBMS 有它的模式，描述表的整体结构的约束。\n它专门创建为宽表。 HBase 是横向扩展。          这些都是细而专为小表。很难形成规模。\n没有任何事务存在于 HBase。                RDBMS 是事务性的。\n它反规范化的数据。                       它具有规范化的数据。\n它用于半结构以及结构化数据是非常好的。             用于结构化数据非常好。\n\n\n# API\n\nJava API 归纳总结在这里：HBase 应用\n\n\n# 附录\n\n\n# 命令行\n\nHBase 命令行可以参考这里：HBase 命令行\n\n\n# 更多内容\n\n\n# 扩展阅读\n\n * HBase 命令\n * HBase 运维\n\n\n# 参考资料\n\n# 官方\n\n * HBase 官网\n * HBase 官方文档\n * HBase 官方文档中文版\n * HBase API\n\n# 文章\n\n * Bigtable: A Distributed Storage System for Structured Data\n * An In-Depth Look at the HBase Architecture",normalizedContent:"# hbase\n\n\n# 简介\n\nhbase 是建立在 hdfs 基础上的面向列的分布式数据库。\n\n * hbase 参考了谷歌的 bigtable 建模，实现的编程语言为 java。\n * 它是 hadoop 项目的子项目，运行于 hdfs 文件系统之上。\n\nhbase 适用场景：实时地随机访问超大数据集。\n\n在 cap 理论中，hbase 属于 cp 类型的系统。\n\n\n# 基础\n\nhbase 维护\n\n\n# 原理\n\n\n# 数据模型\n\nhbase 是一个面向列的数据库，在表中它由行排序。\n\nhbase 表模型结构为：\n\n * 表（table）是行的集合。\n * 行（row）是列族的集合。\n * 列族（column family）是列的集合。\n * 列（row）是键值对的集合。\n\n\n\nhbase 表的单元格（cell）由行和列的坐标交叉决定，是有版本的。默认情况下，版本号是自动分配的，为 hbase 插入单元格时的时间戳。单元格的内容是未解释的字节数组。\n\n行的键也是未解释的字节数组，所以理论上，任何数据都可以通过序列化表示成字符串或二进制，从而存为 hbase 的键值。\n\n\n\n\n# hbase 架构\n\n\n\n和 hdfs、yarn 一样，hbase 也采用 master / slave 架构：\n\n * hbase 有一个 master 节点。master 节点负责将区域（region）分配给 region 节点；恢复 region 节点的故障。\n * hbase 有多个 region 节点。region 节点负责零个或多个区域（region）的管理并相应客户端的读写请求。region 节点还负责区域的划分并通知 master 节点有了新的子区域。\n\nhbase 依赖 zookeeper 来实现故障恢复。\n\n# regin\n\nhbase 表按行键范围水平自动划分为区域（region）。每个区域由表中行的子集构成。每个区域由它所属的表、它所含的第一行及最后一行来表示。\n\n区域只不过是表被拆分，并分布在区域服务器。\n\n\n\n# master 服务器\n\n区域分配、ddl(create、delete)操作由 hbase master 服务器处理。\n\n * master 服务器负责协调 region 服务器\n   * 协助区域启动，出现故障恢复或负载均衡情况时，重新分配 region 服务器\n   * 监控集群中的所有 region 服务器\n * 支持 ddl 接口（创建、删除、更新表）\n\n\n\n# regin 服务器\n\n区域服务器运行在 hdfs 数据节点上，具有以下组件\n\n * wal - write ahead log 是 hdfs 上的文件。wal 存储尚未持久存储到永久存储的新数据，它用于在发生故障时进行恢复。\n\n * blockcache - 是读缓存。它将频繁读取的数据存储在内存中。至少最近使用的数据在完整时被逐出。\n\n * memstore - 是写缓存。它存储尚未写入磁盘的新数据。在写入磁盘之前对其进行排序。每个区域每个列族有一个 memstore。\n\n * hfiles - 将行存储为磁盘上的排序键值对。\n\n\n\n# zookeeper\n\nhbase 使用 zookeeper 作为分布式协调服务来维护集群中的服务器状态。zookeeper 维护哪些服务器是活动的和可用的，并提供服务器故障通知。集群至少应该有 3 个节点。\n\n\n\n\n# hbase 和 rdbms\n\nhbase                           rdbms\nhbase 无模式，它不具有固定列模式的概念;仅定义列族。   rdbms 有它的模式，描述表的整体结构的约束。\n它专门创建为宽表。 hbase 是横向扩展。          这些都是细而专为小表。很难形成规模。\n没有任何事务存在于 hbase。                rdbms 是事务性的。\n它反规范化的数据。                       它具有规范化的数据。\n它用于半结构以及结构化数据是非常好的。             用于结构化数据非常好。\n\n\n# api\n\njava api 归纳总结在这里：hbase 应用\n\n\n# 附录\n\n\n# 命令行\n\nhbase 命令行可以参考这里：hbase 命令行\n\n\n# 更多内容\n\n\n# 扩展阅读\n\n * hbase 命令\n * hbase 运维\n\n\n# 参考资料\n\n# 官方\n\n * hbase 官网\n * hbase 官方文档\n * hbase 官方文档中文版\n * hbase api\n\n# 文章\n\n * bigtable: a distributed storage system for structured data\n * an in-depth look at the hbase architecture",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Cassandra",frontmatter:{title:"Cassandra",date:"2019-08-22T09:02:39.000Z",categories:["数据库","列式数据库"],tags:["数据库","列式数据库","Cassandra"],permalink:"/pages/ca3ca5/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/06.%E5%88%97%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/02.Cassandra.html",relativePath:"12.数据库/06.列式数据库/02.Cassandra.md",key:"v-1c459100",path:"/pages/ca3ca5/",headers:[{level:2,title:"Quick Start",slug:"quick-start",normalizedTitle:"quick start",charIndex:81},{level:3,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:97},{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:142},{level:3,title:"特性",slug:"特性",normalizedTitle:"特性",charIndex:220},{level:4,title:"主要特性",slug:"主要特性",normalizedTitle:"主要特性",charIndex:226},{level:4,title:"突出特性",slug:"突出特性",normalizedTitle:"突出特性",charIndex:435},{level:2,title:"更多内容",slug:"更多内容",normalizedTitle:"更多内容",charIndex:810},{level:2,title:"🚪 传送门",slug:"传送门",normalizedTitle:"🚪 传送门",charIndex:856}],headersStr:"Quick Start 安装 简介 特性 主要特性 突出特性 更多内容 🚪 传送门",content:"# Cassandra\n\n> Apache Cassandra 是一个高度可扩展的分区行存储。行被组织成具有所需主键的表。\n> \n> 最新版本：v4.0\n\n\n# Quick Start\n\n\n# 安装\n\n> 先决条件\n> \n>  * JDK8+\n>  * Python 2.7\n\n\n# 简介\n\nApache Cassandra 是一套开源分布式 Key-Value 存储系统。它最初由 Facebook 开发，用于储存特别大的数据。\n\n\n# 特性\n\n# 主要特性\n\n * 分布式\n * 基于 column 的结构化\n * 高伸展性\n\nCassandra 的主要特点就是它不是一个数据库，而是由一堆数据库节点共同构成的一个分布式网络服务，对 Cassandra 的一个写操作，会被复制到其他节点上去，对 Cassandra 的读操作，也会被路由到某个节点上面去读取。对于一个 Cassandra 群集来说，扩展性能 是比较简单的事情，只管在群集里面添加节点就可以了。\n\n# 突出特性\n\n * 模式灵活 - 使用 Cassandra，像文档存储，不必提前解决记录中的字段。你可以在系统运行时随意的添加或移除字段。这是一个惊人的效率提升，特别是在大型部署上。\n * 真正的可扩展性 - Cassandra 是纯粹意义上的水平扩展。为给集群添加更多容量，可以指向另一台电脑。你不必重启任何进程，改变应用查询，或手动迁移任何数据。\n * 多数据中心识别 - 你可以调整你的节点布局来避免某一个数据中心起火，一个备用的数据中心将至少有每条记录的完全复制。\n * 范围查询 - 如果你不喜欢全部的键值查询，则可以设置键的范围来查询。\n * 列表数据结构 - 在混合模式可以将超级列添加到 5 维。对于每个用户的索引，这是非常方便的。\n * 分布式写操作 - 有可以在任何地方任何时间集中读或写任何数据。并且不会有任何单点失败。\n\n\n# 更多内容\n\n * Cassandra 官网\n * Cassandra Github\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |",normalizedContent:"# cassandra\n\n> apache cassandra 是一个高度可扩展的分区行存储。行被组织成具有所需主键的表。\n> \n> 最新版本：v4.0\n\n\n# quick start\n\n\n# 安装\n\n> 先决条件\n> \n>  * jdk8+\n>  * python 2.7\n\n\n# 简介\n\napache cassandra 是一套开源分布式 key-value 存储系统。它最初由 facebook 开发，用于储存特别大的数据。\n\n\n# 特性\n\n# 主要特性\n\n * 分布式\n * 基于 column 的结构化\n * 高伸展性\n\ncassandra 的主要特点就是它不是一个数据库，而是由一堆数据库节点共同构成的一个分布式网络服务，对 cassandra 的一个写操作，会被复制到其他节点上去，对 cassandra 的读操作，也会被路由到某个节点上面去读取。对于一个 cassandra 群集来说，扩展性能 是比较简单的事情，只管在群集里面添加节点就可以了。\n\n# 突出特性\n\n * 模式灵活 - 使用 cassandra，像文档存储，不必提前解决记录中的字段。你可以在系统运行时随意的添加或移除字段。这是一个惊人的效率提升，特别是在大型部署上。\n * 真正的可扩展性 - cassandra 是纯粹意义上的水平扩展。为给集群添加更多容量，可以指向另一台电脑。你不必重启任何进程，改变应用查询，或手动迁移任何数据。\n * 多数据中心识别 - 你可以调整你的节点布局来避免某一个数据中心起火，一个备用的数据中心将至少有每条记录的完全复制。\n * 范围查询 - 如果你不喜欢全部的键值查询，则可以设置键的范围来查询。\n * 列表数据结构 - 在混合模式可以将超级列添加到 5 维。对于每个用户的索引，这是非常方便的。\n * 分布式写操作 - 有可以在任何地方任何时间集中读或写任何数据。并且不会有任何单点失败。\n\n\n# 更多内容\n\n * cassandra 官网\n * cassandra github\n\n\n# 🚪 传送门\n\n| 钝悟的博客 | db-tutorial 首页 |",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 面试总结",frontmatter:{title:"Elasticsearch 面试总结",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","面试"],permalink:"/pages/0cb563/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/01.Elasticsearch%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/01.Elasticsearch面试总结.md",key:"v-04b75150",path:"/pages/0cb563/",headers:[{level:2,title:"集群部署",slug:"集群部署",normalizedTitle:"集群部署",charIndex:25},{level:2,title:"性能优化",slug:"性能优化",normalizedTitle:"性能优化",charIndex:127},{level:3,title:"filesystem cache",slug:"filesystem-cache",normalizedTitle:"filesystem cache",charIndex:136},{level:3,title:"数据预热",slug:"数据预热",normalizedTitle:"数据预热",charIndex:1707},{level:3,title:"冷热分离",slug:"冷热分离",normalizedTitle:"冷热分离",charIndex:2177},{level:3,title:"document 模型设计",slug:"document-模型设计",normalizedTitle:"document 模型设计",charIndex:2652},{level:3,title:"分页性能优化",slug:"分页性能优化",normalizedTitle:"分页性能优化",charIndex:3002},{level:4,title:"不允许深度分页（默认深度分页性能很差）",slug:"不允许深度分页-默认深度分页性能很差",normalizedTitle:"不允许深度分页（默认深度分页性能很差）",charIndex:3493},{level:4,title:"类似于 app 里的推荐商品不断下拉出来一页一页的",slug:"类似于-app-里的推荐商品不断下拉出来一页一页的",normalizedTitle:"类似于 app 里的推荐商品不断下拉出来一页一页的",charIndex:3551},{level:2,title:"工作原理",slug:"工作原理",normalizedTitle:"工作原理",charIndex:4678},{level:3,title:"es 写数据过程",slug:"es-写数据过程",normalizedTitle:"es 写数据过程",charIndex:4687},{level:3,title:"es 读数据过程",slug:"es-读数据过程",normalizedTitle:"es 读数据过程",charIndex:4966},{level:3,title:"es 搜索数据过程",slug:"es-搜索数据过程",normalizedTitle:"es 搜索数据过程",charIndex:5311},{level:3,title:"写数据底层原理",slug:"写数据底层原理",normalizedTitle:"写数据底层原理",charIndex:5805},{level:3,title:"删除/更新数据底层原理",slug:"删除-更新数据底层原理",normalizedTitle:"删除/更新数据底层原理",charIndex:8075},{level:3,title:"底层 lucene",slug:"底层-lucene",normalizedTitle:"底层 lucene",charIndex:8520},{level:3,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:4542},{level:2,title:"elasticsearch 的倒排索引是什么",slug:"elasticsearch-的倒排索引是什么",normalizedTitle:"elasticsearch 的倒排索引是什么",charIndex:9624},{level:2,title:"3、elasticsearch 索引数据多了怎么办，如何调优，部署",slug:"_3、elasticsearch-索引数据多了怎么办-如何调优-部署",normalizedTitle:"3、elasticsearch 索引数据多了怎么办，如何调优，部署",charIndex:10036},{level:2,title:"4、elasticsearch 是如何实现 master 选举的",slug:"_4、elasticsearch-是如何实现-master-选举的",normalizedTitle:"4、elasticsearch 是如何实现 master 选举的",charIndex:10603},{level:2,title:"详细描述一下 Elasticsearch 索引文档的过程",slug:"详细描述一下-elasticsearch-索引文档的过程",normalizedTitle:"详细描述一下 elasticsearch 索引文档的过程",charIndex:11110},{level:2,title:"详细描述一下 Elasticsearch 搜索的过程？",slug:"详细描述一下-elasticsearch-搜索的过程",normalizedTitle:"详细描述一下 elasticsearch 搜索的过程？",charIndex:11636},{level:2,title:"Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法",slug:"elasticsearch-在部署时-对-linux-的设置有哪些优化方法",normalizedTitle:"elasticsearch 在部署时，对 linux 的设置有哪些优化方法",charIndex:11931},{level:2,title:"lucence 内部结构是什么？",slug:"lucence-内部结构是什么",normalizedTitle:"lucence 内部结构是什么？",charIndex:12136},{level:2,title:"Elasticsearch 是如何实现 Master 选举的？",slug:"elasticsearch-是如何实现-master-选举的",normalizedTitle:"elasticsearch 是如何实现 master 选举的？",charIndex:12237},{level:2,title:"10、Elasticsearch 中的节点（比如共 20 个），其中的 10 个",slug:"_10、elasticsearch-中的节点-比如共-20-个-其中的-10-个",normalizedTitle:"10、elasticsearch 中的节点（比如共 20 个），其中的 10 个",charIndex:12659},{level:2,title:"客户端在和集群连接时，如何选择特定的节点执行请求的？",slug:"客户端在和集群连接时-如何选择特定的节点执行请求的",normalizedTitle:"客户端在和集群连接时，如何选择特定的节点执行请求的？",charIndex:12897},{level:2,title:"详细描述一下 Elasticsearch 索引文档的过程。",slug:"详细描述一下-elasticsearch-索引文档的过程。",normalizedTitle:"详细描述一下 elasticsearch 索引文档的过程。",charIndex:13045},{level:2,title:"详细描述一下 Elasticsearch 更新和删除文档的过程。",slug:"详细描述一下-elasticsearch-更新和删除文档的过程。",normalizedTitle:"详细描述一下 elasticsearch 更新和删除文档的过程。",charIndex:13899},{level:2,title:"详细描述一下 Elasticsearch 搜索的过程。",slug:"详细描述一下-elasticsearch-搜索的过程。",normalizedTitle:"详细描述一下 elasticsearch 搜索的过程。",charIndex:14231},{level:2,title:"在 Elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？",slug:"在-elasticsearch-中-是怎么根据一个词找到对应的倒排索引的",normalizedTitle:"在 elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？",charIndex:14793},{level:2,title:"Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法？",slug:"elasticsearch-在部署时-对-linux-的设置有哪些优化方法-2",normalizedTitle:"elasticsearch 在部署时，对 linux 的设置有哪些优化方法？",charIndex:14942},{level:2,title:"对于 GC 方面，在使用 Elasticsearch 时要注意什么？",slug:"对于-gc-方面-在使用-elasticsearch-时要注意什么",normalizedTitle:"对于 gc 方面，在使用 elasticsearch 时要注意什么？",charIndex:16250},{level:2,title:"18、Elasticsearch 对于大数据量（上亿量级）的聚合如何实现？",slug:"_18、elasticsearch-对于大数据量-上亿量级-的聚合如何实现",normalizedTitle:"18、elasticsearch 对于大数据量（上亿量级）的聚合如何实现？",charIndex:16751},{level:2,title:"19、在并发情况下，Elasticsearch 如果保证读写一致？",slug:"_19、在并发情况下-elasticsearch-如果保证读写一致",normalizedTitle:"19、在并发情况下，elasticsearch 如果保证读写一致？",charIndex:17043},{level:2,title:"20、如何监控 Elasticsearch 集群状态？",slug:"_20、如何监控-elasticsearch-集群状态",normalizedTitle:"20、如何监控 elasticsearch 集群状态？",charIndex:17394},{level:2,title:"21、介绍下你们电商搜索的整体技术架构。",slug:"_21、介绍下你们电商搜索的整体技术架构。",normalizedTitle:"21、介绍下你们电商搜索的整体技术架构。",charIndex:17508},{level:2,title:"介绍一下你们的个性化搜索方案？",slug:"介绍一下你们的个性化搜索方案",normalizedTitle:"介绍一下你们的个性化搜索方案？",charIndex:17535},{level:2,title:"是否了解字典树？",slug:"是否了解字典树",normalizedTitle:"是否了解字典树？",charIndex:17816},{level:2,title:"拼写纠错是如何实现的？",slug:"拼写纠错是如何实现的",normalizedTitle:"拼写纠错是如何实现的？",charIndex:18209}],headersStr:"集群部署 性能优化 filesystem cache 数据预热 冷热分离 document 模型设计 分页性能优化 不允许深度分页（默认深度分页性能很差） 类似于 app 里的推荐商品不断下拉出来一页一页的 工作原理 es 写数据过程 es 读数据过程 es 搜索数据过程 写数据底层原理 删除/更新数据底层原理 底层 lucene 倒排索引 elasticsearch 的倒排索引是什么 3、elasticsearch 索引数据多了怎么办，如何调优，部署 4、elasticsearch 是如何实现 master 选举的 详细描述一下 Elasticsearch 索引文档的过程 详细描述一下 Elasticsearch 搜索的过程？ Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法 lucence 内部结构是什么？ Elasticsearch 是如何实现 Master 选举的？ 10、Elasticsearch 中的节点（比如共 20 个），其中的 10 个 客户端在和集群连接时，如何选择特定的节点执行请求的？ 详细描述一下 Elasticsearch 索引文档的过程。 详细描述一下 Elasticsearch 更新和删除文档的过程。 详细描述一下 Elasticsearch 搜索的过程。 在 Elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？ Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法？ 对于 GC 方面，在使用 Elasticsearch 时要注意什么？ 18、Elasticsearch 对于大数据量（上亿量级）的聚合如何实现？ 19、在并发情况下，Elasticsearch 如果保证读写一致？ 20、如何监控 Elasticsearch 集群状态？ 21、介绍下你们电商搜索的整体技术架构。 介绍一下你们的个性化搜索方案？ 是否了解字典树？ 拼写纠错是如何实现的？",content:"# Elasticsearch 面试总结\n\n\n# 集群部署\n\nES 部署情况：\n\n5 节点（配置：8 核 64 G 1T），总计 320 G，5 T。\n\n约 10+ 索引，5 分片，每日新增数据量约为 2G，4000w 条。记录保存 30 天。\n\n\n# 性能优化\n\n\n# filesystem cache\n\n你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。\n\n\n\nes 的搜索引擎严重依赖于底层的 filesystem cache ，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。\n\n性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1 秒、5 秒、10 秒。但如果是走 filesystem cache ，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。\n\n这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G 。每台机器给 es jvm heap 是 32G ，那么剩下来留给 filesystem cache 的就是每台机器才 32G ，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T ，那么每台机器的数据量是 300G 。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。\n\n归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。\n\n根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。\n\n比如说你现在有一行数据。 id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。\n\nhbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id ，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。\n\n写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。\n\n\n# 数据预热\n\n假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。\n\n其实可以做数据预热。\n\n举个例子，拿微博来说，你可以把一些大 V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。\n\n或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。\n\n对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。\n\n\n# 冷热分离\n\nes 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。\n\n你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。\n\n\n# document 模型设计\n\n对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。\n\n最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。\n\ndocument 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。\n\n\n# 分页性能优化\n\nes 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。\n\n分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。\n\n我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。\n\n有什么解决方案吗？\n\n# 不允许深度分页（默认深度分页性能很差）\n\n跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。\n\n# 类似于 app 里的推荐商品不断下拉出来一页一页的\n\n类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api ，关于如何使用，自行上网搜索。\n\nscroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。\n\n但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。\n\n初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。\n\n除了用 scroll api ，你也可以用 search_after 来做， search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。\n\n1.1、设计阶段调优\n\n（1）根据业务增量需求，采取基于日期模板创建索引，通过 roll over API 滚动索引；\n\n（2）使用别名进行索引管理；\n\n（3）每天凌晨定时对索引做 force_merge 操作，以释放空间；\n\n（4）采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink 操作，以缩减存储；\n\n（5）采取 curator 进行索引的生命周期管理；\n\n（6）仅针对需要分词的字段，合理的设置分词器；\n\n（7）Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。……..\n\n1.2、写入调优\n\n（1）写入前副本数设置为 0；\n\n（2）写入前关闭 refresh_interval 设置为-1，禁用刷新机制；\n\n（3）写入过程中：采取 bulk 批量写入；\n\n（4）写入后恢复副本数和刷新间隔；\n\n（5）尽量使用自动生成的 id。\n\n1.3、查询调优\n\n（1）禁用 wildcard；\n\n（2）禁用批量 terms（成百上千的场景）；\n\n（3）充分利用倒排索引机制，能 keyword 类型尽量 keyword；\n\n（4）数据量大时候，可以先基于时间敲定索引再检索；\n\n（5）设置合理的路由机制。\n\n1.4、其他调优\n\n部署调优，业务调优等。\n\n上面的提及一部分，面试者就基本对你之前的实践或者运维经验有所评估了。\n\n\n# 工作原理\n\n\n# es 写数据过程\n\n * 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。\n * coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。\n * 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。\n * coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。\n\n\n\n\n# es 读数据过程\n\n可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n\n * 客户端发送请求到任意一个 node，成为 coordinate node 。\n * coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n * 接收请求的 node 返回 document 给 coordinate node 。\n * coordinate node 返回 document 给客户端。\n\n\n# es 搜索数据过程\n\nes 最强大的是做全文检索，就是比如你有三条数据：\n\njava真好玩儿啊\njava好难学啊\nj2ee特别牛\n\n\n你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n\n * 客户端发送请求到一个 coordinate node 。\n * 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。\n * query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。\n * fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。\n\n> 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n\n\n# 写数据底层原理\n\n\n\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。\n\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。\n\n为什么叫 es 是准实时的？ NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\n\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n\n这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\n\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\n\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n\n实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n\n> 数据写入 segment file 之后，同时就建立好了倒排索引。\n\n\n# 删除/更新数据底层原理\n\n如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。\n\n\n# 底层 lucene\n\n简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n\n\n# 倒排索引\n\n在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n\n那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n\n举个栗子。\n\n有以下文档：\n\nDOCID   DOC\n1       谷歌地图之父跳槽 Facebook\n2       谷歌地图之父加盟 Facebook\n3       谷歌地图创始人拉斯离开谷歌加盟 Facebook\n4       谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关\n5       谷歌地图之父拉斯加盟社交网站 Facebook\n\n对文档进行分词之后，得到以下倒排索引。\n\nWORDID   WORD       DOCIDS\n1        谷歌         1, 2, 3, 4, 5\n2        地图         1, 2, 3, 4, 5\n3        之父         1, 2, 4, 5\n4        跳槽         1, 4\n5        Facebook   1, 2, 3, 4, 5\n6        加盟         2, 3, 5\n7        创始人        3\n8        拉斯         3, 5\n9        离开         3\n10       与          4\n..       ..         ..\n\n另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n\n要注意倒排索引的两个重要细节：\n\n * 倒排索引中的所有词项对应一个或多个文档；\n * 倒排索引中的词项根据字典顺序升序排列\n\n> 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n\n\n# elasticsearch 的倒排索引是什么\n\n面试官：想了解你对基础概念的认知。\n\n解答：通俗解释一下就可以。\n\n传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。\n\n而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表即为倒排索引。有了倒排索引，就能实现 o（1）时间复杂度的效率检索文章了，极大的提高了检索效率。\n\n\n\n学术的解答方式：\n\n倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。\n\n加分项：倒排索引的底层实现是基于：FST（Finite State Transducer）数据结构。\n\nlucene 从 4+版本后开始大量使用的数据结构是 FST。FST 有两个优点：\n\n（1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；\n\n（2）查询速度快。O(len(str))的查询时间复杂度。\n\n\n# 3、elasticsearch 索引数据多了怎么办，如何调优，部署\n\n面试官：想了解大数据量的运维能力。\n\n解答：索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。\n\n如何调优，正如问题 1 所说，这里细化一下：\n\n3.1 动态索引层面\n\n基于模板+时间+rollover api 滚动创建索引，举例：设计阶段定义：blog 索引的模板格式为：blogindex时间戳的形式，每天递增数据。这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线 2 的 32 次幂-1，索引存储达到了 TB+甚至更大。\n\n一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。\n\n3.2 存储层面\n\n冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。\n\n对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。\n\n3.3 部署层面\n\n一旦之前没有规划，这里就属于应急策略。\n\n结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。\n\n\n# 4、elasticsearch 是如何实现 master 选举的\n\n面试官：想了解 ES 集群的底层原理，不再只关注业务层面了。\n\n解答：\n\n前置前提：\n\n（1）只有候选主节点（master：true）的节点才能成为主节点。\n\n（2）最小主节点数（min_master_nodes）的目的是防止脑裂。\n\n核对了一下代码，核心入口为 findMaster，选择主节点成功返回对应 Master，否则返回 null。选举流程大致描述如下：\n\n第一步：确认候选主节点数达标，elasticsearch.yml 设置的值\n\ndiscovery.zen.minimum_master_nodes；\n\n第二步：比较：先判定是否具备 master 资格，具备候选主节点资格的优先返回；\n\n若两节点都为候选主节点，则 id 小的值会主节点。注意这里的 id 为 string 类型。\n\n题外话：获取节点 id 的方法。\n\n1GET /_cat/nodes?v&amp;h=ip,port,heapPercent,heapMax,id,name\n\n2ip port heapPercent heapMax id name\n\n\n\n# 详细描述一下 Elasticsearch 索引文档的过程\n\n面试官：想了解 ES 的底层原理，不再只关注业务层面了。\n\n解答：\n\n这里的索引文档应该理解为文档写入 ES，创建索引的过程。\n\n文档写入包含：单文档写入和批量 bulk 写入，这里只解释一下：单文档写入流程。\n\n记住官方文档中的这个图。\n\n\n\n第一步：客户写集群某节点写入数据，发送请求。（如果没有指定路由/协调节点，请求的节点扮演路由节点的角色。）\n\n第二步：节点 1 接受到请求后，使用文档_id 来确定文档属于分片 0。请求会被转到另外的节点，假定节点 3。因此分片 0 的主分片分配到节点 3 上。\n\n第三步：节点 3 在主分片上执行写操作，如果成功，则将请求并行转发到节点 1 和节点 2 的副本分片上，等待结果返回。所有的副本分片都报告成功，节点 3 将向协调节点（节点 1）报告成功，节点 1 向请求客户端报告写入成功。\n\n如果面试官再问：第二步中的文档获取分片的过程？\n\n回答：借助路由算法获取，路由算法就是根据路由和文档 id 计算目标的分片 id 的过程。\n\n1shard = hash(_routing) % (num_of_primary_shards)\n\n\n\n# 详细描述一下 Elasticsearch 搜索的过程？\n\n面试官：想了解 ES 搜索的底层原理，不再只关注业务层面了。\n\n解答：\n\n搜索拆解为“query then fetch” 两个阶段。\n\nquery 阶段的目的：定位到位置，但不取。\n\n步骤拆解如下：\n\n（1）假设一个索引数据有 5 主+1 副本 共 10 分片，一次请求会命中（主或者副本分片中）的一个。\n\n（2）每个分片在本地进行查询，结果返回到本地有序的优先队列中。\n\n（3）第 2）步骤的结果发送到协调节点，协调节点产生一个全局的排序列表。\n\nfetch 阶段的目的：取数据。\n\n路由节点获取所有文档，返回给客户端。\n\n\n# Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法\n\n面试官：想了解对 ES 集群的运维能力。\n\n解答：\n\n（1）关闭缓存 swap;\n\n（2）堆内存设置为：Min（节点内存/2, 32GB）;\n\n（3）设置最大文件句柄数；\n\n（4）线程池+队列大小根据业务需要做调整；\n\n（5）磁盘存储 raid 方式——存储有条件使用 RAID10，增加单节点性能以及避免单节点存储故障。\n\n\n# lucence 内部结构是什么？\n\n面试官：想了解你的知识面的广度和深度。\n\n解答：\n\n\n\nLucene 是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。可以基于这个脉络展开一些。\n\n\n# Elasticsearch 是如何实现 Master 选举的？\n\n（1）Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；\n\n（2）对所有可以成为 master 的节点（node.master: true）根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。\n\n（3）如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。\n\n（4）补充：master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能*。\n\n\n# 10、Elasticsearch 中的节点（比如共 20 个），其中的 10 个\n\n选了一个 master，另外 10 个选了另一个 master，怎么办？\n\n（1）当集群 master 候选数量不小于 3 个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题；\n\n（3）当候选数量为两个时，只能修改为唯一的一个 master 候选，其他作为 data 节点，避免脑裂问题。\n\n\n# 客户端在和集群连接时，如何选择特定的节点执行请求的？\n\nTransportClient 利用 transport 模块远程连接一个 elasticsearch 集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的 transport 地址，并以 轮询 的方式与这些地址进行通信。\n\n\n# 详细描述一下 Elasticsearch 索引文档的过程。\n\n协调节点默认使用文档 ID 参与计算（也支持通过 routing），以便为路由提供合适的分片。\n\nshard = hash(document_id) % (num_of_primary_shards)\n\n\n（1）当分片所在的节点接收到来自协调节点的请求后，会将请求写入到 MemoryBuffer，然后定时（默认是每隔 1 秒）写入到 Filesystem Cache，这个从 MomeryBuffer 到 Filesystem Cache 的过程就叫做 refresh；\n\n（2）当然在某些情况下，存在 Momery Buffer 和 Filesystem Cache 的数据可能会丢失，ES 是通过 translog 的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到 translog 中 ，当 Filesystem cache 中的数据写入到磁盘中时，才会清除掉，这个过程叫做 flush；\n\n（3）在 flush 过程中，内存中的缓冲将被清除，内容被写入一个新段，段的 fsync 将创建一个新的提交点，并将内容刷新到磁盘，旧的 translog 将被删除并开始一个新的 translog。\n\n（4）flush 触发的时机是定时触发（默认 30 分钟）或者 translog 变得太大（默认为 512M）时；\n\n\n\n补充：关于 Lucene 的 Segement：\n\n（1）Lucene 索引是由多个段组成，段本身是一个功能齐全的倒排索引。\n\n（2）段是不可变的，允许 Lucene 将新的文档增量地添加到索引中，而不用从头重建索引。\n\n（3）对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗 CPU 的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。\n\n（4）为了解决这个问题，Elasticsearch 会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。\n\n\n# 详细描述一下 Elasticsearch 更新和删除文档的过程。\n\n（1）删除和更新也都是写操作，但是 Elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更；\n\n（2）磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后，文档并没有真的被删除，而是在.del 文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段。\n\n（3）在新的文档被创建时，Elasticsearch 会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del 文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。\n\n\n# 详细描述一下 Elasticsearch 搜索的过程。\n\n（1）搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch；\n\n（2）在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。\n\nPS：在搜索的时候是会查询 Filesystem Cache 的，但是有部分数据还在 MemoryBuffer，所以搜索是近实时的。\n\n（3）每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。\n\n（4）接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰 富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。\n\n（5）补充：Query Then Fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch 增加了一个预查询的处理，询问 Term 和 Document frequency，这个评分更准确，但是性能会变差。*\n\n\n\n\n# 在 Elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？\n\n（1）Lucene 的索引过程，就是按照全文检索的基本过程，将倒排表写成此文件格式的过程。\n\n（2）Lucene 的搜索过程，就是按照此文件格式将索引进去的信息读出来，然后计算每篇文档打分(score)的过程。\n\n\n# Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法？\n\n（1）64 GB 内存的机器是非常理想的， 但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反。\n\n（2）如果你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。\n\n（3）如果你负担得起 SSD，它将远远超出任何旋转介质。 基于 SSD 的节点，查询和索引性能都有提升。如果你负担得起，SSD 是一个好的选择。\n\n（4）即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。\n\n（5）请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在 Elasticsearch 的几个地方，使用 Java 的本地序列化。\n\n（6）通过设置 gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time 可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。\n\n（7）Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。\n\n（8）不要随意修改垃圾回收器（CMS）和各个线程池的大小。\n\n（9）把你的内存的（少于）一半给 Lucene（但不要超过 32 GB！），通过 ES_HEAP_SIZE 环境变量设置。\n\n（10）内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个 100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。\n\n（11）Lucene 使用了大 量 的文件。同时，Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如 64,000。\n\n补充：索引阶段性能提升方法\n\n（1）使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点。\n\n（2）存储：使用 SSD\n\n（3）段和合并：Elasticsearch 默认值是 20 MB/s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100–200 MB/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置，从默认的 512 MB 到更大一些的值，比如 1 GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。\n\n（4）如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s。\n\n（5）如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0 关闭副本。\n\n\n# 对于 GC 方面，在使用 Elasticsearch 时要注意什么？\n\n（1）倒排词典的索引需要常驻内存，无法 GC，需要监控 data node 上 segmentmemory 增长趋势。\n\n（2）各类缓存，field cache, filter cache, indexing cache, bulk queue 等等，要设置合理的大小，并且要应该根据最坏的情况来看 heap 是否够用，也就是各类缓存全部占满的时候，还有 heap 空间可以分配给其他任务吗？避免采用 clear cache 等“自欺欺人”的方式来释放内存。\n\n（3）避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用 scan & scroll api 来实现。\n\n（4）cluster stats 驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。\n\n（5）想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用情况做持续的监控。\n\n（6）根据监控数据理解内存需求，合理配置各类 circuit breaker，将内存溢出风险降低到最低\n\n\n# 18、Elasticsearch 对于大数据量（上亿量级）的聚合如何实现？\n\nElasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数，即该字段的 distinct 或者 unique 值的数目。它是基于 HLL 算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。\n\n\n# 19、在并发情况下，Elasticsearch 如果保证读写一致？\n\n（1）可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；\n\n（2）另外对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。\n\n（3）对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本。\n\n\n# 20、如何监控 Elasticsearch 集群状态？\n\nMarvel 让你可以很简单的通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。\n\n\n# 21、介绍下你们电商搜索的整体技术架构。\n\n\n\n\n# 介绍一下你们的个性化搜索方案？\n\n基于 word2vec 和 Elasticsearch 实现个性化搜索\n\n（1）基于 word2vec、Elasticsearch 和自定义的脚本插件，我们就实现了一个个性化的搜索服务，相对于原有的实现，新版的点击率和转化率都有大幅的提升；\n\n（2）基于 word2vec 的商品向量还有一个可用之处，就是可以用来实现相似商品的推荐；\n\n（3）使用 word2vec 来实现个性化搜索或个性化推荐是有一定局限性的，因为它只能处理用户点击历史这样的时序数据，而无法全面的去考虑用户偏好，这个还是有很大的改进和提升的空间；\n\n\n# 是否了解字典树？\n\n常用字典数据结构如下所示：\n\n\n\nTrie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有 3 个基本性质：\n\n1）根节点不包含字符，除根节点外每一个节点都只包含一个字符。\n\n2）从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。\n\n3）每个节点的所有子节点包含的字符都不相同。\n\n\n\n（1）可以看到，trie 树每一层的节点数是 26^i 级别的。所以为了节省空间，我们还可以用动态链表，或者用数组来模拟动态。而空间的花费，不会超过单词数 × 单词长度。\n\n（2）实现：对每个结点开一个字母集大小的数组，每个结点挂一个链表，使用左儿子右兄弟表示法记录这棵树；\n\n（3）对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上可以保留哈希的复杂度 O(1)。\n\n\n# 拼写纠错是如何实现的？\n\n（1）拼写纠错是基于编辑距离来实现；编辑距离是一种标准的方法，它用来表示经过插入、删除和替换操作从一个字符串转换到另外一个字符串的最小操作步数；\n\n（2）编辑距离的计算过程：比如要计算 batyu 和 beauty 的编辑距离，先创建一个 7×8 的表（batyu 长度为 5，coffee 长度为 6，各加 2），接着，在如下位置填入黑色数字。其他格的计算过程是取以下三个值的最小值：\n\n如果最上方的字符等于最左方的字符，则为左上方的数字。否则为左上方的数字+1。（对于 3,3 来说为 0）\n\n左方数字+1（对于 3,3 格来说为 2）\n\n上方数字+1（对于 3,3 格来说为 2）\n\n最终取右下角的值即为编辑距离的值 3。\n\n\n\n对于拼写纠错，我们考虑构造一个度量空间（Metric Space），该空间内任何关系满足以下三条基本条件：\n\nd(x,y) = 0 -- 假如 x 与 y 的距离为 0，则 x=y\n\nd(x,y) = d(y,x) -- x 到 y 的距离等同于 y 到 x 的距离\n\nd(x,y) + d(y,z) >= d(x,z) -- 三角不等式\n\n（1）根据三角不等式，则满足与 query 距离在 n 范围内的另一个字符转 B，其与 A 的距离最大为 d+n，最小为 d-n。\n\n（2）BK 树的构造就过程如下：每个节点有任意个子节点，每条边有个值表示编辑距离。所有子节点到父节点的边上标注 n 表示编辑距离恰好为 n。比如，我们有棵树父节点是”book”和两个子节点”cake”和”books”，”book”到”books”的边标号 1，”book”到”cake”的边上标号 4。从字典里构造好树后，无论何时你想插入新单词时，计算该单词与根节点的编辑距离，并且查找数值为 d(neweord, root)的边。递归得与各子节点进行比较，直到没有子节点，你就可以创建新的子节点并将新单词保存在那。比如，插入”boo”到刚才上述例子的树中，我们先检查根节点，查找 d(“book”, “boo”) = 1 的边，然后检查标号为 1 的边的子节点，得到单词”books”。我们再计算距离 d(“books”, “boo”)=2，则将新单词插在”books”之后，边标号为 2。\n\n（3）查询相似词如下：计算单词与根节点的编辑距离 d，然后递归查找每个子节点标号为 d-n 到 d+n（包含）的边。假如被检查的节点与搜索单词的距离 d 小于 n，则返回该节点并继续查询。比如输入 cape 且最大容忍距离为 1，则先计算和根的编辑距离 d(“book”, “cape”)=4，然后接着找和根节点之间编辑距离为 3 到 5 的，这个就找到了 cake 这个节点，计算 d(“cake”, “cape”)=1，满足条件所以返回 cake，然后再找和 cake 节点编辑距离是 0 到 2 的，分别找到 cape 和 cart 节点，这样就得到 cape 这个满足条件的结果。\n\n",normalizedContent:"# elasticsearch 面试总结\n\n\n# 集群部署\n\nes 部署情况：\n\n5 节点（配置：8 核 64 g 1t），总计 320 g，5 t。\n\n约 10+ 索引，5 分片，每日新增数据量约为 2g，4000w 条。记录保存 30 天。\n\n\n# 性能优化\n\n\n# filesystem cache\n\n你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。\n\n\n\nes 的搜索引擎严重依赖于底层的 filesystem cache ，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。\n\n性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1 秒、5 秒、10 秒。但如果是走 filesystem cache ，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。\n\n这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64g，总内存就是 64 * 3 = 192g 。每台机器给 es jvm heap 是 32g ，那么剩下来留给 filesystem cache 的就是每台机器才 32g ，总共集群里给 filesystem cache 的就是 32 * 3 = 96g 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1t 的磁盘容量，es 数据量是 1t ，那么每台机器的数据量是 300g 。这样性能好吗？ filesystem cache 的内存才 100g，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。\n\n归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。\n\n根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100g，那么你就将索引数据控制在 100g 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。\n\n比如说你现在有一行数据。 id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。\n\nhbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id ，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。\n\n写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1t 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。\n\n\n# 数据预热\n\n假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60g 数据，结果 filesystem cache 就 30g，还是有 30g 数据留在了磁盘上。\n\n其实可以做数据预热。\n\n举个例子，拿微博来说，你可以把一些大 v，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。\n\n或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。\n\n对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。\n\n\n# 冷热分离\n\nes 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。\n\n你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。\n\n\n# document 模型设计\n\n对于 mysql，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。\n\n最好是先在 java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。\n\ndocument 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。\n\n\n# 分页性能优化\n\nes 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。\n\n分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。\n\n我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。\n\n有什么解决方案吗？\n\n# 不允许深度分页（默认深度分页性能很差）\n\n跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。\n\n# 类似于 app 里的推荐商品不断下拉出来一页一页的\n\n类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api ，关于如何使用，自行上网搜索。\n\nscroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。\n\n但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。\n\n初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。\n\n除了用 scroll api ，你也可以用 search_after 来做， search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。\n\n1.1、设计阶段调优\n\n（1）根据业务增量需求，采取基于日期模板创建索引，通过 roll over api 滚动索引；\n\n（2）使用别名进行索引管理；\n\n（3）每天凌晨定时对索引做 force_merge 操作，以释放空间；\n\n（4）采取冷热分离机制，热数据存储到 ssd，提高检索效率；冷数据定期进行 shrink 操作，以缩减存储；\n\n（5）采取 curator 进行索引的生命周期管理；\n\n（6）仅针对需要分词的字段，合理的设置分词器；\n\n（7）mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。……..\n\n1.2、写入调优\n\n（1）写入前副本数设置为 0；\n\n（2）写入前关闭 refresh_interval 设置为-1，禁用刷新机制；\n\n（3）写入过程中：采取 bulk 批量写入；\n\n（4）写入后恢复副本数和刷新间隔；\n\n（5）尽量使用自动生成的 id。\n\n1.3、查询调优\n\n（1）禁用 wildcard；\n\n（2）禁用批量 terms（成百上千的场景）；\n\n（3）充分利用倒排索引机制，能 keyword 类型尽量 keyword；\n\n（4）数据量大时候，可以先基于时间敲定索引再检索；\n\n（5）设置合理的路由机制。\n\n1.4、其他调优\n\n部署调优，业务调优等。\n\n上面的提及一部分，面试者就基本对你之前的实践或者运维经验有所评估了。\n\n\n# 工作原理\n\n\n# es 写数据过程\n\n * 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。\n * coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。\n * 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。\n * coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。\n\n\n\n\n# es 读数据过程\n\n可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n\n * 客户端发送请求到任意一个 node，成为 coordinate node 。\n * coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n * 接收请求的 node 返回 document 给 coordinate node 。\n * coordinate node 返回 document 给客户端。\n\n\n# es 搜索数据过程\n\nes 最强大的是做全文检索，就是比如你有三条数据：\n\njava真好玩儿啊\njava好难学啊\nj2ee特别牛\n\n\n你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n\n * 客户端发送请求到一个 coordinate node 。\n * 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。\n * query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。\n * fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。\n\n> 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n\n\n# 写数据底层原理\n\n\n\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。\n\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。\n\n为什么叫 es 是准实时的？ nrt ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\n\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n\n这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\n\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\n\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n\n实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n\n> 数据写入 segment file 之后，同时就建立好了倒排索引。\n\n\n# 删除/更新数据底层原理\n\n如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。\n\n\n# 底层 lucene\n\n简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n\n\n# 倒排索引\n\n在搜索引擎中，每个文档都有一个对应的文档 id，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n\n那么，倒排索引就是关键词到文档 id 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n\n举个栗子。\n\n有以下文档：\n\ndocid   doc\n1       谷歌地图之父跳槽 facebook\n2       谷歌地图之父加盟 facebook\n3       谷歌地图创始人拉斯离开谷歌加盟 facebook\n4       谷歌地图之父跳槽 facebook 与 wave 项目取消有关\n5       谷歌地图之父拉斯加盟社交网站 facebook\n\n对文档进行分词之后，得到以下倒排索引。\n\nwordid   word       docids\n1        谷歌         1, 2, 3, 4, 5\n2        地图         1, 2, 3, 4, 5\n3        之父         1, 2, 4, 5\n4        跳槽         1, 4\n5        facebook   1, 2, 3, 4, 5\n6        加盟         2, 3, 5\n7        创始人        3\n8        拉斯         3, 5\n9        离开         3\n10       与          4\n..       ..         ..\n\n另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 facebook ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n\n要注意倒排索引的两个重要细节：\n\n * 倒排索引中的所有词项对应一个或多个文档；\n * 倒排索引中的词项根据字典顺序升序排列\n\n> 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n\n\n# elasticsearch 的倒排索引是什么\n\n面试官：想了解你对基础概念的认知。\n\n解答：通俗解释一下就可以。\n\n传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。\n\n而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表即为倒排索引。有了倒排索引，就能实现 o（1）时间复杂度的效率检索文章了，极大的提高了检索效率。\n\n\n\n学术的解答方式：\n\n倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。\n\n加分项：倒排索引的底层实现是基于：fst（finite state transducer）数据结构。\n\nlucene 从 4+版本后开始大量使用的数据结构是 fst。fst 有两个优点：\n\n（1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间；\n\n（2）查询速度快。o(len(str))的查询时间复杂度。\n\n\n# 3、elasticsearch 索引数据多了怎么办，如何调优，部署\n\n面试官：想了解大数据量的运维能力。\n\n解答：索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。\n\n如何调优，正如问题 1 所说，这里细化一下：\n\n3.1 动态索引层面\n\n基于模板+时间+rollover api 滚动创建索引，举例：设计阶段定义：blog 索引的模板格式为：blogindex时间戳的形式，每天递增数据。这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线 2 的 32 次幂-1，索引存储达到了 tb+甚至更大。\n\n一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。\n\n3.2 存储层面\n\n冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。\n\n对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。\n\n3.3 部署层面\n\n一旦之前没有规划，这里就属于应急策略。\n\n结合 es 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。\n\n\n# 4、elasticsearch 是如何实现 master 选举的\n\n面试官：想了解 es 集群的底层原理，不再只关注业务层面了。\n\n解答：\n\n前置前提：\n\n（1）只有候选主节点（master：true）的节点才能成为主节点。\n\n（2）最小主节点数（min_master_nodes）的目的是防止脑裂。\n\n核对了一下代码，核心入口为 findmaster，选择主节点成功返回对应 master，否则返回 null。选举流程大致描述如下：\n\n第一步：确认候选主节点数达标，elasticsearch.yml 设置的值\n\ndiscovery.zen.minimum_master_nodes；\n\n第二步：比较：先判定是否具备 master 资格，具备候选主节点资格的优先返回；\n\n若两节点都为候选主节点，则 id 小的值会主节点。注意这里的 id 为 string 类型。\n\n题外话：获取节点 id 的方法。\n\n1get /_cat/nodes?v&amp;h=ip,port,heappercent,heapmax,id,name\n\n2ip port heappercent heapmax id name\n\n\n\n# 详细描述一下 elasticsearch 索引文档的过程\n\n面试官：想了解 es 的底层原理，不再只关注业务层面了。\n\n解答：\n\n这里的索引文档应该理解为文档写入 es，创建索引的过程。\n\n文档写入包含：单文档写入和批量 bulk 写入，这里只解释一下：单文档写入流程。\n\n记住官方文档中的这个图。\n\n\n\n第一步：客户写集群某节点写入数据，发送请求。（如果没有指定路由/协调节点，请求的节点扮演路由节点的角色。）\n\n第二步：节点 1 接受到请求后，使用文档_id 来确定文档属于分片 0。请求会被转到另外的节点，假定节点 3。因此分片 0 的主分片分配到节点 3 上。\n\n第三步：节点 3 在主分片上执行写操作，如果成功，则将请求并行转发到节点 1 和节点 2 的副本分片上，等待结果返回。所有的副本分片都报告成功，节点 3 将向协调节点（节点 1）报告成功，节点 1 向请求客户端报告写入成功。\n\n如果面试官再问：第二步中的文档获取分片的过程？\n\n回答：借助路由算法获取，路由算法就是根据路由和文档 id 计算目标的分片 id 的过程。\n\n1shard = hash(_routing) % (num_of_primary_shards)\n\n\n\n# 详细描述一下 elasticsearch 搜索的过程？\n\n面试官：想了解 es 搜索的底层原理，不再只关注业务层面了。\n\n解答：\n\n搜索拆解为“query then fetch” 两个阶段。\n\nquery 阶段的目的：定位到位置，但不取。\n\n步骤拆解如下：\n\n（1）假设一个索引数据有 5 主+1 副本 共 10 分片，一次请求会命中（主或者副本分片中）的一个。\n\n（2）每个分片在本地进行查询，结果返回到本地有序的优先队列中。\n\n（3）第 2）步骤的结果发送到协调节点，协调节点产生一个全局的排序列表。\n\nfetch 阶段的目的：取数据。\n\n路由节点获取所有文档，返回给客户端。\n\n\n# elasticsearch 在部署时，对 linux 的设置有哪些优化方法\n\n面试官：想了解对 es 集群的运维能力。\n\n解答：\n\n（1）关闭缓存 swap;\n\n（2）堆内存设置为：min（节点内存/2, 32gb）;\n\n（3）设置最大文件句柄数；\n\n（4）线程池+队列大小根据业务需要做调整；\n\n（5）磁盘存储 raid 方式——存储有条件使用 raid10，增加单节点性能以及避免单节点存储故障。\n\n\n# lucence 内部结构是什么？\n\n面试官：想了解你的知识面的广度和深度。\n\n解答：\n\n\n\nlucene 是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。可以基于这个脉络展开一些。\n\n\n# elasticsearch 是如何实现 master 选举的？\n\n（1）elasticsearch 的选主是 zendiscovery 模块负责的，主要包含 ping（节点之间通过这个 rpc 来发现彼此）和 unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分；\n\n（2）对所有可以成为 master 的节点（node.master: true）根据 nodeid 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。\n\n（3）如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n/2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。\n\n（4）补充：master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能*。\n\n\n# 10、elasticsearch 中的节点（比如共 20 个），其中的 10 个\n\n选了一个 master，另外 10 个选了另一个 master，怎么办？\n\n（1）当集群 master 候选数量不小于 3 个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题；\n\n（3）当候选数量为两个时，只能修改为唯一的一个 master 候选，其他作为 data 节点，避免脑裂问题。\n\n\n# 客户端在和集群连接时，如何选择特定的节点执行请求的？\n\ntransportclient 利用 transport 模块远程连接一个 elasticsearch 集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的 transport 地址，并以 轮询 的方式与这些地址进行通信。\n\n\n# 详细描述一下 elasticsearch 索引文档的过程。\n\n协调节点默认使用文档 id 参与计算（也支持通过 routing），以便为路由提供合适的分片。\n\nshard = hash(document_id) % (num_of_primary_shards)\n\n\n（1）当分片所在的节点接收到来自协调节点的请求后，会将请求写入到 memorybuffer，然后定时（默认是每隔 1 秒）写入到 filesystem cache，这个从 momerybuffer 到 filesystem cache 的过程就叫做 refresh；\n\n（2）当然在某些情况下，存在 momery buffer 和 filesystem cache 的数据可能会丢失，es 是通过 translog 的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到 translog 中 ，当 filesystem cache 中的数据写入到磁盘中时，才会清除掉，这个过程叫做 flush；\n\n（3）在 flush 过程中，内存中的缓冲将被清除，内容被写入一个新段，段的 fsync 将创建一个新的提交点，并将内容刷新到磁盘，旧的 translog 将被删除并开始一个新的 translog。\n\n（4）flush 触发的时机是定时触发（默认 30 分钟）或者 translog 变得太大（默认为 512m）时；\n\n\n\n补充：关于 lucene 的 segement：\n\n（1）lucene 索引是由多个段组成，段本身是一个功能齐全的倒排索引。\n\n（2）段是不可变的，允许 lucene 将新的文档增量地添加到索引中，而不用从头重建索引。\n\n（3）对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗 cpu 的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。\n\n（4）为了解决这个问题，elasticsearch 会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。\n\n\n# 详细描述一下 elasticsearch 更新和删除文档的过程。\n\n（1）删除和更新也都是写操作，但是 elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更；\n\n（2）磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后，文档并没有真的被删除，而是在.del 文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段。\n\n（3）在新的文档被创建时，elasticsearch 会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del 文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。\n\n\n# 详细描述一下 elasticsearch 搜索的过程。\n\n（1）搜索被执行成一个两阶段过程，我们称之为 query then fetch；\n\n（2）在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。\n\nps：在搜索的时候是会查询 filesystem cache 的，但是有部分数据还在 memorybuffer，所以搜索是近实时的。\n\n（3）每个分片返回各自优先队列中 所有文档的 id 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。\n\n（4）接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 get 请求。每个分片加载并 丰 富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。\n\n（5）补充：query then fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，dfs query then fetch 增加了一个预查询的处理，询问 term 和 document frequency，这个评分更准确，但是性能会变差。*\n\n\n\n\n# 在 elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？\n\n（1）lucene 的索引过程，就是按照全文检索的基本过程，将倒排表写成此文件格式的过程。\n\n（2）lucene 的搜索过程，就是按照此文件格式将索引进去的信息读出来，然后计算每篇文档打分(score)的过程。\n\n\n# elasticsearch 在部署时，对 linux 的设置有哪些优化方法？\n\n（1）64 gb 内存的机器是非常理想的， 但是 32 gb 和 16 gb 机器也是很常见的。少于 8 gb 会适得其反。\n\n（2）如果你要在更快的 cpus 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。\n\n（3）如果你负担得起 ssd，它将远远超出任何旋转介质。 基于 ssd 的节点，查询和索引性能都有提升。如果你负担得起，ssd 是一个好的选择。\n\n（4）即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。\n\n（5）请确保运行你应用程序的 jvm 和服务器的 jvm 是完全一样的。 在 elasticsearch 的几个地方，使用 java 的本地序列化。\n\n（6）通过设置 gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time 可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。\n\n（7）elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。\n\n（8）不要随意修改垃圾回收器（cms）和各个线程池的大小。\n\n（9）把你的内存的（少于）一半给 lucene（但不要超过 32 gb！），通过 es_heap_size 环境变量设置。\n\n（10）内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个 100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。\n\n（11）lucene 使用了大 量 的文件。同时，elasticsearch 在节点和 http 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如 64,000。\n\n补充：索引阶段性能提升方法\n\n（1）使用批量请求并调整其大小：每次批量数据 5–15 mb 大是个不错的起始点。\n\n（2）存储：使用 ssd\n\n（3）段和合并：elasticsearch 默认值是 20 mb/s，对机械磁盘应该是个不错的设置。如果你用的是 ssd，可以考虑提高到 100–200 mb/s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置，从默认的 512 mb 到更大一些的值，比如 1 gb，这可以在一次清空触发的时候在事务日志里积累出更大的段。\n\n（4）如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s。\n\n（5）如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0 关闭副本。\n\n\n# 对于 gc 方面，在使用 elasticsearch 时要注意什么？\n\n（1）倒排词典的索引需要常驻内存，无法 gc，需要监控 data node 上 segmentmemory 增长趋势。\n\n（2）各类缓存，field cache, filter cache, indexing cache, bulk queue 等等，要设置合理的大小，并且要应该根据最坏的情况来看 heap 是否够用，也就是各类缓存全部占满的时候，还有 heap 空间可以分配给其他任务吗？避免采用 clear cache 等“自欺欺人”的方式来释放内存。\n\n（3）避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用 scan & scroll api 来实现。\n\n（4）cluster stats 驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。\n\n（5）想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用情况做持续的监控。\n\n（6）根据监控数据理解内存需求，合理配置各类 circuit breaker，将内存溢出风险降低到最低\n\n\n# 18、elasticsearch 对于大数据量（上亿量级）的聚合如何实现？\n\nelasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数，即该字段的 distinct 或者 unique 值的数目。它是基于 hll 算法的。hll 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。\n\n\n# 19、在并发情况下，elasticsearch 如果保证读写一致？\n\n（1）可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突；\n\n（2）另外对于写操作，一致性级别支持 quorum/one/all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。\n\n（3）对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本。\n\n\n# 20、如何监控 elasticsearch 集群状态？\n\nmarvel 让你可以很简单的通过 kibana 监控 elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。\n\n\n# 21、介绍下你们电商搜索的整体技术架构。\n\n\n\n\n# 介绍一下你们的个性化搜索方案？\n\n基于 word2vec 和 elasticsearch 实现个性化搜索\n\n（1）基于 word2vec、elasticsearch 和自定义的脚本插件，我们就实现了一个个性化的搜索服务，相对于原有的实现，新版的点击率和转化率都有大幅的提升；\n\n（2）基于 word2vec 的商品向量还有一个可用之处，就是可以用来实现相似商品的推荐；\n\n（3）使用 word2vec 来实现个性化搜索或个性化推荐是有一定局限性的，因为它只能处理用户点击历史这样的时序数据，而无法全面的去考虑用户偏好，这个还是有很大的改进和提升的空间；\n\n\n# 是否了解字典树？\n\n常用字典数据结构如下所示：\n\n\n\ntrie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有 3 个基本性质：\n\n1）根节点不包含字符，除根节点外每一个节点都只包含一个字符。\n\n2）从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。\n\n3）每个节点的所有子节点包含的字符都不相同。\n\n\n\n（1）可以看到，trie 树每一层的节点数是 26^i 级别的。所以为了节省空间，我们还可以用动态链表，或者用数组来模拟动态。而空间的花费，不会超过单词数 × 单词长度。\n\n（2）实现：对每个结点开一个字母集大小的数组，每个结点挂一个链表，使用左儿子右兄弟表示法记录这棵树；\n\n（3）对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上可以保留哈希的复杂度 o(1)。\n\n\n# 拼写纠错是如何实现的？\n\n（1）拼写纠错是基于编辑距离来实现；编辑距离是一种标准的方法，它用来表示经过插入、删除和替换操作从一个字符串转换到另外一个字符串的最小操作步数；\n\n（2）编辑距离的计算过程：比如要计算 batyu 和 beauty 的编辑距离，先创建一个 7×8 的表（batyu 长度为 5，coffee 长度为 6，各加 2），接着，在如下位置填入黑色数字。其他格的计算过程是取以下三个值的最小值：\n\n如果最上方的字符等于最左方的字符，则为左上方的数字。否则为左上方的数字+1。（对于 3,3 来说为 0）\n\n左方数字+1（对于 3,3 格来说为 2）\n\n上方数字+1（对于 3,3 格来说为 2）\n\n最终取右下角的值即为编辑距离的值 3。\n\n\n\n对于拼写纠错，我们考虑构造一个度量空间（metric space），该空间内任何关系满足以下三条基本条件：\n\nd(x,y) = 0 -- 假如 x 与 y 的距离为 0，则 x=y\n\nd(x,y) = d(y,x) -- x 到 y 的距离等同于 y 到 x 的距离\n\nd(x,y) + d(y,z) >= d(x,z) -- 三角不等式\n\n（1）根据三角不等式，则满足与 query 距离在 n 范围内的另一个字符转 b，其与 a 的距离最大为 d+n，最小为 d-n。\n\n（2）bk 树的构造就过程如下：每个节点有任意个子节点，每条边有个值表示编辑距离。所有子节点到父节点的边上标注 n 表示编辑距离恰好为 n。比如，我们有棵树父节点是”book”和两个子节点”cake”和”books”，”book”到”books”的边标号 1，”book”到”cake”的边上标号 4。从字典里构造好树后，无论何时你想插入新单词时，计算该单词与根节点的编辑距离，并且查找数值为 d(neweord, root)的边。递归得与各子节点进行比较，直到没有子节点，你就可以创建新的子节点并将新单词保存在那。比如，插入”boo”到刚才上述例子的树中，我们先检查根节点，查找 d(“book”, “boo”) = 1 的边，然后检查标号为 1 的边的子节点，得到单词”books”。我们再计算距离 d(“books”, “boo”)=2，则将新单词插在”books”之后，边标号为 2。\n\n（3）查询相似词如下：计算单词与根节点的编辑距离 d，然后递归查找每个子节点标号为 d-n 到 d+n（包含）的边。假如被检查的节点与搜索单词的距离 d 小于 n，则返回该节点并继续查询。比如输入 cape 且最大容忍距离为 1，则先计算和根的编辑距离 d(“book”, “cape”)=4，然后接着找和根节点之间编辑距离为 3 到 5 的，这个就找到了 cake 这个节点，计算 d(“cake”, “cape”)=1，满足条件所以返回 cake，然后再找和 cake 节点编辑距离是 0 到 2 的，分别找到 cape 和 cart 节点，这样就得到 cape 这个满足条件的结果。\n\n",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 快速入门",frontmatter:{title:"Elasticsearch 快速入门",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch"],permalink:"/pages/98c3a5/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/02.Elasticsearch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/02.Elasticsearch快速入门.md",key:"v-41d7fd45",path:"/pages/98c3a5/",headers:[{level:2,title:"Elasticsearch 简介",slug:"elasticsearch-简介",normalizedTitle:"elasticsearch 简介",charIndex:267},{level:3,title:"什么是 Elasticsearch",slug:"什么是-elasticsearch",normalizedTitle:"什么是 elasticsearch",charIndex:288},{level:3,title:"核心概念",slug:"核心概念",normalizedTitle:"核心概念",charIndex:669},{level:4,title:"Cluster",slug:"cluster",normalizedTitle:"cluster",charIndex:725},{level:4,title:"Node",slug:"node",normalizedTitle:"node",charIndex:796},{level:4,title:"Index",slug:"index",normalizedTitle:"index",charIndex:940},{level:4,title:"Type",slug:"type",normalizedTitle:"type",charIndex:1103},{level:4,title:"Document",slug:"document",normalizedTitle:"document",charIndex:1369},{level:4,title:"Field",slug:"field",normalizedTitle:"field",charIndex:1618},{level:4,title:"Shard",slug:"shard",normalizedTitle:"shard",charIndex:1708},{level:4,title:"Replica",slug:"replica",normalizedTitle:"replica",charIndex:1882},{level:4,title:"ES 核心概念 vs. DB 核心概念",slug:"es-核心概念-vs-db-核心概念",normalizedTitle:"es 核心概念 vs. db 核心概念",charIndex:2161},{level:2,title:"ElasticSearch 基本原理",slug:"elasticsearch-基本原理",normalizedTitle:"elasticsearch 基本原理",charIndex:2246},{level:3,title:"ES 写数据过程",slug:"es-写数据过程",normalizedTitle:"es 写数据过程",charIndex:2269},{level:3,title:"ES 读数据过程",slug:"es-读数据过程",normalizedTitle:"es 读数据过程",charIndex:2546},{level:3,title:"es 搜索数据过程",slug:"es-搜索数据过程",normalizedTitle:"es 搜索数据过程",charIndex:2887},{level:3,title:"写数据底层原理",slug:"写数据底层原理",normalizedTitle:"写数据底层原理",charIndex:3381},{level:3,title:"删除/更新数据底层原理",slug:"删除-更新数据底层原理",normalizedTitle:"删除/更新数据底层原理",charIndex:5637},{level:3,title:"底层 lucene",slug:"底层-lucene",normalizedTitle:"底层 lucene",charIndex:6077},{level:3,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:5627},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7161}],headersStr:"Elasticsearch 简介 什么是 Elasticsearch 核心概念 Cluster Node Index Type Document Field Shard Replica ES 核心概念 vs. DB 核心概念 ElasticSearch 基本原理 ES 写数据过程 ES 读数据过程 es 搜索数据过程 写数据底层原理 删除/更新数据底层原理 底层 lucene 倒排索引 参考资料",content:'# Elasticsearch 快速入门\n\n> Elasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n> \n> Elasticsearch 基于搜索库 Lucene 开发。ElasticSearch 隐藏了 Lucene 的复杂性，提供了简单易用的 REST API / Java API 接口（另外还有其他语言的 API 接口）。\n> \n> 以下简称 ES。\n\n\n# Elasticsearch 简介\n\n\n# 什么是 Elasticsearch\n\nElasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n\nElasticsearch 基于搜索库 Lucene 开发。ElasticSearch 隐藏了 Lucene 的复杂性，提供了简单易用的 REST API / Java API 接口（另外还有其他语言的 API 接口）。\n\nElasticSearch 可以视为一个文档存储，它将复杂数据结构序列化为 JSON 存储。\n\nElasticSearch 是近乎于实时的全文搜素，这是指：\n\n * 从写入数据到数据可以被搜索，存在较小的延迟（大概是 1s）\n * 基于 ES 执行搜索和分析可以达到秒级\n\n\n# 核心概念\n\nindex -> type -> mapping -> document -> field\n\n\n# Cluster\n\n集群包含多个节点，每个节点属于哪个集群都是通过一个配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。\n\n# Node\n\nNode 是集群中的一个节点，节点也有一个名称，默认是随机分配的。默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。\n\n# Index\n\n可以认为是文档（document）的优化集合。\n\nES 会为所有字段建立索引，经过处理后写入一个反向索引（Inverted Index）。查找数据的时候，直接查找该索引。\n\n所以，ES 数据管理的顶层单位就叫做 Index（索引）。它是单个数据库的同义词。每个 Index （即数据库）的名字必须是小写。\n\n# Type\n\n每个索引里可以有一个或者多个类型（type）。类型（type） 是 index 的一个逻辑分类。\n\n不同的 Type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 Index，而不是一个 Index 里面的两个 Type（虽然可以做到）。\n\n> 注意：根据规划，Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。\n\n# Document\n\nIndex 里面单条的记录称为 Document（文档）。许多条 Document 构成了一个 Index。\n\n每个 文档（document） 都是字段（field）的集合。\n\nDocument 使用 JSON 格式表示，下面是一个例子。\n\n{\n"user": "张三",\n"title": "工程师",\n"desc": "数据库管理"\n}\n\n\n同一个 Index 里面的 Document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。\n\n# Field\n\n字段（field） 是包含数据的键值对。\n\n默认情况下，Elasticsearch 对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。\n\n# Shard\n\n当单台机器不足以存储大量数据时，Elasticsearch 可以将一个索引中的数据切分为多个 分片（shard） 。 分片（shard） 分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。\n\n# Replica\n\n任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 副本（replica）。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。\n\n# ES 核心概念 vs. DB 核心概念\n\nES         DB\nindex      数据库\ntype       数据表\ndocuemnt   一行数据\n\n\n# ElasticSearch 基本原理\n\n\n# ES 写数据过程\n\n * 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。\n * coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。\n * 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。\n * coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。\n\n\n\n\n# ES 读数据过程\n\n可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n\n * 客户端发送请求到任意一个 node，成为 coordinate node。\n * coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n * 接收请求的 node 返回 document 给 coordinate node。\n * coordinate node 返回 document 给客户端。\n\n\n# es 搜索数据过程\n\nes 最强大的是做全文检索，就是比如你有三条数据：\n\njava真好玩儿啊\njava好难学啊\nj2ee特别牛\n\n\n你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n\n * 客户端发送请求到一个 coordinate node 。\n * 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。\n * query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。\n * fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。\n\n> 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n\n\n# 写数据底层原理\n\n\n\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。\n\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。\n\n为什么叫 es 是准实时的？ NRT，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\n\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n\n这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\n\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\n\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n\n实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n\n> 数据写入 segment file 之后，同时就建立好了倒排索引。\n\n\n# 删除/更新数据底层原理\n\n如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。\n\n\n# 底层 lucene\n\n简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n\n\n# 倒排索引\n\n在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n\n那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n\n举个栗子。\n\n有以下文档：\n\nDOCID   DOC\n1       谷歌地图之父跳槽 Facebook\n2       谷歌地图之父加盟 Facebook\n3       谷歌地图创始人拉斯离开谷歌加盟 Facebook\n4       谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关\n5       谷歌地图之父拉斯加盟社交网站 Facebook\n\n对文档进行分词之后，得到以下倒排索引。\n\nWORDID   WORD       DOCIDS\n1        谷歌         1,2,3,4,5\n2        地图         1,2,3,4,5\n3        之父         1,2,4,5\n4        跳槽         1,4\n5        Facebook   1,2,3,4,5\n6        加盟         2,3,5\n7        创始人        3\n8        拉斯         3,5\n9        离开         3\n10       与          4\n..       ..         ..\n\n另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n\n要注意倒排索引的两个重要细节：\n\n * 倒排索引中的所有词项对应一个或多个文档；\n * 倒排索引中的词项根据字典顺序升序排列\n\n> 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n\n\n# 参考资料\n\n * 官方\n   * Elasticsearch 官网\n   * Elasticsearch Github\n   * Elasticsearch 官方文档\n * 文章\n   * Install Elasticsearch with RPM\n   * https://www.ruanyifeng.com/blog/2017/08/elasticsearch.html\n   * es-introduction\n   * es-write-query-search',normalizedContent:'# elasticsearch 快速入门\n\n> elasticsearch 是一个分布式、restful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 elastic stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n> \n> elasticsearch 基于搜索库 lucene 开发。elasticsearch 隐藏了 lucene 的复杂性，提供了简单易用的 rest api / java api 接口（另外还有其他语言的 api 接口）。\n> \n> 以下简称 es。\n\n\n# elasticsearch 简介\n\n\n# 什么是 elasticsearch\n\nelasticsearch 是一个分布式、restful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 elastic stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n\nelasticsearch 基于搜索库 lucene 开发。elasticsearch 隐藏了 lucene 的复杂性，提供了简单易用的 rest api / java api 接口（另外还有其他语言的 api 接口）。\n\nelasticsearch 可以视为一个文档存储，它将复杂数据结构序列化为 json 存储。\n\nelasticsearch 是近乎于实时的全文搜素，这是指：\n\n * 从写入数据到数据可以被搜索，存在较小的延迟（大概是 1s）\n * 基于 es 执行搜索和分析可以达到秒级\n\n\n# 核心概念\n\nindex -> type -> mapping -> document -> field\n\n\n# cluster\n\n集群包含多个节点，每个节点属于哪个集群都是通过一个配置来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。\n\n# node\n\nnode 是集群中的一个节点，节点也有一个名称，默认是随机分配的。默认节点会去加入一个名称为 elasticsearch 的集群。如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成 elasticsearch 集群。\n\n# index\n\n可以认为是文档（document）的优化集合。\n\nes 会为所有字段建立索引，经过处理后写入一个反向索引（inverted index）。查找数据的时候，直接查找该索引。\n\n所以，es 数据管理的顶层单位就叫做 index（索引）。它是单个数据库的同义词。每个 index （即数据库）的名字必须是小写。\n\n# type\n\n每个索引里可以有一个或者多个类型（type）。类型（type） 是 index 的一个逻辑分类。\n\n不同的 type 应该有相似的结构（schema），举例来说，id字段不能在这个组是字符串，在另一个组是数值。这是与关系型数据库的表的一个区别。性质完全不同的数据（比如products和logs）应该存成两个 index，而不是一个 index 里面的两个 type（虽然可以做到）。\n\n> 注意：根据规划，elastic 6.x 版只允许每个 index 包含一个 type，7.x 版将会彻底移除 type。\n\n# document\n\nindex 里面单条的记录称为 document（文档）。许多条 document 构成了一个 index。\n\n每个 文档（document） 都是字段（field）的集合。\n\ndocument 使用 json 格式表示，下面是一个例子。\n\n{\n"user": "张三",\n"title": "工程师",\n"desc": "数据库管理"\n}\n\n\n同一个 index 里面的 document，不要求有相同的结构（scheme），但是最好保持相同，这样有利于提高搜索效率。\n\n# field\n\n字段（field） 是包含数据的键值对。\n\n默认情况下，elasticsearch 对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。\n\n# shard\n\n当单台机器不足以存储大量数据时，elasticsearch 可以将一个索引中的数据切分为多个 分片（shard） 。 分片（shard） 分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。\n\n# replica\n\n任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 副本（replica）。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。\n\n# es 核心概念 vs. db 核心概念\n\nes         db\nindex      数据库\ntype       数据表\ndocuemnt   一行数据\n\n\n# elasticsearch 基本原理\n\n\n# es 写数据过程\n\n * 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。\n * coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。\n * 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。\n * coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。\n\n\n\n\n# es 读数据过程\n\n可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n\n * 客户端发送请求到任意一个 node，成为 coordinate node。\n * coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n * 接收请求的 node 返回 document 给 coordinate node。\n * coordinate node 返回 document 给客户端。\n\n\n# es 搜索数据过程\n\nes 最强大的是做全文检索，就是比如你有三条数据：\n\njava真好玩儿啊\njava好难学啊\nj2ee特别牛\n\n\n你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。\n\n * 客户端发送请求到一个 coordinate node 。\n * 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。\n * query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。\n * fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。\n\n> 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。\n\n\n# 写数据底层原理\n\n\n\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。\n\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。\n\n为什么叫 es 是准实时的？ nrt，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\n\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n\n这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\n\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\n\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n\n实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n\n> 数据写入 segment file 之后，同时就建立好了倒排索引。\n\n\n# 删除/更新数据底层原理\n\n如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。\n\n\n# 底层 lucene\n\n简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n\n\n# 倒排索引\n\n在搜索引擎中，每个文档都有一个对应的文档 id，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n\n那么，倒排索引就是关键词到文档 id 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n\n举个栗子。\n\n有以下文档：\n\ndocid   doc\n1       谷歌地图之父跳槽 facebook\n2       谷歌地图之父加盟 facebook\n3       谷歌地图创始人拉斯离开谷歌加盟 facebook\n4       谷歌地图之父跳槽 facebook 与 wave 项目取消有关\n5       谷歌地图之父拉斯加盟社交网站 facebook\n\n对文档进行分词之后，得到以下倒排索引。\n\nwordid   word       docids\n1        谷歌         1,2,3,4,5\n2        地图         1,2,3,4,5\n3        之父         1,2,4,5\n4        跳槽         1,4\n5        facebook   1,2,3,4,5\n6        加盟         2,3,5\n7        创始人        3\n8        拉斯         3,5\n9        离开         3\n10       与          4\n..       ..         ..\n\n另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n\n要注意倒排索引的两个重要细节：\n\n * 倒排索引中的所有词项对应一个或多个文档；\n * 倒排索引中的词项根据字典顺序升序排列\n\n> 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n\n\n# 参考资料\n\n * 官方\n   * elasticsearch 官网\n   * elasticsearch github\n   * elasticsearch 官方文档\n * 文章\n   * install elasticsearch with rpm\n   * https://www.ruanyifeng.com/blog/2017/08/elasticsearch.html\n   * es-introduction\n   * es-write-query-search',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 简介",frontmatter:{title:"Elasticsearch 简介",date:"2022-02-22T21:01:01.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch"],permalink:"/pages/0fb506/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/03.Elasticsearch%E7%AE%80%E4%BB%8B.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/03.Elasticsearch简介.md",key:"v-83a32830",path:"/pages/0fb506/",headers:[{level:2,title:"Elasticsearch 特点",slug:"elasticsearch-特点",normalizedTitle:"elasticsearch 特点",charIndex:429},{level:2,title:"Elasticsearch 发展历史",slug:"elasticsearch-发展历史",normalizedTitle:"elasticsearch 发展历史",charIndex:600},{level:2,title:"Elasticsearch 概念",slug:"elasticsearch-概念",normalizedTitle:"elasticsearch 概念",charIndex:3168},{level:3,title:"近实时（NRT）",slug:"近实时-nrt",normalizedTitle:"近实时（nrt）",charIndex:3258},{level:3,title:"索引（Index）",slug:"索引-index",normalizedTitle:"索引（index）",charIndex:3331},{level:4,title:"倒排索引",slug:"倒排索引",normalizedTitle:"倒排索引",charIndex:3574},{level:4,title:"index template",slug:"index-template",normalizedTitle:"index template",charIndex:3930},{level:4,title:"dynamic template",slug:"dynamic-template",normalizedTitle:"dynamic template",charIndex:5130},{level:3,title:"~~类型（Type）~~",slug:"类型-type",normalizedTitle:"<s>类型（type）</s>",charIndex:null},{level:3,title:"文档（Document）",slug:"文档-document",normalizedTitle:"文档（document）",charIndex:6584},{level:4,title:"文档的元数据",slug:"文档的元数据",normalizedTitle:"文档的元数据",charIndex:6759},{level:3,title:"节点（Node）",slug:"节点-node",normalizedTitle:"节点（node）",charIndex:7215},{level:4,title:"节点简介",slug:"节点简介",normalizedTitle:"节点简介",charIndex:7227},{level:4,title:"节点类型",slug:"节点类型",normalizedTitle:"节点类型",charIndex:7447},{level:4,title:"节点配置",slug:"节点配置",normalizedTitle:"节点配置",charIndex:8023},{level:3,title:"集群（Cluster）",slug:"集群-cluster",normalizedTitle:"集群（cluster）",charIndex:8232},{level:4,title:"集群简介",slug:"集群简介",normalizedTitle:"集群简介",charIndex:8247},{level:4,title:"集群健康",slug:"集群健康",normalizedTitle:"集群健康",charIndex:8704},{level:3,title:"分片（Shards）",slug:"分片-shards",normalizedTitle:"分片（shards）",charIndex:9438},{level:4,title:"分片简介",slug:"分片简介",normalizedTitle:"分片简介",charIndex:9452},{level:4,title:"主分片和副分片",slug:"主分片和副分片",normalizedTitle:"主分片和副分片",charIndex:9730},{level:3,title:"副本（Replicas）",slug:"副本-replicas",normalizedTitle:"副本（replicas）",charIndex:10156},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10505}],headersStr:"Elasticsearch 特点 Elasticsearch 发展历史 Elasticsearch 概念 近实时（NRT） 索引（Index） 倒排索引 index template dynamic template ~~类型（Type）~~ 文档（Document） 文档的元数据 节点（Node） 节点简介 节点类型 节点配置 集群（Cluster） 集群简介 集群健康 分片（Shards） 分片简介 主分片和副分片 副本（Replicas） 参考资料",content:'# Elasticsearch 简介\n\nElasticsearch 是一个基于 Lucene 的搜索和数据分析工具，它提供了一个分布式服务。Elasticsearch 是遵从 Apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n它用于全文搜索、结构化搜索、分析以及将这三者混合使用：\n\n * 维基百科使用 Elasticsearch 提供全文搜索并高亮关键字，以及**输入实时搜索(search-as-you-type)和搜索纠错(did-you-mean)**等搜索建议功能。\n * 英国卫报使用 Elasticsearch 结合用户日志和社交网络数据提供给他们的编辑以实时的反馈，以便及时了解公众对新发表的文章的回应。\n * StackOverflow 结合全文搜索与地理位置查询，以及more-like-this功能来找到相关的问题和答案。\n * Github 使用 Elasticsearch 检索 1300 亿行的代码。\n\n\n# Elasticsearch 特点\n\n * 分布式的实时文件存储，每个字段都被索引并可被搜索；\n * 分布式的实时分析搜索引擎；\n * 可弹性扩展到上百台服务器规模，处理 PB 级结构化或非结构化数据；\n * 开箱即用（安装即可使用），它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。只需很少的学习既可在生产环境中使用。\n\n\n# Elasticsearch 发展历史\n\n * 2010 年 2 月 8 日，Elasticsearch 第一个公开版本发布。\n\n * 2010 年 5 月 14 日，发布第一个具有里程碑意义的初始版本 0.7.0 ，具有如下特征：\n\n * Zen Discovery 自动发现模块；\n   \n   * 支持 Groovy Client；\n\n * 简单的插件管理机制；\n   \n   * 更好地支持 icu 分词器；\n\n * 更多的管理 api。\n\n * 2013 年初，GitHub 抛弃了 Solr，采取 ElasticSearch 来做其 PB 级的搜索。\n\n * 2014 年 2 月 14 日，发布 1.0.0 版本，增加如下重要特性：\n\n * 支持 Snapshot/Restore API 备份恢复 API；\n   \n   * 支持聚合分析 Aggregations；\n\n * 支持 cat api；\n   \n   * 支持断路器；\n\n * 引入 Doc values。\n\n * 2015 年 10 月 28 日，发布 2.0.0 版本，有如下重要特性：\n\n * 增加了 Pipleline Aggregations；\n   \n   * query/filter 查询合并，都合并到 query 中，根据不同的上下文执行不同的查询；\n\n * 压缩存储可配置；\n   \n   * Rivers 模块被移除；\n\n * Multicast 组播发现被移除，成为一个插件，生产环境必须配置单播地址。\n\n * 2016 年 10 月 26 日，发布 5.0.0 版本，有如下重大特性变化：\n\n * Lucene 6.x 的支持，磁盘空间少一半；索引时间少一半；查询性能提升 25%；支持 IPV6；\n   \n   * Internal Engine 级别移除了用于避免同一文档并发更新的竞争锁，带来 15%-20% 的性能提升；\n\n * Shrink API，它可将分片数进行收缩成它的因数，如之前你是 15 个分片，你可以收缩成 5 个或者 3 个又或者 1 个，那么我们就可以想象成这样一种场景，在写入压力非常大的收集阶段，设置足够多的索引，充分利用 shard 的并行写能力，索引写完之后收缩成更少的 shard，提高查询性能；\n   \n   * 提供了第一个 Java 原生的 REST 客户端 SDK；\n\n * IngestNode，之前如果需要对数据进行加工，都是在索引之前进行处理，比如 logstash 可以对日志进行结构化和转换，现在直接在 es 就可以处理了；\n   \n   * 提供了 Painless 脚本，代替 Groovy 脚本；\n   * 移除 site plugins，就是说 head、bigdesk 都不能直接装 es 里面了，不过可以部署独立站点（反正都是静态文件）或开发 kibana 插件；\n   * 新增 Sliced Scroll 类型，现在 Scroll 接口可以并发来进行数据遍历了。每个 Scroll 请求，可以分成多个 Slice 请求，可以理解为切片，各 Slice 独立并行，利用 Scroll 重建或者遍历要快很多倍；\n   * 新增了 Profile API；\n   * 新增了 Rollover API；\n   * 新增 Reindex；\n   * 引入新的字段类型 Text/Keyword 来替换 String；\n   * 限制索引请求大小，避免大量并发请求压垮 ES；\n   * 限制单个请求的 shards 数量，默认 1000 个。\n\n * 2017 年 8 月 31 日，发布 6.0.0 版本，具有如下重要特性：\n\n * 稀疏性 Doc Values 的支持；\n   \n   * Index Sorting，即索引阶段的排序；\n\n * 顺序号的支持，每个 es 的操作都有一个顺序编号（类似增量设计）；\n   \n   * 无缝滚动升级；\n\n * 从 6.0 开始不支持一个 index 里面存在多个 type；\n   \n   * Index-template inheritance，索引版本的继承，目前索引模板是所有匹配的都会合并，这样会造成索引模板有一些冲突问题， 6.0 将会只匹配一个，索引创建时也会进行验证；\n   * Load aware shard routing， 基于负载的请求路由，目前的搜索请求是全节点轮询，那么性能最慢的节点往往会造成整体的延迟增加，新的实现方式将基于队列的耗费时间自动调节队列长度，负载高的节点的队列长度将减少，让其他节点分摊更多的压力，搜索和索引都将基于这种机制；\n   * 已经关闭的索引将也支持 replica 的自动处理，确保数据可靠。\n\n * 2019 年 4 月 10 日，发布 7.0.0 版本，具有如下重要特性：\n\n * 集群连接变化：TransportClient 被废弃，es7 的 java 代码，只能使用 restclient；对于 java 编程，建议采用 High-level-rest-client 的方式操作 ES 集群；\n   \n   * ES 程序包默认打包 jdk：7.x 版本的程序包大小变成 300MB+，对比 6.x，包大了 200MB+，这正是 JDK 的大小；\n\n * 采用基于 Lucene 9.0；\n   \n   * 正式废除单个索引下多 Type 的支持，es6 时，官方就提到了 es7 会删除 type，并且 es6 时，已经规定每一个 index 只能有一个 type。在 es7 中，使用默认的 _doc 作为 type，官方说在 8.x 版本会彻底移除 type。api 请求方式也发送变化，如获得某索引的某 ID 的文档：GET index/_doc/id 其中 index 和 id 为具体的值；\n\n * 引入了真正的内存断路器，它可以更精准地检测出无法处理的请求，并防止它们使单个节点不稳定；\n   \n   * Zen2 是 Elasticsearch 的全新集群协调层，提高了可靠性、性能和用户体验，变得更快、更安全，并更易于使用。\n\n\n# Elasticsearch 概念\n\n下列有一些概念是 Elasticsearch 的核心。从一开始就理解这些概念将极大地帮助简化学习 Elasticsearch 的过程。\n\n\n# 近实时（NRT）\n\nElasticsearch 是一个近乎实时的搜索平台。这意味着从索引文档到可搜索文档的时间有一点延迟（通常是一秒）。\n\n\n# 索引（Index）\n\n索引在不同语境，有着不同的含义\n\n * 索引（名词）：一个 索引 类似于传统关系数据库中的一个 数据库 ，是一个存储关系型文档的容器。 索引 (index) 的复数词为 indices 或 indexes 。索引实际上是指向一个或者多个物理分片的逻辑命名空间 。\n * 索引（动词）：索引一个文档 就是存储一个文档到一个 索引 （名词）中以便被检索和查询。这非常类似于 SQL 语句中的 INSERT 关键词，除了文档已存在时，新文档会替换旧文档情况之外。\n * 倒排索引：关系型数据库通过增加一个索引比如一个 B 树索引到指定的列上，以便提升数据检索速度。Elasticsearch 和 Lucene 使用了一个叫做 倒排索引 的结构来达到相同的目的。\n\n索引的 Mapping 和 Setting\n\n * Mapping 定义文档字段的类型\n * Setting 定义不同的数据分布\n\n示例：\n\n{\n    "settings": { ... any settings ... },\n    "mappings": {\n        "type_one": { ... any mappings ... },\n        "type_two": { ... any mappings ... },\n        ...\n    }\n}\n\n\n# 倒排索引\n\n\n\n# index template\n\nindex template（索引模板）帮助用户设定 Mapping 和 Setting，并按照一定的规则，自动匹配到新创建的索引之上。\n\n * 模板仅在一个索引被创建时，才会产生作用。修改模板不会影响已创建的索引。\n * 你可以设定多个索引模板，这些设置会被 merge 在一起。\n * 你可以指定 order 的数值，控制 merge 的过程。\n\n当新建一个索引时\n\n * 应用 ES 默认的 Mapping 和 Setting\n * 应用 order 数值低的 index template 中的设定\n * 应用 order 数值高的 index template 中的设定，之前的设定会被覆盖\n * 应用创建索引是，用户所指定的 Mapping 和 Setting，并覆盖之前模板中的设定。\n\n示例：创建默认索引模板\n\nPUT _template/template_default\n{\n  "index_patterns": ["*"],\n  "order": 0,\n  "version": 1,\n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas": 1\n  }\n}\n\nPUT /_template/template_test\n{\n  "index_patterns": ["test*"],\n  "order": 1,\n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas": 2\n  },\n  "mappings": {\n    "date_detection": false,\n    "numeric_detection": true\n  }\n}\n\n# 查看索引模板\nGET /_template/template_default\nGET /_template/temp*\n\n#写入新的数据，index以test开头\nPUT testtemplate/_doc/1\n{\n  "someNumber": "1",\n  "someDate": "2019/01/01"\n}\nGET testtemplate/_mapping\nGET testtemplate/_settings\n\nPUT testmy\n{\n\t"settings":{\n\t\t"number_of_replicas":5\n\t}\n}\n\nPUT testmy/_doc/1\n{\n  "key": "value"\n}\n\nGET testmy/_settings\nDELETE testmy\nDELETE /_template/template_default\nDELETE /_template/template_test\n\n\n# dynamic template\n\n * 根据 ES 识别的数据类型，结合字段名称，来动态设定字段类型\n   * 所有的字符串类型都设定成 Keyword，或者关闭 keyword 字段。\n   * is 开头的字段都设置成 boolean\n   * long_ 开头的都设置成 long 类型\n * dynamic template 是定义在某个索引的 Mapping 中\n * template 有一个名称\n * 匹配规则是一个数组\n * 为匹配到字段设置 Mapping\n\n示例：\n\n#Dynaminc Mapping 根据类型和字段名\nDELETE my_index\n\nPUT my_index/_doc/1\n{\n  "firstName": "Ruan",\n  "isVIP": "true"\n}\n\nGET my_index/_mapping\n\nDELETE my_index\nPUT my_index\n{\n  "mappings": {\n    "dynamic_templates": [\n      {\n        "strings_as_boolean": {\n          "match_mapping_type": "string",\n          "match": "is*",\n          "mapping": {\n            "type": "boolean"\n          }\n        }\n      },\n      {\n        "strings_as_keywords": {\n          "match_mapping_type": "string",\n          "mapping": {\n            "type": "keyword"\n          }\n        }\n      }\n    ]\n  }\n}\nGET my_index/_mapping\n\nDELETE my_index\n#结合路径\nPUT my_index\n{\n  "mappings": {\n    "dynamic_templates": [\n      {\n        "full_name": {\n          "path_match": "name.*",\n          "path_unmatch": "*.middle",\n          "mapping": {\n            "type": "text",\n            "copy_to": "full_name"\n          }\n        }\n      }\n    ]\n  }\n}\nGET my_index/_mapping\n\n\nPUT my_index/_doc/1\n{\n  "name": {\n    "first": "John",\n    "middle": "Winston",\n    "last": "Lennon"\n  }\n}\n\nGET my_index/_search?q=full_name:John\nDELETE my_index\n\n\n\n# 类型（Type）\n\ntype 是一个逻辑意义上的分类或者叫分区，允许在同一索引中建立多个 type。本质是相当于一个过滤条件，高版本将会废弃 type 概念。\n\n> 6.0.0 版本及之后，废弃 type\n\n\n# 文档（Document）\n\nElasticsearch 是面向文档的，文档是所有可搜索数据的最小单位。\n\nElasticsearch 使用 JSON 作为文档的序列化格式。\n\n在索引/类型中，可以根据需要存储任意数量的文档。\n\n每个文档都有一个 Unique ID\n\n * 用户可以自己指定\n * 或通过 Elasticsearch 自动生成\n\n# 文档的元数据\n\n一个文档不仅仅包含它的数据 ，也包含元数据 —— 有关文档的信息。\n\n * _index：文档在哪存放\n * _type：文档表示的对象类别\n * _id：文档唯一标识\n * _source：文档的原始 Json 数据\n * _all：整合所有字段内容到该字段，已被废除\n * _version：文档的版本信息\n * _score：相关性打分\n\n示例：\n\n{\n  "_index": "megacorp",\n  "_type": "employee",\n  "_id": "1",\n  "_version": 1,\n  "found": true,\n  "_source": {\n    "first_name": "John",\n    "last_name": "Smith",\n    "age": 25,\n    "about": "I love to go rock climbing",\n    "interests": ["sports", "music"]\n  }\n}\n\n\n\n# 节点（Node）\n\n# 节点简介\n\n一个运行中的 Elasticsearch 实例称为一个节点。\n\nElasticsearch 实例本质上是一个 Java 进程。一台机器上可以运行多个 Elasticsearch 进程，但是生产环境建议一台机器上只运行一个 Elasticsearch 进程\n\n每个节点都有名字，通过配置文件配置，或启动时通过 -E node.name=node1 指定。\n\n每个节点在启动后，会分配一个 UID，保存在 data 目录下。\n\n# 节点类型\n\n * 主节点（master node）：每个节点都保存了集群的状态，只有 master 节点才能修改集群的状态信息（保证数据一致性）。集群状态，维护了以下信息：\n   * 所有的节点信息\n   * 所有的索引和其相关的 mapping 和 setting 信息\n   * 分片的路由信息\n * 候选节点（master eligible node）：master eligible 节点可以参加选主流程。第一个启动的节点，会将自己选举为 mater 节点。\n   * 每个节点启动后，默认为 master eligible 节点，可以通过配置 node.master: false 禁止\n * 数据节点（data node）：负责保存分片数据。\n * 协调节点（coordinating node）：负责接收客户端的请求，将请求分发到合适的接地那，最终把结果汇集到一起。每个 Elasticsearch 节点默认都是协调节点（coordinating node）。\n * 冷/热节点（warm/hot node）：针对不同硬件配置的数据节点（data node），用来实现 Hot & Warm 架构，降低集群部署的成本。\n * 机器学习节点（machine learning node）：负责执行机器学习的 Job，用来做异常检测。\n\n# 节点配置\n\n配置参数          默认值    说明\nnode.master   true   是否为主节点\nnode.data     true   是否为数据节点\nnode.ingest   true   \nnode.ml       true   是否为机器学习节点（需要开启 x-pack）\n\n> 建议\n> \n> 开发环境中一个节点可以承担多种角色。但是，在生产环境中，节点应该设置为单一角色。\n\n\n# 集群（Cluster）\n\n# 集群简介\n\n拥有相同 cluster.name 配置的 Elasticsearch 节点组成一个集群。 cluster.name 默认名为 elasticsearch，可以通过配置文件修改，或启动时通过 -E cluster.name=xxx 指定。\n\n当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n\n当一个节点被选举成为主节点时，它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量增加，它也不会成为瓶颈。 任何节点都可以成为主节点。\n\n作为用户，我们可以将请求发送到集群中的任何节点 ，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。\n\n# 集群健康\n\nElasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red 。\n\n在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容：\n\n{\n  "cluster_name" : "elasticsearch",\n  "status" : "green",\n  "timed_out" : false,\n  "number_of_nodes" : 1,\n  "number_of_data_nodes" : 1,\n  "active_primary_shards" : 5,\n  "active_shards" : 5,\n  "relocating_shards" : 0,\n  "initializing_shards" : 0,\n  "unassigned_shards" : 0,\n  "delayed_unassigned_shards" : 0,\n  "number_of_pending_tasks" : 0,\n  "number_of_in_flight_fetch" : 0,\n  "task_max_waiting_in_queue_millis" : 0,\n  "active_shards_percent_as_number" : 100.0\n}\n\n\nstatus 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：\n\n * green：所有的主分片和副本分片都正常运行。\n * yellow：所有的主分片都正常运行，但不是所有的副本分片都正常运行。\n * red：有主分片没能正常运行。\n\n\n# 分片（Shards）\n\n# 分片简介\n\n索引实际上是指向一个或者多个物理分片的逻辑命名空间 。\n\n一个分片是一个底层的工作单元 ，它仅保存了全部数据中的一部分。一个分片可以视为一个 Lucene 的实例，并且它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。\n\nElasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。\n\n# 主分片和副分片\n\n分片分为主分片（Primary Shard）和副分片（Replica Shard）。\n\n主分片：用于解决数据水平扩展的问题。通过主分片，可以将数据分布到集群内不同节点上。\n\n * 索引内任意一个文档都归属于一个主分片。\n * 主分片数在索引创建时指定，后序不允许修改，除非 Reindex\n\n副分片（Replica Shard）：用于解决数据高可用的问题。副分片是主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n\n * 副分片数可以动态调整\n * 增加副本数，还可以在一定程度上提高服务的可用性（读取的吞吐）\n\n对于生产环境中分片的设定，需要提前做好容量规划\n\n分片数过小\n\n * 无法水平扩展\n * 单个分片的数量太大，导致数据重新分配耗时\n\n分片数过大\n\n * 影响搜索结果的相关性打分，影响统计结果的准确性\n * 单节点上过多的分片，会导致资源浪费，同时也会影响性能\n\n\n# 副本（Replicas）\n\n副本主要是针对主分片（Shards）的复制，Elasticsearch 中主分片可以拥有 0 个或多个的副本。\n\n副本分片的主要目的就是为了故障转移。\n\n分片副本很重要，主要有两个原因：\n\n * 它在分片或节点发生故障时提供高可用性。因此，副本分片永远不会在与其复制的主分片相同的节点；\n * 副本分片也可以接受搜索的请求，可以并行搜索，从而提高系统的吞吐量。\n\n> 每个 Elasticsearch 分片都是 Lucene 索引。单个 Lucene 索引中可以包含最大数量的文档。截止 LUCENE-5843，限制是 2,147,483,519（= Integer.MAX_VALUE - 128）文档。您可以使用_cat/shardsAPI 监控分片大小。\n\n\n# 参考资料\n\n * Elasticsearch 官网\n * Elasticsearch 简介',normalizedContent:'# elasticsearch 简介\n\nelasticsearch 是一个基于 lucene 的搜索和数据分析工具，它提供了一个分布式服务。elasticsearch 是遵从 apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n它用于全文搜索、结构化搜索、分析以及将这三者混合使用：\n\n * 维基百科使用 elasticsearch 提供全文搜索并高亮关键字，以及**输入实时搜索(search-as-you-type)和搜索纠错(did-you-mean)**等搜索建议功能。\n * 英国卫报使用 elasticsearch 结合用户日志和社交网络数据提供给他们的编辑以实时的反馈，以便及时了解公众对新发表的文章的回应。\n * stackoverflow 结合全文搜索与地理位置查询，以及more-like-this功能来找到相关的问题和答案。\n * github 使用 elasticsearch 检索 1300 亿行的代码。\n\n\n# elasticsearch 特点\n\n * 分布式的实时文件存储，每个字段都被索引并可被搜索；\n * 分布式的实时分析搜索引擎；\n * 可弹性扩展到上百台服务器规模，处理 pb 级结构化或非结构化数据；\n * 开箱即用（安装即可使用），它提供了许多合理的缺省值，并对初学者隐藏了复杂的搜索引擎理论。只需很少的学习既可在生产环境中使用。\n\n\n# elasticsearch 发展历史\n\n * 2010 年 2 月 8 日，elasticsearch 第一个公开版本发布。\n\n * 2010 年 5 月 14 日，发布第一个具有里程碑意义的初始版本 0.7.0 ，具有如下特征：\n\n * zen discovery 自动发现模块；\n   \n   * 支持 groovy client；\n\n * 简单的插件管理机制；\n   \n   * 更好地支持 icu 分词器；\n\n * 更多的管理 api。\n\n * 2013 年初，github 抛弃了 solr，采取 elasticsearch 来做其 pb 级的搜索。\n\n * 2014 年 2 月 14 日，发布 1.0.0 版本，增加如下重要特性：\n\n * 支持 snapshot/restore api 备份恢复 api；\n   \n   * 支持聚合分析 aggregations；\n\n * 支持 cat api；\n   \n   * 支持断路器；\n\n * 引入 doc values。\n\n * 2015 年 10 月 28 日，发布 2.0.0 版本，有如下重要特性：\n\n * 增加了 pipleline aggregations；\n   \n   * query/filter 查询合并，都合并到 query 中，根据不同的上下文执行不同的查询；\n\n * 压缩存储可配置；\n   \n   * rivers 模块被移除；\n\n * multicast 组播发现被移除，成为一个插件，生产环境必须配置单播地址。\n\n * 2016 年 10 月 26 日，发布 5.0.0 版本，有如下重大特性变化：\n\n * lucene 6.x 的支持，磁盘空间少一半；索引时间少一半；查询性能提升 25%；支持 ipv6；\n   \n   * internal engine 级别移除了用于避免同一文档并发更新的竞争锁，带来 15%-20% 的性能提升；\n\n * shrink api，它可将分片数进行收缩成它的因数，如之前你是 15 个分片，你可以收缩成 5 个或者 3 个又或者 1 个，那么我们就可以想象成这样一种场景，在写入压力非常大的收集阶段，设置足够多的索引，充分利用 shard 的并行写能力，索引写完之后收缩成更少的 shard，提高查询性能；\n   \n   * 提供了第一个 java 原生的 rest 客户端 sdk；\n\n * ingestnode，之前如果需要对数据进行加工，都是在索引之前进行处理，比如 logstash 可以对日志进行结构化和转换，现在直接在 es 就可以处理了；\n   \n   * 提供了 painless 脚本，代替 groovy 脚本；\n   * 移除 site plugins，就是说 head、bigdesk 都不能直接装 es 里面了，不过可以部署独立站点（反正都是静态文件）或开发 kibana 插件；\n   * 新增 sliced scroll 类型，现在 scroll 接口可以并发来进行数据遍历了。每个 scroll 请求，可以分成多个 slice 请求，可以理解为切片，各 slice 独立并行，利用 scroll 重建或者遍历要快很多倍；\n   * 新增了 profile api；\n   * 新增了 rollover api；\n   * 新增 reindex；\n   * 引入新的字段类型 text/keyword 来替换 string；\n   * 限制索引请求大小，避免大量并发请求压垮 es；\n   * 限制单个请求的 shards 数量，默认 1000 个。\n\n * 2017 年 8 月 31 日，发布 6.0.0 版本，具有如下重要特性：\n\n * 稀疏性 doc values 的支持；\n   \n   * index sorting，即索引阶段的排序；\n\n * 顺序号的支持，每个 es 的操作都有一个顺序编号（类似增量设计）；\n   \n   * 无缝滚动升级；\n\n * 从 6.0 开始不支持一个 index 里面存在多个 type；\n   \n   * index-template inheritance，索引版本的继承，目前索引模板是所有匹配的都会合并，这样会造成索引模板有一些冲突问题， 6.0 将会只匹配一个，索引创建时也会进行验证；\n   * load aware shard routing， 基于负载的请求路由，目前的搜索请求是全节点轮询，那么性能最慢的节点往往会造成整体的延迟增加，新的实现方式将基于队列的耗费时间自动调节队列长度，负载高的节点的队列长度将减少，让其他节点分摊更多的压力，搜索和索引都将基于这种机制；\n   * 已经关闭的索引将也支持 replica 的自动处理，确保数据可靠。\n\n * 2019 年 4 月 10 日，发布 7.0.0 版本，具有如下重要特性：\n\n * 集群连接变化：transportclient 被废弃，es7 的 java 代码，只能使用 restclient；对于 java 编程，建议采用 high-level-rest-client 的方式操作 es 集群；\n   \n   * es 程序包默认打包 jdk：7.x 版本的程序包大小变成 300mb+，对比 6.x，包大了 200mb+，这正是 jdk 的大小；\n\n * 采用基于 lucene 9.0；\n   \n   * 正式废除单个索引下多 type 的支持，es6 时，官方就提到了 es7 会删除 type，并且 es6 时，已经规定每一个 index 只能有一个 type。在 es7 中，使用默认的 _doc 作为 type，官方说在 8.x 版本会彻底移除 type。api 请求方式也发送变化，如获得某索引的某 id 的文档：get index/_doc/id 其中 index 和 id 为具体的值；\n\n * 引入了真正的内存断路器，它可以更精准地检测出无法处理的请求，并防止它们使单个节点不稳定；\n   \n   * zen2 是 elasticsearch 的全新集群协调层，提高了可靠性、性能和用户体验，变得更快、更安全，并更易于使用。\n\n\n# elasticsearch 概念\n\n下列有一些概念是 elasticsearch 的核心。从一开始就理解这些概念将极大地帮助简化学习 elasticsearch 的过程。\n\n\n# 近实时（nrt）\n\nelasticsearch 是一个近乎实时的搜索平台。这意味着从索引文档到可搜索文档的时间有一点延迟（通常是一秒）。\n\n\n# 索引（index）\n\n索引在不同语境，有着不同的含义\n\n * 索引（名词）：一个 索引 类似于传统关系数据库中的一个 数据库 ，是一个存储关系型文档的容器。 索引 (index) 的复数词为 indices 或 indexes 。索引实际上是指向一个或者多个物理分片的逻辑命名空间 。\n * 索引（动词）：索引一个文档 就是存储一个文档到一个 索引 （名词）中以便被检索和查询。这非常类似于 sql 语句中的 insert 关键词，除了文档已存在时，新文档会替换旧文档情况之外。\n * 倒排索引：关系型数据库通过增加一个索引比如一个 b 树索引到指定的列上，以便提升数据检索速度。elasticsearch 和 lucene 使用了一个叫做 倒排索引 的结构来达到相同的目的。\n\n索引的 mapping 和 setting\n\n * mapping 定义文档字段的类型\n * setting 定义不同的数据分布\n\n示例：\n\n{\n    "settings": { ... any settings ... },\n    "mappings": {\n        "type_one": { ... any mappings ... },\n        "type_two": { ... any mappings ... },\n        ...\n    }\n}\n\n\n# 倒排索引\n\n\n\n# index template\n\nindex template（索引模板）帮助用户设定 mapping 和 setting，并按照一定的规则，自动匹配到新创建的索引之上。\n\n * 模板仅在一个索引被创建时，才会产生作用。修改模板不会影响已创建的索引。\n * 你可以设定多个索引模板，这些设置会被 merge 在一起。\n * 你可以指定 order 的数值，控制 merge 的过程。\n\n当新建一个索引时\n\n * 应用 es 默认的 mapping 和 setting\n * 应用 order 数值低的 index template 中的设定\n * 应用 order 数值高的 index template 中的设定，之前的设定会被覆盖\n * 应用创建索引是，用户所指定的 mapping 和 setting，并覆盖之前模板中的设定。\n\n示例：创建默认索引模板\n\nput _template/template_default\n{\n  "index_patterns": ["*"],\n  "order": 0,\n  "version": 1,\n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas": 1\n  }\n}\n\nput /_template/template_test\n{\n  "index_patterns": ["test*"],\n  "order": 1,\n  "settings": {\n    "number_of_shards": 1,\n    "number_of_replicas": 2\n  },\n  "mappings": {\n    "date_detection": false,\n    "numeric_detection": true\n  }\n}\n\n# 查看索引模板\nget /_template/template_default\nget /_template/temp*\n\n#写入新的数据，index以test开头\nput testtemplate/_doc/1\n{\n  "somenumber": "1",\n  "somedate": "2019/01/01"\n}\nget testtemplate/_mapping\nget testtemplate/_settings\n\nput testmy\n{\n\t"settings":{\n\t\t"number_of_replicas":5\n\t}\n}\n\nput testmy/_doc/1\n{\n  "key": "value"\n}\n\nget testmy/_settings\ndelete testmy\ndelete /_template/template_default\ndelete /_template/template_test\n\n\n# dynamic template\n\n * 根据 es 识别的数据类型，结合字段名称，来动态设定字段类型\n   * 所有的字符串类型都设定成 keyword，或者关闭 keyword 字段。\n   * is 开头的字段都设置成 boolean\n   * long_ 开头的都设置成 long 类型\n * dynamic template 是定义在某个索引的 mapping 中\n * template 有一个名称\n * 匹配规则是一个数组\n * 为匹配到字段设置 mapping\n\n示例：\n\n#dynaminc mapping 根据类型和字段名\ndelete my_index\n\nput my_index/_doc/1\n{\n  "firstname": "ruan",\n  "isvip": "true"\n}\n\nget my_index/_mapping\n\ndelete my_index\nput my_index\n{\n  "mappings": {\n    "dynamic_templates": [\n      {\n        "strings_as_boolean": {\n          "match_mapping_type": "string",\n          "match": "is*",\n          "mapping": {\n            "type": "boolean"\n          }\n        }\n      },\n      {\n        "strings_as_keywords": {\n          "match_mapping_type": "string",\n          "mapping": {\n            "type": "keyword"\n          }\n        }\n      }\n    ]\n  }\n}\nget my_index/_mapping\n\ndelete my_index\n#结合路径\nput my_index\n{\n  "mappings": {\n    "dynamic_templates": [\n      {\n        "full_name": {\n          "path_match": "name.*",\n          "path_unmatch": "*.middle",\n          "mapping": {\n            "type": "text",\n            "copy_to": "full_name"\n          }\n        }\n      }\n    ]\n  }\n}\nget my_index/_mapping\n\n\nput my_index/_doc/1\n{\n  "name": {\n    "first": "john",\n    "middle": "winston",\n    "last": "lennon"\n  }\n}\n\nget my_index/_search?q=full_name:john\ndelete my_index\n\n\n\n# 类型（type）\n\ntype 是一个逻辑意义上的分类或者叫分区，允许在同一索引中建立多个 type。本质是相当于一个过滤条件，高版本将会废弃 type 概念。\n\n> 6.0.0 版本及之后，废弃 type\n\n\n# 文档（document）\n\nelasticsearch 是面向文档的，文档是所有可搜索数据的最小单位。\n\nelasticsearch 使用 json 作为文档的序列化格式。\n\n在索引/类型中，可以根据需要存储任意数量的文档。\n\n每个文档都有一个 unique id\n\n * 用户可以自己指定\n * 或通过 elasticsearch 自动生成\n\n# 文档的元数据\n\n一个文档不仅仅包含它的数据 ，也包含元数据 —— 有关文档的信息。\n\n * _index：文档在哪存放\n * _type：文档表示的对象类别\n * _id：文档唯一标识\n * _source：文档的原始 json 数据\n * _all：整合所有字段内容到该字段，已被废除\n * _version：文档的版本信息\n * _score：相关性打分\n\n示例：\n\n{\n  "_index": "megacorp",\n  "_type": "employee",\n  "_id": "1",\n  "_version": 1,\n  "found": true,\n  "_source": {\n    "first_name": "john",\n    "last_name": "smith",\n    "age": 25,\n    "about": "i love to go rock climbing",\n    "interests": ["sports", "music"]\n  }\n}\n\n\n\n# 节点（node）\n\n# 节点简介\n\n一个运行中的 elasticsearch 实例称为一个节点。\n\nelasticsearch 实例本质上是一个 java 进程。一台机器上可以运行多个 elasticsearch 进程，但是生产环境建议一台机器上只运行一个 elasticsearch 进程\n\n每个节点都有名字，通过配置文件配置，或启动时通过 -e node.name=node1 指定。\n\n每个节点在启动后，会分配一个 uid，保存在 data 目录下。\n\n# 节点类型\n\n * 主节点（master node）：每个节点都保存了集群的状态，只有 master 节点才能修改集群的状态信息（保证数据一致性）。集群状态，维护了以下信息：\n   * 所有的节点信息\n   * 所有的索引和其相关的 mapping 和 setting 信息\n   * 分片的路由信息\n * 候选节点（master eligible node）：master eligible 节点可以参加选主流程。第一个启动的节点，会将自己选举为 mater 节点。\n   * 每个节点启动后，默认为 master eligible 节点，可以通过配置 node.master: false 禁止\n * 数据节点（data node）：负责保存分片数据。\n * 协调节点（coordinating node）：负责接收客户端的请求，将请求分发到合适的接地那，最终把结果汇集到一起。每个 elasticsearch 节点默认都是协调节点（coordinating node）。\n * 冷/热节点（warm/hot node）：针对不同硬件配置的数据节点（data node），用来实现 hot & warm 架构，降低集群部署的成本。\n * 机器学习节点（machine learning node）：负责执行机器学习的 job，用来做异常检测。\n\n# 节点配置\n\n配置参数          默认值    说明\nnode.master   true   是否为主节点\nnode.data     true   是否为数据节点\nnode.ingest   true   \nnode.ml       true   是否为机器学习节点（需要开启 x-pack）\n\n> 建议\n> \n> 开发环境中一个节点可以承担多种角色。但是，在生产环境中，节点应该设置为单一角色。\n\n\n# 集群（cluster）\n\n# 集群简介\n\n拥有相同 cluster.name 配置的 elasticsearch 节点组成一个集群。 cluster.name 默认名为 elasticsearch，可以通过配置文件修改，或启动时通过 -e cluster.name=xxx 指定。\n\n当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n\n当一个节点被选举成为主节点时，它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量增加，它也不会成为瓶颈。 任何节点都可以成为主节点。\n\n作为用户，我们可以将请求发送到集群中的任何节点 ，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 elasticsearch 对这一切的管理都是透明的。\n\n# 集群健康\n\nelasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red 。\n\n在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容：\n\n{\n  "cluster_name" : "elasticsearch",\n  "status" : "green",\n  "timed_out" : false,\n  "number_of_nodes" : 1,\n  "number_of_data_nodes" : 1,\n  "active_primary_shards" : 5,\n  "active_shards" : 5,\n  "relocating_shards" : 0,\n  "initializing_shards" : 0,\n  "unassigned_shards" : 0,\n  "delayed_unassigned_shards" : 0,\n  "number_of_pending_tasks" : 0,\n  "number_of_in_flight_fetch" : 0,\n  "task_max_waiting_in_queue_millis" : 0,\n  "active_shards_percent_as_number" : 100.0\n}\n\n\nstatus 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：\n\n * green：所有的主分片和副本分片都正常运行。\n * yellow：所有的主分片都正常运行，但不是所有的副本分片都正常运行。\n * red：有主分片没能正常运行。\n\n\n# 分片（shards）\n\n# 分片简介\n\n索引实际上是指向一个或者多个物理分片的逻辑命名空间 。\n\n一个分片是一个底层的工作单元 ，它仅保存了全部数据中的一部分。一个分片可以视为一个 lucene 的实例，并且它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。\n\nelasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。\n\n# 主分片和副分片\n\n分片分为主分片（primary shard）和副分片（replica shard）。\n\n主分片：用于解决数据水平扩展的问题。通过主分片，可以将数据分布到集群内不同节点上。\n\n * 索引内任意一个文档都归属于一个主分片。\n * 主分片数在索引创建时指定，后序不允许修改，除非 reindex\n\n副分片（replica shard）：用于解决数据高可用的问题。副分片是主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n\n * 副分片数可以动态调整\n * 增加副本数，还可以在一定程度上提高服务的可用性（读取的吞吐）\n\n对于生产环境中分片的设定，需要提前做好容量规划\n\n分片数过小\n\n * 无法水平扩展\n * 单个分片的数量太大，导致数据重新分配耗时\n\n分片数过大\n\n * 影响搜索结果的相关性打分，影响统计结果的准确性\n * 单节点上过多的分片，会导致资源浪费，同时也会影响性能\n\n\n# 副本（replicas）\n\n副本主要是针对主分片（shards）的复制，elasticsearch 中主分片可以拥有 0 个或多个的副本。\n\n副本分片的主要目的就是为了故障转移。\n\n分片副本很重要，主要有两个原因：\n\n * 它在分片或节点发生故障时提供高可用性。因此，副本分片永远不会在与其复制的主分片相同的节点；\n * 副本分片也可以接受搜索的请求，可以并行搜索，从而提高系统的吞吐量。\n\n> 每个 elasticsearch 分片都是 lucene 索引。单个 lucene 索引中可以包含最大数量的文档。截止 lucene-5843，限制是 2,147,483,519（= integer.max_value - 128）文档。您可以使用_cat/shardsapi 监控分片大小。\n\n\n# 参考资料\n\n * elasticsearch 官网\n * elasticsearch 简介',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 索引",frontmatter:{title:"Elasticsearch 索引",date:"2022-02-22T21:01:01.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","索引"],permalink:"/pages/293175/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/04.Elasticsearch%E7%B4%A2%E5%BC%95.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/04.Elasticsearch索引.md",key:"v-0f15ab6e",path:"/pages/293175/",headers:[{level:2,title:"索引管理操作",slug:"索引管理操作",normalizedTitle:"索引管理操作",charIndex:23},{level:3,title:"索引删除",slug:"索引删除",normalizedTitle:"索引删除",charIndex:99},{level:3,title:"索引别名",slug:"索引别名",normalizedTitle:"索引别名",charIndex:82},{level:2,title:"Settings 详解",slug:"settings-详解",normalizedTitle:"settings 详解",charIndex:996},{level:3,title:"固定属性",slug:"固定属性",normalizedTitle:"固定属性",charIndex:2731},{level:3,title:"索引静态配置",slug:"索引静态配置",normalizedTitle:"索引静态配置",charIndex:2838},{level:3,title:"索引动态配置",slug:"索引动态配置",normalizedTitle:"索引动态配置",charIndex:3574},{level:2,title:"Mapping 详解",slug:"mapping-详解",normalizedTitle:"mapping 详解",charIndex:3788},{level:3,title:"映射分类",slug:"映射分类",normalizedTitle:"映射分类",charIndex:4064},{level:4,title:"静态映射",slug:"静态映射",normalizedTitle:"静态映射",charIndex:4093},{level:4,title:"动态映射",slug:"动态映射",normalizedTitle:"动态映射",charIndex:4098},{level:3,title:"基础类型",slug:"基础类型",normalizedTitle:"基础类型",charIndex:5937},{level:3,title:"复杂类型",slug:"复杂类型",normalizedTitle:"复杂类型",charIndex:6114},{level:3,title:"特殊类型",slug:"特殊类型",normalizedTitle:"特殊类型",charIndex:6176},{level:3,title:"Mapping 属性",slug:"mapping-属性",normalizedTitle:"mapping 属性",charIndex:6330},{level:2,title:"索引查询",slug:"索引查询",normalizedTitle:"索引查询",charIndex:7561},{level:3,title:"多个 index、多个 type 查询",slug:"多个-index、多个-type-查询",normalizedTitle:"多个 index、多个 type 查询",charIndex:7570},{level:3,title:"URI 搜索",slug:"uri-搜索",normalizedTitle:"uri 搜索",charIndex:8028},{level:3,title:"查询流程",slug:"查询流程",normalizedTitle:"查询流程",charIndex:10336},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10604}],headersStr:"索引管理操作 索引删除 索引别名 Settings 详解 固定属性 索引静态配置 索引动态配置 Mapping 详解 映射分类 静态映射 动态映射 基础类型 复杂类型 特殊类型 Mapping 属性 索引查询 多个 index、多个 type 查询 URI 搜索 查询流程 参考资料",content:'# Elasticsearch 索引\n\n\n# 索引管理操作\n\nElasticsearch 索引管理主要包括如何进行索引的创建、索引的删除、副本的更新、索引读写权限、索引别名的配置等等内容。\n\n\n# 索引删除\n\nES 索引删除操作向 ES 集群的 http 接口发送指定索引的 delete http 请求即可，可以通过 curl 命令，具体如下：\n\ncurl -X DELETE http://{es_host}:{es_http_port}/{index}\n\n\n如果删除成功，它会返回如下信息，具体示例如下：\n\ncurl -X DELETE http://10.10.10.66:9200/my_index?pretty\n\n\n为了返回的信息便于读取，增加了 pretty 参数：\n\n{\n  "acknowledged" : true\n}\n\n\n\n# 索引别名\n\nES 的索引别名就是给一个索引或者多个索引起的另一个名字，典型的应用场景是针对索引使用的平滑切换。\n\n首先，创建索引 my_index，然后将别名 my_alias 指向它，示例如下：\n\nPUT /my_index\nPUT /my_index/_alias/my_alias\n\n\n也可以通过如下形式：\n\nPOST /_aliases\n{\n  "actions": [\n    { "add": { "index": "my_index", "alias": "my_alias" }}\n  ]\n}\n\n\n也可以在一次请求中增加别名和移除别名混合使用：\n\nPOST /_aliases\n{\n  "actions": [\n    { "remove": { "index": "my_index", "alias": "my_alias" }}\n    { "add": { "index": "my_index_v2", "alias": "my_alias" }}\n  ]\n}\n\n\n> 需要注意的是，如果别名与索引是一对一的，使用别名索引文档或者查询文档是可以的，但是如果别名和索引是一对多的，使用别名会发生错误，因为 ES 不知道把文档写入哪个索引中去或者从哪个索引中读取文档。\n\nES 索引别名有个典型的应用场景是平滑切换，更多细节可以查看 Elasticsearch（ES）索引零停机（无需重启）无缝平滑切换的方法。\n\n\n# Settings 详解\n\nElasticsearch 索引的配置项主要分为静态配置属性和动态配置属性，静态配置属性是索引创建后不能修改，而动态配置属性则可以随时修改。\n\nES 索引设置的 api 为 _settings，完整的示例如下：\n\nPUT /my_index\n{\n  "settings": {\n    "index": {\n      "number_of_shards": "1",\n      "number_of_replicas": "1",\n      "refresh_interval": "60s",\n      "analysis": {\n        "filter": {\n          "tsconvert": {\n            "type": "stconvert",\n            "convert_type": "t2s",\n            "delimiter": ","\n          },\n          "synonym": {\n            "type": "synonym",\n            "synonyms_path": "analysis/synonyms.txt"\n          }\n        },\n        "analyzer": {\n          "ik_max_word_synonym": {\n            "filter": [\n              "synonym",\n              "tsconvert",\n              "standard",\n              "lowercase",\n              "stop"\n            ],\n            "tokenizer": "ik_max_word"\n          },\n          "ik_smart_synonym": {\n            "filter": [\n              "synonym",\n              "standard",\n              "lowercase",\n              "stop"\n            ],\n            "tokenizer": "ik_smart"\n          }\n        },\n\t\t\t"mapping": {\n\t\t\t\t"coerce": "false",\n\t\t\t\t"ignore_malformed": "false"\n\t\t\t},\n\t\t\t"indexing": {\n\t\t\t\t"slowlog": {\n\t\t\t\t\t"threshold": {\n\t\t\t\t\t\t"index": {\n\t\t\t\t\t\t\t"warn": "2s",\n\t\t\t\t\t\t\t"info": "1s"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t"provided_name": "hospital_202101070533",\n\t\t\t"query": {\n\t\t\t\t"default_field": "timestamp",\n\t\t\t\t"parse": {\n\t\t\t\t\t"allow_unmapped_fields": "false"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"requests": {\n\t\t\t\t"cache": {\n\t\t\t\t\t"enable": "true"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"search": {\n\t\t\t\t"slowlog": {\n\t\t\t\t\t"threshold": {\n\t\t\t\t\t\t"fetch": {\n\t\t\t\t\t\t\t"warn": "1s",\n\t\t\t\t\t\t\t"info": "200ms"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t"query": {\n\t\t\t\t\t\t\t"warn": "1s",\n\t\t\t\t\t\t\t"info": "500ms"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 固定属性\n\n * index.creation_date：顾名思义索引的创建时间戳。\n * index.uuid：索引的 uuid 信息。\n * index.version.created：索引的版本号。\n\n\n# 索引静态配置\n\n * index.number_of_shards：索引的主分片数，默认值是 5。这个配置在索引创建后不能修改；在 es 层面，可以通过 es.index.max_number_of_shards 属性设置索引最大的分片数，默认为 1024。\n * index.codec：数据存储的压缩算法，默认值为 LZ4，可选择值还有 best_compression，它比 LZ4 可以获得更好的压缩比（即占据较小的磁盘空间，但存储性能比 LZ4 低）。\n * index.routing_partition_size：路由分区数，如果设置了该参数，其路由算法为：( hash(_routing) + hash(_id) % index.routing_parttion_size ) % number_of_shards。如果该值不设置，则路由算法为 hash(_routing) % number_of_shardings，_routing 默认值为 _id。\n\n静态配置里，有重要的部分是配置分析器（config analyzers）。\n\n * index.analysis\n   \n   ：分析器最外层的配置项，内部主要分为 char_filter、tokenizer、filter 和 analyzer。\n   \n   * char_filter：定义新的字符过滤器件。\n   * tokenizer：定义新的分词器。\n   * filter：定义新的 token filter，如同义词 filter。\n   * analyzer：配置新的分析器，一般是 char_filter、tokenizer 和一些 token filter 的组合。\n\n\n# 索引动态配置\n\n * index.number_of_replicas：索引主分片的副本数，默认值是 1，该值必须大于等于 0，这个配置可以随时修改。\n * index.refresh_interval：执行新索引数据的刷新操作频率，该操作使对索引的最新更改对搜索可见，默认为 1s。也可以设置为 -1 以禁用刷新。更详细信息参考 Elasticsearch 动态修改 refresh_interval 刷新间隔设置。\n\n\n# Mapping 详解\n\n在 Elasticsearch 中，Mapping（映射），用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、字段的权重、分词器等属性，就如同在关系型数据库中创建数据表时会设置字段的类型。\n\nMapping 会把 json 文档映射成 Lucene 所需要的扁平格式\n\n一个 Mapping 属于一个索引的 Type\n\n * 每个文档都属于一个 Type\n * 一个 Type 有一个 Mapping 定义\n * 7.0 开始，不需要在 Mapping 定义中指定 type 信息\n\n\n# 映射分类\n\n在 Elasticsearch 中，映射可分为静态映射和动态映射。在关系型数据库中写入数据之前首先要建表，在建表语句中声明字段的属性，在 Elasticsearch 中，则不必如此，Elasticsearch 最重要的功能之一就是让你尽可能快地开始探索数据，文档写入 Elasticsearch 中，它会根据字段的类型自动识别，这种机制称为动态映射，而静态映射则是写入数据之前对字段的属性进行手工设置。\n\n# 静态映射\n\n静态映射是在创建索引时手工指定索引映射。静态映射和 SQL 中在建表语句中指定字段属性类似。相比动态映射，通过静态映射可以添加更详细、更精准的配置信息。\n\n如何定义一个 Mapping\n\nPUT /books\n{\n    "mappings": {\n        "type_one": { ... any mappings ... },\n        "type_two": { ... any mappings ... },\n        ...\n    }\n}\n\n\n# 动态映射\n\n动态映射是一种偷懒的方式，可直接创建索引并写入文档，文档中字段的类型是 Elasticsearch 自动识别的，不需要在创建索引的时候设置字段的类型。在实际项目中，如果遇到的业务在导入数据之前不确定有哪些字段，也不清楚字段的类型是什么，使用动态映射非常合适。当 Elasticsearch 在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动把该字段添加到映射中，根据字段的取值自动推测字段类型的规则见下表：\n\nJSON 格式的数据      自动推测的字段类型\nnull            没有字段被添加\ntrue or false   boolean 类型\n浮点类型数字          float 类型\n数字              long 类型\nJSON 对象         object 类型\n数组              由数组中第一个非空值决定\nstring          有可能是 date 类型（若开启日期检测）、double 或 long 类型、text 类型、keyword 类型\n\n下面举一个例子认识动态 mapping，在 Elasticsearch 中创建一个新的索引并查看它的 mapping，命令如下：\n\nPUT books\nGET books/_mapping\n\n\n此时 books 索引的 mapping 是空的，返回结果如下：\n\n{\n  "books": {\n    "mappings": {}\n  }\n}\n\n\n再往 books 索引中写入一条文档，命令如下：\n\nPUT books/it/1\n{\n\t"id": 1,\n\t"publish_date": "2019-11-10",\n\t"name": "master Elasticsearch"\n}\n\n\n文档写入完成之后，再次查看 mapping，返回结果如下：\n\n{\n  "books": {\n    "mappings": {\n      "properties": {\n        "id": {\n          "type": "long"\n        },\n        "name": {\n          "type": "text",\n          "fields": {\n            "keyword": {\n              "type": "keyword",\n              "ignore_above": 256\n            }\n          }\n        },\n        "publish_date": {\n          "type": "date"\n        }\n      }\n    }\n  }\n}\n\n\n使用动态 mapping 要结合实际业务需求来综合考虑，如果将 Elasticsearch 当作主要的数据存储使用，并且希望出现未知字段时抛出异常来提醒你注意这一问题，那么开启动态 mapping 并不适用。在 mapping 中可以通过 dynamic 设置来控制是否自动新增字段，接受以下参数：\n\n * true：默认值为 true，自动添加字段。\n * false：忽略新的字段。\n * strict：严格模式，发现新的字段抛出异常。\n\n\n# 基础类型\n\n类型      关键字\n字符串类型   string、text、keyword\n数字类型    long、integer、short、byte、double、float、half_float、scaled_float\n日期类型    date\n布尔类型    boolean\n二进制类型   binary\n范围类型    range\n\n\n# 复杂类型\n\n类型     关键字\n数组类型   array\n对象类型   object\n嵌套类型   nested\n\n\n# 特殊类型\n\n类型       关键字\n地理类型     geo_point\n地理图形类型   geo_shape\nIP 类型    ip\n范围类型     completion\n令牌计数类型   token_count\n附件类型     attachment\n抽取类型     percolator\n\n\n# Mapping 属性\n\nElasticsearch 的 mapping 中的字段属性非常多，具体如下表格：\n\n| 属性名 | 描述 | | :- | :- | | | type | 字段类型，常用的有 text、integer 等等。 | | index | 当前字段是否被作为索引。可选值为 true，默认为 true。 | | store | 是否存储指定字段，可选值为 true | false，设置 true 意味着需要开辟单独的存储空间为这个字段做存储，而且这个存储是独立于 _source 的存储的。 | | norms | 是否使用归一化因子，可选值为 true | false，不需要对某字段进行打分排序时，可禁用它，节省空间；type 为 text 时，默认为 true；而 type 为 keyword 时，默认为 false。 | | index_options | 索引选项控制添加到倒排索引（Inverted Index）的信息，这些信息用于搜索（Search）和高亮显示：docs：只索引文档编号(Doc Number)；freqs：索引文档编号和词频率（term frequency）；positions：索引文档编号，词频率和词位置（序号）；offsets：索引文档编号，词频率，词偏移量（开始和结束位置）和词位置（序号）。默认情况下，被分析的字符串（analyzed string）字段使用 positions，其他字段默认使用 docs。此外，需要注意的是 index_option 是 elasticsearch 特有的设置属性；临近搜索和短语查询时，index_option 必须设置为 offsets，同时高亮也可使用 postings highlighter。 | | term_vector | 索引选项控制词向量相关信息：no：默认值，表示不存储词向量相关信息；yes：只存储词向量信息；with_positions：存储词项和词项位置；with_offsets：存储词项和字符偏移位置；with_positions_offsets：存储词项、词项位置、字符偏移位置。term_vector 是 lucene 层面的索引设置。 | | similarity | 指定文档相似度算法（也可以叫评分模型）：BM25：ES 5 之后的默认设置。 | | copy_to | 复制到自定义 _all 字段，值是数组形式，即表明可以指定多个自定义的字段。 | | analyzer | 指定索引和搜索时的分析器，如果同时指定 search_analyzer 则搜索时会优先使用 search_analyzer。 | | search_analyzer | 指定搜索时的分析器，搜索时的优先级最高。 | | null_value | 用于需要对 Null 值实现搜索的场景，只有 Keyword 类型支持此配置。 |\n\n\n# 索引查询\n\n\n# 多个 index、多个 type 查询\n\nElasticsearch 的搜索 api 支持一个索引（index）的多个类型（type）查询以及**多个索引（index）**的查询。\n\n例如，我们可以搜索 twitter 索引下面所有匹配条件的所有类型中文档，如下：\n\nGET /twitter/_search?q=user:shay\n\n\n我们也可以搜索一个索引下面指定多个 type 下匹配条件的文档，如下：\n\nGET /twitter/tweet,user/_search?q=user:banon\n\n\n我们也可以搜索多个索引下匹配条件的文档，如下：\n\nGET /twitter,elasticsearch/_search?q=tag:wow\n\n\n此外我们也可以搜索所有索引下匹配条件的文档，用_all 表示所有索引，如下：\n\nGET /_all/_search?q=tag:wow\n\n\n甚至我们可以搜索所有索引及所有 type 下匹配条件的文档，如下：\n\nGET /_search?q=tag:wow\n\n\n\n# URI 搜索\n\nElasticsearch 支持用 uri 搜索，可用 get 请求里面拼接相关的参数，并用 curl 相关的命令就可以进行测试。\n\n如下有一个示例：\n\nGET twitter/_search?q=user:kimchy\n\n\n如下是上一个请求的相应实体：\n\n{\n  "timed_out": false,\n  "took": 62,\n  "_shards": {\n    "total": 1,\n    "successful": 1,\n    "skipped": 0,\n    "failed": 0\n  },\n  "hits": {\n    "total": 1,\n    "max_score": 1.3862944,\n    "hits": [\n      {\n        "_index": "twitter",\n        "_type": "_doc",\n        "_id": "0",\n        "_score": 1.3862944,\n        "_source": {\n          "user": "kimchy",\n          "date": "2009-11-15T14:12:12",\n          "message": "trying out Elasticsearch",\n          "likes": 0\n        }\n      }\n    ]\n  }\n}\n\n\nURI 中允许的参数：\n\n名称                             描述\nq                              查询字符串，映射到 query_string 查询\ndf                             在查询中未定义字段前缀时使用的默认字段\nanalyzer                       查询字符串时指定的分词器\nanalyze_wildcard               是否允许通配符和前缀查询，默认设置为 false\nbatched_reduce_size            应在协调节点上一次减少的分片结果数。如果请求中潜在的分片数量很大，则应将此值用作保护机制，以减少每个搜索请求的内存开销\ndefault_operator               默认使用的匹配运算符，可以是AND或者OR，默认是OR\nlenient                        如果设置为 true，将会忽略由于格式化引起的问题（如向数据字段提供文本），默认为 false\nexplain                        对于每个 hit，包含了具体如何计算得分的解释\n_source                        请求文档内容的参数，默认 true；设置 false 的话，不返回_source\n                               字段，可以使用**_source_include和_source_exclude**参数分别指定返回字段和不返回的字段\nstored_fields                  指定每个匹配返回的文档中的存储字段，多个用逗号分隔。不指定任何值将导致没有字段返回\nsort                           排序方式，可以是fieldName、fieldName:asc或者fieldName:desc的形式。fieldName\n                               可以是文档中的实际字段，也可以是诸如_score 字段，其表示基于分数的排序。此外可以指定多个 sort\n                               参数（顺序很重要）\ntrack_scores                   当排序时，若设置 true，返回每个命中文档的分数\ntrack_total_hits               是否返回匹配条件命中的总文档数，默认为 true\ntimeout                        设置搜索的超时时间，默认无超时时间\nterminate_after                在达到查询终止条件之前，指定每个分片收集的最大文档数。如果设置，则在响应中多了一个 terminated_early\n                               的布尔字段，以指示查询执行是否实际上已终止。默认为 no terminate_after\nfrom                           从第几条（索引以 0 开始）结果开始返回，默认为 0\nsize                           返回命中的文档数，默认为 10\nsearch_type                    搜索的方式，可以是dfs_query_then_fetch或query_then_fetch。默认为query_then_fetch\nallow_partial_search_results   是否可以返回部分结果。如设置为 false，表示如果请求产生部分结果，则设置为返回整体故障；默认为\n                               true，表示允许请求在超时或部分失败的情况下获得部分结果\n\n\n# 查询流程\n\n在 Elasticsearch 中，查询是一个比较复杂的执行模式，因为我们不知道那些 document 会被匹配到，任何一个 shard 上都有可能，所以一个 search 请求必须查询一个索引或多个索引里面的所有 shard 才能完整的查询到我们想要的结果。\n\n找到所有匹配的结果是查询的第一步，来自多个 shard 上的数据集在分页返回到客户端之前会被合并到一个排序后的 list 列表，由于需要经过一步取 top N 的操作，所以 search 需要进过两个阶段才能完成，分别是 query 和 fetch。\n\n\n# 参考资料\n\n * Elasticsearch 官网\n * Elasticsearch 索引映射类型及 mapping 属性详解',normalizedContent:'# elasticsearch 索引\n\n\n# 索引管理操作\n\nelasticsearch 索引管理主要包括如何进行索引的创建、索引的删除、副本的更新、索引读写权限、索引别名的配置等等内容。\n\n\n# 索引删除\n\nes 索引删除操作向 es 集群的 http 接口发送指定索引的 delete http 请求即可，可以通过 curl 命令，具体如下：\n\ncurl -x delete http://{es_host}:{es_http_port}/{index}\n\n\n如果删除成功，它会返回如下信息，具体示例如下：\n\ncurl -x delete http://10.10.10.66:9200/my_index?pretty\n\n\n为了返回的信息便于读取，增加了 pretty 参数：\n\n{\n  "acknowledged" : true\n}\n\n\n\n# 索引别名\n\nes 的索引别名就是给一个索引或者多个索引起的另一个名字，典型的应用场景是针对索引使用的平滑切换。\n\n首先，创建索引 my_index，然后将别名 my_alias 指向它，示例如下：\n\nput /my_index\nput /my_index/_alias/my_alias\n\n\n也可以通过如下形式：\n\npost /_aliases\n{\n  "actions": [\n    { "add": { "index": "my_index", "alias": "my_alias" }}\n  ]\n}\n\n\n也可以在一次请求中增加别名和移除别名混合使用：\n\npost /_aliases\n{\n  "actions": [\n    { "remove": { "index": "my_index", "alias": "my_alias" }}\n    { "add": { "index": "my_index_v2", "alias": "my_alias" }}\n  ]\n}\n\n\n> 需要注意的是，如果别名与索引是一对一的，使用别名索引文档或者查询文档是可以的，但是如果别名和索引是一对多的，使用别名会发生错误，因为 es 不知道把文档写入哪个索引中去或者从哪个索引中读取文档。\n\nes 索引别名有个典型的应用场景是平滑切换，更多细节可以查看 elasticsearch（es）索引零停机（无需重启）无缝平滑切换的方法。\n\n\n# settings 详解\n\nelasticsearch 索引的配置项主要分为静态配置属性和动态配置属性，静态配置属性是索引创建后不能修改，而动态配置属性则可以随时修改。\n\nes 索引设置的 api 为 _settings，完整的示例如下：\n\nput /my_index\n{\n  "settings": {\n    "index": {\n      "number_of_shards": "1",\n      "number_of_replicas": "1",\n      "refresh_interval": "60s",\n      "analysis": {\n        "filter": {\n          "tsconvert": {\n            "type": "stconvert",\n            "convert_type": "t2s",\n            "delimiter": ","\n          },\n          "synonym": {\n            "type": "synonym",\n            "synonyms_path": "analysis/synonyms.txt"\n          }\n        },\n        "analyzer": {\n          "ik_max_word_synonym": {\n            "filter": [\n              "synonym",\n              "tsconvert",\n              "standard",\n              "lowercase",\n              "stop"\n            ],\n            "tokenizer": "ik_max_word"\n          },\n          "ik_smart_synonym": {\n            "filter": [\n              "synonym",\n              "standard",\n              "lowercase",\n              "stop"\n            ],\n            "tokenizer": "ik_smart"\n          }\n        },\n\t\t\t"mapping": {\n\t\t\t\t"coerce": "false",\n\t\t\t\t"ignore_malformed": "false"\n\t\t\t},\n\t\t\t"indexing": {\n\t\t\t\t"slowlog": {\n\t\t\t\t\t"threshold": {\n\t\t\t\t\t\t"index": {\n\t\t\t\t\t\t\t"warn": "2s",\n\t\t\t\t\t\t\t"info": "1s"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t"provided_name": "hospital_202101070533",\n\t\t\t"query": {\n\t\t\t\t"default_field": "timestamp",\n\t\t\t\t"parse": {\n\t\t\t\t\t"allow_unmapped_fields": "false"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"requests": {\n\t\t\t\t"cache": {\n\t\t\t\t\t"enable": "true"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"search": {\n\t\t\t\t"slowlog": {\n\t\t\t\t\t"threshold": {\n\t\t\t\t\t\t"fetch": {\n\t\t\t\t\t\t\t"warn": "1s",\n\t\t\t\t\t\t\t"info": "200ms"\n\t\t\t\t\t\t},\n\t\t\t\t\t\t"query": {\n\t\t\t\t\t\t\t"warn": "1s",\n\t\t\t\t\t\t\t"info": "500ms"\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 固定属性\n\n * index.creation_date：顾名思义索引的创建时间戳。\n * index.uuid：索引的 uuid 信息。\n * index.version.created：索引的版本号。\n\n\n# 索引静态配置\n\n * index.number_of_shards：索引的主分片数，默认值是 5。这个配置在索引创建后不能修改；在 es 层面，可以通过 es.index.max_number_of_shards 属性设置索引最大的分片数，默认为 1024。\n * index.codec：数据存储的压缩算法，默认值为 lz4，可选择值还有 best_compression，它比 lz4 可以获得更好的压缩比（即占据较小的磁盘空间，但存储性能比 lz4 低）。\n * index.routing_partition_size：路由分区数，如果设置了该参数，其路由算法为：( hash(_routing) + hash(_id) % index.routing_parttion_size ) % number_of_shards。如果该值不设置，则路由算法为 hash(_routing) % number_of_shardings，_routing 默认值为 _id。\n\n静态配置里，有重要的部分是配置分析器（config analyzers）。\n\n * index.analysis\n   \n   ：分析器最外层的配置项，内部主要分为 char_filter、tokenizer、filter 和 analyzer。\n   \n   * char_filter：定义新的字符过滤器件。\n   * tokenizer：定义新的分词器。\n   * filter：定义新的 token filter，如同义词 filter。\n   * analyzer：配置新的分析器，一般是 char_filter、tokenizer 和一些 token filter 的组合。\n\n\n# 索引动态配置\n\n * index.number_of_replicas：索引主分片的副本数，默认值是 1，该值必须大于等于 0，这个配置可以随时修改。\n * index.refresh_interval：执行新索引数据的刷新操作频率，该操作使对索引的最新更改对搜索可见，默认为 1s。也可以设置为 -1 以禁用刷新。更详细信息参考 elasticsearch 动态修改 refresh_interval 刷新间隔设置。\n\n\n# mapping 详解\n\n在 elasticsearch 中，mapping（映射），用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、字段的权重、分词器等属性，就如同在关系型数据库中创建数据表时会设置字段的类型。\n\nmapping 会把 json 文档映射成 lucene 所需要的扁平格式\n\n一个 mapping 属于一个索引的 type\n\n * 每个文档都属于一个 type\n * 一个 type 有一个 mapping 定义\n * 7.0 开始，不需要在 mapping 定义中指定 type 信息\n\n\n# 映射分类\n\n在 elasticsearch 中，映射可分为静态映射和动态映射。在关系型数据库中写入数据之前首先要建表，在建表语句中声明字段的属性，在 elasticsearch 中，则不必如此，elasticsearch 最重要的功能之一就是让你尽可能快地开始探索数据，文档写入 elasticsearch 中，它会根据字段的类型自动识别，这种机制称为动态映射，而静态映射则是写入数据之前对字段的属性进行手工设置。\n\n# 静态映射\n\n静态映射是在创建索引时手工指定索引映射。静态映射和 sql 中在建表语句中指定字段属性类似。相比动态映射，通过静态映射可以添加更详细、更精准的配置信息。\n\n如何定义一个 mapping\n\nput /books\n{\n    "mappings": {\n        "type_one": { ... any mappings ... },\n        "type_two": { ... any mappings ... },\n        ...\n    }\n}\n\n\n# 动态映射\n\n动态映射是一种偷懒的方式，可直接创建索引并写入文档，文档中字段的类型是 elasticsearch 自动识别的，不需要在创建索引的时候设置字段的类型。在实际项目中，如果遇到的业务在导入数据之前不确定有哪些字段，也不清楚字段的类型是什么，使用动态映射非常合适。当 elasticsearch 在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动把该字段添加到映射中，根据字段的取值自动推测字段类型的规则见下表：\n\njson 格式的数据      自动推测的字段类型\nnull            没有字段被添加\ntrue or false   boolean 类型\n浮点类型数字          float 类型\n数字              long 类型\njson 对象         object 类型\n数组              由数组中第一个非空值决定\nstring          有可能是 date 类型（若开启日期检测）、double 或 long 类型、text 类型、keyword 类型\n\n下面举一个例子认识动态 mapping，在 elasticsearch 中创建一个新的索引并查看它的 mapping，命令如下：\n\nput books\nget books/_mapping\n\n\n此时 books 索引的 mapping 是空的，返回结果如下：\n\n{\n  "books": {\n    "mappings": {}\n  }\n}\n\n\n再往 books 索引中写入一条文档，命令如下：\n\nput books/it/1\n{\n\t"id": 1,\n\t"publish_date": "2019-11-10",\n\t"name": "master elasticsearch"\n}\n\n\n文档写入完成之后，再次查看 mapping，返回结果如下：\n\n{\n  "books": {\n    "mappings": {\n      "properties": {\n        "id": {\n          "type": "long"\n        },\n        "name": {\n          "type": "text",\n          "fields": {\n            "keyword": {\n              "type": "keyword",\n              "ignore_above": 256\n            }\n          }\n        },\n        "publish_date": {\n          "type": "date"\n        }\n      }\n    }\n  }\n}\n\n\n使用动态 mapping 要结合实际业务需求来综合考虑，如果将 elasticsearch 当作主要的数据存储使用，并且希望出现未知字段时抛出异常来提醒你注意这一问题，那么开启动态 mapping 并不适用。在 mapping 中可以通过 dynamic 设置来控制是否自动新增字段，接受以下参数：\n\n * true：默认值为 true，自动添加字段。\n * false：忽略新的字段。\n * strict：严格模式，发现新的字段抛出异常。\n\n\n# 基础类型\n\n类型      关键字\n字符串类型   string、text、keyword\n数字类型    long、integer、short、byte、double、float、half_float、scaled_float\n日期类型    date\n布尔类型    boolean\n二进制类型   binary\n范围类型    range\n\n\n# 复杂类型\n\n类型     关键字\n数组类型   array\n对象类型   object\n嵌套类型   nested\n\n\n# 特殊类型\n\n类型       关键字\n地理类型     geo_point\n地理图形类型   geo_shape\nip 类型    ip\n范围类型     completion\n令牌计数类型   token_count\n附件类型     attachment\n抽取类型     percolator\n\n\n# mapping 属性\n\nelasticsearch 的 mapping 中的字段属性非常多，具体如下表格：\n\n| 属性名 | 描述 | | :- | :- | | | type | 字段类型，常用的有 text、integer 等等。 | | index | 当前字段是否被作为索引。可选值为 true，默认为 true。 | | store | 是否存储指定字段，可选值为 true | false，设置 true 意味着需要开辟单独的存储空间为这个字段做存储，而且这个存储是独立于 _source 的存储的。 | | norms | 是否使用归一化因子，可选值为 true | false，不需要对某字段进行打分排序时，可禁用它，节省空间；type 为 text 时，默认为 true；而 type 为 keyword 时，默认为 false。 | | index_options | 索引选项控制添加到倒排索引（inverted index）的信息，这些信息用于搜索（search）和高亮显示：docs：只索引文档编号(doc number)；freqs：索引文档编号和词频率（term frequency）；positions：索引文档编号，词频率和词位置（序号）；offsets：索引文档编号，词频率，词偏移量（开始和结束位置）和词位置（序号）。默认情况下，被分析的字符串（analyzed string）字段使用 positions，其他字段默认使用 docs。此外，需要注意的是 index_option 是 elasticsearch 特有的设置属性；临近搜索和短语查询时，index_option 必须设置为 offsets，同时高亮也可使用 postings highlighter。 | | term_vector | 索引选项控制词向量相关信息：no：默认值，表示不存储词向量相关信息；yes：只存储词向量信息；with_positions：存储词项和词项位置；with_offsets：存储词项和字符偏移位置；with_positions_offsets：存储词项、词项位置、字符偏移位置。term_vector 是 lucene 层面的索引设置。 | | similarity | 指定文档相似度算法（也可以叫评分模型）：bm25：es 5 之后的默认设置。 | | copy_to | 复制到自定义 _all 字段，值是数组形式，即表明可以指定多个自定义的字段。 | | analyzer | 指定索引和搜索时的分析器，如果同时指定 search_analyzer 则搜索时会优先使用 search_analyzer。 | | search_analyzer | 指定搜索时的分析器，搜索时的优先级最高。 | | null_value | 用于需要对 null 值实现搜索的场景，只有 keyword 类型支持此配置。 |\n\n\n# 索引查询\n\n\n# 多个 index、多个 type 查询\n\nelasticsearch 的搜索 api 支持一个索引（index）的多个类型（type）查询以及**多个索引（index）**的查询。\n\n例如，我们可以搜索 twitter 索引下面所有匹配条件的所有类型中文档，如下：\n\nget /twitter/_search?q=user:shay\n\n\n我们也可以搜索一个索引下面指定多个 type 下匹配条件的文档，如下：\n\nget /twitter/tweet,user/_search?q=user:banon\n\n\n我们也可以搜索多个索引下匹配条件的文档，如下：\n\nget /twitter,elasticsearch/_search?q=tag:wow\n\n\n此外我们也可以搜索所有索引下匹配条件的文档，用_all 表示所有索引，如下：\n\nget /_all/_search?q=tag:wow\n\n\n甚至我们可以搜索所有索引及所有 type 下匹配条件的文档，如下：\n\nget /_search?q=tag:wow\n\n\n\n# uri 搜索\n\nelasticsearch 支持用 uri 搜索，可用 get 请求里面拼接相关的参数，并用 curl 相关的命令就可以进行测试。\n\n如下有一个示例：\n\nget twitter/_search?q=user:kimchy\n\n\n如下是上一个请求的相应实体：\n\n{\n  "timed_out": false,\n  "took": 62,\n  "_shards": {\n    "total": 1,\n    "successful": 1,\n    "skipped": 0,\n    "failed": 0\n  },\n  "hits": {\n    "total": 1,\n    "max_score": 1.3862944,\n    "hits": [\n      {\n        "_index": "twitter",\n        "_type": "_doc",\n        "_id": "0",\n        "_score": 1.3862944,\n        "_source": {\n          "user": "kimchy",\n          "date": "2009-11-15t14:12:12",\n          "message": "trying out elasticsearch",\n          "likes": 0\n        }\n      }\n    ]\n  }\n}\n\n\nuri 中允许的参数：\n\n名称                             描述\nq                              查询字符串，映射到 query_string 查询\ndf                             在查询中未定义字段前缀时使用的默认字段\nanalyzer                       查询字符串时指定的分词器\nanalyze_wildcard               是否允许通配符和前缀查询，默认设置为 false\nbatched_reduce_size            应在协调节点上一次减少的分片结果数。如果请求中潜在的分片数量很大，则应将此值用作保护机制，以减少每个搜索请求的内存开销\ndefault_operator               默认使用的匹配运算符，可以是and或者or，默认是or\nlenient                        如果设置为 true，将会忽略由于格式化引起的问题（如向数据字段提供文本），默认为 false\nexplain                        对于每个 hit，包含了具体如何计算得分的解释\n_source                        请求文档内容的参数，默认 true；设置 false 的话，不返回_source\n                               字段，可以使用**_source_include和_source_exclude**参数分别指定返回字段和不返回的字段\nstored_fields                  指定每个匹配返回的文档中的存储字段，多个用逗号分隔。不指定任何值将导致没有字段返回\nsort                           排序方式，可以是fieldname、fieldname:asc或者fieldname:desc的形式。fieldname\n                               可以是文档中的实际字段，也可以是诸如_score 字段，其表示基于分数的排序。此外可以指定多个 sort\n                               参数（顺序很重要）\ntrack_scores                   当排序时，若设置 true，返回每个命中文档的分数\ntrack_total_hits               是否返回匹配条件命中的总文档数，默认为 true\ntimeout                        设置搜索的超时时间，默认无超时时间\nterminate_after                在达到查询终止条件之前，指定每个分片收集的最大文档数。如果设置，则在响应中多了一个 terminated_early\n                               的布尔字段，以指示查询执行是否实际上已终止。默认为 no terminate_after\nfrom                           从第几条（索引以 0 开始）结果开始返回，默认为 0\nsize                           返回命中的文档数，默认为 10\nsearch_type                    搜索的方式，可以是dfs_query_then_fetch或query_then_fetch。默认为query_then_fetch\nallow_partial_search_results   是否可以返回部分结果。如设置为 false，表示如果请求产生部分结果，则设置为返回整体故障；默认为\n                               true，表示允许请求在超时或部分失败的情况下获得部分结果\n\n\n# 查询流程\n\n在 elasticsearch 中，查询是一个比较复杂的执行模式，因为我们不知道那些 document 会被匹配到，任何一个 shard 上都有可能，所以一个 search 请求必须查询一个索引或多个索引里面的所有 shard 才能完整的查询到我们想要的结果。\n\n找到所有匹配的结果是查询的第一步，来自多个 shard 上的数据集在分页返回到客户端之前会被合并到一个排序后的 list 列表，由于需要经过一步取 top n 的操作，所以 search 需要进过两个阶段才能完成，分别是 query 和 fetch。\n\n\n# 参考资料\n\n * elasticsearch 官网\n * elasticsearch 索引映射类型及 mapping 属性详解',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 映射",frontmatter:{title:"Elasticsearch 映射",date:"2022-05-16T19:54:24.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","索引"],permalink:"/pages/d1bae4/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/05.Elasticsearch%E6%98%A0%E5%B0%84.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/05.Elasticsearch映射.md",key:"v-a1e79a52",path:"/pages/d1bae4/",headers:[{level:2,title:"映射方式",slug:"映射方式",normalizedTitle:"映射方式",charIndex:433},{level:3,title:"静态映射",slug:"静态映射",normalizedTitle:"静态映射",charIndex:462},{level:3,title:"动态映射",slug:"动态映射",normalizedTitle:"动态映射",charIndex:467},{level:4,title:"动态字段映射",slug:"动态字段映射",normalizedTitle:"动态字段映射",charIndex:1611},{level:4,title:"动态模板",slug:"动态模板",normalizedTitle:"动态模板",charIndex:3687},{level:2,title:"运行时字段",slug:"运行时字段",normalizedTitle:"运行时字段",charIndex:1875},{level:3,title:"运行时字段的优点",slug:"运行时字段的优点",normalizedTitle:"运行时字段的优点",charIndex:4770},{level:2,title:"字段数据类型",slug:"字段数据类型",normalizedTitle:"字段数据类型",charIndex:5126},{level:2,title:"元数据字段",slug:"元数据字段",normalizedTitle:"元数据字段",charIndex:6401},{level:2,title:"映射参数",slug:"映射参数",normalizedTitle:"映射参数",charIndex:6855},{level:2,title:"映射配置",slug:"映射配置",normalizedTitle:"映射配置",charIndex:9217},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9749}],headersStr:"映射方式 静态映射 动态映射 动态字段映射 动态模板 运行时字段 运行时字段的优点 字段数据类型 元数据字段 映射参数 映射配置 参考资料",content:'# Elasticsearch 映射\n\n在 Elasticsearch 中，Mapping（映射），用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、字段的权重、分词器等属性，就如同在关系型数据库中创建数据表时会设置字段的类型。\n\nMapping 会把 JSON 文档映射成 Lucene 所需要的扁平格式\n\n一个 Mapping 属于一个索引的 Type\n\n * 每个文档都属于一个 Type\n * 一个 Type 有一个 Mapping 定义\n * 7.0 开始，不需要在 Mapping 定义中指定 type 信息\n\n每个 document 都是 field 的集合，每个 field 都有自己的数据类型。映射数据时，可以创建一个 mapping，其中包含与 document 相关的 field 列表。映射定义还包括元数据 field，例如 _source ，它自定义如何处理 document 的关联元数据。\n\n\n# 映射方式\n\n在 Elasticsearch 中，映射可分为静态映射和动态映射。在关系型数据库中写入数据之前首先要建表，在建表语句中声明字段的属性，在 Elasticsearch 中，则不必如此，Elasticsearch 最重要的功能之一就是让你尽可能快地开始探索数据，文档写入 Elasticsearch 中，它会根据字段的类型自动识别，这种机制称为动态映射，而静态映射则是写入数据之前对字段的属性进行手工设置。\n\n\n# 静态映射\n\nES 官方将静态映射称为显式映射（Explicit mapping）。静态映射是在创建索引时显示的指定索引映射。静态映射和 SQL 中在建表语句中指定字段属性类似。相比动态映射，通过静态映射可以添加更详细、更精准的配置信息。例如：\n\n * 哪些字符串字段应被视为全文字段。\n * 哪些字段包含数字、日期或地理位置。\n * 日期值的格式。\n * 用于控制动态添加字段的自定义规则。\n\n【示例】创建索引时，显示指定 mapping\n\nPUT /my-index-000001\n{\n  "mappings": {\n    "properties": {\n      "age":    { "type": "integer" },\n      "email":  { "type": "keyword"  },\n      "name":   { "type": "text"  }\n    }\n  }\n}\n\n\n【示例】在已存在的索引中，指定一个 field 的属性\n\nPUT /my-index-000001/_mapping\n{\n  "properties": {\n    "employee-id": {\n      "type": "keyword",\n      "index": false\n    }\n  }\n}\n\n\n【示例】查看 mapping\n\nGET /my-index-000001/_mapping\n\n\n【示例】查看指定 field 的 mapping\n\nGET /my-index-000001/_mapping/field/employee-id\n\n\n\n# 动态映射\n\n动态映射机制，允许用户不手动定义映射，Elasticsearch 会自动识别字段类型。在实际项目中，如果遇到的业务在导入数据之前不确定有哪些字段，也不清楚字段的类型是什么，使用动态映射非常合适。当 Elasticsearch 在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动把该字段添加到映射中。\n\n示例：创建一个名为 data 的索引、其 mapping 类型为 _doc，并且有一个类型为 long 的字段 count。\n\nPUT data/_doc/1\n{ "count": 5 }\n\n\n# 动态字段映射\n\n动态字段映射（Dynamic field mappings）是用于管理动态字段检测的规则。当 Elasticsearch 在文档中检测到新字段时，默认情况下会动态将该字段添加到类型映射中。\n\n在 mapping 中可以通过将 dynamic 参数设置为 true 或 runtime 来开启动态映射。\n\ndynamic 不同设置的作用：\n\n可选值       说明\ntrue      新字段被添加到 mapping 中。mapping 的默认设置。\nruntime   新字段被添加到 mapping 中并作为运行时字段——这些字段不会被索引，但是可以在查询时出现在 _source 中。\nfalse     新字段不会被索引或搜索，但仍会出现在返回匹配的 _source\n          字段中。这些字段不会添加到映射中，并且必须显式添加新字段。\nstrict    如果检测到新字段，则会抛出异常并拒绝文档。必须将新字段显式添加到映射中。\n\n> 需要注意的是：对已有字段，一旦已经有数据写入，就不再支持修改字段定义。如果希望改变字段类型，必须重建索引。这是由于 Lucene 实现的倒排索引，一旦生成后，就不允许修改。如果修改了字段的数据类型，会导致已被索引的字段无法被搜索。\n\n启用动态字段映射后，Elasticsearch 使用内置规则来确定如何映射每个字段的数据类型。规则如下：\n\nJSON 数据类型       "DYNAMIC":"TRUE"                "DYNAMIC":"RUNTIME"\nnull            没有字段被添加                         没有字段被添加\ntrue or false   boolean 类型                      boolean 类型\n浮点型数字           float 类型                        double 类型\n数字              数字型                             long 类型\nJSON 对象         object 类型                       没有字段被添加\n数组              由数组中第一个非空值决定                    由数组中第一个非空值决定\n开启日期检测的字符串      date 类型                         date 类型\n开启数字检测的字符串      float 类型或 long类型                double 类型或 long 类型\n什么也没开启的字符串      带有 .keyword 子 field 的 text 类型   keyword 类型\n\n下面举一个例子认识动态 mapping，在 Elasticsearch 中创建一个新的索引并查看它的 mapping，命令如下：\n\nPUT books\nGET books/_mapping\n\n\n此时 books 索引的 mapping 是空的，返回结果如下：\n\n{\n  "books": {\n    "mappings": {}\n  }\n}\n\n\n再往 books 索引中写入一条文档，命令如下：\n\nPUT books/it/1\n{\n\t"id": 1,\n\t"publish_date": "2019-11-10",\n\t"name": "master Elasticsearch"\n}\n\n\n文档写入完成之后，再次查看 mapping，返回结果如下：\n\n{\n  "books": {\n    "mappings": {\n      "properties": {\n        "id": {\n          "type": "long"\n        },\n        "name": {\n          "type": "text",\n          "fields": {\n            "keyword": {\n              "type": "keyword",\n              "ignore_above": 256\n            }\n          }\n        },\n        "publish_date": {\n          "type": "date"\n        }\n      }\n    }\n  }\n}\n\n\n动态映射有时可能会错误的识别字段类型，这种情况下，可能会导致一些功能无法正常使用，如 Range 查询。所以，使用动态 mapping 要结合实际业务需求来综合考虑，如果将 Elasticsearch 当作主要的数据存储使用，并且希望出现未知字段时抛出异常来提醒你注意这一问题，那么开启动态 mapping 并不适用。\n\n# 动态模板\n\n**动态模板（dynamic templates）**是用于给 mapping 动态添加字段的自定义规则。\n\n动态模板可以设置匹配条件，只有匹配的情况下才使用动态模板：\n\n * match_mapping_type 对 Elasticsearch 检测到的数据类型进行操作\n * match 和 unmatch 使用模式匹配字段名称\n * path_match 和 path_unmatch 对字段的完整虚线路径进行操作\n * 如果动态模板没有定义 match_mapping_type、match 或 path_match，则不会匹配任何字段。您仍然可以在批量请求的 dynamic_templates 部分按名称引用模板。\n\n【示例】当设置 \'dynamic\':\'true\' 时，Elasticsearch 会将字符串字段映射为带有关键字子字段的文本字段。如果只是索引结构化内容并且对全文搜索不感兴趣，可以让 Elasticsearch 仅将字段映射为关键字字段。这种情况下，只有完全匹配才能搜索到这些字段。\n\nPUT my-index-000001\n{\n  "mappings": {\n    "dynamic_templates": [\n      {\n        "strings_as_keywords": {\n          "match_mapping_type": "string",\n          "mapping": {\n            "type": "keyword"\n          }\n        }\n      }\n    ]\n  }\n}\n\n\n\n# 运行时字段\n\n运行时字段是在查询时评估的字段。运行时字段有以下作用：\n\n * 在不重新索引数据的情况下，向现有文档添加字段\n * 在不了解数据结构的情况下，也可以处理数据\n * 在查询时覆盖从索引字段返回的值\n * 为特定用途定义字段而不修改底层架构\n\n检索 Elasticsearch 时，运行时字段和其他字段并没有什么不同。\n\n需要注意的是：使用 _search API 上的 fields 参数来检索运行时字段的值。运行时字段不会显示在 _source 中，但 fields API 适用于所有字段，即使是那些未作为原始 _source 的一部分发送的字段。\n\n运行时字段在处理日志数据时很有用，尤其是当日志是不确定的数据结构时：这种情况下，会降低搜索速度，但您的索引大小要小得多，您可以更快地处理日志，而无需为它们设置索引。\n\n\n# 运行时字段的优点\n\n因为运行时字段没有被索引，所以添加运行时字段不会增加索引大小。用户可以直接在 mapping 中定义运行时字段，从而节省存储成本并提高采集数据的速度。定义了运行时字段后，可以立即在搜索请求、聚合、过滤和排序中使用它。\n\n如果将运行时字段设为索引字段，则无需修改任何引用运行时字段的查询。更好的是，您可以引用字段是运行时字段的一些索引，以及字段是索引字段的其他索引。您可以灵活地选择要索引哪些字段以及保留哪些字段作为运行时字段。\n\n就其核心而言，运行时字段最重要的好处是能够在您提取字段后将字段添加到文档中。此功能简化了映射决策，因为您不必预先决定如何解析数据，并且可以使用运行时字段随时修改映射。使用运行时字段允许更小的索引和更快的摄取时间，这结合使用更少的资源并降低您的运营成本。\n\n\n# 字段数据类型\n\n在 Elasticsearch 中，每个字段都有一个字段数据类型或字段类型，用于指示字段包含的数据类型（例如字符串或布尔值）及其预期用途。字段类型按系列分组。同一族中的类型具有完全相同的搜索行为，但可能具有不同的空间使用或性能特征。\n\nElasticsearch 提供了非常丰富的数据类型，官方将其分为以下几类：\n\n * 普通类型\n   \n   * binary：编码为 Base64 字符串的二进制值。\n   * boolean：布尔类型，值为 true 或 false。\n   * Keywords：keyword 族类型，包括 keyword、constant_keyword 和 wildcard。\n   * Numbers：数字类型，如 long 和 double\n   * Dates：日期类型，包括 date 和 date_nanos。\n   * alias：用于定义存在字段的别名。\n\n * 对象类型\n   \n   * object：JSON 对象\n   * flattened：整个 JSON 对象作为单个字段值。\n   * nested：保留其子字段之间关系的 JSON 对象。\n   * join：为同一索引中的文档定义父/子关系。\n\n * 结构化数据类型\n   \n   * Range：范围类型，例如：long_range、double_range、date_range 和 ip_range。\n   * ip：IPv4 和 IPv6 地址。\n   * version：版本号。支持 Semantic Versioning 优先规则。\n   * murmur3：计算并存储 hash 值。\n\n * 聚合数据类型\n   \n   * aggregate_metric_double：预先聚合的指标值\n   * histogram：直方图式的预聚合数值。\n\n * 文本搜索类型\n   \n   * text fields：text 族类型，包括 text 和 match_only_text。\n   * annotated-text：包含特殊标记的文本。用于识别命名实体。\n   * completion：用于自动补全。\n   * search_as_you_type：键入时完成的类似文本的类型。\n   * token_count：文本中标记的计数。\n\n * 文档排名类型\n   \n   * dense_vector：记录浮点数的密集向量。\n   * rank_feature：记录一个数字特征，为了在查询时提高命中率。\n   * rank_features：记录多个数字特征，为了在查询时提高命中率。\n\n * 空间数据类型\n   \n   * geo_point：地理经纬度\n   * geo_shape：复杂的形状，例如多边形\n   * point：任意笛卡尔点\n   * shape：任意笛卡尔几何形状\n\n * 其他类型\n   \n   * percolator：使用 Query DSL 编写的索引查询\n\n\n# 元数据字段\n\n一个文档中，不仅仅包含数据 ，也包含元数据。元数据是用于描述文档的信息。\n\n * 标识元数据字段\n   * _index：文档所属的索引。\n   * _id：文档的 ID。\n * 文档 source 元数据字段\n   * _source：文档正文的原始 JSON。\n   * _size：_source 字段的大小（以字节为单位），由 mapper-size 插件提供。\n * 文档计数元数据字段\n   * _doc_count：当文档表示预聚合数据时，用于存储文档计数的自定义字段。\n * 索引元数据字段\n   * _field_names：文档中的所有非空字段。\n   * _ignored：文档中所有的由于 ignore_malformed 而在索引时被忽略的字段。\n * 路由元数据字段\n   * _routing：将文档路由到特定分片的自定义路由值。\n * 其他元数据字段\n   * _meta：应用程序特定的元数据。\n   * _tier：文档所属索引的当前数据层首选项。\n\n\n# 映射参数\n\nElasticsearch 提供了以下映射参数：\n\n * analyzer：指定在索引或搜索文本字段时用于文本分析的分析器。\n * coerce：如果开启，Elasticsearch 将尝试清理脏数据以适应字段的数据类型。\n * copy_to：允许将多个字段的值复制到一个组字段中，然后可以将其作为单个字段进行查询。\n * doc_values：默认情况下，所有字段都是被\n * dynamic：是否开启动态映射。\n * eager_global_ordinals：当在 global ordinals 的时候，refresh 以后下一次查询字典就需要重新构建，在追求查询的场景下很影响查询性能。可以使用 eager_global_ordinals，即在每次 refresh 以后即可更新字典，字典常驻内存，减少了查询的时候构建字典的耗时。\n * enabled：只能应用于顶级 mapping 定义和 object 字段。设置为 false 后，Elasticsearch 解析时，会完全跳过该字段。\n * fielddata：默认情况下， text 字段是可搜索的，但不可用于聚合、排序或脚本。如果为字段设置 fielddata=true，就会通过反转倒排索引将 fielddata 加载到内存中。请注意，这可能会占用大量内存。如果想对 text 字段进行聚合、排序或脚本操作，fielddata 是唯一方法。\n * fields：有时候，同一个字段需要以不同目的进行索引，此时可以通过 fields 进行配置。\n * format：用于格式化日期类型。\n * ignore_above：字符串长度大于 ignore_above 所设，则不会被索引或存储。\n * ignore_malformed：有时候，同一个字段，可能会存储不同的数据类型。默认情况下，Elasticsearch 解析字段数据类型失败时，会引发异常，并拒绝整个文档。 如果设置 ignore_malformed 为 true，则允许忽略异常。这种情况下，格式错误的字段不会被索引，但文档中的其他字段可以正常处理。\n * index_options 用于控制将哪些信息添加到倒排索引以进行搜索和突出显示。只有 text 和 keyword 等基于术语（term）的字段类型支持此配置。\n * index_phrases：如果启用，两个词的组合（shingles）将被索引到一个单独的字段中。这允许以更大的索引为代价，更有效地运行精确的短语查询（无 slop）。请注意，当停用词未被删除时，此方法效果最佳，因为包含停用词的短语将不使用辅助字段，并将回退到标准短语查询。接受真或假（默认）。\n * index_prefixes：index_prefixes 参数启用 term 前缀索引以加快前缀搜索。\n * index：index 选项控制字段值是否被索引。默认为 true。\n * meta：附加到字段的元数据。此元数据对 Elasticsearch 是不透明的，它仅适用于多个应用共享相同索引的元数据信息，例如：单位。\n * normalizer：keyword 字段的 normalizer 属性类似于 analyzer ，只是它保证分析链只产生单个标记。 normalizer 在索引 keyword 之前应用，以及在搜索时通过查询解析器（例如匹配查询）或通过术语级别查询（例如术语查询）搜索关键字字段时应用。\n * norms：norms 存储在查询时使用的各种规范化因子，以便计算文档的相关性评分。\n * null_value：null 值无法被索引和搜索。当一个字段被设为 null，则被视为没有值。null_value 允许将空值替换为指定值，以便对其进行索引和搜索。\n * position_increment_gap：分析的文本字段会考虑术语位置，以便能够支持邻近或短语查询。当索引具有多个值的文本字段时，值之间会添加一个“假”间隙，以防止大多数短语查询在值之间匹配。此间隙的大小使用 position_increment_gap 配置，默认为 100。\n * properties：类型映射、对象字段和嵌套字段包含的子字段，都称为属性。这些属性可以是任何数据类型，包括对象和嵌套。\n * search_analyzer：通常，在索引时和搜索时应使用相同的分析器，以确保查询中的术语与倒排索引中的术语格式相同。但是，有时在搜索时使用不同的分析器可能是有意义的，例如使用 edge_ngram 标记器实现自动补全或使用同义词搜索时。\n * similarity：Elasticsearch 允许为每个字段配置文本评分算法或相似度。相似度设置提供了一种选择文本相似度算法的简单方法，而不是默认的 BM25，例如布尔值。只有 text 和 keyword 等基于文本的字段类型支持此配置。\n * store：默认情况下，对字段值进行索引以使其可搜索，但不会存储它们。这意味着可以查询该字段，但无法检索原始字段值。通常这不重要，字段值已经是默认存储的 _source 字段的一部分。如果您只想检索单个字段或几个字段的值，而不是整个 _source，则可以通过 source filtering 来实现。\n * term_vector：term_vector 包含有关分析过程产生的术语的信息，包括：\n   * 术语列表\n   * 每个 term 的位置（或顺序）\n   * 起始和结束字符偏移量，用于将 term 和原始字符串进行映射\n   * 有效负载（如果可用） - 用户定义的，与 term 位置相关的二进制数据\n\n\n# 映射配置\n\n * index.mapping.total_fields.limit：索引中的最大字段数。字段和对象映射以及字段别名计入此限制。默认值为 1000。\n * index.mapping.depth.limit：字段的最大深度，以内部对象的数量来衡量。例如，如果所有字段都在根对象级别定义，则深度为 1。如果有一个对象映射，则深度为 2，以此类推。默认值为 20。\n * index.mapping.nested_fields.limit：索引中不同 nested 映射的最大数量。 nested 类型只应在特殊情况下使用，即需要相互独立地查询对象数组。为了防止设计不佳的映射，此设置限制了每个索引的唯一 nested 类型的数量。默认值为 50。\n * index.mapping.nested_objects.limit：单个文档中，所有 nested 类型中包含的最大嵌套 JSON 对象数。当文档包含太多 nested 对象时，此限制有助于防止出现内存溢出。默认值为 10000。\n * index.mapping.field_name_length.limit：设置字段名称的最大长度。默认为 Long.MAX_VALUE（无限制）。\n\n\n# 参考资料\n\n * Elasticsearch 官方文档之 Mapping',normalizedContent:'# elasticsearch 映射\n\n在 elasticsearch 中，mapping（映射），用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、字段的权重、分词器等属性，就如同在关系型数据库中创建数据表时会设置字段的类型。\n\nmapping 会把 json 文档映射成 lucene 所需要的扁平格式\n\n一个 mapping 属于一个索引的 type\n\n * 每个文档都属于一个 type\n * 一个 type 有一个 mapping 定义\n * 7.0 开始，不需要在 mapping 定义中指定 type 信息\n\n每个 document 都是 field 的集合，每个 field 都有自己的数据类型。映射数据时，可以创建一个 mapping，其中包含与 document 相关的 field 列表。映射定义还包括元数据 field，例如 _source ，它自定义如何处理 document 的关联元数据。\n\n\n# 映射方式\n\n在 elasticsearch 中，映射可分为静态映射和动态映射。在关系型数据库中写入数据之前首先要建表，在建表语句中声明字段的属性，在 elasticsearch 中，则不必如此，elasticsearch 最重要的功能之一就是让你尽可能快地开始探索数据，文档写入 elasticsearch 中，它会根据字段的类型自动识别，这种机制称为动态映射，而静态映射则是写入数据之前对字段的属性进行手工设置。\n\n\n# 静态映射\n\nes 官方将静态映射称为显式映射（explicit mapping）。静态映射是在创建索引时显示的指定索引映射。静态映射和 sql 中在建表语句中指定字段属性类似。相比动态映射，通过静态映射可以添加更详细、更精准的配置信息。例如：\n\n * 哪些字符串字段应被视为全文字段。\n * 哪些字段包含数字、日期或地理位置。\n * 日期值的格式。\n * 用于控制动态添加字段的自定义规则。\n\n【示例】创建索引时，显示指定 mapping\n\nput /my-index-000001\n{\n  "mappings": {\n    "properties": {\n      "age":    { "type": "integer" },\n      "email":  { "type": "keyword"  },\n      "name":   { "type": "text"  }\n    }\n  }\n}\n\n\n【示例】在已存在的索引中，指定一个 field 的属性\n\nput /my-index-000001/_mapping\n{\n  "properties": {\n    "employee-id": {\n      "type": "keyword",\n      "index": false\n    }\n  }\n}\n\n\n【示例】查看 mapping\n\nget /my-index-000001/_mapping\n\n\n【示例】查看指定 field 的 mapping\n\nget /my-index-000001/_mapping/field/employee-id\n\n\n\n# 动态映射\n\n动态映射机制，允许用户不手动定义映射，elasticsearch 会自动识别字段类型。在实际项目中，如果遇到的业务在导入数据之前不确定有哪些字段，也不清楚字段的类型是什么，使用动态映射非常合适。当 elasticsearch 在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动把该字段添加到映射中。\n\n示例：创建一个名为 data 的索引、其 mapping 类型为 _doc，并且有一个类型为 long 的字段 count。\n\nput data/_doc/1\n{ "count": 5 }\n\n\n# 动态字段映射\n\n动态字段映射（dynamic field mappings）是用于管理动态字段检测的规则。当 elasticsearch 在文档中检测到新字段时，默认情况下会动态将该字段添加到类型映射中。\n\n在 mapping 中可以通过将 dynamic 参数设置为 true 或 runtime 来开启动态映射。\n\ndynamic 不同设置的作用：\n\n可选值       说明\ntrue      新字段被添加到 mapping 中。mapping 的默认设置。\nruntime   新字段被添加到 mapping 中并作为运行时字段——这些字段不会被索引，但是可以在查询时出现在 _source 中。\nfalse     新字段不会被索引或搜索，但仍会出现在返回匹配的 _source\n          字段中。这些字段不会添加到映射中，并且必须显式添加新字段。\nstrict    如果检测到新字段，则会抛出异常并拒绝文档。必须将新字段显式添加到映射中。\n\n> 需要注意的是：对已有字段，一旦已经有数据写入，就不再支持修改字段定义。如果希望改变字段类型，必须重建索引。这是由于 lucene 实现的倒排索引，一旦生成后，就不允许修改。如果修改了字段的数据类型，会导致已被索引的字段无法被搜索。\n\n启用动态字段映射后，elasticsearch 使用内置规则来确定如何映射每个字段的数据类型。规则如下：\n\njson 数据类型       "dynamic":"true"                "dynamic":"runtime"\nnull            没有字段被添加                         没有字段被添加\ntrue or false   boolean 类型                      boolean 类型\n浮点型数字           float 类型                        double 类型\n数字              数字型                             long 类型\njson 对象         object 类型                       没有字段被添加\n数组              由数组中第一个非空值决定                    由数组中第一个非空值决定\n开启日期检测的字符串      date 类型                         date 类型\n开启数字检测的字符串      float 类型或 long类型                double 类型或 long 类型\n什么也没开启的字符串      带有 .keyword 子 field 的 text 类型   keyword 类型\n\n下面举一个例子认识动态 mapping，在 elasticsearch 中创建一个新的索引并查看它的 mapping，命令如下：\n\nput books\nget books/_mapping\n\n\n此时 books 索引的 mapping 是空的，返回结果如下：\n\n{\n  "books": {\n    "mappings": {}\n  }\n}\n\n\n再往 books 索引中写入一条文档，命令如下：\n\nput books/it/1\n{\n\t"id": 1,\n\t"publish_date": "2019-11-10",\n\t"name": "master elasticsearch"\n}\n\n\n文档写入完成之后，再次查看 mapping，返回结果如下：\n\n{\n  "books": {\n    "mappings": {\n      "properties": {\n        "id": {\n          "type": "long"\n        },\n        "name": {\n          "type": "text",\n          "fields": {\n            "keyword": {\n              "type": "keyword",\n              "ignore_above": 256\n            }\n          }\n        },\n        "publish_date": {\n          "type": "date"\n        }\n      }\n    }\n  }\n}\n\n\n动态映射有时可能会错误的识别字段类型，这种情况下，可能会导致一些功能无法正常使用，如 range 查询。所以，使用动态 mapping 要结合实际业务需求来综合考虑，如果将 elasticsearch 当作主要的数据存储使用，并且希望出现未知字段时抛出异常来提醒你注意这一问题，那么开启动态 mapping 并不适用。\n\n# 动态模板\n\n**动态模板（dynamic templates）**是用于给 mapping 动态添加字段的自定义规则。\n\n动态模板可以设置匹配条件，只有匹配的情况下才使用动态模板：\n\n * match_mapping_type 对 elasticsearch 检测到的数据类型进行操作\n * match 和 unmatch 使用模式匹配字段名称\n * path_match 和 path_unmatch 对字段的完整虚线路径进行操作\n * 如果动态模板没有定义 match_mapping_type、match 或 path_match，则不会匹配任何字段。您仍然可以在批量请求的 dynamic_templates 部分按名称引用模板。\n\n【示例】当设置 \'dynamic\':\'true\' 时，elasticsearch 会将字符串字段映射为带有关键字子字段的文本字段。如果只是索引结构化内容并且对全文搜索不感兴趣，可以让 elasticsearch 仅将字段映射为关键字字段。这种情况下，只有完全匹配才能搜索到这些字段。\n\nput my-index-000001\n{\n  "mappings": {\n    "dynamic_templates": [\n      {\n        "strings_as_keywords": {\n          "match_mapping_type": "string",\n          "mapping": {\n            "type": "keyword"\n          }\n        }\n      }\n    ]\n  }\n}\n\n\n\n# 运行时字段\n\n运行时字段是在查询时评估的字段。运行时字段有以下作用：\n\n * 在不重新索引数据的情况下，向现有文档添加字段\n * 在不了解数据结构的情况下，也可以处理数据\n * 在查询时覆盖从索引字段返回的值\n * 为特定用途定义字段而不修改底层架构\n\n检索 elasticsearch 时，运行时字段和其他字段并没有什么不同。\n\n需要注意的是：使用 _search api 上的 fields 参数来检索运行时字段的值。运行时字段不会显示在 _source 中，但 fields api 适用于所有字段，即使是那些未作为原始 _source 的一部分发送的字段。\n\n运行时字段在处理日志数据时很有用，尤其是当日志是不确定的数据结构时：这种情况下，会降低搜索速度，但您的索引大小要小得多，您可以更快地处理日志，而无需为它们设置索引。\n\n\n# 运行时字段的优点\n\n因为运行时字段没有被索引，所以添加运行时字段不会增加索引大小。用户可以直接在 mapping 中定义运行时字段，从而节省存储成本并提高采集数据的速度。定义了运行时字段后，可以立即在搜索请求、聚合、过滤和排序中使用它。\n\n如果将运行时字段设为索引字段，则无需修改任何引用运行时字段的查询。更好的是，您可以引用字段是运行时字段的一些索引，以及字段是索引字段的其他索引。您可以灵活地选择要索引哪些字段以及保留哪些字段作为运行时字段。\n\n就其核心而言，运行时字段最重要的好处是能够在您提取字段后将字段添加到文档中。此功能简化了映射决策，因为您不必预先决定如何解析数据，并且可以使用运行时字段随时修改映射。使用运行时字段允许更小的索引和更快的摄取时间，这结合使用更少的资源并降低您的运营成本。\n\n\n# 字段数据类型\n\n在 elasticsearch 中，每个字段都有一个字段数据类型或字段类型，用于指示字段包含的数据类型（例如字符串或布尔值）及其预期用途。字段类型按系列分组。同一族中的类型具有完全相同的搜索行为，但可能具有不同的空间使用或性能特征。\n\nelasticsearch 提供了非常丰富的数据类型，官方将其分为以下几类：\n\n * 普通类型\n   \n   * binary：编码为 base64 字符串的二进制值。\n   * boolean：布尔类型，值为 true 或 false。\n   * keywords：keyword 族类型，包括 keyword、constant_keyword 和 wildcard。\n   * numbers：数字类型，如 long 和 double\n   * dates：日期类型，包括 date 和 date_nanos。\n   * alias：用于定义存在字段的别名。\n\n * 对象类型\n   \n   * object：json 对象\n   * flattened：整个 json 对象作为单个字段值。\n   * nested：保留其子字段之间关系的 json 对象。\n   * join：为同一索引中的文档定义父/子关系。\n\n * 结构化数据类型\n   \n   * range：范围类型，例如：long_range、double_range、date_range 和 ip_range。\n   * ip：ipv4 和 ipv6 地址。\n   * version：版本号。支持 semantic versioning 优先规则。\n   * murmur3：计算并存储 hash 值。\n\n * 聚合数据类型\n   \n   * aggregate_metric_double：预先聚合的指标值\n   * histogram：直方图式的预聚合数值。\n\n * 文本搜索类型\n   \n   * text fields：text 族类型，包括 text 和 match_only_text。\n   * annotated-text：包含特殊标记的文本。用于识别命名实体。\n   * completion：用于自动补全。\n   * search_as_you_type：键入时完成的类似文本的类型。\n   * token_count：文本中标记的计数。\n\n * 文档排名类型\n   \n   * dense_vector：记录浮点数的密集向量。\n   * rank_feature：记录一个数字特征，为了在查询时提高命中率。\n   * rank_features：记录多个数字特征，为了在查询时提高命中率。\n\n * 空间数据类型\n   \n   * geo_point：地理经纬度\n   * geo_shape：复杂的形状，例如多边形\n   * point：任意笛卡尔点\n   * shape：任意笛卡尔几何形状\n\n * 其他类型\n   \n   * percolator：使用 query dsl 编写的索引查询\n\n\n# 元数据字段\n\n一个文档中，不仅仅包含数据 ，也包含元数据。元数据是用于描述文档的信息。\n\n * 标识元数据字段\n   * _index：文档所属的索引。\n   * _id：文档的 id。\n * 文档 source 元数据字段\n   * _source：文档正文的原始 json。\n   * _size：_source 字段的大小（以字节为单位），由 mapper-size 插件提供。\n * 文档计数元数据字段\n   * _doc_count：当文档表示预聚合数据时，用于存储文档计数的自定义字段。\n * 索引元数据字段\n   * _field_names：文档中的所有非空字段。\n   * _ignored：文档中所有的由于 ignore_malformed 而在索引时被忽略的字段。\n * 路由元数据字段\n   * _routing：将文档路由到特定分片的自定义路由值。\n * 其他元数据字段\n   * _meta：应用程序特定的元数据。\n   * _tier：文档所属索引的当前数据层首选项。\n\n\n# 映射参数\n\nelasticsearch 提供了以下映射参数：\n\n * analyzer：指定在索引或搜索文本字段时用于文本分析的分析器。\n * coerce：如果开启，elasticsearch 将尝试清理脏数据以适应字段的数据类型。\n * copy_to：允许将多个字段的值复制到一个组字段中，然后可以将其作为单个字段进行查询。\n * doc_values：默认情况下，所有字段都是被\n * dynamic：是否开启动态映射。\n * eager_global_ordinals：当在 global ordinals 的时候，refresh 以后下一次查询字典就需要重新构建，在追求查询的场景下很影响查询性能。可以使用 eager_global_ordinals，即在每次 refresh 以后即可更新字典，字典常驻内存，减少了查询的时候构建字典的耗时。\n * enabled：只能应用于顶级 mapping 定义和 object 字段。设置为 false 后，elasticsearch 解析时，会完全跳过该字段。\n * fielddata：默认情况下， text 字段是可搜索的，但不可用于聚合、排序或脚本。如果为字段设置 fielddata=true，就会通过反转倒排索引将 fielddata 加载到内存中。请注意，这可能会占用大量内存。如果想对 text 字段进行聚合、排序或脚本操作，fielddata 是唯一方法。\n * fields：有时候，同一个字段需要以不同目的进行索引，此时可以通过 fields 进行配置。\n * format：用于格式化日期类型。\n * ignore_above：字符串长度大于 ignore_above 所设，则不会被索引或存储。\n * ignore_malformed：有时候，同一个字段，可能会存储不同的数据类型。默认情况下，elasticsearch 解析字段数据类型失败时，会引发异常，并拒绝整个文档。 如果设置 ignore_malformed 为 true，则允许忽略异常。这种情况下，格式错误的字段不会被索引，但文档中的其他字段可以正常处理。\n * index_options 用于控制将哪些信息添加到倒排索引以进行搜索和突出显示。只有 text 和 keyword 等基于术语（term）的字段类型支持此配置。\n * index_phrases：如果启用，两个词的组合（shingles）将被索引到一个单独的字段中。这允许以更大的索引为代价，更有效地运行精确的短语查询（无 slop）。请注意，当停用词未被删除时，此方法效果最佳，因为包含停用词的短语将不使用辅助字段，并将回退到标准短语查询。接受真或假（默认）。\n * index_prefixes：index_prefixes 参数启用 term 前缀索引以加快前缀搜索。\n * index：index 选项控制字段值是否被索引。默认为 true。\n * meta：附加到字段的元数据。此元数据对 elasticsearch 是不透明的，它仅适用于多个应用共享相同索引的元数据信息，例如：单位。\n * normalizer：keyword 字段的 normalizer 属性类似于 analyzer ，只是它保证分析链只产生单个标记。 normalizer 在索引 keyword 之前应用，以及在搜索时通过查询解析器（例如匹配查询）或通过术语级别查询（例如术语查询）搜索关键字字段时应用。\n * norms：norms 存储在查询时使用的各种规范化因子，以便计算文档的相关性评分。\n * null_value：null 值无法被索引和搜索。当一个字段被设为 null，则被视为没有值。null_value 允许将空值替换为指定值，以便对其进行索引和搜索。\n * position_increment_gap：分析的文本字段会考虑术语位置，以便能够支持邻近或短语查询。当索引具有多个值的文本字段时，值之间会添加一个“假”间隙，以防止大多数短语查询在值之间匹配。此间隙的大小使用 position_increment_gap 配置，默认为 100。\n * properties：类型映射、对象字段和嵌套字段包含的子字段，都称为属性。这些属性可以是任何数据类型，包括对象和嵌套。\n * search_analyzer：通常，在索引时和搜索时应使用相同的分析器，以确保查询中的术语与倒排索引中的术语格式相同。但是，有时在搜索时使用不同的分析器可能是有意义的，例如使用 edge_ngram 标记器实现自动补全或使用同义词搜索时。\n * similarity：elasticsearch 允许为每个字段配置文本评分算法或相似度。相似度设置提供了一种选择文本相似度算法的简单方法，而不是默认的 bm25，例如布尔值。只有 text 和 keyword 等基于文本的字段类型支持此配置。\n * store：默认情况下，对字段值进行索引以使其可搜索，但不会存储它们。这意味着可以查询该字段，但无法检索原始字段值。通常这不重要，字段值已经是默认存储的 _source 字段的一部分。如果您只想检索单个字段或几个字段的值，而不是整个 _source，则可以通过 source filtering 来实现。\n * term_vector：term_vector 包含有关分析过程产生的术语的信息，包括：\n   * 术语列表\n   * 每个 term 的位置（或顺序）\n   * 起始和结束字符偏移量，用于将 term 和原始字符串进行映射\n   * 有效负载（如果可用） - 用户定义的，与 term 位置相关的二进制数据\n\n\n# 映射配置\n\n * index.mapping.total_fields.limit：索引中的最大字段数。字段和对象映射以及字段别名计入此限制。默认值为 1000。\n * index.mapping.depth.limit：字段的最大深度，以内部对象的数量来衡量。例如，如果所有字段都在根对象级别定义，则深度为 1。如果有一个对象映射，则深度为 2，以此类推。默认值为 20。\n * index.mapping.nested_fields.limit：索引中不同 nested 映射的最大数量。 nested 类型只应在特殊情况下使用，即需要相互独立地查询对象数组。为了防止设计不佳的映射，此设置限制了每个索引的唯一 nested 类型的数量。默认值为 50。\n * index.mapping.nested_objects.limit：单个文档中，所有 nested 类型中包含的最大嵌套 json 对象数。当文档包含太多 nested 对象时，此限制有助于防止出现内存溢出。默认值为 10000。\n * index.mapping.field_name_length.limit：设置字段名称的最大长度。默认为 long.max_value（无限制）。\n\n\n# 参考资料\n\n * elasticsearch 官方文档之 mapping',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 查询",frontmatter:{title:"Elasticsearch 查询",date:"2022-01-18T08:01:08.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","查询"],permalink:"/pages/83bd15/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/05.Elasticsearch%E6%9F%A5%E8%AF%A2.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/05.Elasticsearch查询.md",key:"v-f6b5c434",path:"/pages/83bd15/",headers:[{level:2,title:"全文查询",slug:"全文查询",normalizedTitle:"全文查询",charIndex:107},{level:3,title:"intervals query",slug:"intervals-query",normalizedTitle:"intervals query",charIndex:305},{level:3,title:"match query",slug:"match-query",normalizedTitle:"match query",charIndex:1226},{level:4,title:"match query 简写",slug:"match-query-简写",normalizedTitle:"match query 简写",charIndex:1748},{level:4,title:"match query 如何工作",slug:"match-query-如何工作",normalizedTitle:"match query 如何工作",charIndex:1899},{level:4,title:"match query 的模糊查询",slug:"match-query-的模糊查询",normalizedTitle:"match query 的模糊查询",charIndex:2371},{level:4,title:"zero terms 查询",slug:"zero-terms-查询",normalizedTitle:"zero terms 查询",charIndex:2802},{level:3,title:"matchboolprefix query",slug:"match-bool-prefix-query",normalizedTitle:"matchboolprefix query",charIndex:null},{level:3,title:"match_phrase query",slug:"match-phrase-query",normalizedTitle:"match_phrase query",charIndex:3828},{level:3,title:"matchphraseprefix query",slug:"match-phrase-prefix-query",normalizedTitle:"matchphraseprefix query",charIndex:null},{level:3,title:"multi_match query",slug:"multi-match-query",normalizedTitle:"multi_match query",charIndex:4644},{level:3,title:"combined_fields query",slug:"combined-fields-query",normalizedTitle:"combined_fields query",charIndex:5436},{level:4,title:"字段前缀权重",slug:"字段前缀权重",normalizedTitle:"字段前缀权重",charIndex:5808},{level:3,title:"common_terms query",slug:"common-terms-query",normalizedTitle:"common_terms query",charIndex:5905},{level:3,title:"query_string query",slug:"query-string-query",normalizedTitle:"query_string query",charIndex:7342},{level:3,title:"simplequerystring query",slug:"simple-query-string-query",normalizedTitle:"simplequerystring query",charIndex:null},{level:4,title:"simplequerystring 语义",slug:"simple-query-string-语义",normalizedTitle:"simplequerystring 语义",charIndex:null},{level:3,title:"全文查询完整示例",slug:"全文查询完整示例",normalizedTitle:"全文查询完整示例",charIndex:8351},{level:2,title:"词项查询",slug:"词项查询",normalizedTitle:"词项查询",charIndex:112},{level:3,title:"exists query",slug:"exists-query",normalizedTitle:"exists query",charIndex:9114},{level:3,title:"fuzzy query",slug:"fuzzy-query",normalizedTitle:"fuzzy query",charIndex:9130},{level:3,title:"ids query",slug:"ids-query",normalizedTitle:"ids query",charIndex:9145},{level:3,title:"prefix query",slug:"prefix-query",normalizedTitle:"prefix query",charIndex:3131},{level:3,title:"range query",slug:"range-query",normalizedTitle:"range query",charIndex:9174},{level:3,title:"regexp query",slug:"regexp-query",normalizedTitle:"regexp query",charIndex:9189},{level:3,title:"term query",slug:"term-query",normalizedTitle:"term query",charIndex:9205},{level:3,title:"terms query",slug:"terms-query",normalizedTitle:"terms query",charIndex:5912},{level:3,title:"type query",slug:"type-query",normalizedTitle:"type query",charIndex:9234},{level:3,title:"wildcard query",slug:"wildcard-query",normalizedTitle:"wildcard query",charIndex:9248},{level:3,title:"词项查询完整示例",slug:"词项查询完整示例",normalizedTitle:"词项查询完整示例",charIndex:14439},{level:2,title:"复合查询",slug:"复合查询",normalizedTitle:"复合查询",charIndex:117},{level:3,title:"bool query",slug:"bool-query",normalizedTitle:"bool query",charIndex:15645},{level:3,title:"boosting query",slug:"boosting-query",normalizedTitle:"boosting query",charIndex:16644},{level:3,title:"constant_score query",slug:"constant-score-query",normalizedTitle:"constant_score query",charIndex:17275},{level:3,title:"dis_max query",slug:"dis-max-query",normalizedTitle:"dis_max query",charIndex:17690},{level:3,title:"function_score query",slug:"function-score-query",normalizedTitle:"function_score query",charIndex:18073},{level:3,title:"indices query",slug:"indices-query",normalizedTitle:"indices query",charIndex:18879},{level:2,title:"嵌套查询",slug:"嵌套查询",normalizedTitle:"嵌套查询",charIndex:122},{level:3,title:"nested query",slug:"nested-query",normalizedTitle:"nested query",charIndex:19493},{level:3,title:"has_child query",slug:"has-child-query",normalizedTitle:"has_child query",charIndex:19576},{level:3,title:"has_parent query",slug:"has-parent-query",normalizedTitle:"has_parent query",charIndex:19599},{level:2,title:"位置查询",slug:"位置查询",normalizedTitle:"位置查询",charIndex:127},{level:3,title:"geo_distance query",slug:"geo-distance-query",normalizedTitle:"geo_distance query",charIndex:23207},{level:3,title:"geoboundingbox query",slug:"geo-bounding-box-query",normalizedTitle:"geoboundingbox query",charIndex:null},{level:3,title:"geo_polygon query",slug:"geo-polygon-query",normalizedTitle:"geo_polygon query",charIndex:24362},{level:3,title:"geo_shape query",slug:"geo-shape-query",normalizedTitle:"geo_shape query",charIndex:24837},{level:2,title:"特殊查询",slug:"特殊查询",normalizedTitle:"特殊查询",charIndex:132},{level:3,title:"morelikethis query",slug:"more-like-this-query",normalizedTitle:"morelikethis query",charIndex:null},{level:3,title:"script query",slug:"script-query",normalizedTitle:"script query",charIndex:26550},{level:3,title:"percolate query",slug:"percolate-query",normalizedTitle:"percolate query",charIndex:26779}],headersStr:"全文查询 intervals query match query match query 简写 match query 如何工作 match query 的模糊查询 zero terms 查询 matchboolprefix query match_phrase query matchphraseprefix query multi_match query combined_fields query 字段前缀权重 common_terms query query_string query simplequerystring query simplequerystring 语义 全文查询完整示例 词项查询 exists query fuzzy query ids query prefix query range query regexp query term query terms query type query wildcard query 词项查询完整示例 复合查询 bool query boosting query constant_score query dis_max query function_score query indices query 嵌套查询 nested query has_child query has_parent query 位置查询 geo_distance query geoboundingbox query geo_polygon query geo_shape query 特殊查询 morelikethis query script query percolate query",content:'# Elasticsearch 查询\n\nElasticsearch 查询语句采用基于 RESTful 风格的接口封装成 JSON 格式的对象，称之为 Query DSL。Elasticsearch 查询分类大致分为全文查询、词项查询、复合查询、嵌套查询、位置查询、特殊查询。Elasticsearch 查询从机制分为两种，一种是根据用户输入的查询词，通过排序模型计算文档与查询词之间的相关度，并根据评分高低排序返回；另一种是过滤机制，只根据过滤条件对文档进行过滤，不计算评分，速度相对较快。\n\n\n# 全文查询\n\nES 全文查询主要用于在全文字段上，主要考虑查询词与文档的相关性（Relevance）。\n\n\n# intervals query\n\nintervals query 根据匹配词的顺序和近似度返回文档。\n\nintervals query 使用匹配规则，这些规则应用于指定字段中的 term。\n\n示例：下面示例搜索 query 字段，搜索值是 my favorite food，没有任何间隙；然后是 my_text 字段搜索匹配 hot water、cold porridge 的 term。\n\n当 my_text 中的值为 my favorite food is cold porridge 时，会匹配成功，但是 when it\'s cold my favorite food is porridge 则匹配失败\n\nPOST _search\n{\n  "query": {\n    "intervals" : {\n      "my_text" : {\n        "all_of" : {\n          "ordered" : true,\n          "intervals" : [\n            {\n              "match" : {\n                "query" : "my favorite food",\n                "max_gaps" : 0,\n                "ordered" : true\n              }\n            },\n            {\n              "any_of" : {\n                "intervals" : [\n                  { "match" : { "query" : "hot water" } },\n                  { "match" : { "query" : "cold porridge" } }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n\n\n\n# match query\n\nmatch query 用于搜索单个字段，首先会针对查询语句进行解析（经过 analyzer），主要是对查询语句进行分词，分词后查询语句的任何一个词项被匹配，文档就会被搜到，默认情况下相当于对分词后词项进行 or 匹配操作。\n\nmatch query 是执行全文搜索的标准查询，包括模糊匹配选项。\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "match": {\n      "customer_full_name": {\n        "query": "George Hubbard"\n      }\n    }\n  }\n}\n\n\n等同于 or 匹配操作，如下：\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "match": {\n      "customer_full_name": {\n        "query": "George Hubbard",\n        "operator": "or"\n      }\n    }\n  }\n}\n\n\n# match query 简写\n\n可以通过组合 <field> 和 query 参数来简化匹配查询语法。\n\n示例：\n\nGET /_search\n{\n  "query": {\n    "match": {\n      "message": "this is a test"\n    }\n  }\n}\n\n\n# match query 如何工作\n\n匹配查询是布尔类型。这意味着会对提供的文本进行分析，分析过程从提供的文本构造一个布尔查询。 operator 参数可以设置为 or 或 and 来控制布尔子句（默认为 or）。可以使用 minimum_should_match 参数设置要匹配的可选 should 子句的最小数量。\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "match": {\n      "customer_full_name": {\n        "query": "George Hubbard",\n        "operator": "and"\n      }\n    }\n  }\n}\n\n\n可以设置 analyzer 来控制哪个分析器将对文本执行分析过程。它默认为字段显式映射定义或默认搜索分析器。\n\nlenient 参数可以设置为 true 以忽略由数据类型不匹配导致的异常，例如尝试使用文本查询字符串查询数字字段。默认为 false。\n\n# match query 的模糊查询\n\nfuzziness 允许基于被查询字段的类型进行模糊匹配。请参阅 Fuzziness 的配置。\n\n在这种情况下可以设置 prefix_length 和 max_expansions 来控制模糊匹配。如果设置了模糊选项，查询将使用 top_terms_blended_freqs_${max_expansions} 作为其重写方法，fuzzy_rewrite 参数允许控制查询将如何被重写。\n\n默认情况下允许模糊倒转 (ab → ba)，但可以通过将 fuzzy_transpositions 设置为 false 来禁用。\n\nGET /_search\n{\n  "query": {\n    "match": {\n      "message": {\n        "query": "this is a testt",\n        "fuzziness": "AUTO"\n      }\n    }\n  }\n}\n\n\n# zero terms 查询\n\n如果使用的分析器像 stop 过滤器一样删除查询中的所有标记，则默认行为是不匹配任何文档。可以使用 zero_terms_query 选项来改变默认行为，它接受 none（默认）和 all （相当于 match_all 查询）。\n\nGET /_search\n{\n  "query": {\n    "match": {\n      "message": {\n        "query": "to be or not to be",\n        "operator": "and",\n        "zero_terms_query": "all"\n      }\n    }\n  }\n}\n\n\n\n# match_bool_prefix query\n\nmatch_bool_prefix query 分析其输入并根据这些词构造一个布尔查询。除了最后一个术语之外的每个术语都用于术语查询。最后一个词用于 prefix query。\n\n示例：\n\nGET /_search\n{\n  "query": {\n    "match_bool_prefix" : {\n      "message" : "quick brown f"\n    }\n  }\n}\n\n\n等价于\n\nGET /_search\n{\n  "query": {\n    "bool" : {\n      "should": [\n        { "term": { "message": "quick" }},\n        { "term": { "message": "brown" }},\n        { "prefix": { "message": "f"}}\n      ]\n    }\n  }\n}\n\n\nmatch_bool_prefix query 和 match_phrase_prefix query 之间的一个重要区别是：match_phrase_prefix query 将其 term 匹配为短语，但 match_bool_prefix query 可以在任何位置匹配其 term。\n\n上面的示例 match_bool_prefix query 查询可以匹配包含 quick brown fox 的字段，但它也可以快速匹配 brown fox。它还可以匹配包含 quick、brown 和以 f 开头的字段，出现在任何位置。\n\n\n# match_phrase query\n\nmatch_phrase query 即短语匹配，首先会把 query 内容分词，分词器可以自定义，同时文档还要满足以下两个条件才会被搜索到：\n\n 1. 分词后所有词项都要出现在该字段中（相当于 and 操作）。\n 2. 字段中的词项顺序要一致。\n\n例如，有以下 3 个文档，使用 match_phrase 查询 "How are you"，只有前两个文档会被匹配：\n\nPUT demo/_create/1\n{ "desc": "How are you" }\n\nPUT demo/_create/2\n{ "desc": "How are you, Jack?"}\n\nPUT demo/_create/3\n{ "desc": "are you"}\n\nGET demo/_search\n{\n  "query": {\n    "match_phrase": {\n      "desc": "How are you"\n    }\n  }\n}\n\n\n> 说明：\n> \n> 一个被认定为和短语 How are you 匹配的文档，必须满足以下这些要求：\n> \n>  * How、 are 和 you 需要全部出现在域中。\n>  * are 的位置应该比 How 的位置大 1 。\n>  * you 的位置应该比 How 的位置大 2 。\n\n\n# match_phrase_prefix query\n\nmatch_phrase_prefix query 和 match_phrase query 类似，只不过 match_phrase_prefix query 最后一个 term 会被作为前缀匹配。\n\nGET demo/_search\n{\n  "query": {\n    "match_phrase_prefix": {\n      "desc": "are yo"\n    }\n  }\n}\n\n\n\n# multi_match query\n\nmulti_match query 是 match query 的升级，用于搜索多个字段。\n\n示例：\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "multi_match": {\n      "query": 34.98,\n      "fields": [\n        "taxful_total_price",\n        "taxless_total_price"\n      ]\n    }\n  }\n}\n\n\nmulti_match query 的搜索字段可以使用通配符指定，示例如下：\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "multi_match": {\n      "query": 34.98,\n      "fields": [\n        "taxful_*",\n        "taxless_total_price"\n      ]\n    }\n  }\n}\n\n\n同时，也可以用指数符指定搜索字段的权重。\n\n示例：指定 taxful_total_price 字段的权重是 taxless_total_price 字段的 3 倍，命令如下：\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "multi_match": {\n      "query": 34.98,\n      "fields": [\n        "taxful_total_price^3",\n        "taxless_total_price"\n      ]\n    }\n  }\n}\n\n\n\n# combined_fields query\n\ncombined_fields query 支持搜索多个文本字段，就好像它们的内容已被索引到一个组合字段中一样。该查询会生成以 term 为中心的输入字符串视图：首先它将查询字符串解析为独立的 term，然后在所有字段中查找每个 term。当匹配结果可能跨越多个文本字段时，此查询特别有用，例如文章的标题、摘要和正文：\n\nGET /_search\n{\n  "query": {\n    "combined_fields" : {\n      "query":      "database systems",\n      "fields":     [ "title", "abstract", "body"],\n      "operator":   "and"\n    }\n  }\n}\n\n\n# 字段前缀权重\n\n字段前缀权重根据组合字段模型进行计算。例如，如果 title 字段的权重为 2，则匹配度打分时会将 title 中的每个 term 形成的组合字段，按出现两次进行打分。\n\n\n# common_terms query\n\n> 7.3.0 废弃\n\ncommon_terms query 是一种在不牺牲性能的情况下替代停用词提高搜索准确率和召回率的方案。\n\n查询中的每个词项都有一定的代价，以搜索“The brown fox”为例，query 会被解析成三个词项“the”“brown”和“fox”，每个词项都会到索引中执行一次查询。很显然包含“the”的文档非常多，相比其他词项，“the”的重要性会低很多。传统的解决方案是把“the”当作停用词处理，去除停用词之后可以减少索引大小，同时在搜索时减少对停用词的收缩。\n\n虽然停用词对文档评分影响不大，但是当停用词仍然有重要意义的时候，去除停用词就不是完美的解决方案了。如果去除停用词，就无法区分“happy”和“not happy”, “The”“To be or not to be”就不会在索引中存在，搜索的准确率和召回率就会降低。\n\ncommon_terms query 提供了一种解决方案，它把 query 分词后的词项分成重要词项（低频词项）和不重要的词项（高频词，也就是之前的停用词）。在搜索的时候，首先搜索和重要词项匹配的文档，这些文档是词项出现较少并且词项对其评分影响较大的文档。然后执行第二次查询，搜索对评分影响较小的高频词项，但是不计算所有文档的评分，而是只计算第一次查询已经匹配的文档得分。如果一个查询中只包含高频词，那么会通过 and 连接符执行一个单独的查询，换言之，会搜索所有的词项。\n\n词项是高频词还是低频词是通过 cutoff frequency 来设置阀值的，取值可以是绝对频率（频率大于 1）或者相对频率（0 ～ 1）。common_terms query 最有趣之处在于它能自适应特定领域的停用词，例如，在视频托管网站上，诸如“clip”或“video”之类的高频词项将自动表现为停用词，无须保留手动列表。\n\n例如，文档频率高于 0.1% 的词项将会被当作高频词项，词频之间可以用 low_freq_operator、high_freq_operator 参数连接。设置低频词操作符为“and”使所有的低频词都是必须搜索的，示例代码如下：\n\nGET books/_search\n{\n\t"query": {\n\t\t"common": {\n\t\t\t"body": {\n\t\t\t\t"query": "nelly the elephant as a cartoon",\n\t\t\t\t"cutoff_frequency": 0.001,\n\t\t\t\t"low_freq_operator": "and"\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n上述操作等价于：\n\nGET books/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": [\n\t\t\t  { "term": { "body": "nelly" } },\n\t\t\t  { "term": { "body": "elephant" } },\n\t\t\t  { "term": { "body": "cartoon" } }\n\t\t\t],\n\t\t\t"should": [\n\t\t\t  { "term": { "body": "the" } },\n\t\t\t  { "term": { "body": "as" } },\n\t\t\t  { "term": { "body": "a" } }\n\t\t\t]\n\t\t}\n\t}\n}\n\n\n\n# query_string query\n\nquery_string query 是与 Lucene 查询语句的语法结合非常紧密的一种查询，允许在一个查询语句中使用多个特殊条件关键字（如：AND | OR | NOT）对多个字段进行查询，建议熟悉 Lucene 查询语法的用户去使用。\n\n用户可以使用 query_string query 来创建包含通配符、跨多个字段的搜索等复杂搜索。虽然通用，但查询是严格的，如果查询字符串包含任何无效语法，则会返回错误。\n\n示例：\n\nGET /_search\n{\n  "query": {\n    "query_string": {\n      "query": "(new york city) OR (big apple)",\n      "default_field": "content"\n    }\n  }\n}\n\n\n\n# simple_query_string query\n\nsimple_query_string query 是一种适合直接暴露给用户，并且具有非常完善的查询语法的查询语句，接受 Lucene 查询语法，解析过程中发生错误不会抛出异常。\n\n虽然语法比 query_string query 更严格，但 simple_query_string query 不会返回无效语法的错误。相反，它会忽略查询字符串的任何无效部分。\n\n示例：\n\nGET /_search\n{\n  "query": {\n    "simple_query_string" : {\n        "query": "\\"fried eggs\\" +(eggplant | potato) -frittata",\n        "fields": ["title^5", "body"],\n        "default_operator": "and"\n    }\n  }\n}\n\n\n# simple_query_string 语义\n\n * +：等价于 AND 操作\n * |：等价于 OR 操作\n * -：相当于 NOT 操作\n * "：包装一些标记以表示用于搜索的短语\n * *：词尾表示前缀查询\n * ( and )：表示优先级\n * ~N：词尾表示表示编辑距离（模糊性）\n * ~N：在一个短语之后表示溢出量\n\n注意：要使用上面的字符，请使用反斜杠 / 对其进行转义。\n\n\n# 全文查询完整示例\n\n#设置 position_increment_gap\nDELETE groups\nPUT groups\n{\n  "mappings": {\n    "properties": {\n      "names":{\n        "type": "text",\n        "position_increment_gap": 0\n      }\n    }\n  }\n}\n\nGET groups/_mapping\n\nPOST groups/_doc\n{\n  "names": [ "John Water", "Water Smith"]\n}\n\nPOST groups/_search\n{\n  "query": {\n    "match_phrase": {\n      "names": {\n        "query": "Water Water",\n        "slop": 100\n      }\n    }\n  }\n}\n\nPOST groups/_search\n{\n  "query": {\n    "match_phrase": {\n      "names": "Water Smith"\n    }\n  }\n}\n\nDELETE groups\n\n\n\n# 词项查询\n\nTerm（词项）是表达语意的最小单位。搜索和利用统计语言模型进行自然语言处理都需要处理 Term。\n\n全文查询在执行查询之前会分析查询字符串。\n\n与全文查询不同，词项查询不会分词，而是将输入作为一个整体，在倒排索引中查找准确的词项。并且使用相关度计算公式为每个包含该词项的文档进行相关度计算。一言以概之：词项查询是对词项进行精确匹配。词项查询通常用于结构化数据，如数字、日期和枚举类型。\n\n词项查询有以下类型：\n\n * exists query\n * fuzzy query\n * ids query\n * prefix query\n * range query\n * regexp query\n * term query\n * terms query\n * type query\n * wildcard query\n\n\n# exists query\n\nexists query 会返回字段中至少有一个非空值的文档。\n\n由于多种原因，文档字段可能不存在索引值：\n\n * JSON 中的字段为 null 或 []\n * 该字段在 mapping 中配置了 "index" : false\n * 字段值的长度超过了 mapping 中的 ignore_above 设置\n * 字段值格式错误，并且在 mapping 中定义了 ignore_malformed\n\n示例：\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "exists": {\n      "field": "email"\n    }\n  }\n}\n\n\n以下文档会匹配上面的查询：\n\n * { "user" : "jane" } 有 user 字段，且不为空。\n * { "user" : "" } 有 user 字段，值为空字符串。\n * { "user" : "-" } 有 user 字段，值不为空。\n * { "user" : [ "jane" ] } 有 user 字段，值不为空。\n * { "user" : [ "jane", null ] } 有 user 字段，至少一个值不为空即可。\n\n下面的文档都不会被匹配：\n\n * { "user" : null } 虽然有 user 字段，但是值为空。\n * { "user" : [] } 虽然有 user 字段，但是值为空。\n * { "user" : [null] } 虽然有 user 字段，但是值为空。\n * { "foo" : "bar" } 没有 user 字段。\n\n\n# fuzzy query\n\nfuzzy query（模糊查询）返回包含与搜索词相似的词的文档。ES 使用 Levenshtein edit distance（Levenshtein 编辑距离）测量相似度或模糊度。\n\n编辑距离是将一个术语转换为另一个术语所需的单个字符更改的数量。这些变化可能包括：\n\n * 改变一个字符：（box -> fox）\n * 删除一个字符：（black -> lack）\n * 插入一个字符：（sic -> sick）\n * 反转两个相邻字符：（act → cat）\n\n为了找到相似的词条，fuzzy query 会在指定的编辑距离内创建搜索词条的所有可能变体或扩展集。然后返回完全匹配任意扩展的文档。\n\nGET books/_search\n{\n  "query": {\n    "fuzzy": {\n      "user.id": {\n        "value": "ki",\n        "fuzziness": "AUTO",\n        "max_expansions": 50,\n        "prefix_length": 0,\n        "transpositions": true,\n        "rewrite": "constant_score"\n      }\n    }\n  }\n}\n\n\n注意：如果配置了 search.allow_expensive_queries ，则 fuzzy query 不能执行。\n\n\n# ids query\n\nids query 根据 ID 返回文档。 此查询使用存储在 _id 字段中的文档 ID。\n\nGET /_search\n{\n  "query": {\n    "ids" : {\n      "values" : ["1", "4", "100"]\n    }\n  }\n}\n\n\n\n# prefix query\n\nprefix query 用于查询某个字段中包含指定前缀的文档。\n\n比如查询 user.id 中含有以 ki 为前缀的关键词的文档，那么含有 kind、kid 等所有以 ki 开头关键词的文档都会被匹配。\n\nGET /_search\n{\n  "query": {\n    "prefix": {\n      "user.id": {\n        "value": "ki"\n      }\n    }\n  }\n}\n\n\n\n# range query\n\nrange query 即范围查询，用于匹配在某一范围内的数值型、日期类型或者字符串型字段的文档。比如搜索哪些书籍的价格在 50 到 100 之间、哪些书籍的出版时间在 2015 年到 2019 年之间。使用 range 查询只能查询一个字段，不能作用在多个字段上。\n\nrange 查询支持的参数有以下几种：\n\n * gt：大于\n\n * gte：大于等于\n\n * lt：小于\n\n * lte：小于等于\n\n * format：如果字段是 Date 类型，可以设置日期格式化\n\n * time_zone：时区\n\n * relation：指示范围查询如何匹配范围字段的值。\n   \n   * INTERSECTS (Default)：匹配与查询字段值范围相交的文档。\n   * CONTAINS：匹配完全包含查询字段值的文档。\n   * WITHIN：匹配具有完全在查询范围内的范围字段值的文档。\n\n示例：数值范围查询\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "range": {\n      "taxful_total_price": {\n        "gt": 10,\n        "lte": 50\n      }\n    }\n  }\n}\n\n\n示例：日期范围查询\n\nGET kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "range": {\n      "order_date": {\n        "time_zone": "+00:00",\n        "gte": "2018-01-01T00:00:00",\n        "lte": "now"\n      }\n    }\n  }\n}\n\n\n\n# regexp query\n\nregexp query 返回与正则表达式相匹配的 term 所属的文档。\n\n正则表达式是一种使用占位符字符匹配数据模式的方法，称为运算符。\n\n示例：以下搜索返回 user.id 字段包含任何以 k 开头并以 y 结尾的文档。 .* 运算符匹配任何长度的任何字符，包括无字符。匹配项可以包括 ky、kay 和 kimchy。\n\nGET /_search\n{\n  "query": {\n    "regexp": {\n      "user.id": {\n        "value": "k.*y",\n        "flags": "ALL",\n        "case_insensitive": true,\n        "max_determinized_states": 10000,\n        "rewrite": "constant_score"\n      }\n    }\n  }\n}\n\n\n> 注意：如果配置了search.allow_expensive_queries ，则 regexp query 会被禁用。\n\n\n# term query\n\nterm query 用来查找指定字段中包含给定单词的文档，term 查询不被解析，只有查询词和文档中的词精确匹配才会被搜索到，应用场景为查询人名、地名等需要精准匹配的需求。\n\n示例：\n\n# 1. 创建一个索引\nDELETE my-index-000001\nPUT my-index-000001\n{\n  "mappings": {\n    "properties": {\n      "full_text": { "type": "text" }\n    }\n  }\n}\n\n# 2. 使用 "Quick Brown Foxes!" 关键字查 "full_text" 字段\nPUT my-index-000001/_doc/1\n{\n  "full_text": "Quick Brown Foxes!"\n}\n\n# 3. 使用 term 查询\nGET my-index-000001/_search?pretty\n{\n  "query": {\n    "term": {\n      "full_text": "Quick Brown Foxes!"\n    }\n  }\n}\n# 因为 full_text 字段不再包含确切的 Term —— "Quick Brown Foxes!"，所以 term query 搜索不到任何结果\n\n# 4. 使用 match 查询\nGET my-index-000001/_search?pretty\n{\n  "query": {\n    "match": {\n      "full_text": "Quick Brown Foxes!"\n    }\n  }\n}\n\nDELETE my-index-000001\n\n\n> ⚠️ 注意：应避免 term 查询对 text 字段使用查询。\n> \n> 默认情况下，Elasticsearch 针对 text 字段的值进行解析分词，这会使查找 text 字段值的精确匹配变得困难。\n> \n> 要搜索 text 字段值，需改用 match 查询。\n\n\n# terms query\n\nterms query 与 term query 相同，但可以搜索多个值。\n\nterms query 查询参数：\n\n * index：索引名\n * id：文档 ID\n * path：要从中获取字段值的字段的名称，即搜索关键字\n * routing（选填）：要从中获取 term 值的文档的自定义路由值。如果在索引文档时提供了自定义路由值，则此参数是必需的。\n\n示例：\n\n# 1. 创建一个索引\nDELETE my-index-000001\nPUT my-index-000001\n{\n  "mappings": {\n    "properties": {\n      "color": { "type": "keyword" }\n    }\n  }\n}\n\n# 2. 写入一个文档\nPUT my-index-000001/_doc/1\n{\n  "color": [\n    "blue",\n    "green"\n  ]\n}\n\n# 3. 写入另一个文档\nPUT my-index-000001/_doc/2\n{\n  "color": "blue"\n}\n\n# 3. 使用 terms query\nGET my-index-000001/_search?pretty\n{\n  "query": {\n    "terms": {\n      "color": {\n        "index": "my-index-000001",\n        "id": "2",\n        "path": "color"\n      }\n    }\n  }\n}\n\nDELETE my-index-000001\n\n\n\n# type query\n\n> 7.0.0 后废弃\n\ntype query 用于查询具有指定类型的文档。\n\n示例：\n\nGET /_search\n{\n  "query": {\n    "type": {\n      "value": "_doc"\n    }\n  }\n}\n\n\n\n# wildcard query\n\nwildcard query 即通配符查询，返回与通配符模式匹配的文档。\n\n? 用来匹配一个任意字符，* 用来匹配零个或者多个字符。\n\n示例：以下搜索返回 user.id 字段包含以 ki 开头并以 y 结尾的术语的文档。这些匹配项可以包括 kiy、kity 或 kimchy。\n\nGET /_search\n{\n  "query": {\n    "wildcard": {\n      "user.id": {\n        "value": "ki*y",\n        "boost": 1.0,\n        "rewrite": "constant_score"\n      }\n    }\n  }\n}\n\n\n> 注意：如果配置了search.allow_expensive_queries ，则wildcard query 会被禁用。\n\n\n# 词项查询完整示例\n\nDELETE products\nPUT products\n{\n  "settings": {\n    "number_of_shards": 1\n  }\n}\n\nPOST /products/_bulk\n{ "index": { "_id": 1 }}\n{ "productID" : "XHDK-A-1293-#fJ3","desc":"iPhone" }\n{ "index": { "_id": 2 }}\n{ "productID" : "KDKE-B-9947-#kL5","desc":"iPad" }\n{ "index": { "_id": 3 }}\n{ "productID" : "JODL-X-1937-#pV7","desc":"MBP" }\n\nGET /products\n\nPOST /products/_search\n{\n  "query": {\n    "term": {\n      "desc": {\n        //"value": "iPhone"\n        "value":"iphone"\n      }\n    }\n  }\n}\n\nPOST /products/_search\n{\n  "query": {\n    "term": {\n      "desc.keyword": {\n        //"value": "iPhone"\n        //"value":"iphone"\n      }\n    }\n  }\n}\n\nPOST /products/_search\n{\n  "query": {\n    "term": {\n      "productID": {\n        "value": "XHDK-A-1293-#fJ3"\n      }\n    }\n  }\n}\n\nPOST /products/_search\n{\n  //"explain": true,\n  "query": {\n    "term": {\n      "productID.keyword": {\n        "value": "XHDK-A-1293-#fJ3"\n      }\n    }\n  }\n}\n\nPOST /products/_search\n{\n  "explain": true,\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "productID.keyword": "XHDK-A-1293-#fJ3"\n        }\n      }\n\n    }\n  }\n}\n\n\n\n# 复合查询\n\n复合查询就是把一些简单查询组合在一起实现更复杂的查询需求，除此之外，复合查询还可以控制另外一个查询的行为。\n\n\n# bool query\n\nbool 查询可以把任意多个简单查询组合在一起，使用 must、should、must_not、filter 选项来表示简单查询之间的逻辑，每个选项都可以出现 0 次到多次，它们的含义如下：\n\n * must 文档必须匹配 must 选项下的查询条件，相当于逻辑运算的 AND，且参与文档相关度的评分。\n * should 文档可以匹配 should 选项下的查询条件也可以不匹配，相当于逻辑运算的 OR，且参与文档相关度的评分。\n * must_not 与 must 相反，匹配该选项下的查询条件的文档不会被返回；需要注意的是，must_not 语句不会影响评分，它的作用只是将不相关的文档排除。\n * filter 和 must 一样，匹配 filter 选项下的查询条件的文档才会被返回，但是 filter 不评分，只起到过滤功能，与 must_not 相反。\n\n假设要查询 title 中包含关键词 java，并且 price 不能高于 70，description 可以包含也可以不包含虚拟机的书籍，构造 bool 查询语句如下：\n\nGET books/_search\n{\n  "query": {\n    "bool": {\n      "filter": {\n        "term": {\n          "status": 1\n        }\n      },\n      "must_not": {\n        "range": {\n          "price": {\n            "gte": 70\n          }\n        }\n      },\n      "must": {\n        "match": {\n          "title": "java"\n        }\n      },\n      "should": [\n        {\n          "match": {\n            "description": "虚拟机"\n          }\n        }\n      ],\n      "minimum_should_match": 1\n    }\n  }\n}\n\n\n有关布尔查询更详细的信息参考 bool query（组合查询）详解。\n\n\n# boosting query\n\nboosting 查询用于需要对两个查询的评分进行调整的场景，boosting 查询会把两个查询封装在一起并降低其中一个查询的评分。\n\nboosting 查询包括 positive、negative 和 negative_boost 三个部分，positive 中的查询评分保持不变，negative 中的查询会降低文档评分，negative_boost 指明 negative 中降低的权值。如果我们想对 2015 年之前出版的书降低评分，可以构造一个 boosting 查询，查询语句如下：\n\nGET books/_search\n{\n\t"query": {\n\t\t"boosting": {\n\t\t\t"positive": {\n\t\t\t\t"match": {\n\t\t\t\t\t"title": "python"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"negative": {\n\t\t\t\t"range": {\n\t\t\t\t\t"publish_time": {\n\t\t\t\t\t\t"lte": "2015-01-01"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t"negative_boost": 0.2\n\t\t}\n\t}\n}\n\n\nboosting 查询中指定了抑制因子为 0.2，publish_time 的值在 2015-01-01 之后的文档得分不变，publish_time 的值在 2015-01-01 之前的文档得分为原得分的 0.2 倍。\n\n\n# constant_score query\n\nconstantscore query 包装一个 filter query，并返回匹配过滤器查询条件的文档，且它们的相关性评分都等于 _boost 参数值（可以理解为原有的基于 tf-idf 或 bm25 的相关分固定为 1.0，所以最终评分为 1.0 * boost，即等于 boost 参数值）。下面的查询语句会返回 title 字段中含有关键词 elasticsearch 的文档，所有文档的评分都是 1.8：\n\nGET books/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "title": "elasticsearch"\n        }\n      },\n      "boost": 1.8\n    }\n  }\n}\n\n\n\n# dis_max query\n\ndis_max query 与 bool query 有一定联系也有一定区别，dis_max query 支持多并发查询，可返回与任意查询条件子句匹配的任何文档类型。与 bool 查询可以将所有匹配查询的分数相结合使用的方式不同，dis_max 查询只使用最佳匹配查询条件的分数。请看下面的例子：\n\nGET books/_search\n{\n\t"query": {\n\t\t"dis_max": {\n\t\t\t"tie_breaker": 0.7,\n\t\t\t"boost": 1.2,\n\t\t\t"queries": [{\n\t\t\t\t\t"term": {\n\t\t\t\t\t\t"age": 34\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t"term": {\n\t\t\t\t\t\t"age": 35\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t}\n}\n\n\n\n# function_score query\n\nfunction_score query 可以修改查询的文档得分，这个查询在有些情况下非常有用，比如通过评分函数计算文档得分代价较高，可以改用过滤器加自定义评分函数的方式来取代传统的评分方式。\n\n使用 function_score query，用户需要定义一个查询和一至多个评分函数，评分函数会对查询到的每个文档分别计算得分。\n\n下面这条查询语句会返回 books 索引中的所有文档，文档的最大得分为 5，每个文档的得分随机生成，权重的计算模式为相乘模式。\n\nGET books/_search\n{\n  "query": {\n    "function_score": {\n      "query": {\n        "match all": {}\n      },\n      "boost": "5",\n      "random_score": {},\n      "boost_mode": "multiply"\n    }\n  }\n}\n\n\n使用脚本自定义评分公式，这里把 price 值的十分之一开方作为每个文档的得分，查询语句如下：\n\nGET books/_search\n{\n  "query": {\n    "function_score": {\n      "query": {\n        "match": {\n          "title": "java"\n        }\n      },\n      "script_score": {\n        "inline": "Math.sqrt(doc[\'price\'].value/10)"\n      }\n    }\n  }\n}\n\n\n关于 function_score 的更多详细内容请查看 Elasticsearch function_score 查询最强详解。\n\n\n# indices query\n\nindices query 适用于需要在多个索引之间进行查询的场景，它允许指定一个索引名字列表和内部查询。indices query 中有 query 和 no_match_query 两部分，query 中用于搜索指定索引列表中的文档，no_match_query 中的查询条件用于搜索指定索引列表之外的文档。下面的查询语句实现了搜索索引 books、books2 中 title 字段包含关键字 javascript，其他索引中 title 字段包含 basketball 的文档，查询语句如下：\n\nGET books/_search\n{\n\t"query": {\n\t\t"indices": {\n\t\t\t"indices": ["books", "books2"],\n\t\t\t"query": {\n\t\t\t\t"match": {\n\t\t\t\t\t"title": "javascript"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"no_match_query": {\n\t\t\t\t"term": {\n\t\t\t\t\t"title": "basketball"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 嵌套查询\n\n在 Elasticsearch 这样的分布式系统中执行全 SQL 风格的连接查询代价昂贵，是不可行的。相应地，为了实现水平规模地扩展，Elasticsearch 提供了以下两种形式的 join：\n\n * nested query（嵌套查询）\n   \n   文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来。\n\n * has_child query（有子查询）和 has_parent query（有父查询）\n   \n   父子关系可以存在单个的索引的两个类型的文档之间。has_child 查询将返回其子文档能满足特定查询的父文档，而 has_parent 则返回其父文档能满足特定查询的子文档。\n\n\n# nested query\n\n文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来（用嵌套查询）。\n\nPUT /my_index\n{\n\t"mappings": {\n\t\t"type1": {\n\t\t\t"properties": {\n\t\t\t\t"obj1": {\n\t\t\t\t\t"type": "nested"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# has_child query\n\n文档的父子关系创建索引时在映射中声明，这里以员工（employee）和工作城市（branch）为例，它们属于不同的类型，相当于数据库中的两张表，如果想把员工和他们工作的城市关联起来，需要告诉 Elasticsearch 文档之间的父子关系，这里 employee 是 child type，branch 是 parent type，在映射中声明，执行命令：\n\nPUT /company\n{\n\t"mappings": {\n\t\t"branch": {},\n\t\t"employee": {\n\t\t\t"parent": { "type": "branch" }\n\t\t}\n\t}\n}\n\n\n使用 bulk api 索引 branch 类型下的文档，命令如下：\n\nPOST company/branch/_bulk\n{ "index": { "_id": "london" }}\n{ "name": "London Westminster","city": "London","country": "UK" }\n{ "index": { "_id": "liverpool" }}\n{ "name": "Liverpool Central","city": "Liverpool","country": "UK" }\n{ "index": { "_id": "paris" }}\n{ "name": "Champs Elysees","city": "Paris","country": "France" }\n\n\n添加员工数据：\n\nPOST company/employee/_bulk\n{ "index": { "_id": 1,"parent":"london" }}\n{ "name": "Alice Smith","dob": "1970-10-24","hobby": "hiking" }\n{ "index": { "_id": 2,"parent":"london" }}\n{ "name": "Mark Tomas","dob": "1982-05-16","hobby": "diving" }\n{ "index": { "_id": 3,"parent":"liverpool" }}\n{ "name": "Barry Smith","dob": "1979-04-01","hobby": "hiking" }\n{ "index": { "_id": 4,"parent":"paris" }}\n{ "name": "Adrien Grand","dob": "1987-05-11","hobby": "horses" }\n\n\n通过子文档查询父文档要使用 has_child 查询。例如，搜索 1980 年以后出生的员工所在的分支机构，employee 中 1980 年以后出生的有 Mark Thomas 和 Adrien Grand，他们分别在 london 和 paris，执行以下查询命令进行验证：\n\nGET company/branch/_search\n{\n\t"query": {\n\t\t"has_child": {\n\t\t\t"type": "employee",\n\t\t\t"query": {\n\t\t\t\t"range": { "dob": { "gte": "1980-01-01" } }\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n搜索哪些机构中有名为 “Alice Smith” 的员工，因为使用 match 查询，会解析为 “Alice” 和 “Smith”，所以 Alice Smith 和 Barry Smith 所在的机构会被匹配，执行以下查询命令进行验证：\n\nGET company/branch/_search\n{\n\t"query": {\n\t\t"has_child": {\n\t\t\t"type": "employee",\n\t\t\t"score_mode": "max",\n\t\t\t"query": {\n\t\t\t\t"match": { "name": "Alice Smith" }\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n可以使用 min_children 指定子文档的最小个数。例如，搜索最少含有两个 employee 的机构，查询命令如下：\n\nGET company/branch/_search?pretty\n{\n\t"query": {\n\t\t"has_child": {\n\t\t\t"type": "employee",\n\t\t\t"min_children": 2,\n\t\t\t"query": {\n\t\t\t\t"match_all": {}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# has_parent query\n\n通过父文档查询子文档使用 has_parent 查询。比如，搜索哪些 employee 工作在 UK，查询命令如下：\n\nGET company/employee/_search\n{\n\t"query": {\n\t\t"has_parent": {\n\t\t\t"parent_type": "branch",\n\t\t\t"query": {\n\t\t\t\t"match": { "country": "UK }\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 位置查询\n\nElasticsearch 可以对地理位置点 geo_point 类型和地理位置形状 geo_shape 类型的数据进行搜索。为了学习方便，这里准备一些城市的地理坐标作为测试数据，每一条文档都包含城市名称和地理坐标这两个字段，这里的坐标点取的是各个城市中心的一个位置。首先把下面的内容保存到 geo.json 文件中：\n\n{"index":{ "_index":"geo","_type":"city","_id":"1" }}\n{"name":"北京","location":"39.9088145109,116.3973999023"}\n{"index":{ "_index":"geo","_type":"city","_id": "2" }}\n{"name":"乌鲁木齐","location":"43.8266300000,87.6168800000"}\n{"index":{ "_index":"geo","_type":"city","_id":"3" }}\n{"name":"西安","location":"34.3412700000,108.9398400000"}\n{"index":{ "_index":"geo","_type":"city","_id":"4" }}\n{"name":"郑州","location":"34.7447157466,113.6587142944"}\n{"index":{ "_index":"geo","_type":"city","_id":"5" }}\n{"name":"杭州","location":"30.2294080260,120.1492309570"}\n{"index":{ "_index":"geo","_type":"city","_id":"6" }}\n{"name":"济南","location":"36.6518400000,117.1200900000"}\n\n\n创建一个索引并设置映射：\n\nPUT geo\n{\n\t"mappings": {\n\t\t"city": {\n\t\t\t"properties": {\n\t\t\t\t"name": {\n\t\t\t\t\t"type": "keyword"\n\t\t\t\t},\n\t\t\t\t"location": {\n\t\t\t\t\t"type": "geo_point"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n然后执行批量导入命令：\n\ncurl -XPOST "http://localhost:9200/_bulk?pretty" --data-binary @geo.json\n\n\n\n# geo_distance query\n\ngeo_distance query 可以查找在一个中心点指定范围内的地理点文档。例如，查找距离天津 200km 以内的城市，搜索结果中会返回北京，命令如下：\n\nGET geo/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"geo_distance": {\n\t\t\t\t\t"distance": "200km",\n\t\t\t\t\t"location": {\n\t\t\t\t\t\t"lat": 39.0851000000,\n\t\t\t\t\t\t"lon": 117.1993700000\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n按各城市离北京的距离排序：\n\nGET geo/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [{\n    "_geo_distance": {\n      "location": "39.9088145109,116.3973999023",\n      "unit": "km",\n      "order": "asc",\n      "distance_type": "plane"\n    }\n  }]\n}\n\n\n其中 location 对应的经纬度字段；unit 为 km 表示将距离以 km 为单位写入到每个返回结果的 sort 键中；distance_type 为 plane 表示使用快速但精度略差的 plane 计算方式。\n\n\n# geo_bounding_box query\n\ngeo_bounding_box query 用于查找落入指定的矩形内的地理坐标。查询中由两个点确定一个矩形，然后在矩形区域内查询匹配的文档。\n\nGET geo/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"geo_bounding_box": {\n\t\t\t\t\t"location": {\n\t\t\t\t\t\t"top_left": {\n\t\t\t\t\t\t\t"lat": 38.4864400000,\n\t\t\t\t\t\t\t"lon": 106.2324800000\n\t\t\t\t\t\t},\n\t\t\t\t\t\t"bottom_right": {\n\t\t\t\t\t\t\t"lat": 28.6820200000,\n\t\t\t\t\t\t\t"lon": 115.8579400000\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# geo_polygon query\n\ngeo_polygon query 用于查找在指定多边形内的地理点。例如，呼和浩特、重庆、上海三地组成一个三角形，查询位置在该三角形区域内的城市，命令如下：\n\nGET geo/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t}\n\t\t},\n\t\t"filter": {\n\t\t\t"geo_polygon": {\n\t\t\t\t"location": {\n\t\t\t\t\t"points": [{\n\t\t\t\t\t\t"lat": 40.8414900000,\n\t\t\t\t\t\t"lon": 111.7519900000\n\t\t\t\t\t}, {\n\t\t\t\t\t\t"lat": 29.5647100000,\n\t\t\t\t\t\t"lon": 106.5507300000\n\t\t\t\t\t}, {\n\t\t\t\t\t\t"lat": 31.2303700000,\n\t\t\t\t\t\t"lon": 121.4737000000\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# geo_shape query\n\ngeo_shape query 用于查询 geo_shape 类型的地理数据，地理形状之间的关系有相交、包含、不相交三种。创建一个新的索引用于测试，其中 location 字段的类型设为 geo_shape 类型。\n\nPUT geoshape\n{\n\t"mappings": {\n\t\t"city": {\n\t\t\t"properties": {\n\t\t\t\t"name": {\n\t\t\t\t\t"type": "keyword"\n\t\t\t\t},\n\t\t\t\t"location": {\n\t\t\t\t\t"type": "geo_shape"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n关于经纬度的顺序这里做一个说明，geo_point 类型的字段纬度在前经度在后，但是对于 geo_shape 类型中的点，是经度在前纬度在后，这一点需要特别注意。\n\n把西安和郑州连成的线写入索引：\n\nPOST geoshape/city/1\n{\n\t"name": "西安-郑州",\n\t"location": {\n\t\t"type": "linestring",\n\t\t"coordinates": [\n\t\t\t[108.9398400000, 34.3412700000],\n\t\t\t[113.6587142944, 34.7447157466]\n\t\t]\n\t}\n}\n\n\n查询包含在由银川和南昌作为对角线上的点组成的矩形的地理形状，由于西安和郑州组成的直线落在该矩形区域内，因此可以被查询到。命令如下：\n\nGET geoshape/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"geo_shape": {\n\t\t\t\t\t"location": {\n\t\t\t\t\t\t"shape": {\n\t\t\t\t\t\t\t"type": "envelope",\n\t\t\t\t\t\t\t"coordinates": [\n\t\t\t\t\t\t\t\t[106.23248, 38.48644],\n\t\t\t\t\t\t\t\t[115.85794, 28.68202]\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t"relation": "within"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 特殊查询\n\n\n# more_like_this query\n\nmore_like_this query 可以查询和提供文本类似的文档，通常用于近似文本的推荐等场景。查询命令如下：\n\nGET books/_search\n{\n\t"query": {\n\t\t"more_like_ this": {\n\t\t\t"fields": ["title", "description"],\n\t\t\t"like": "java virtual machine",\n\t\t\t"min_term_freq": 1,\n\t\t\t"max_query_terms": 12\n\t\t}\n\t}\n}\n\n\n可选的参数及取值说明如下：\n\n * fields 要匹配的字段，默认是 _all 字段。\n * like 要匹配的文本。\n * min_term_freq 文档中词项的最低频率，默认是 2，低于此频率的文档会被忽略。\n * max_query_terms query 中能包含的最大词项数目，默认为 25。\n * min_doc_freq 最小的文档频率，默认为 5。\n * max_doc_freq 最大文档频率。\n * min_word length 单词的最小长度。\n * max_word length 单词的最大长度。\n * stop_words 停用词列表。\n * analyzer 分词器。\n * minimum_should_match 文档应匹配的最小词项数，默认为 query 分词后词项数的 30%。\n * boost terms 词项的权重。\n * include 是否把输入文档作为结果返回。\n * boost 整个 query 的权重，默认为 1.0。\n\n\n# script query\n\nElasticsearch 支持使用脚本进行查询。例如，查询价格大于 180 的文档，命令如下：\n\nGET books/_search\n{\n  "query": {\n    "script": {\n      "script": {\n        "inline": "doc[\'price\'].value > 180",\n        "lang": "painless"\n      }\n    }\n  }\n}\n\n\n\n# percolate query\n\n一般情况下，我们是先把文档写入到 Elasticsearch 中，通过查询语句对文档进行搜索。percolate query 则是反其道而行之的做法，它会先注册查询条件，根据文档来查询 query。例如，在 my-index 索引中有一个 laptop 类型，文档有 price 和 name 两个字段，在映射中声明一个 percolator 类型的 query，命令如下：\n\nPUT my-index\n{\n\t"mappings": {\n\t\t"laptop": {\n\t\t\t"properties": {\n\t\t\t\t"price": { "type": "long" },\n\t\t\t\t"name": { "type": "text" }\n\t\t\t},\n\t\t\t"queries": {\n\t\t\t\t"properties": {\n\t\t\t\t\t"query": { "type": "percolator" }\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n注册一个 bool query，bool query 中包含一个 range query，要求 price 字段的取值小于等于 10000，并且 name 字段中含有关键词 macbook：\n\nPUT /my-index/queries/1?refresh\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": [{\n\t\t\t\t"range": { "price": { "lte": 10000 } }\n\t\t\t}, {\n\t\t\t\t"match": { "name": "macbook" }\n\t\t\t}]\n\t\t}\n\t}\n}\n\n\n通过文档查询 query：\n\nGET /my-index/_search\n{\n\t"query": {\n\t\t"percolate": {\n\t\t\t"field": "query",\n\t\t\t"document_type": "laptop",\n\t\t\t"document": {\n\t\t\t\t"price": 9999,\n\t\t\t\t"name": "macbook pro on sale"\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n文档符合 query 中的条件，返回结果中可以查到上文中注册的 bool query。percolate query 的这种特性适用于数据分类、数据路由、事件监控和预警等场景。',normalizedContent:'# elasticsearch 查询\n\nelasticsearch 查询语句采用基于 restful 风格的接口封装成 json 格式的对象，称之为 query dsl。elasticsearch 查询分类大致分为全文查询、词项查询、复合查询、嵌套查询、位置查询、特殊查询。elasticsearch 查询从机制分为两种，一种是根据用户输入的查询词，通过排序模型计算文档与查询词之间的相关度，并根据评分高低排序返回；另一种是过滤机制，只根据过滤条件对文档进行过滤，不计算评分，速度相对较快。\n\n\n# 全文查询\n\nes 全文查询主要用于在全文字段上，主要考虑查询词与文档的相关性（relevance）。\n\n\n# intervals query\n\nintervals query 根据匹配词的顺序和近似度返回文档。\n\nintervals query 使用匹配规则，这些规则应用于指定字段中的 term。\n\n示例：下面示例搜索 query 字段，搜索值是 my favorite food，没有任何间隙；然后是 my_text 字段搜索匹配 hot water、cold porridge 的 term。\n\n当 my_text 中的值为 my favorite food is cold porridge 时，会匹配成功，但是 when it\'s cold my favorite food is porridge 则匹配失败\n\npost _search\n{\n  "query": {\n    "intervals" : {\n      "my_text" : {\n        "all_of" : {\n          "ordered" : true,\n          "intervals" : [\n            {\n              "match" : {\n                "query" : "my favorite food",\n                "max_gaps" : 0,\n                "ordered" : true\n              }\n            },\n            {\n              "any_of" : {\n                "intervals" : [\n                  { "match" : { "query" : "hot water" } },\n                  { "match" : { "query" : "cold porridge" } }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n\n\n\n# match query\n\nmatch query 用于搜索单个字段，首先会针对查询语句进行解析（经过 analyzer），主要是对查询语句进行分词，分词后查询语句的任何一个词项被匹配，文档就会被搜到，默认情况下相当于对分词后词项进行 or 匹配操作。\n\nmatch query 是执行全文搜索的标准查询，包括模糊匹配选项。\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "match": {\n      "customer_full_name": {\n        "query": "george hubbard"\n      }\n    }\n  }\n}\n\n\n等同于 or 匹配操作，如下：\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "match": {\n      "customer_full_name": {\n        "query": "george hubbard",\n        "operator": "or"\n      }\n    }\n  }\n}\n\n\n# match query 简写\n\n可以通过组合 <field> 和 query 参数来简化匹配查询语法。\n\n示例：\n\nget /_search\n{\n  "query": {\n    "match": {\n      "message": "this is a test"\n    }\n  }\n}\n\n\n# match query 如何工作\n\n匹配查询是布尔类型。这意味着会对提供的文本进行分析，分析过程从提供的文本构造一个布尔查询。 operator 参数可以设置为 or 或 and 来控制布尔子句（默认为 or）。可以使用 minimum_should_match 参数设置要匹配的可选 should 子句的最小数量。\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "match": {\n      "customer_full_name": {\n        "query": "george hubbard",\n        "operator": "and"\n      }\n    }\n  }\n}\n\n\n可以设置 analyzer 来控制哪个分析器将对文本执行分析过程。它默认为字段显式映射定义或默认搜索分析器。\n\nlenient 参数可以设置为 true 以忽略由数据类型不匹配导致的异常，例如尝试使用文本查询字符串查询数字字段。默认为 false。\n\n# match query 的模糊查询\n\nfuzziness 允许基于被查询字段的类型进行模糊匹配。请参阅 fuzziness 的配置。\n\n在这种情况下可以设置 prefix_length 和 max_expansions 来控制模糊匹配。如果设置了模糊选项，查询将使用 top_terms_blended_freqs_${max_expansions} 作为其重写方法，fuzzy_rewrite 参数允许控制查询将如何被重写。\n\n默认情况下允许模糊倒转 (ab → ba)，但可以通过将 fuzzy_transpositions 设置为 false 来禁用。\n\nget /_search\n{\n  "query": {\n    "match": {\n      "message": {\n        "query": "this is a testt",\n        "fuzziness": "auto"\n      }\n    }\n  }\n}\n\n\n# zero terms 查询\n\n如果使用的分析器像 stop 过滤器一样删除查询中的所有标记，则默认行为是不匹配任何文档。可以使用 zero_terms_query 选项来改变默认行为，它接受 none（默认）和 all （相当于 match_all 查询）。\n\nget /_search\n{\n  "query": {\n    "match": {\n      "message": {\n        "query": "to be or not to be",\n        "operator": "and",\n        "zero_terms_query": "all"\n      }\n    }\n  }\n}\n\n\n\n# match_bool_prefix query\n\nmatch_bool_prefix query 分析其输入并根据这些词构造一个布尔查询。除了最后一个术语之外的每个术语都用于术语查询。最后一个词用于 prefix query。\n\n示例：\n\nget /_search\n{\n  "query": {\n    "match_bool_prefix" : {\n      "message" : "quick brown f"\n    }\n  }\n}\n\n\n等价于\n\nget /_search\n{\n  "query": {\n    "bool" : {\n      "should": [\n        { "term": { "message": "quick" }},\n        { "term": { "message": "brown" }},\n        { "prefix": { "message": "f"}}\n      ]\n    }\n  }\n}\n\n\nmatch_bool_prefix query 和 match_phrase_prefix query 之间的一个重要区别是：match_phrase_prefix query 将其 term 匹配为短语，但 match_bool_prefix query 可以在任何位置匹配其 term。\n\n上面的示例 match_bool_prefix query 查询可以匹配包含 quick brown fox 的字段，但它也可以快速匹配 brown fox。它还可以匹配包含 quick、brown 和以 f 开头的字段，出现在任何位置。\n\n\n# match_phrase query\n\nmatch_phrase query 即短语匹配，首先会把 query 内容分词，分词器可以自定义，同时文档还要满足以下两个条件才会被搜索到：\n\n 1. 分词后所有词项都要出现在该字段中（相当于 and 操作）。\n 2. 字段中的词项顺序要一致。\n\n例如，有以下 3 个文档，使用 match_phrase 查询 "how are you"，只有前两个文档会被匹配：\n\nput demo/_create/1\n{ "desc": "how are you" }\n\nput demo/_create/2\n{ "desc": "how are you, jack?"}\n\nput demo/_create/3\n{ "desc": "are you"}\n\nget demo/_search\n{\n  "query": {\n    "match_phrase": {\n      "desc": "how are you"\n    }\n  }\n}\n\n\n> 说明：\n> \n> 一个被认定为和短语 how are you 匹配的文档，必须满足以下这些要求：\n> \n>  * how、 are 和 you 需要全部出现在域中。\n>  * are 的位置应该比 how 的位置大 1 。\n>  * you 的位置应该比 how 的位置大 2 。\n\n\n# match_phrase_prefix query\n\nmatch_phrase_prefix query 和 match_phrase query 类似，只不过 match_phrase_prefix query 最后一个 term 会被作为前缀匹配。\n\nget demo/_search\n{\n  "query": {\n    "match_phrase_prefix": {\n      "desc": "are yo"\n    }\n  }\n}\n\n\n\n# multi_match query\n\nmulti_match query 是 match query 的升级，用于搜索多个字段。\n\n示例：\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "multi_match": {\n      "query": 34.98,\n      "fields": [\n        "taxful_total_price",\n        "taxless_total_price"\n      ]\n    }\n  }\n}\n\n\nmulti_match query 的搜索字段可以使用通配符指定，示例如下：\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "multi_match": {\n      "query": 34.98,\n      "fields": [\n        "taxful_*",\n        "taxless_total_price"\n      ]\n    }\n  }\n}\n\n\n同时，也可以用指数符指定搜索字段的权重。\n\n示例：指定 taxful_total_price 字段的权重是 taxless_total_price 字段的 3 倍，命令如下：\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "multi_match": {\n      "query": 34.98,\n      "fields": [\n        "taxful_total_price^3",\n        "taxless_total_price"\n      ]\n    }\n  }\n}\n\n\n\n# combined_fields query\n\ncombined_fields query 支持搜索多个文本字段，就好像它们的内容已被索引到一个组合字段中一样。该查询会生成以 term 为中心的输入字符串视图：首先它将查询字符串解析为独立的 term，然后在所有字段中查找每个 term。当匹配结果可能跨越多个文本字段时，此查询特别有用，例如文章的标题、摘要和正文：\n\nget /_search\n{\n  "query": {\n    "combined_fields" : {\n      "query":      "database systems",\n      "fields":     [ "title", "abstract", "body"],\n      "operator":   "and"\n    }\n  }\n}\n\n\n# 字段前缀权重\n\n字段前缀权重根据组合字段模型进行计算。例如，如果 title 字段的权重为 2，则匹配度打分时会将 title 中的每个 term 形成的组合字段，按出现两次进行打分。\n\n\n# common_terms query\n\n> 7.3.0 废弃\n\ncommon_terms query 是一种在不牺牲性能的情况下替代停用词提高搜索准确率和召回率的方案。\n\n查询中的每个词项都有一定的代价，以搜索“the brown fox”为例，query 会被解析成三个词项“the”“brown”和“fox”，每个词项都会到索引中执行一次查询。很显然包含“the”的文档非常多，相比其他词项，“the”的重要性会低很多。传统的解决方案是把“the”当作停用词处理，去除停用词之后可以减少索引大小，同时在搜索时减少对停用词的收缩。\n\n虽然停用词对文档评分影响不大，但是当停用词仍然有重要意义的时候，去除停用词就不是完美的解决方案了。如果去除停用词，就无法区分“happy”和“not happy”, “the”“to be or not to be”就不会在索引中存在，搜索的准确率和召回率就会降低。\n\ncommon_terms query 提供了一种解决方案，它把 query 分词后的词项分成重要词项（低频词项）和不重要的词项（高频词，也就是之前的停用词）。在搜索的时候，首先搜索和重要词项匹配的文档，这些文档是词项出现较少并且词项对其评分影响较大的文档。然后执行第二次查询，搜索对评分影响较小的高频词项，但是不计算所有文档的评分，而是只计算第一次查询已经匹配的文档得分。如果一个查询中只包含高频词，那么会通过 and 连接符执行一个单独的查询，换言之，会搜索所有的词项。\n\n词项是高频词还是低频词是通过 cutoff frequency 来设置阀值的，取值可以是绝对频率（频率大于 1）或者相对频率（0 ～ 1）。common_terms query 最有趣之处在于它能自适应特定领域的停用词，例如，在视频托管网站上，诸如“clip”或“video”之类的高频词项将自动表现为停用词，无须保留手动列表。\n\n例如，文档频率高于 0.1% 的词项将会被当作高频词项，词频之间可以用 low_freq_operator、high_freq_operator 参数连接。设置低频词操作符为“and”使所有的低频词都是必须搜索的，示例代码如下：\n\nget books/_search\n{\n\t"query": {\n\t\t"common": {\n\t\t\t"body": {\n\t\t\t\t"query": "nelly the elephant as a cartoon",\n\t\t\t\t"cutoff_frequency": 0.001,\n\t\t\t\t"low_freq_operator": "and"\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n上述操作等价于：\n\nget books/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": [\n\t\t\t  { "term": { "body": "nelly" } },\n\t\t\t  { "term": { "body": "elephant" } },\n\t\t\t  { "term": { "body": "cartoon" } }\n\t\t\t],\n\t\t\t"should": [\n\t\t\t  { "term": { "body": "the" } },\n\t\t\t  { "term": { "body": "as" } },\n\t\t\t  { "term": { "body": "a" } }\n\t\t\t]\n\t\t}\n\t}\n}\n\n\n\n# query_string query\n\nquery_string query 是与 lucene 查询语句的语法结合非常紧密的一种查询，允许在一个查询语句中使用多个特殊条件关键字（如：and | or | not）对多个字段进行查询，建议熟悉 lucene 查询语法的用户去使用。\n\n用户可以使用 query_string query 来创建包含通配符、跨多个字段的搜索等复杂搜索。虽然通用，但查询是严格的，如果查询字符串包含任何无效语法，则会返回错误。\n\n示例：\n\nget /_search\n{\n  "query": {\n    "query_string": {\n      "query": "(new york city) or (big apple)",\n      "default_field": "content"\n    }\n  }\n}\n\n\n\n# simple_query_string query\n\nsimple_query_string query 是一种适合直接暴露给用户，并且具有非常完善的查询语法的查询语句，接受 lucene 查询语法，解析过程中发生错误不会抛出异常。\n\n虽然语法比 query_string query 更严格，但 simple_query_string query 不会返回无效语法的错误。相反，它会忽略查询字符串的任何无效部分。\n\n示例：\n\nget /_search\n{\n  "query": {\n    "simple_query_string" : {\n        "query": "\\"fried eggs\\" +(eggplant | potato) -frittata",\n        "fields": ["title^5", "body"],\n        "default_operator": "and"\n    }\n  }\n}\n\n\n# simple_query_string 语义\n\n * +：等价于 and 操作\n * |：等价于 or 操作\n * -：相当于 not 操作\n * "：包装一些标记以表示用于搜索的短语\n * *：词尾表示前缀查询\n * ( and )：表示优先级\n * ~n：词尾表示表示编辑距离（模糊性）\n * ~n：在一个短语之后表示溢出量\n\n注意：要使用上面的字符，请使用反斜杠 / 对其进行转义。\n\n\n# 全文查询完整示例\n\n#设置 position_increment_gap\ndelete groups\nput groups\n{\n  "mappings": {\n    "properties": {\n      "names":{\n        "type": "text",\n        "position_increment_gap": 0\n      }\n    }\n  }\n}\n\nget groups/_mapping\n\npost groups/_doc\n{\n  "names": [ "john water", "water smith"]\n}\n\npost groups/_search\n{\n  "query": {\n    "match_phrase": {\n      "names": {\n        "query": "water water",\n        "slop": 100\n      }\n    }\n  }\n}\n\npost groups/_search\n{\n  "query": {\n    "match_phrase": {\n      "names": "water smith"\n    }\n  }\n}\n\ndelete groups\n\n\n\n# 词项查询\n\nterm（词项）是表达语意的最小单位。搜索和利用统计语言模型进行自然语言处理都需要处理 term。\n\n全文查询在执行查询之前会分析查询字符串。\n\n与全文查询不同，词项查询不会分词，而是将输入作为一个整体，在倒排索引中查找准确的词项。并且使用相关度计算公式为每个包含该词项的文档进行相关度计算。一言以概之：词项查询是对词项进行精确匹配。词项查询通常用于结构化数据，如数字、日期和枚举类型。\n\n词项查询有以下类型：\n\n * exists query\n * fuzzy query\n * ids query\n * prefix query\n * range query\n * regexp query\n * term query\n * terms query\n * type query\n * wildcard query\n\n\n# exists query\n\nexists query 会返回字段中至少有一个非空值的文档。\n\n由于多种原因，文档字段可能不存在索引值：\n\n * json 中的字段为 null 或 []\n * 该字段在 mapping 中配置了 "index" : false\n * 字段值的长度超过了 mapping 中的 ignore_above 设置\n * 字段值格式错误，并且在 mapping 中定义了 ignore_malformed\n\n示例：\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "exists": {\n      "field": "email"\n    }\n  }\n}\n\n\n以下文档会匹配上面的查询：\n\n * { "user" : "jane" } 有 user 字段，且不为空。\n * { "user" : "" } 有 user 字段，值为空字符串。\n * { "user" : "-" } 有 user 字段，值不为空。\n * { "user" : [ "jane" ] } 有 user 字段，值不为空。\n * { "user" : [ "jane", null ] } 有 user 字段，至少一个值不为空即可。\n\n下面的文档都不会被匹配：\n\n * { "user" : null } 虽然有 user 字段，但是值为空。\n * { "user" : [] } 虽然有 user 字段，但是值为空。\n * { "user" : [null] } 虽然有 user 字段，但是值为空。\n * { "foo" : "bar" } 没有 user 字段。\n\n\n# fuzzy query\n\nfuzzy query（模糊查询）返回包含与搜索词相似的词的文档。es 使用 levenshtein edit distance（levenshtein 编辑距离）测量相似度或模糊度。\n\n编辑距离是将一个术语转换为另一个术语所需的单个字符更改的数量。这些变化可能包括：\n\n * 改变一个字符：（box -> fox）\n * 删除一个字符：（black -> lack）\n * 插入一个字符：（sic -> sick）\n * 反转两个相邻字符：（act → cat）\n\n为了找到相似的词条，fuzzy query 会在指定的编辑距离内创建搜索词条的所有可能变体或扩展集。然后返回完全匹配任意扩展的文档。\n\nget books/_search\n{\n  "query": {\n    "fuzzy": {\n      "user.id": {\n        "value": "ki",\n        "fuzziness": "auto",\n        "max_expansions": 50,\n        "prefix_length": 0,\n        "transpositions": true,\n        "rewrite": "constant_score"\n      }\n    }\n  }\n}\n\n\n注意：如果配置了 search.allow_expensive_queries ，则 fuzzy query 不能执行。\n\n\n# ids query\n\nids query 根据 id 返回文档。 此查询使用存储在 _id 字段中的文档 id。\n\nget /_search\n{\n  "query": {\n    "ids" : {\n      "values" : ["1", "4", "100"]\n    }\n  }\n}\n\n\n\n# prefix query\n\nprefix query 用于查询某个字段中包含指定前缀的文档。\n\n比如查询 user.id 中含有以 ki 为前缀的关键词的文档，那么含有 kind、kid 等所有以 ki 开头关键词的文档都会被匹配。\n\nget /_search\n{\n  "query": {\n    "prefix": {\n      "user.id": {\n        "value": "ki"\n      }\n    }\n  }\n}\n\n\n\n# range query\n\nrange query 即范围查询，用于匹配在某一范围内的数值型、日期类型或者字符串型字段的文档。比如搜索哪些书籍的价格在 50 到 100 之间、哪些书籍的出版时间在 2015 年到 2019 年之间。使用 range 查询只能查询一个字段，不能作用在多个字段上。\n\nrange 查询支持的参数有以下几种：\n\n * gt：大于\n\n * gte：大于等于\n\n * lt：小于\n\n * lte：小于等于\n\n * format：如果字段是 date 类型，可以设置日期格式化\n\n * time_zone：时区\n\n * relation：指示范围查询如何匹配范围字段的值。\n   \n   * intersects (default)：匹配与查询字段值范围相交的文档。\n   * contains：匹配完全包含查询字段值的文档。\n   * within：匹配具有完全在查询范围内的范围字段值的文档。\n\n示例：数值范围查询\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "range": {\n      "taxful_total_price": {\n        "gt": 10,\n        "lte": 50\n      }\n    }\n  }\n}\n\n\n示例：日期范围查询\n\nget kibana_sample_data_ecommerce/_search\n{\n  "query": {\n    "range": {\n      "order_date": {\n        "time_zone": "+00:00",\n        "gte": "2018-01-01t00:00:00",\n        "lte": "now"\n      }\n    }\n  }\n}\n\n\n\n# regexp query\n\nregexp query 返回与正则表达式相匹配的 term 所属的文档。\n\n正则表达式是一种使用占位符字符匹配数据模式的方法，称为运算符。\n\n示例：以下搜索返回 user.id 字段包含任何以 k 开头并以 y 结尾的文档。 .* 运算符匹配任何长度的任何字符，包括无字符。匹配项可以包括 ky、kay 和 kimchy。\n\nget /_search\n{\n  "query": {\n    "regexp": {\n      "user.id": {\n        "value": "k.*y",\n        "flags": "all",\n        "case_insensitive": true,\n        "max_determinized_states": 10000,\n        "rewrite": "constant_score"\n      }\n    }\n  }\n}\n\n\n> 注意：如果配置了search.allow_expensive_queries ，则 regexp query 会被禁用。\n\n\n# term query\n\nterm query 用来查找指定字段中包含给定单词的文档，term 查询不被解析，只有查询词和文档中的词精确匹配才会被搜索到，应用场景为查询人名、地名等需要精准匹配的需求。\n\n示例：\n\n# 1. 创建一个索引\ndelete my-index-000001\nput my-index-000001\n{\n  "mappings": {\n    "properties": {\n      "full_text": { "type": "text" }\n    }\n  }\n}\n\n# 2. 使用 "quick brown foxes!" 关键字查 "full_text" 字段\nput my-index-000001/_doc/1\n{\n  "full_text": "quick brown foxes!"\n}\n\n# 3. 使用 term 查询\nget my-index-000001/_search?pretty\n{\n  "query": {\n    "term": {\n      "full_text": "quick brown foxes!"\n    }\n  }\n}\n# 因为 full_text 字段不再包含确切的 term —— "quick brown foxes!"，所以 term query 搜索不到任何结果\n\n# 4. 使用 match 查询\nget my-index-000001/_search?pretty\n{\n  "query": {\n    "match": {\n      "full_text": "quick brown foxes!"\n    }\n  }\n}\n\ndelete my-index-000001\n\n\n> ⚠️ 注意：应避免 term 查询对 text 字段使用查询。\n> \n> 默认情况下，elasticsearch 针对 text 字段的值进行解析分词，这会使查找 text 字段值的精确匹配变得困难。\n> \n> 要搜索 text 字段值，需改用 match 查询。\n\n\n# terms query\n\nterms query 与 term query 相同，但可以搜索多个值。\n\nterms query 查询参数：\n\n * index：索引名\n * id：文档 id\n * path：要从中获取字段值的字段的名称，即搜索关键字\n * routing（选填）：要从中获取 term 值的文档的自定义路由值。如果在索引文档时提供了自定义路由值，则此参数是必需的。\n\n示例：\n\n# 1. 创建一个索引\ndelete my-index-000001\nput my-index-000001\n{\n  "mappings": {\n    "properties": {\n      "color": { "type": "keyword" }\n    }\n  }\n}\n\n# 2. 写入一个文档\nput my-index-000001/_doc/1\n{\n  "color": [\n    "blue",\n    "green"\n  ]\n}\n\n# 3. 写入另一个文档\nput my-index-000001/_doc/2\n{\n  "color": "blue"\n}\n\n# 3. 使用 terms query\nget my-index-000001/_search?pretty\n{\n  "query": {\n    "terms": {\n      "color": {\n        "index": "my-index-000001",\n        "id": "2",\n        "path": "color"\n      }\n    }\n  }\n}\n\ndelete my-index-000001\n\n\n\n# type query\n\n> 7.0.0 后废弃\n\ntype query 用于查询具有指定类型的文档。\n\n示例：\n\nget /_search\n{\n  "query": {\n    "type": {\n      "value": "_doc"\n    }\n  }\n}\n\n\n\n# wildcard query\n\nwildcard query 即通配符查询，返回与通配符模式匹配的文档。\n\n? 用来匹配一个任意字符，* 用来匹配零个或者多个字符。\n\n示例：以下搜索返回 user.id 字段包含以 ki 开头并以 y 结尾的术语的文档。这些匹配项可以包括 kiy、kity 或 kimchy。\n\nget /_search\n{\n  "query": {\n    "wildcard": {\n      "user.id": {\n        "value": "ki*y",\n        "boost": 1.0,\n        "rewrite": "constant_score"\n      }\n    }\n  }\n}\n\n\n> 注意：如果配置了search.allow_expensive_queries ，则wildcard query 会被禁用。\n\n\n# 词项查询完整示例\n\ndelete products\nput products\n{\n  "settings": {\n    "number_of_shards": 1\n  }\n}\n\npost /products/_bulk\n{ "index": { "_id": 1 }}\n{ "productid" : "xhdk-a-1293-#fj3","desc":"iphone" }\n{ "index": { "_id": 2 }}\n{ "productid" : "kdke-b-9947-#kl5","desc":"ipad" }\n{ "index": { "_id": 3 }}\n{ "productid" : "jodl-x-1937-#pv7","desc":"mbp" }\n\nget /products\n\npost /products/_search\n{\n  "query": {\n    "term": {\n      "desc": {\n        //"value": "iphone"\n        "value":"iphone"\n      }\n    }\n  }\n}\n\npost /products/_search\n{\n  "query": {\n    "term": {\n      "desc.keyword": {\n        //"value": "iphone"\n        //"value":"iphone"\n      }\n    }\n  }\n}\n\npost /products/_search\n{\n  "query": {\n    "term": {\n      "productid": {\n        "value": "xhdk-a-1293-#fj3"\n      }\n    }\n  }\n}\n\npost /products/_search\n{\n  //"explain": true,\n  "query": {\n    "term": {\n      "productid.keyword": {\n        "value": "xhdk-a-1293-#fj3"\n      }\n    }\n  }\n}\n\npost /products/_search\n{\n  "explain": true,\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "productid.keyword": "xhdk-a-1293-#fj3"\n        }\n      }\n\n    }\n  }\n}\n\n\n\n# 复合查询\n\n复合查询就是把一些简单查询组合在一起实现更复杂的查询需求，除此之外，复合查询还可以控制另外一个查询的行为。\n\n\n# bool query\n\nbool 查询可以把任意多个简单查询组合在一起，使用 must、should、must_not、filter 选项来表示简单查询之间的逻辑，每个选项都可以出现 0 次到多次，它们的含义如下：\n\n * must 文档必须匹配 must 选项下的查询条件，相当于逻辑运算的 and，且参与文档相关度的评分。\n * should 文档可以匹配 should 选项下的查询条件也可以不匹配，相当于逻辑运算的 or，且参与文档相关度的评分。\n * must_not 与 must 相反，匹配该选项下的查询条件的文档不会被返回；需要注意的是，must_not 语句不会影响评分，它的作用只是将不相关的文档排除。\n * filter 和 must 一样，匹配 filter 选项下的查询条件的文档才会被返回，但是 filter 不评分，只起到过滤功能，与 must_not 相反。\n\n假设要查询 title 中包含关键词 java，并且 price 不能高于 70，description 可以包含也可以不包含虚拟机的书籍，构造 bool 查询语句如下：\n\nget books/_search\n{\n  "query": {\n    "bool": {\n      "filter": {\n        "term": {\n          "status": 1\n        }\n      },\n      "must_not": {\n        "range": {\n          "price": {\n            "gte": 70\n          }\n        }\n      },\n      "must": {\n        "match": {\n          "title": "java"\n        }\n      },\n      "should": [\n        {\n          "match": {\n            "description": "虚拟机"\n          }\n        }\n      ],\n      "minimum_should_match": 1\n    }\n  }\n}\n\n\n有关布尔查询更详细的信息参考 bool query（组合查询）详解。\n\n\n# boosting query\n\nboosting 查询用于需要对两个查询的评分进行调整的场景，boosting 查询会把两个查询封装在一起并降低其中一个查询的评分。\n\nboosting 查询包括 positive、negative 和 negative_boost 三个部分，positive 中的查询评分保持不变，negative 中的查询会降低文档评分，negative_boost 指明 negative 中降低的权值。如果我们想对 2015 年之前出版的书降低评分，可以构造一个 boosting 查询，查询语句如下：\n\nget books/_search\n{\n\t"query": {\n\t\t"boosting": {\n\t\t\t"positive": {\n\t\t\t\t"match": {\n\t\t\t\t\t"title": "python"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"negative": {\n\t\t\t\t"range": {\n\t\t\t\t\t"publish_time": {\n\t\t\t\t\t\t"lte": "2015-01-01"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t\t"negative_boost": 0.2\n\t\t}\n\t}\n}\n\n\nboosting 查询中指定了抑制因子为 0.2，publish_time 的值在 2015-01-01 之后的文档得分不变，publish_time 的值在 2015-01-01 之前的文档得分为原得分的 0.2 倍。\n\n\n# constant_score query\n\nconstantscore query 包装一个 filter query，并返回匹配过滤器查询条件的文档，且它们的相关性评分都等于 _boost 参数值（可以理解为原有的基于 tf-idf 或 bm25 的相关分固定为 1.0，所以最终评分为 1.0 * boost，即等于 boost 参数值）。下面的查询语句会返回 title 字段中含有关键词 elasticsearch 的文档，所有文档的评分都是 1.8：\n\nget books/_search\n{\n  "query": {\n    "constant_score": {\n      "filter": {\n        "term": {\n          "title": "elasticsearch"\n        }\n      },\n      "boost": 1.8\n    }\n  }\n}\n\n\n\n# dis_max query\n\ndis_max query 与 bool query 有一定联系也有一定区别，dis_max query 支持多并发查询，可返回与任意查询条件子句匹配的任何文档类型。与 bool 查询可以将所有匹配查询的分数相结合使用的方式不同，dis_max 查询只使用最佳匹配查询条件的分数。请看下面的例子：\n\nget books/_search\n{\n\t"query": {\n\t\t"dis_max": {\n\t\t\t"tie_breaker": 0.7,\n\t\t\t"boost": 1.2,\n\t\t\t"queries": [{\n\t\t\t\t\t"term": {\n\t\t\t\t\t\t"age": 34\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t"term": {\n\t\t\t\t\t\t"age": 35\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t}\n}\n\n\n\n# function_score query\n\nfunction_score query 可以修改查询的文档得分，这个查询在有些情况下非常有用，比如通过评分函数计算文档得分代价较高，可以改用过滤器加自定义评分函数的方式来取代传统的评分方式。\n\n使用 function_score query，用户需要定义一个查询和一至多个评分函数，评分函数会对查询到的每个文档分别计算得分。\n\n下面这条查询语句会返回 books 索引中的所有文档，文档的最大得分为 5，每个文档的得分随机生成，权重的计算模式为相乘模式。\n\nget books/_search\n{\n  "query": {\n    "function_score": {\n      "query": {\n        "match all": {}\n      },\n      "boost": "5",\n      "random_score": {},\n      "boost_mode": "multiply"\n    }\n  }\n}\n\n\n使用脚本自定义评分公式，这里把 price 值的十分之一开方作为每个文档的得分，查询语句如下：\n\nget books/_search\n{\n  "query": {\n    "function_score": {\n      "query": {\n        "match": {\n          "title": "java"\n        }\n      },\n      "script_score": {\n        "inline": "math.sqrt(doc[\'price\'].value/10)"\n      }\n    }\n  }\n}\n\n\n关于 function_score 的更多详细内容请查看 elasticsearch function_score 查询最强详解。\n\n\n# indices query\n\nindices query 适用于需要在多个索引之间进行查询的场景，它允许指定一个索引名字列表和内部查询。indices query 中有 query 和 no_match_query 两部分，query 中用于搜索指定索引列表中的文档，no_match_query 中的查询条件用于搜索指定索引列表之外的文档。下面的查询语句实现了搜索索引 books、books2 中 title 字段包含关键字 javascript，其他索引中 title 字段包含 basketball 的文档，查询语句如下：\n\nget books/_search\n{\n\t"query": {\n\t\t"indices": {\n\t\t\t"indices": ["books", "books2"],\n\t\t\t"query": {\n\t\t\t\t"match": {\n\t\t\t\t\t"title": "javascript"\n\t\t\t\t}\n\t\t\t},\n\t\t\t"no_match_query": {\n\t\t\t\t"term": {\n\t\t\t\t\t"title": "basketball"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 嵌套查询\n\n在 elasticsearch 这样的分布式系统中执行全 sql 风格的连接查询代价昂贵，是不可行的。相应地，为了实现水平规模地扩展，elasticsearch 提供了以下两种形式的 join：\n\n * nested query（嵌套查询）\n   \n   文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来。\n\n * has_child query（有子查询）和 has_parent query（有父查询）\n   \n   父子关系可以存在单个的索引的两个类型的文档之间。has_child 查询将返回其子文档能满足特定查询的父文档，而 has_parent 则返回其父文档能满足特定查询的子文档。\n\n\n# nested query\n\n文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来（用嵌套查询）。\n\nput /my_index\n{\n\t"mappings": {\n\t\t"type1": {\n\t\t\t"properties": {\n\t\t\t\t"obj1": {\n\t\t\t\t\t"type": "nested"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# has_child query\n\n文档的父子关系创建索引时在映射中声明，这里以员工（employee）和工作城市（branch）为例，它们属于不同的类型，相当于数据库中的两张表，如果想把员工和他们工作的城市关联起来，需要告诉 elasticsearch 文档之间的父子关系，这里 employee 是 child type，branch 是 parent type，在映射中声明，执行命令：\n\nput /company\n{\n\t"mappings": {\n\t\t"branch": {},\n\t\t"employee": {\n\t\t\t"parent": { "type": "branch" }\n\t\t}\n\t}\n}\n\n\n使用 bulk api 索引 branch 类型下的文档，命令如下：\n\npost company/branch/_bulk\n{ "index": { "_id": "london" }}\n{ "name": "london westminster","city": "london","country": "uk" }\n{ "index": { "_id": "liverpool" }}\n{ "name": "liverpool central","city": "liverpool","country": "uk" }\n{ "index": { "_id": "paris" }}\n{ "name": "champs elysees","city": "paris","country": "france" }\n\n\n添加员工数据：\n\npost company/employee/_bulk\n{ "index": { "_id": 1,"parent":"london" }}\n{ "name": "alice smith","dob": "1970-10-24","hobby": "hiking" }\n{ "index": { "_id": 2,"parent":"london" }}\n{ "name": "mark tomas","dob": "1982-05-16","hobby": "diving" }\n{ "index": { "_id": 3,"parent":"liverpool" }}\n{ "name": "barry smith","dob": "1979-04-01","hobby": "hiking" }\n{ "index": { "_id": 4,"parent":"paris" }}\n{ "name": "adrien grand","dob": "1987-05-11","hobby": "horses" }\n\n\n通过子文档查询父文档要使用 has_child 查询。例如，搜索 1980 年以后出生的员工所在的分支机构，employee 中 1980 年以后出生的有 mark thomas 和 adrien grand，他们分别在 london 和 paris，执行以下查询命令进行验证：\n\nget company/branch/_search\n{\n\t"query": {\n\t\t"has_child": {\n\t\t\t"type": "employee",\n\t\t\t"query": {\n\t\t\t\t"range": { "dob": { "gte": "1980-01-01" } }\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n搜索哪些机构中有名为 “alice smith” 的员工，因为使用 match 查询，会解析为 “alice” 和 “smith”，所以 alice smith 和 barry smith 所在的机构会被匹配，执行以下查询命令进行验证：\n\nget company/branch/_search\n{\n\t"query": {\n\t\t"has_child": {\n\t\t\t"type": "employee",\n\t\t\t"score_mode": "max",\n\t\t\t"query": {\n\t\t\t\t"match": { "name": "alice smith" }\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n可以使用 min_children 指定子文档的最小个数。例如，搜索最少含有两个 employee 的机构，查询命令如下：\n\nget company/branch/_search?pretty\n{\n\t"query": {\n\t\t"has_child": {\n\t\t\t"type": "employee",\n\t\t\t"min_children": 2,\n\t\t\t"query": {\n\t\t\t\t"match_all": {}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# has_parent query\n\n通过父文档查询子文档使用 has_parent 查询。比如，搜索哪些 employee 工作在 uk，查询命令如下：\n\nget company/employee/_search\n{\n\t"query": {\n\t\t"has_parent": {\n\t\t\t"parent_type": "branch",\n\t\t\t"query": {\n\t\t\t\t"match": { "country": "uk }\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 位置查询\n\nelasticsearch 可以对地理位置点 geo_point 类型和地理位置形状 geo_shape 类型的数据进行搜索。为了学习方便，这里准备一些城市的地理坐标作为测试数据，每一条文档都包含城市名称和地理坐标这两个字段，这里的坐标点取的是各个城市中心的一个位置。首先把下面的内容保存到 geo.json 文件中：\n\n{"index":{ "_index":"geo","_type":"city","_id":"1" }}\n{"name":"北京","location":"39.9088145109,116.3973999023"}\n{"index":{ "_index":"geo","_type":"city","_id": "2" }}\n{"name":"乌鲁木齐","location":"43.8266300000,87.6168800000"}\n{"index":{ "_index":"geo","_type":"city","_id":"3" }}\n{"name":"西安","location":"34.3412700000,108.9398400000"}\n{"index":{ "_index":"geo","_type":"city","_id":"4" }}\n{"name":"郑州","location":"34.7447157466,113.6587142944"}\n{"index":{ "_index":"geo","_type":"city","_id":"5" }}\n{"name":"杭州","location":"30.2294080260,120.1492309570"}\n{"index":{ "_index":"geo","_type":"city","_id":"6" }}\n{"name":"济南","location":"36.6518400000,117.1200900000"}\n\n\n创建一个索引并设置映射：\n\nput geo\n{\n\t"mappings": {\n\t\t"city": {\n\t\t\t"properties": {\n\t\t\t\t"name": {\n\t\t\t\t\t"type": "keyword"\n\t\t\t\t},\n\t\t\t\t"location": {\n\t\t\t\t\t"type": "geo_point"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n然后执行批量导入命令：\n\ncurl -xpost "http://localhost:9200/_bulk?pretty" --data-binary @geo.json\n\n\n\n# geo_distance query\n\ngeo_distance query 可以查找在一个中心点指定范围内的地理点文档。例如，查找距离天津 200km 以内的城市，搜索结果中会返回北京，命令如下：\n\nget geo/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"geo_distance": {\n\t\t\t\t\t"distance": "200km",\n\t\t\t\t\t"location": {\n\t\t\t\t\t\t"lat": 39.0851000000,\n\t\t\t\t\t\t"lon": 117.1993700000\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n按各城市离北京的距离排序：\n\nget geo/_search\n{\n  "query": {\n    "match_all": {}\n  },\n  "sort": [{\n    "_geo_distance": {\n      "location": "39.9088145109,116.3973999023",\n      "unit": "km",\n      "order": "asc",\n      "distance_type": "plane"\n    }\n  }]\n}\n\n\n其中 location 对应的经纬度字段；unit 为 km 表示将距离以 km 为单位写入到每个返回结果的 sort 键中；distance_type 为 plane 表示使用快速但精度略差的 plane 计算方式。\n\n\n# geo_bounding_box query\n\ngeo_bounding_box query 用于查找落入指定的矩形内的地理坐标。查询中由两个点确定一个矩形，然后在矩形区域内查询匹配的文档。\n\nget geo/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"geo_bounding_box": {\n\t\t\t\t\t"location": {\n\t\t\t\t\t\t"top_left": {\n\t\t\t\t\t\t\t"lat": 38.4864400000,\n\t\t\t\t\t\t\t"lon": 106.2324800000\n\t\t\t\t\t\t},\n\t\t\t\t\t\t"bottom_right": {\n\t\t\t\t\t\t\t"lat": 28.6820200000,\n\t\t\t\t\t\t\t"lon": 115.8579400000\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# geo_polygon query\n\ngeo_polygon query 用于查找在指定多边形内的地理点。例如，呼和浩特、重庆、上海三地组成一个三角形，查询位置在该三角形区域内的城市，命令如下：\n\nget geo/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t}\n\t\t},\n\t\t"filter": {\n\t\t\t"geo_polygon": {\n\t\t\t\t"location": {\n\t\t\t\t\t"points": [{\n\t\t\t\t\t\t"lat": 40.8414900000,\n\t\t\t\t\t\t"lon": 111.7519900000\n\t\t\t\t\t}, {\n\t\t\t\t\t\t"lat": 29.5647100000,\n\t\t\t\t\t\t"lon": 106.5507300000\n\t\t\t\t\t}, {\n\t\t\t\t\t\t"lat": 31.2303700000,\n\t\t\t\t\t\t"lon": 121.4737000000\n\t\t\t\t\t}]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# geo_shape query\n\ngeo_shape query 用于查询 geo_shape 类型的地理数据，地理形状之间的关系有相交、包含、不相交三种。创建一个新的索引用于测试，其中 location 字段的类型设为 geo_shape 类型。\n\nput geoshape\n{\n\t"mappings": {\n\t\t"city": {\n\t\t\t"properties": {\n\t\t\t\t"name": {\n\t\t\t\t\t"type": "keyword"\n\t\t\t\t},\n\t\t\t\t"location": {\n\t\t\t\t\t"type": "geo_shape"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n关于经纬度的顺序这里做一个说明，geo_point 类型的字段纬度在前经度在后，但是对于 geo_shape 类型中的点，是经度在前纬度在后，这一点需要特别注意。\n\n把西安和郑州连成的线写入索引：\n\npost geoshape/city/1\n{\n\t"name": "西安-郑州",\n\t"location": {\n\t\t"type": "linestring",\n\t\t"coordinates": [\n\t\t\t[108.9398400000, 34.3412700000],\n\t\t\t[113.6587142944, 34.7447157466]\n\t\t]\n\t}\n}\n\n\n查询包含在由银川和南昌作为对角线上的点组成的矩形的地理形状，由于西安和郑州组成的直线落在该矩形区域内，因此可以被查询到。命令如下：\n\nget geoshape/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match_all": {}\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"geo_shape": {\n\t\t\t\t\t"location": {\n\t\t\t\t\t\t"shape": {\n\t\t\t\t\t\t\t"type": "envelope",\n\t\t\t\t\t\t\t"coordinates": [\n\t\t\t\t\t\t\t\t[106.23248, 38.48644],\n\t\t\t\t\t\t\t\t[115.85794, 28.68202]\n\t\t\t\t\t\t\t]\n\t\t\t\t\t\t},\n\t\t\t\t\t\t"relation": "within"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n\n# 特殊查询\n\n\n# more_like_this query\n\nmore_like_this query 可以查询和提供文本类似的文档，通常用于近似文本的推荐等场景。查询命令如下：\n\nget books/_search\n{\n\t"query": {\n\t\t"more_like_ this": {\n\t\t\t"fields": ["title", "description"],\n\t\t\t"like": "java virtual machine",\n\t\t\t"min_term_freq": 1,\n\t\t\t"max_query_terms": 12\n\t\t}\n\t}\n}\n\n\n可选的参数及取值说明如下：\n\n * fields 要匹配的字段，默认是 _all 字段。\n * like 要匹配的文本。\n * min_term_freq 文档中词项的最低频率，默认是 2，低于此频率的文档会被忽略。\n * max_query_terms query 中能包含的最大词项数目，默认为 25。\n * min_doc_freq 最小的文档频率，默认为 5。\n * max_doc_freq 最大文档频率。\n * min_word length 单词的最小长度。\n * max_word length 单词的最大长度。\n * stop_words 停用词列表。\n * analyzer 分词器。\n * minimum_should_match 文档应匹配的最小词项数，默认为 query 分词后词项数的 30%。\n * boost terms 词项的权重。\n * include 是否把输入文档作为结果返回。\n * boost 整个 query 的权重，默认为 1.0。\n\n\n# script query\n\nelasticsearch 支持使用脚本进行查询。例如，查询价格大于 180 的文档，命令如下：\n\nget books/_search\n{\n  "query": {\n    "script": {\n      "script": {\n        "inline": "doc[\'price\'].value > 180",\n        "lang": "painless"\n      }\n    }\n  }\n}\n\n\n\n# percolate query\n\n一般情况下，我们是先把文档写入到 elasticsearch 中，通过查询语句对文档进行搜索。percolate query 则是反其道而行之的做法，它会先注册查询条件，根据文档来查询 query。例如，在 my-index 索引中有一个 laptop 类型，文档有 price 和 name 两个字段，在映射中声明一个 percolator 类型的 query，命令如下：\n\nput my-index\n{\n\t"mappings": {\n\t\t"laptop": {\n\t\t\t"properties": {\n\t\t\t\t"price": { "type": "long" },\n\t\t\t\t"name": { "type": "text" }\n\t\t\t},\n\t\t\t"queries": {\n\t\t\t\t"properties": {\n\t\t\t\t\t"query": { "type": "percolator" }\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n注册一个 bool query，bool query 中包含一个 range query，要求 price 字段的取值小于等于 10000，并且 name 字段中含有关键词 macbook：\n\nput /my-index/queries/1?refresh\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": [{\n\t\t\t\t"range": { "price": { "lte": 10000 } }\n\t\t\t}, {\n\t\t\t\t"match": { "name": "macbook" }\n\t\t\t}]\n\t\t}\n\t}\n}\n\n\n通过文档查询 query：\n\nget /my-index/_search\n{\n\t"query": {\n\t\t"percolate": {\n\t\t\t"field": "query",\n\t\t\t"document_type": "laptop",\n\t\t\t"document": {\n\t\t\t\t"price": 9999,\n\t\t\t\t"name": "macbook pro on sale"\n\t\t\t}\n\t\t}\n\t}\n}\n\n\n文档符合 query 中的条件，返回结果中可以查到上文中注册的 bool query。percolate query 的这种特性适用于数据分类、数据路由、事件监控和预警等场景。',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 高亮搜索及显示",frontmatter:{title:"Elasticsearch 高亮搜索及显示",date:"2022-02-22T21:01:01.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","高亮"],permalink:"/pages/e1b769/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/06.Elasticsearch%E9%AB%98%E4%BA%AE.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/06.Elasticsearch高亮.md",key:"v-957680f8",path:"/pages/e1b769/",headers:[{level:2,title:"高亮参数",slug:"高亮参数",normalizedTitle:"高亮参数",charIndex:187},{level:2,title:"自定义高亮片段",slug:"自定义高亮片段",normalizedTitle:"自定义高亮片段",charIndex:2379},{level:2,title:"多字段高亮",slug:"多字段高亮",normalizedTitle:"多字段高亮",charIndex:2728},{level:2,title:"高亮性能分析",slug:"高亮性能分析",normalizedTitle:"高亮性能分析",charIndex:3111}],headersStr:"高亮参数 自定义高亮片段 多字段高亮 高亮性能分析",content:'# Elasticsearch 高亮搜索及显示\n\nElasticsearch 的高亮（highlight）可以让您从搜索结果中的一个或多个字段中获取突出显示的摘要，以便向用户显示查询匹配的位置。当您请求突出显示（即高亮）时，响应结果的 highlight 字段中包括高亮的字段和高亮的片段。Elasticsearch 默认会用 <em></em> 标签标记关键字。\n\n\n# 高亮参数\n\nES 提供了如下高亮参数：\n\n参数                        说明\nboundary_chars            包含每个边界字符的字符串。默认为,! ?\\ \\ n。\nboundary_max_scan         扫描边界字符的距离。默认为 20。\nboundary_scanner          指定如何分割突出显示的片段，支持 chars、sentence、word 三种方式。\nboundary_scanner_locale   用来设置搜索和确定单词边界的本地化设置，此参数使用语言标记的形式（“en-US”, “fr-FR”, “ja-JP”）\nencoder                   表示代码段应该是 HTML 编码的:默认(无编码)还是 HTML (HTML-转义代码段文本，然后插入高亮标记)\nfields                    指定检索高亮显示的字段。可以使用通配符来指定字段。例如，可以指定 comment**来获取以\n                          comment*开头的所有文本和关键字字段的高亮显示。\nforce_source              根据源高亮显示。默认值为 false。\nfragmenter                指定文本应如何在突出显示片段中拆分:支持参数 simple 或者 span。\nfragment_offset           控制要开始突出显示的空白。仅在使用 fvh highlighter 时有效。\nfragment_size             字符中突出显示的片段的大小。默认为 100。\nhighlight_query           突出显示搜索查询之外的其他查询的匹配项。这在使用重打分查询时特别有用，因为默认情况下高亮显示不会考虑这些问题。\nmatched_fields            组合多个匹配结果以突出显示单个字段，对于使用不同方式分析同一字符串的多字段。所有的 matched_fields 必须将\n                          term_vector 设置为\n                          with_positions_offsets，但是只有将匹配项组合到的字段才会被加载，因此只有将 store 设置为\n                          yes 才能使该字段受益。只适用于 fvh highlighter。\nno_match_size             如果没有要突出显示的匹配片段，则希望从字段开头返回的文本量。默认为 0(不返回任何内容)。\nnumber_of_fragments       返回的片段的最大数量。如果片段的数量设置为\n                          0，则不会返回任何片段。相反，突出显示并返回整个字段内容。当需要突出显示短文本(如标题或地址)，但不需要分段时，使用此配置非常方便。如果\n                          number_of_fragments 为 0，则忽略 fragment_size。默认为 5。\norder                     设置为 score\n                          时，按分数对突出显示的片段进行排序。默认情况下，片段将按照它们在字段中出现的顺序输出(order:none)。将此选项设置为\n                          score 将首先输出最相关的片段。每个高亮应用自己的逻辑来计算相关性得分。\nphrase_limit              控制文档中所考虑的匹配短语的数量。防止 fvh highlighter\n                          分析太多的短语和消耗太多的内存。提高限制会增加查询时间并消耗更多内存。默认为 256。\npre_tags                  与 post_tags 一起使用，定义用于突出显示文本的 HTML\n                          标记。默认情况下，突出显示的文本被包装在和标记中。指定为字符串数组。\npost_tags                 与 pre_tags 一起使用，定义用于突出显示文本的 HTML\n                          标记。默认情况下，突出显示的文本被包装在和标记中。指定为字符串数组。\nrequire_field_match       默认情况下，只突出显示包含查询匹配的字段。将 require_field_match 设置为 false\n                          以突出显示所有字段。默认值为 true。\ntags_schema               设置为使用内置标记模式的样式。\ntype                      使用的高亮模式，可选项为**unified、plain或fvh**。默认为 unified。\n\n\n# 自定义高亮片段\n\n如果我们想使用自定义标签，在高亮属性中给需要高亮的字段加上 pre_tags 和 post_tags 即可。例如，搜索 title 字段中包含关键词 javascript 的书籍并使用自定义 HTML 标签高亮关键词，查询语句如下：\n\nGET /books/_search\n{\n  "query": {\n    "match": { "title": "javascript" }\n  },\n  "highlight": {\n    "fields": {\n      "title": {\n        "pre_tags": ["<strong>"],\n        "post_tags": ["</strong>"]\n      }\n    }\n  }\n}\n\n\n\n# 多字段高亮\n\n关于搜索高亮，还需要掌握如何设置多字段搜索高亮。比如，搜索 title 字段的时候，我们期望 description 字段中的关键字也可以高亮，这时候就需要把 require_field_match 属性的取值设置为 fasle。require_field_match 的默认值为 true，只会高亮匹配的字段。多字段高亮的查询语句如下：\n\nGET /books/_search\n{\n  "query": {\n    "match": { "title": "javascript" }\n  },\n  "highlight": {\n    "require_field_match": false,\n    "fields": {\n      "title": {},\n      "description": {}\n    }\n  }\n}\n\n\n\n# 高亮性能分析\n\nElasticsearch 提供了三种高亮器，分别是默认的 highlighter 高亮器、postings-highlighter 高亮器和 fast-vector-highlighter 高亮器。\n\n默认的 highlighter 是最基本的高亮器。highlighter 高亮器实现高亮功能需要对 _source 中保存的原始文档进行二次分析，其速度在三种高亮器里最慢，优点是不需要额外的存储空间。\n\npostings-highlighter 高亮器实现高亮功能不需要二次分析，但是需要在字段的映射中设置 index_options 参数的取值为 offsets，即保存关键词的偏移量，速度快于默认的 highlighter 高亮器。例如，配置 comment 字段使用 postings-highlighter 高亮器，映射如下：\n\nPUT /example\n{\n  "mappings": {\n    "doc": {\n      "properties": {\n        "comment": {\n          "type": "text",\n          "index_options": "offsets"\n        }\n      }\n    }\n  }\n}\n\n\nfast-vector-highlighter 高亮器实现高亮功能速度最快，但是需要在字段的映射中设置 term_vector 参数的取值为 with_positions_offsets，即保存关键词的位置和偏移信息，占用的存储空间最大，是典型的空间换时间的做法。例如，配置 comment 字段使用 fast-vector-highlighter 高亮器，映射如下：\n\nPUT /example\n{\n  "mappings": {\n    "doc": {\n      "properties": {\n        "comment": {\n          "type": "text",\n          "term_vector": "with_positions_offsets"\n        }\n      }\n    }\n  }\n}\n',normalizedContent:'# elasticsearch 高亮搜索及显示\n\nelasticsearch 的高亮（highlight）可以让您从搜索结果中的一个或多个字段中获取突出显示的摘要，以便向用户显示查询匹配的位置。当您请求突出显示（即高亮）时，响应结果的 highlight 字段中包括高亮的字段和高亮的片段。elasticsearch 默认会用 <em></em> 标签标记关键字。\n\n\n# 高亮参数\n\nes 提供了如下高亮参数：\n\n参数                        说明\nboundary_chars            包含每个边界字符的字符串。默认为,! ?\\ \\ n。\nboundary_max_scan         扫描边界字符的距离。默认为 20。\nboundary_scanner          指定如何分割突出显示的片段，支持 chars、sentence、word 三种方式。\nboundary_scanner_locale   用来设置搜索和确定单词边界的本地化设置，此参数使用语言标记的形式（“en-us”, “fr-fr”, “ja-jp”）\nencoder                   表示代码段应该是 html 编码的:默认(无编码)还是 html (html-转义代码段文本，然后插入高亮标记)\nfields                    指定检索高亮显示的字段。可以使用通配符来指定字段。例如，可以指定 comment**来获取以\n                          comment*开头的所有文本和关键字字段的高亮显示。\nforce_source              根据源高亮显示。默认值为 false。\nfragmenter                指定文本应如何在突出显示片段中拆分:支持参数 simple 或者 span。\nfragment_offset           控制要开始突出显示的空白。仅在使用 fvh highlighter 时有效。\nfragment_size             字符中突出显示的片段的大小。默认为 100。\nhighlight_query           突出显示搜索查询之外的其他查询的匹配项。这在使用重打分查询时特别有用，因为默认情况下高亮显示不会考虑这些问题。\nmatched_fields            组合多个匹配结果以突出显示单个字段，对于使用不同方式分析同一字符串的多字段。所有的 matched_fields 必须将\n                          term_vector 设置为\n                          with_positions_offsets，但是只有将匹配项组合到的字段才会被加载，因此只有将 store 设置为\n                          yes 才能使该字段受益。只适用于 fvh highlighter。\nno_match_size             如果没有要突出显示的匹配片段，则希望从字段开头返回的文本量。默认为 0(不返回任何内容)。\nnumber_of_fragments       返回的片段的最大数量。如果片段的数量设置为\n                          0，则不会返回任何片段。相反，突出显示并返回整个字段内容。当需要突出显示短文本(如标题或地址)，但不需要分段时，使用此配置非常方便。如果\n                          number_of_fragments 为 0，则忽略 fragment_size。默认为 5。\norder                     设置为 score\n                          时，按分数对突出显示的片段进行排序。默认情况下，片段将按照它们在字段中出现的顺序输出(order:none)。将此选项设置为\n                          score 将首先输出最相关的片段。每个高亮应用自己的逻辑来计算相关性得分。\nphrase_limit              控制文档中所考虑的匹配短语的数量。防止 fvh highlighter\n                          分析太多的短语和消耗太多的内存。提高限制会增加查询时间并消耗更多内存。默认为 256。\npre_tags                  与 post_tags 一起使用，定义用于突出显示文本的 html\n                          标记。默认情况下，突出显示的文本被包装在和标记中。指定为字符串数组。\npost_tags                 与 pre_tags 一起使用，定义用于突出显示文本的 html\n                          标记。默认情况下，突出显示的文本被包装在和标记中。指定为字符串数组。\nrequire_field_match       默认情况下，只突出显示包含查询匹配的字段。将 require_field_match 设置为 false\n                          以突出显示所有字段。默认值为 true。\ntags_schema               设置为使用内置标记模式的样式。\ntype                      使用的高亮模式，可选项为**unified、plain或fvh**。默认为 unified。\n\n\n# 自定义高亮片段\n\n如果我们想使用自定义标签，在高亮属性中给需要高亮的字段加上 pre_tags 和 post_tags 即可。例如，搜索 title 字段中包含关键词 javascript 的书籍并使用自定义 html 标签高亮关键词，查询语句如下：\n\nget /books/_search\n{\n  "query": {\n    "match": { "title": "javascript" }\n  },\n  "highlight": {\n    "fields": {\n      "title": {\n        "pre_tags": ["<strong>"],\n        "post_tags": ["</strong>"]\n      }\n    }\n  }\n}\n\n\n\n# 多字段高亮\n\n关于搜索高亮，还需要掌握如何设置多字段搜索高亮。比如，搜索 title 字段的时候，我们期望 description 字段中的关键字也可以高亮，这时候就需要把 require_field_match 属性的取值设置为 fasle。require_field_match 的默认值为 true，只会高亮匹配的字段。多字段高亮的查询语句如下：\n\nget /books/_search\n{\n  "query": {\n    "match": { "title": "javascript" }\n  },\n  "highlight": {\n    "require_field_match": false,\n    "fields": {\n      "title": {},\n      "description": {}\n    }\n  }\n}\n\n\n\n# 高亮性能分析\n\nelasticsearch 提供了三种高亮器，分别是默认的 highlighter 高亮器、postings-highlighter 高亮器和 fast-vector-highlighter 高亮器。\n\n默认的 highlighter 是最基本的高亮器。highlighter 高亮器实现高亮功能需要对 _source 中保存的原始文档进行二次分析，其速度在三种高亮器里最慢，优点是不需要额外的存储空间。\n\npostings-highlighter 高亮器实现高亮功能不需要二次分析，但是需要在字段的映射中设置 index_options 参数的取值为 offsets，即保存关键词的偏移量，速度快于默认的 highlighter 高亮器。例如，配置 comment 字段使用 postings-highlighter 高亮器，映射如下：\n\nput /example\n{\n  "mappings": {\n    "doc": {\n      "properties": {\n        "comment": {\n          "type": "text",\n          "index_options": "offsets"\n        }\n      }\n    }\n  }\n}\n\n\nfast-vector-highlighter 高亮器实现高亮功能速度最快，但是需要在字段的映射中设置 term_vector 参数的取值为 with_positions_offsets，即保存关键词的位置和偏移信息，占用的存储空间最大，是典型的空间换时间的做法。例如，配置 comment 字段使用 fast-vector-highlighter 高亮器，映射如下：\n\nput /example\n{\n  "mappings": {\n    "doc": {\n      "properties": {\n        "comment": {\n          "type": "text",\n          "term_vector": "with_positions_offsets"\n        }\n      }\n    }\n  }\n}\n',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 排序",frontmatter:{title:"Elasticsearch 排序",date:"2022-01-19T22:49:16.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","排序"],permalink:"/pages/24baff/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/07.Elasticsearch%E6%8E%92%E5%BA%8F.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/07.Elasticsearch排序.md",key:"v-2ac3b3cb",path:"/pages/24baff/",headers:[{level:2,title:"默认相关性排序",slug:"默认相关性排序",normalizedTitle:"默认相关性排序",charIndex:216},{level:3,title:"TF-IDF 模型",slug:"tf-idf-模型",normalizedTitle:"tf-idf 模型",charIndex:459},{level:3,title:"BM25 模型",slug:"bm25-模型",normalizedTitle:"bm25 模型",charIndex:1851},{level:2,title:"字段的值排序",slug:"字段的值排序",normalizedTitle:"字段的值排序",charIndex:71},{level:2,title:"多字段排序",slug:"多字段排序",normalizedTitle:"多字段排序",charIndex:2770},{level:2,title:"多值字段的排序",slug:"多值字段的排序",normalizedTitle:"多值字段的排序",charIndex:3296},{level:2,title:"地理位置上的距离排序",slug:"地理位置上的距离排序",normalizedTitle:"地理位置上的距离排序",charIndex:3532},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:4519}],headersStr:"默认相关性排序 TF-IDF 模型 BM25 模型 字段的值排序 多字段排序 多值字段的排序 地理位置上的距离排序 参考资料",content:'# Elasticsearch 排序\n\n在 Elasticsearch 中，默认排序是按照相关性的评分（_score）进行降序排序，也可以按照字段的值排序、多级排序、多值字段排序、基于 geo（地理位置）排序以及自定义脚本排序，除此之外，对于相关性的评分也可以用 rescore 二次、三次打分，它可以限定重新打分的窗口大小（window size），并针对作用范围内的文档修改其得分，从而达到精细化控制结果相关性的目的。\n\n\n# 默认相关性排序\n\n在 Elasticsearch 中，默认情况下，文档是按照相关性得分倒序排列的，其对应的相关性得分字段用 _score 来表示，它是浮点数类型，_score 评分越高，相关性越高。评分模型的选择可以通过 similarity 参数在映射中指定。\n\n相似度算法可以按字段指定，只需在映射中为不同字段选定即可，如果要修改已有字段的相似度算法，只能通过为数据重新建立索引来达到目的。关于更多 es 相似度算法可以参考 深入理解 es 相似度算法（相关性得分计算）。\n\n\n# TF-IDF 模型\n\nElasticsearch 在 5.4 版本以前，text 类型的字段，默认采用基于 tf-idf 的向量空间模型。\n\n在开始计算得分之时，Elasticsearch 使用了被搜索词条的频率以及它有多常见来影响得分。一个简短的解释是，一个词条出现在某个文档中的次数越多，它就越相关；但是，如果该词条出现在不同的文档的次数越多，它就越不相关。这一点被称为 TF-IDF，TF 是词频（term frequency），IDF 是逆文档频率（inverse document frequency）。\n\n考虑给一篇文档打分的首要方式，是统计一个词条在文本中出现的次数。举个例子，如果在用户的区域搜索关于 Elasticsearch 的 get-together，用户希望频繁提及 Elasticsearch 的分组被优先展示出来。\n\n"We will discuss Elasticsearch at the next Big Data group."\n"Tuesday the Elasticsearch team will gather to answer questions about Elasticsearch."\n\n\n第一个句子提到 Elasticsearch 一次，而第二个句子提到 Elasticsearch 两次，所以包含第二句话的文档应该比包含第一句话的文档拥有更高的得分。如果我们要按照数量来讨论，第一句话的词频（TF）是 1，而第二句话的词频将是 2。\n\n逆文档频率比文档词频稍微复杂一点。这个听上去很酷炫的描述意味着，如果一个分词（通常是单词，但不一定是）在索引的不同文档中出现越多的次数，那么它就越不重要。使用如下例子更容易解释这一点。\n\n"We use Elasticsearch to power the search for our website."\n"The developers like Elasticsearch so far."\n"The scoring of documents is calculated by the scoring formula."\n\n\n如上述例子，需要理解以下几点：\n\n * 词条 “Elasticsearch” 的文档频率是 2（因为它出现在两篇文档中）。文档频率的逆源自得分乘以 1/DF，这里 DF 是该词条的文档频率。这就意味着，由于词条拥有更高的文档频率，它的权重就会降低。\n * 词条 “the” 的文档频率是 3，因为它出现在所有的三篇文档中。请注意，尽管 “the” 在最后一篇文档中出现了两次，它的文档频率还是 3。这是因为，逆文档频率只检查一个词条是否出现在某文档中，而不检查它出现多少次。那个应该是词频所关心的事情。\n\n逆文档频率是一个重要的因素，用于平衡词条的词频。举个例子，考虑有一个用户搜索词条 “the score”，单词 the 几乎出现在每个普通的英语文本中，如果它不被均衡一下，单词 the 的频率要完全淹没单词 score 的频率。逆文档频率 IDF 均衡了 the 这种常见词的相关性影响，所以实际的相关性得分将会对查询的词条有一个更准确的描述。\n\n一旦词频 TF 和逆文档频率 IDF 计算完成，就可以使用 TF-IDF 公式来计算文档的得分。\n\n\n# BM25 模型\n\nElasticsearch 在 5.4 版本之后，针对 text 类型的字段，默认采用的是 BM25 评分模型，而不是基于 tf-idf 的向量空间模型，评分模型的选择可以通过 similarity 参数在映射中指定。\n\n\n# 字段的值排序\n\n在 Elasticsearch 中按照字段的值排序，可以利用 sort 参数实现。\n\nGET books/_search\n{\n  "sort": {\n    "price": {\n      "order": "desc"\n    }\n  }\n}\n\n\n返回结果如下：\n\n{\n  "took": 132,\n  "timed_out": false,\n  "_shards": {\n    "total": 10,\n    "successful": 10,\n    "skipped": 0,\n    "failed": 0\n  },\n  "hits": {\n    "total": 749244,\n    "max_score": null,\n    "hits": [\n      {\n        "_index": "books",\n        "_type": "book",\n        "_id": "8456479",\n        "_score": null,\n        "_source": {\n          "id": 8456479,\n          "price": 1580.00,\n          ...\n        },\n        "sort": [\n          1580.00\n        ]\n      },\n      ...\n    ]\n  }\n}\n\n\n从如上返回结果，可以看出，max_score 和 _score 字段都返回 null，返回字段多出 sort 字段，包含排序字段的分值。计算 _score 的花销巨大，如果不根据相关性排序，记录 _score 是没有意义的。如果无论如何都要计算 _score，可以将 track_scores 参数设置为 true。\n\n\n# 多字段排序\n\n如果我们想要结合使用 price、date 和 _score 进行查询，并且匹配的结果首先按照价格排序，然后按照日期排序，最后按照相关性排序，具体示例如下：\n\nGET books/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match": { "content": "java" }\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"term": { "user_id": 4868438 }\n\t\t\t}\n\t\t}\n\t},\n\t"sort": [{\n\t\t\t"price": {\n\t\t\t\t"order": "desc"\n\t\t\t}\n\t\t}, {\n\t\t\t"date": {\n\t\t\t\t"order": "desc"\n\t\t\t}\n\t\t}, {\n\t\t\t"_score": {\n\t\t\t\t"order": "desc"\n\t\t\t}\n\t\t}\n\t]\n}\n\n\n排序条件的顺序是很重要的。结果首先按第一个条件排序，仅当结果集的第一个 sort 值完全相同时才会按照第二个条件进行排序，以此类推。\n\n多级排序并不一定包含 _score。你可以根据一些不同的字段进行排序，如地理距离或是脚本计算的特定值。\n\n\n# 多值字段的排序\n\n一种情形是字段有多个值的排序，需要记住这些值并没有固有的顺序；一个多值的字段仅仅是多个值的包装，这时应该选择哪个进行排序呢？\n\n对于数字或日期，你可以将多值字段减为单值，这可以通过使用 min、max、avg 或是 sum 排序模式。例如你可以按照每个 date 字段中的最早日期进行排序，通过以下方法：\n\n"sort": {\n  "dates": {\n    "order": "asc",\n    "mode":  "min"\n  }\n}\n\n\n\n# 地理位置上的距离排序\n\nes 的地理位置排序使用 _geo_distance 来进行距离排序，如下示例：\n\n{\n  "sort" : [\n    {\n      "_geo_distance" : {\n        "es_location_field" : [116.407526, 39.904030],\n        "order" : "asc",\n        "unit" : "km",\n        "mode" : "min",\n        "distance_type" : "plane"\n      }\n    }\n  ],\n  "query" : {\n    ......\n  }\n}\n\n\n_geo_distance 的选项具体如下：\n\n * 如上的 es_location_field 指的是 es 存储经纬度数据的字段名。\n * order：指定按距离升序或降序，分别对应 asc 和 desc。\n * unit：计算距离值的单位，默认是 m，表示米（meters），其它可选项有 mi、cm、mm、NM、km、ft、yd 和 in。\n * mode：针对数组数据（多个值）时，指定的取值模式，可选值有 min、max、sum、avg 和 median，当排序采用升序时，默认为 min；排序采用降序时，默认为 max。\n * distance_type：用来设置如何计算距离，它的可选项有 sloppy_arc、arc 和 plane，默认为 sloppy_arc，arc 它相对更精确些，但速度会明显下降，plane 则是计算快，但是长距离计算相对不准确。\n * ignore_unmapped：未映射字段时，是否忽略处理，可选项有 true 和 false；默认为 false，表示如果未映射字段，查询将引发异常；若设置 true，将忽略未映射的字段，并且不匹配此查询的任何文档。\n * validation_method：指定检验经纬度数据的方式，可选项有 IGNORE_MALFORMED、COERCE 和 STRICT；IGNORE_MALFORMED 表示可接受纬度或经度无效的地理点，即忽略数据；COERCE 表示另外尝试并推断正确的地理坐标；STRICT 为默认值，表示遇到不正确的地理坐标直接抛出异常。\n\n\n# 参考资料\n\n * Elasticsearch 教程',normalizedContent:'# elasticsearch 排序\n\n在 elasticsearch 中，默认排序是按照相关性的评分（_score）进行降序排序，也可以按照字段的值排序、多级排序、多值字段排序、基于 geo（地理位置）排序以及自定义脚本排序，除此之外，对于相关性的评分也可以用 rescore 二次、三次打分，它可以限定重新打分的窗口大小（window size），并针对作用范围内的文档修改其得分，从而达到精细化控制结果相关性的目的。\n\n\n# 默认相关性排序\n\n在 elasticsearch 中，默认情况下，文档是按照相关性得分倒序排列的，其对应的相关性得分字段用 _score 来表示，它是浮点数类型，_score 评分越高，相关性越高。评分模型的选择可以通过 similarity 参数在映射中指定。\n\n相似度算法可以按字段指定，只需在映射中为不同字段选定即可，如果要修改已有字段的相似度算法，只能通过为数据重新建立索引来达到目的。关于更多 es 相似度算法可以参考 深入理解 es 相似度算法（相关性得分计算）。\n\n\n# tf-idf 模型\n\nelasticsearch 在 5.4 版本以前，text 类型的字段，默认采用基于 tf-idf 的向量空间模型。\n\n在开始计算得分之时，elasticsearch 使用了被搜索词条的频率以及它有多常见来影响得分。一个简短的解释是，一个词条出现在某个文档中的次数越多，它就越相关；但是，如果该词条出现在不同的文档的次数越多，它就越不相关。这一点被称为 tf-idf，tf 是词频（term frequency），idf 是逆文档频率（inverse document frequency）。\n\n考虑给一篇文档打分的首要方式，是统计一个词条在文本中出现的次数。举个例子，如果在用户的区域搜索关于 elasticsearch 的 get-together，用户希望频繁提及 elasticsearch 的分组被优先展示出来。\n\n"we will discuss elasticsearch at the next big data group."\n"tuesday the elasticsearch team will gather to answer questions about elasticsearch."\n\n\n第一个句子提到 elasticsearch 一次，而第二个句子提到 elasticsearch 两次，所以包含第二句话的文档应该比包含第一句话的文档拥有更高的得分。如果我们要按照数量来讨论，第一句话的词频（tf）是 1，而第二句话的词频将是 2。\n\n逆文档频率比文档词频稍微复杂一点。这个听上去很酷炫的描述意味着，如果一个分词（通常是单词，但不一定是）在索引的不同文档中出现越多的次数，那么它就越不重要。使用如下例子更容易解释这一点。\n\n"we use elasticsearch to power the search for our website."\n"the developers like elasticsearch so far."\n"the scoring of documents is calculated by the scoring formula."\n\n\n如上述例子，需要理解以下几点：\n\n * 词条 “elasticsearch” 的文档频率是 2（因为它出现在两篇文档中）。文档频率的逆源自得分乘以 1/df，这里 df 是该词条的文档频率。这就意味着，由于词条拥有更高的文档频率，它的权重就会降低。\n * 词条 “the” 的文档频率是 3，因为它出现在所有的三篇文档中。请注意，尽管 “the” 在最后一篇文档中出现了两次，它的文档频率还是 3。这是因为，逆文档频率只检查一个词条是否出现在某文档中，而不检查它出现多少次。那个应该是词频所关心的事情。\n\n逆文档频率是一个重要的因素，用于平衡词条的词频。举个例子，考虑有一个用户搜索词条 “the score”，单词 the 几乎出现在每个普通的英语文本中，如果它不被均衡一下，单词 the 的频率要完全淹没单词 score 的频率。逆文档频率 idf 均衡了 the 这种常见词的相关性影响，所以实际的相关性得分将会对查询的词条有一个更准确的描述。\n\n一旦词频 tf 和逆文档频率 idf 计算完成，就可以使用 tf-idf 公式来计算文档的得分。\n\n\n# bm25 模型\n\nelasticsearch 在 5.4 版本之后，针对 text 类型的字段，默认采用的是 bm25 评分模型，而不是基于 tf-idf 的向量空间模型，评分模型的选择可以通过 similarity 参数在映射中指定。\n\n\n# 字段的值排序\n\n在 elasticsearch 中按照字段的值排序，可以利用 sort 参数实现。\n\nget books/_search\n{\n  "sort": {\n    "price": {\n      "order": "desc"\n    }\n  }\n}\n\n\n返回结果如下：\n\n{\n  "took": 132,\n  "timed_out": false,\n  "_shards": {\n    "total": 10,\n    "successful": 10,\n    "skipped": 0,\n    "failed": 0\n  },\n  "hits": {\n    "total": 749244,\n    "max_score": null,\n    "hits": [\n      {\n        "_index": "books",\n        "_type": "book",\n        "_id": "8456479",\n        "_score": null,\n        "_source": {\n          "id": 8456479,\n          "price": 1580.00,\n          ...\n        },\n        "sort": [\n          1580.00\n        ]\n      },\n      ...\n    ]\n  }\n}\n\n\n从如上返回结果，可以看出，max_score 和 _score 字段都返回 null，返回字段多出 sort 字段，包含排序字段的分值。计算 _score 的花销巨大，如果不根据相关性排序，记录 _score 是没有意义的。如果无论如何都要计算 _score，可以将 track_scores 参数设置为 true。\n\n\n# 多字段排序\n\n如果我们想要结合使用 price、date 和 _score 进行查询，并且匹配的结果首先按照价格排序，然后按照日期排序，最后按照相关性排序，具体示例如下：\n\nget books/_search\n{\n\t"query": {\n\t\t"bool": {\n\t\t\t"must": {\n\t\t\t\t"match": { "content": "java" }\n\t\t\t},\n\t\t\t"filter": {\n\t\t\t\t"term": { "user_id": 4868438 }\n\t\t\t}\n\t\t}\n\t},\n\t"sort": [{\n\t\t\t"price": {\n\t\t\t\t"order": "desc"\n\t\t\t}\n\t\t}, {\n\t\t\t"date": {\n\t\t\t\t"order": "desc"\n\t\t\t}\n\t\t}, {\n\t\t\t"_score": {\n\t\t\t\t"order": "desc"\n\t\t\t}\n\t\t}\n\t]\n}\n\n\n排序条件的顺序是很重要的。结果首先按第一个条件排序，仅当结果集的第一个 sort 值完全相同时才会按照第二个条件进行排序，以此类推。\n\n多级排序并不一定包含 _score。你可以根据一些不同的字段进行排序，如地理距离或是脚本计算的特定值。\n\n\n# 多值字段的排序\n\n一种情形是字段有多个值的排序，需要记住这些值并没有固有的顺序；一个多值的字段仅仅是多个值的包装，这时应该选择哪个进行排序呢？\n\n对于数字或日期，你可以将多值字段减为单值，这可以通过使用 min、max、avg 或是 sum 排序模式。例如你可以按照每个 date 字段中的最早日期进行排序，通过以下方法：\n\n"sort": {\n  "dates": {\n    "order": "asc",\n    "mode":  "min"\n  }\n}\n\n\n\n# 地理位置上的距离排序\n\nes 的地理位置排序使用 _geo_distance 来进行距离排序，如下示例：\n\n{\n  "sort" : [\n    {\n      "_geo_distance" : {\n        "es_location_field" : [116.407526, 39.904030],\n        "order" : "asc",\n        "unit" : "km",\n        "mode" : "min",\n        "distance_type" : "plane"\n      }\n    }\n  ],\n  "query" : {\n    ......\n  }\n}\n\n\n_geo_distance 的选项具体如下：\n\n * 如上的 es_location_field 指的是 es 存储经纬度数据的字段名。\n * order：指定按距离升序或降序，分别对应 asc 和 desc。\n * unit：计算距离值的单位，默认是 m，表示米（meters），其它可选项有 mi、cm、mm、nm、km、ft、yd 和 in。\n * mode：针对数组数据（多个值）时，指定的取值模式，可选值有 min、max、sum、avg 和 median，当排序采用升序时，默认为 min；排序采用降序时，默认为 max。\n * distance_type：用来设置如何计算距离，它的可选项有 sloppy_arc、arc 和 plane，默认为 sloppy_arc，arc 它相对更精确些，但速度会明显下降，plane 则是计算快，但是长距离计算相对不准确。\n * ignore_unmapped：未映射字段时，是否忽略处理，可选项有 true 和 false；默认为 false，表示如果未映射字段，查询将引发异常；若设置 true，将忽略未映射的字段，并且不匹配此查询的任何文档。\n * validation_method：指定检验经纬度数据的方式，可选项有 ignore_malformed、coerce 和 strict；ignore_malformed 表示可接受纬度或经度无效的地理点，即忽略数据；coerce 表示另外尝试并推断正确的地理坐标；strict 为默认值，表示遇到不正确的地理坐标直接抛出异常。\n\n\n# 参考资料\n\n * elasticsearch 教程',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 聚合",frontmatter:{title:"Elasticsearch 聚合",date:"2022-01-19T22:49:16.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","聚合"],permalink:"/pages/f89f66/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/08.Elasticsearch%E8%81%9A%E5%90%88.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/08.Elasticsearch聚合.md",key:"v-164e75b2",path:"/pages/f89f66/",headers:[{level:2,title:"聚合的具体结构",slug:"聚合的具体结构",normalizedTitle:"聚合的具体结构",charIndex:327},{level:2,title:"指标聚合",slug:"指标聚合",normalizedTitle:"指标聚合",charIndex:159},{level:3,title:"Max Aggregation",slug:"max-aggregation",normalizedTitle:"max aggregation",charIndex:1837},{level:3,title:"Min Aggregation",slug:"min-aggregation",normalizedTitle:"min aggregation",charIndex:2375},{level:3,title:"Avg Aggregation",slug:"avg-aggregation",normalizedTitle:"avg aggregation",charIndex:2668},{level:3,title:"Sum Aggregation",slug:"sum-aggregation",normalizedTitle:"sum aggregation",charIndex:3150},{level:3,title:"Value Count Aggregation",slug:"value-count-aggregation",normalizedTitle:"value count aggregation",charIndex:3557},{level:3,title:"Cardinality Aggregation",slug:"cardinality-aggregation",normalizedTitle:"cardinality aggregation",charIndex:3869},{level:3,title:"Stats Aggregation",slug:"stats-aggregation",normalizedTitle:"stats aggregation",charIndex:4440},{level:3,title:"Extended Stats Aggregation",slug:"extended-stats-aggregation",normalizedTitle:"extended stats aggregation",charIndex:4854},{level:3,title:"Percentiles Aggregation",slug:"percentiles-aggregation",normalizedTitle:"percentiles aggregation",charIndex:5546},{level:3,title:"Percentiles Ranks Aggregation",slug:"percentiles-ranks-aggregation",normalizedTitle:"percentiles ranks aggregation",charIndex:6434},{level:2,title:"桶聚合",slug:"桶聚合",normalizedTitle:"桶聚合",charIndex:186},{level:3,title:"Terms Aggregation",slug:"terms-aggregation",normalizedTitle:"terms aggregation",charIndex:7707},{level:3,title:"Filter Aggregation",slug:"filter-aggregation",normalizedTitle:"filter aggregation",charIndex:7872},{level:3,title:"Filters Aggregation",slug:"filters-aggregation",normalizedTitle:"filters aggregation",charIndex:7964},{level:3,title:"Range Aggregation",slug:"range-aggregation",normalizedTitle:"range aggregation",charIndex:8029},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:12009}],headersStr:"聚合的具体结构 指标聚合 Max Aggregation Min Aggregation Avg Aggregation Sum Aggregation Value Count Aggregation Cardinality Aggregation Stats Aggregation Extended Stats Aggregation Percentiles Aggregation Percentiles Ranks Aggregation 桶聚合 Terms Aggregation Filter Aggregation Filters Aggregation Range Aggregation 参考资料",content:'# Elasticsearch 聚合\n\nElasticsearch 是一个分布式的全文搜索引擎，索引和搜索是 Elasticsearch 的基本功能。事实上，Elasticsearch 的聚合（Aggregations）功能也十分强大，允许在数据上做复杂的分析统计。Elasticsearch 提供的聚合分析功能主要有指标聚合(metrics aggregations)、桶聚合(bucket aggregations)、管道聚合(pipeline aggregations) 和 矩阵聚合(matrix aggregations) 四大类，管道聚合和矩阵聚合官方说明是在试验阶段，后期会完全更改或者移除，这里不再对管道聚合和矩阵聚合进行讲解。\n\n\n# 聚合的具体结构\n\n所有的聚合，无论它们是什么类型，都遵从以下的规则。\n\n * 使用查询中同样的 JSON 请求来定义它们，而且你是使用键 aggregations 或者是 aggs 来进行标记。需要给每个聚合起一个名字，指定它的类型以及和该类型相关的选项。\n * 它们运行在查询的结果之上。和查询不匹配的文档不会计算在内，除非你使用 global 聚集将不匹配的文档囊括其中。\n * 可以进一步过滤查询的结果，而不影响聚集。\n\n以下是聚合的基本结构：\n\n"aggregations" : { \x3c!-- 最外层的聚合键，也可以缩写为 aggs --\x3e\n    "<aggregation_name>" : { \x3c!-- 聚合的自定义名字 --\x3e\n        "<aggregation_type>" : { \x3c!-- 聚合的类型，指标相关的，如 max、min、avg、sum，桶相关的 terms、filter 等 --\x3e\n            <aggregation_body> \x3c!-- 聚合体：对哪些字段进行聚合，可以取字段的值，也可以是脚本计算的结果 --\x3e\n        }\n        [,"meta" : {  [<meta_data_body>] } ]? \x3c!-- 元 --\x3e\n        [,"aggregations" : { [<sub_aggregation>]+ } ]? \x3c!-- 在聚合里面在定义子聚合 --\x3e\n    }\n    [,"<aggregation_name_2>" : { ... } ]* \x3c!-- 聚合的自定义名字 2 --\x3e\n}\n\n\n * 在最上层有一个 aggregations 的键，可以缩写为 aggs。\n * 在下面一层，需要为聚合指定一个名字。可以在请求的返回中看到这个名字。在同一个请求中使用多个聚合时，这一点非常有用，它让你可以很容易地理解每组结果的含义。\n * 最后，必须要指定聚合的类型。\n\n> 关于聚合分析的值来源，可以取字段的值，也可以是脚本计算的结果。\n> \n> 但是用脚本计算的结果时，需要注意脚本的性能和安全性；尽管多数聚集类型允许使用脚本，但是脚本使得聚集变得缓慢，因为脚本必须在每篇文档上运行。为了避免脚本的运行，可以在索引阶段进行计算。\n> \n> 此外，脚本也可以被人可能利用进行恶意代码攻击，尽量使用沙盒（sandbox）内的脚本语言。\n\n示例：查询所有球员的平均年龄是多少，并对球员的平均薪水加 188（也可以理解为每名球员加 188 后的平均薪水）。\n\nPOST /player/_search?size=0\n{\n  "aggs": {\n    "avg_age": {\n      "avg": {\n        "field": "age"\n      }\n    },\n    "avg_salary_188": {\n      "avg": {\n        "script": {\n          "source": "doc.salary.value + 188"\n        }\n      }\n    }\n  }\n}\n\n\n\n# 指标聚合\n\n指标聚合（又称度量聚合）主要从不同文档的分组中提取统计数据，或者，从来自其他聚合的文档桶来提取统计数据。\n\n这些统计数据通常来自数值型字段，如最小或者平均价格。用户可以单独获取每项统计数据，或者也可以使用 stats 聚合来同时获取它们。更高级的统计数据，如平方和或者是标准差，可以通过 extended stats 聚合来获取。\n\n\n# Max Aggregation\n\nMax Aggregation 用于最大值统计。例如，统计 sales 索引中价格最高的是哪本书，并且计算出对应的价格的 2 倍值，查询语句如下：\n\nGET /sales/_search?size=0\n{\n  "aggs" : {\n    "max_price" : {\n      "max" : {\n        "field" : "price"\n      }\n    },\n    "max_price_2" : {\n      "max" : {\n        "field" : "price",\n        "script": {\n          "source": "_value * 2.0"\n        }\n      }\n    }\n  }\n}\n\n\n指定的 field，在脚本中可以用 _value 取字段的值。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "max_price": {\n      "value": 188.0\n    },\n    "max_price_2": {\n      "value": 376.0\n    }\n  }\n}\n\n\n\n# Min Aggregation\n\nMin Aggregation 用于最小值统计。例如，统计 sales 索引中价格最低的是哪本书，查询语句如下：\n\nGET /sales/_search?size=0\n{\n  "aggs" : {\n    "min_price" : {\n      "min" : {\n        "field" : "price"\n      }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "min_price": {\n      "value": 18.0\n    }\n  }\n}\n\n\n\n# Avg Aggregation\n\nAvg Aggregation 用于计算平均值。例如，统计 exams 索引中考试的平均分数，如未存在分数，默认为 60 分，查询语句如下：\n\nGET /exams/_search?size=0\n{\n  "aggs" : {\n    "avg_grade" : {\n      "avg" : {\n        "field" : "grade",\n        "missing": 60\n      }\n    }\n  }\n}\n\n\n如果指定字段没有值，可以通过 missing 指定默认值；若未指定默认值，缺失该字段值的文档将被忽略（计算）。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "avg_grade": {\n      "value": 78.0\n    }\n  }\n}\n\n\n除了常规的平均值聚合计算外，elasticsearch 还提供了加权平均值的聚合计算，详情参见 Elasticsearch 指标聚合之 Weighted Avg Aggregation。\n\n\n# Sum Aggregation\n\nSum Aggregation 用于计算总和。例如，统计 sales 索引中 type 字段中匹配 hat 的价格总和，查询语句如下：\n\nGET /exams/_search?size=0\n{\n  "query" : {\n    "constant_score" : {\n      "filter" : {\n        "match" : { "type" : "hat" }\n      }\n    }\n  },\n  "aggs" : {\n    "hat_prices" : {\n      "sum" : { "field" : "price" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "hat_prices": {\n      "value": 567.0\n    }\n  }\n}\n\n\n\n# Value Count Aggregation\n\nValue Count Aggregation 可按字段统计文档数量。例如，统计 books 索引中包含 author 字段的文档数量，查询语句如下：\n\nGET /books/_search?size=0\n{\n  "aggs" : {\n    "doc_count" : {\n      "value_count" : { "field" : "author" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "doc_count": {\n      "value": 5\n    }\n  }\n}\n\n\n\n# Cardinality Aggregation\n\nCardinality Aggregation 用于基数统计，其作用是先执行类似 SQL 中的 distinct 操作，去掉集合中的重复项，然后统计去重后的集合长度。例如，在 books 索引中对 language 字段进行 cardinality 操作可以统计出编程语言的种类数，查询语句如下：\n\nGET /books/_search?size=0\n{\n  "aggs" : {\n    "all_lan" : {\n      "cardinality" : { "field" : "language" }\n    },\n    "title_cnt" : {\n      "cardinality" : { "field" : "title.keyword" }\n    }\n  }\n}\n\n\n假设 title 字段为文本类型（text），去重时需要指定 keyword，表示把 title 作为整体去重，即不分词统计。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "all_lan": {\n      "value": 8\n    },\n    "title_cnt": {\n      "value": 18\n    }\n  }\n}\n\n\n\n# Stats Aggregation\n\nStats Aggregation 用于基本统计，会一次返回 count、max、min、avg 和 sum 这 5 个指标。例如，在 exams 索引中对 grade 字段进行分数相关的基本统计，查询语句如下：\n\nGET /exams/_search?size=0\n{\n  "aggs" : {\n    "grades_stats" : {\n      "stats" : { "field" : "grade" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "grades_stats": {\n      "count": 2,\n      "min": 50.0,\n      "max": 100.0,\n      "avg": 75.0,\n      "sum": 150.0\n    }\n  }\n}\n\n\n\n# Extended Stats Aggregation\n\nExtended Stats Aggregation 用于高级统计，和基本统计功能类似，但是会比基本统计多出以下几个统计结果，sum_of_squares（平方和）、variance（方差）、std_deviation（标准差）、std_deviation_bounds（平均值加/减两个标准差的区间）。在 exams 索引中对 grade 字段进行分数相关的高级统计，查询语句如下：\n\nGET /exams/_search?size=0\n{\n  "aggs" : {\n    "grades_stats" : {\n      "extended_stats" : { "field" : "grade" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "grades_stats": {\n      "count": 2,\n      "min": 50.0,\n      "max": 100.0,\n      "avg": 75.0,\n      "sum": 150.0,\n      "sum_of_squares": 12500.0,\n      "variance": 625.0,\n      "std_deviation": 25.0,\n      "std_deviation_bounds": {\n        "upper": 125.0,\n        "lower": 25.0\n      }\n    }\n  }\n}\n\n\n\n# Percentiles Aggregation\n\nPercentiles Aggregation 用于百分位统计。百分位数是一个统计学术语，如果将一组数据从大到小排序，并计算相应的累计百分位，某一百分位所对应数据的值就称为这一百分位的百分位数。默认情况下，累计百分位为 [ 1, 5, 25, 50, 75, 95, 99 ]。以下例子给出了在 latency 索引中对 load_time 字段进行加载时间的百分位统计，查询语句如下：\n\nGET latency/_search\n{\n  "size": 0,\n  "aggs" : {\n    "load_time_outlier" : {\n      "percentiles" : {\n        "field" : "load_time"\n      }\n    }\n  }\n}\n\n\n需要注意的是，如上的 load_time 字段必须是数字类型。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "load_time_outlier": {\n      "values" : {\n        "1.0": 5.0,\n        "5.0": 25.0,\n        "25.0": 165.0,\n        "50.0": 445.0,\n        "75.0": 725.0,\n        "95.0": 945.0,\n        "99.0": 985.0\n      }\n    }\n  }\n}\n\n\n百分位的统计也可以指定 percents 参数指定百分位，如下：\n\nGET latency/_search\n{\n  "size": 0,\n  "aggs" : {\n    "load_time_outlier" : {\n      "percentiles" : {\n        "field" : "load_time",\n        "percents": [60, 80, 95]\n      }\n    }\n  }\n}\n\n\n\n# Percentiles Ranks Aggregation\n\nPercentiles Ranks Aggregation 与 Percentiles Aggregation 统计恰恰相反，就是想看当前数值处在什么范围内（百分位）， 假如你查一下当前值 500 和 600 所处的百分位，发现是 90.01 和 100，那么说明有 90.01 % 的数值都在 500 以内，100 % 的数值在 600 以内。\n\nGET latency/_search\n{\n  "size": 0,\n    "aggs" : {\n      "load_time_ranks" : {\n        "percentile_ranks" : {\n          "field" : "load_time",\n          "values" : [500, 600]\n        }\n      }\n  }\n}\n\n\n同样 load_time 字段必须是数字类型。\n\n返回结果大概类似如下：\n\n{\n  ...\n  "aggregations": {\n    "load_time_ranks": {\n      "values" : {\n        "500.0": 90.01,\n        "600.0": 100.0\n      }\n    }\n  }\n}\n\n\n可以设置 keyed 参数为 true，将对应的 values 作为桶 key 一起返回，默认是 false。\n\nGET latency/_search\n{\n  "size": 0,\n  "aggs": {\n    "load_time_ranks": {\n      "percentile_ranks": {\n        "field": "load_time",\n        "values": [500, 600],\n        "keyed": true\n      }\n    }\n  }\n}\n\n\n返回结果如下：\n\n{\n  ...\n  "aggregations": {\n    "load_time_ranks": {\n      "values": [\n        {\n          "key": 500.0,\n          "value": 90.01\n        },\n        {\n          "key": 600.0,\n          "value": 100.0\n        }\n      ]\n    }\n  }\n}\n\n\n\n# 桶聚合\n\nbucket 可以理解为一个桶，它会遍历文档中的内容，凡是符合某一要求的就放入一个桶中，分桶相当于 SQL 中的 group by。从另外一个角度，可以将指标聚合看成单桶聚合，即把所有文档放到一个桶中，而桶聚合是多桶型聚合，它根据相应的条件进行分组。\n\n种类                                      描述/场景\n词项聚合（Terms Aggregation）                 用于分组聚合，让用户得知文档中每个词项的频率，它返回每个词项出现的次数。\n差异词项聚合（Significant Terms Aggregation）   它会返回某个词项在整个索引中和在查询结果中的词频差异，这有助于我们发现搜索场景中有意义的词。\n过滤器聚合（Filter Aggregation）               指定过滤器匹配的所有文档到单个桶（bucket），通常这将用于将当前聚合上下文缩小到一组特定的文档。\n多过滤器聚合（Filters Aggregation）             指定多个过滤器匹配所有文档到多个桶（bucket）。\n范围聚合（Range Aggregation）                 范围聚合，用于反映数据的分布情况。\n日期范围聚合（Date Range Aggregation）          专门用于日期类型的范围聚合。\nIP 范围聚合（IP Range Aggregation）           用于对 IP 类型数据范围聚合。\n直方图聚合（Histogram Aggregation）            可能是数值，或者日期型，和范围聚集类似。\n时间直方图聚合（Date Histogram Aggregation）     时间直方图聚合，常用于按照日期对文档进行统计并绘制条形图。\n空值聚合（Missing Aggregation）               空值聚合，可以把文档集中所有缺失字段的文档分到一个桶中。\n地理点范围聚合（Geo Distance Aggregation）       用于对地理点（geo point）做范围统计。\n\n\n# Terms Aggregation\n\nTerms Aggregation 用于词项的分组聚合。最为经典的用例是获取 X 中最频繁（top frequent）的项目，其中 X 是文档中的某个字段，如用户的名称、标签或分类。由于 terms 聚集统计的是每个词条，而不是整个字段值，因此通常需要在一个非分析型的字段上运行这种聚集。原因是, 你期望“big data”作为词组统计，而不是“big”单独统计一次，“data”再单独统计一次。\n\n用户可以使用 terms 聚集，从分析型字段（如内容）中抽取最为频繁的词条。还可以使用这种信息来生成一个单词云。\n\n{\n  "aggs": {\n    "profit_terms": {\n      "terms": { // terms 聚合 关键字\n        "field": "profit",\n        ......\n      }\n    }\n  }\n}\n\n\n在 terms 分桶的基础上，还可以对每个桶进行指标统计，也可以基于一些指标或字段值进行排序。示例如下：\n\n{\n  "aggs": {\n    "item_terms": {\n      "terms": {\n        "field": "item_id",\n        "size": 1000,\n        "order":[{\n          "gmv_stat": "desc"\n        },{\n          "gmv_180d": "desc"\n        }]\n      },\n      "aggs": {\n        "gmv_stat": {\n          "sum": {\n            "field": "gmv"\n          }\n        },\n        "gmv_180d": {\n          "sum": {\n            "script": "doc[\'gmv_90d\'].value*2"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n返回的结果如下：\n\n{\n  ...\n  "aggregations": {\n    "hospital_id_agg": {\n      "doc_count_error_upper_bound": 0,\n      "sum_other_doc_count": 260,\n      "buckets": [\n        {\n          "key": 23388,\n          "doc_count": 18,\n          "gmv_stat": {\n            "value": 176220\n          },\n          "gmv_180d": {\n            "value": 89732\n          }\n        },\n        {\n          "key": 96117,\n          "doc_count": 16,\n          "gmv_stat": {\n            "value": 129306\n          },\n          "gmv_180d": {\n            "value": 56988\n          }\n        },\n        ...\n      ]\n    }\n  }\n}\n\n\n默认情况下返回按文档计数从高到低的前 10 个分组，可以通过 size 参数指定返回的分组数。\n\n\n# Filter Aggregation\n\nFilter Aggregation 是过滤器聚合，可以把符合过滤器中的条件的文档分到一个桶中，即是单分组聚合。\n\n{\n  "aggs": {\n    "age_terms": {\n      "filter": {"match":{"gender":"F"}},\n      "aggs": {\n        "avg_age": {\n          "avg": {\n            "field": "age"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n# Filters Aggregation\n\nFilters Aggregation 是多过滤器聚合，可以把符合多个过滤条件的文档分到不同的桶中，即每个分组关联一个过滤条件，并收集所有满足自身过滤条件的文档。\n\n{\n  "size": 0,\n  "aggs": {\n    "messages": {\n      "filters": {\n        "filters": {\n          "errors": { "match": { "body": "error" } },\n          "warnings": { "match": { "body": "warning" } }\n        }\n      }\n    }\n  }\n}\n\n\n在这个例子里，我们分析日志信息。聚合会创建两个关于日志数据的分组，一个收集包含错误信息的文档，另一个收集包含告警信息的文档。而且每个分组会按月份划分。\n\n{\n  ...\n  "aggregations": {\n    "messages": {\n      "buckets": {\n        "errors": {\n          "doc_count": 1\n        },\n        "warnings": {\n          "doc_count": 2\n        }\n      }\n    }\n  }\n}\n\n\n\n# Range Aggregation\n\nRange Aggregation 范围聚合是一个基于多组值来源的聚合，可以让用户定义一系列范围，每个范围代表一个分组。在聚合执行的过程中，从每个文档提取出来的值都会检查每个分组的范围，并且使相关的文档落入分组中。注意，范围聚合的每个范围内包含 from 值但是排除 to 值。\n\n{\n  "aggs": {\n    "age_range": {\n      "range": {\n        "field": "age",\n          "ranges": [{\n            "to": 25\n          },\n          {\n            "from": 25,\n            "to": 35\n          },\n          {\n            "from": 35\n          }]\n        },\n        "aggs": {\n          "bmax": {\n            "max": {\n              "field": "balance"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n返回结果如下：\n\n{\n  ...\n  "aggregations": {\n    "age_range": {\n      "buckets": [{\n        "key": "*-25.0",\n        "to": 25,\n        "doc_count": 225,\n        "bmax": {\n          "value": 49587\n        }\n      },\n      {\n        "key": "25.0-35.0",\n        "from": 25,\n        "to": 35,\n        "doc_count": 485,\n        "bmax": {\n          "value": 49795\n        }\n      },\n      {\n        "key": "35.0-*",\n        "from": 35,\n        "doc_count": 290,\n        "bmax": {\n          "value": 49989\n        }\n      }]\n    }\n  }\n}\n\n\n\n# 参考资料\n\n * Elasticsearch 教程',normalizedContent:'# elasticsearch 聚合\n\nelasticsearch 是一个分布式的全文搜索引擎，索引和搜索是 elasticsearch 的基本功能。事实上，elasticsearch 的聚合（aggregations）功能也十分强大，允许在数据上做复杂的分析统计。elasticsearch 提供的聚合分析功能主要有指标聚合(metrics aggregations)、桶聚合(bucket aggregations)、管道聚合(pipeline aggregations) 和 矩阵聚合(matrix aggregations) 四大类，管道聚合和矩阵聚合官方说明是在试验阶段，后期会完全更改或者移除，这里不再对管道聚合和矩阵聚合进行讲解。\n\n\n# 聚合的具体结构\n\n所有的聚合，无论它们是什么类型，都遵从以下的规则。\n\n * 使用查询中同样的 json 请求来定义它们，而且你是使用键 aggregations 或者是 aggs 来进行标记。需要给每个聚合起一个名字，指定它的类型以及和该类型相关的选项。\n * 它们运行在查询的结果之上。和查询不匹配的文档不会计算在内，除非你使用 global 聚集将不匹配的文档囊括其中。\n * 可以进一步过滤查询的结果，而不影响聚集。\n\n以下是聚合的基本结构：\n\n"aggregations" : { \x3c!-- 最外层的聚合键，也可以缩写为 aggs --\x3e\n    "<aggregation_name>" : { \x3c!-- 聚合的自定义名字 --\x3e\n        "<aggregation_type>" : { \x3c!-- 聚合的类型，指标相关的，如 max、min、avg、sum，桶相关的 terms、filter 等 --\x3e\n            <aggregation_body> \x3c!-- 聚合体：对哪些字段进行聚合，可以取字段的值，也可以是脚本计算的结果 --\x3e\n        }\n        [,"meta" : {  [<meta_data_body>] } ]? \x3c!-- 元 --\x3e\n        [,"aggregations" : { [<sub_aggregation>]+ } ]? \x3c!-- 在聚合里面在定义子聚合 --\x3e\n    }\n    [,"<aggregation_name_2>" : { ... } ]* \x3c!-- 聚合的自定义名字 2 --\x3e\n}\n\n\n * 在最上层有一个 aggregations 的键，可以缩写为 aggs。\n * 在下面一层，需要为聚合指定一个名字。可以在请求的返回中看到这个名字。在同一个请求中使用多个聚合时，这一点非常有用，它让你可以很容易地理解每组结果的含义。\n * 最后，必须要指定聚合的类型。\n\n> 关于聚合分析的值来源，可以取字段的值，也可以是脚本计算的结果。\n> \n> 但是用脚本计算的结果时，需要注意脚本的性能和安全性；尽管多数聚集类型允许使用脚本，但是脚本使得聚集变得缓慢，因为脚本必须在每篇文档上运行。为了避免脚本的运行，可以在索引阶段进行计算。\n> \n> 此外，脚本也可以被人可能利用进行恶意代码攻击，尽量使用沙盒（sandbox）内的脚本语言。\n\n示例：查询所有球员的平均年龄是多少，并对球员的平均薪水加 188（也可以理解为每名球员加 188 后的平均薪水）。\n\npost /player/_search?size=0\n{\n  "aggs": {\n    "avg_age": {\n      "avg": {\n        "field": "age"\n      }\n    },\n    "avg_salary_188": {\n      "avg": {\n        "script": {\n          "source": "doc.salary.value + 188"\n        }\n      }\n    }\n  }\n}\n\n\n\n# 指标聚合\n\n指标聚合（又称度量聚合）主要从不同文档的分组中提取统计数据，或者，从来自其他聚合的文档桶来提取统计数据。\n\n这些统计数据通常来自数值型字段，如最小或者平均价格。用户可以单独获取每项统计数据，或者也可以使用 stats 聚合来同时获取它们。更高级的统计数据，如平方和或者是标准差，可以通过 extended stats 聚合来获取。\n\n\n# max aggregation\n\nmax aggregation 用于最大值统计。例如，统计 sales 索引中价格最高的是哪本书，并且计算出对应的价格的 2 倍值，查询语句如下：\n\nget /sales/_search?size=0\n{\n  "aggs" : {\n    "max_price" : {\n      "max" : {\n        "field" : "price"\n      }\n    },\n    "max_price_2" : {\n      "max" : {\n        "field" : "price",\n        "script": {\n          "source": "_value * 2.0"\n        }\n      }\n    }\n  }\n}\n\n\n指定的 field，在脚本中可以用 _value 取字段的值。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "max_price": {\n      "value": 188.0\n    },\n    "max_price_2": {\n      "value": 376.0\n    }\n  }\n}\n\n\n\n# min aggregation\n\nmin aggregation 用于最小值统计。例如，统计 sales 索引中价格最低的是哪本书，查询语句如下：\n\nget /sales/_search?size=0\n{\n  "aggs" : {\n    "min_price" : {\n      "min" : {\n        "field" : "price"\n      }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "min_price": {\n      "value": 18.0\n    }\n  }\n}\n\n\n\n# avg aggregation\n\navg aggregation 用于计算平均值。例如，统计 exams 索引中考试的平均分数，如未存在分数，默认为 60 分，查询语句如下：\n\nget /exams/_search?size=0\n{\n  "aggs" : {\n    "avg_grade" : {\n      "avg" : {\n        "field" : "grade",\n        "missing": 60\n      }\n    }\n  }\n}\n\n\n如果指定字段没有值，可以通过 missing 指定默认值；若未指定默认值，缺失该字段值的文档将被忽略（计算）。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "avg_grade": {\n      "value": 78.0\n    }\n  }\n}\n\n\n除了常规的平均值聚合计算外，elasticsearch 还提供了加权平均值的聚合计算，详情参见 elasticsearch 指标聚合之 weighted avg aggregation。\n\n\n# sum aggregation\n\nsum aggregation 用于计算总和。例如，统计 sales 索引中 type 字段中匹配 hat 的价格总和，查询语句如下：\n\nget /exams/_search?size=0\n{\n  "query" : {\n    "constant_score" : {\n      "filter" : {\n        "match" : { "type" : "hat" }\n      }\n    }\n  },\n  "aggs" : {\n    "hat_prices" : {\n      "sum" : { "field" : "price" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "hat_prices": {\n      "value": 567.0\n    }\n  }\n}\n\n\n\n# value count aggregation\n\nvalue count aggregation 可按字段统计文档数量。例如，统计 books 索引中包含 author 字段的文档数量，查询语句如下：\n\nget /books/_search?size=0\n{\n  "aggs" : {\n    "doc_count" : {\n      "value_count" : { "field" : "author" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "doc_count": {\n      "value": 5\n    }\n  }\n}\n\n\n\n# cardinality aggregation\n\ncardinality aggregation 用于基数统计，其作用是先执行类似 sql 中的 distinct 操作，去掉集合中的重复项，然后统计去重后的集合长度。例如，在 books 索引中对 language 字段进行 cardinality 操作可以统计出编程语言的种类数，查询语句如下：\n\nget /books/_search?size=0\n{\n  "aggs" : {\n    "all_lan" : {\n      "cardinality" : { "field" : "language" }\n    },\n    "title_cnt" : {\n      "cardinality" : { "field" : "title.keyword" }\n    }\n  }\n}\n\n\n假设 title 字段为文本类型（text），去重时需要指定 keyword，表示把 title 作为整体去重，即不分词统计。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "all_lan": {\n      "value": 8\n    },\n    "title_cnt": {\n      "value": 18\n    }\n  }\n}\n\n\n\n# stats aggregation\n\nstats aggregation 用于基本统计，会一次返回 count、max、min、avg 和 sum 这 5 个指标。例如，在 exams 索引中对 grade 字段进行分数相关的基本统计，查询语句如下：\n\nget /exams/_search?size=0\n{\n  "aggs" : {\n    "grades_stats" : {\n      "stats" : { "field" : "grade" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "grades_stats": {\n      "count": 2,\n      "min": 50.0,\n      "max": 100.0,\n      "avg": 75.0,\n      "sum": 150.0\n    }\n  }\n}\n\n\n\n# extended stats aggregation\n\nextended stats aggregation 用于高级统计，和基本统计功能类似，但是会比基本统计多出以下几个统计结果，sum_of_squares（平方和）、variance（方差）、std_deviation（标准差）、std_deviation_bounds（平均值加/减两个标准差的区间）。在 exams 索引中对 grade 字段进行分数相关的高级统计，查询语句如下：\n\nget /exams/_search?size=0\n{\n  "aggs" : {\n    "grades_stats" : {\n      "extended_stats" : { "field" : "grade" }\n    }\n  }\n}\n\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "grades_stats": {\n      "count": 2,\n      "min": 50.0,\n      "max": 100.0,\n      "avg": 75.0,\n      "sum": 150.0,\n      "sum_of_squares": 12500.0,\n      "variance": 625.0,\n      "std_deviation": 25.0,\n      "std_deviation_bounds": {\n        "upper": 125.0,\n        "lower": 25.0\n      }\n    }\n  }\n}\n\n\n\n# percentiles aggregation\n\npercentiles aggregation 用于百分位统计。百分位数是一个统计学术语，如果将一组数据从大到小排序，并计算相应的累计百分位，某一百分位所对应数据的值就称为这一百分位的百分位数。默认情况下，累计百分位为 [ 1, 5, 25, 50, 75, 95, 99 ]。以下例子给出了在 latency 索引中对 load_time 字段进行加载时间的百分位统计，查询语句如下：\n\nget latency/_search\n{\n  "size": 0,\n  "aggs" : {\n    "load_time_outlier" : {\n      "percentiles" : {\n        "field" : "load_time"\n      }\n    }\n  }\n}\n\n\n需要注意的是，如上的 load_time 字段必须是数字类型。\n\n聚合结果如下：\n\n{\n  ...\n  "aggregations": {\n    "load_time_outlier": {\n      "values" : {\n        "1.0": 5.0,\n        "5.0": 25.0,\n        "25.0": 165.0,\n        "50.0": 445.0,\n        "75.0": 725.0,\n        "95.0": 945.0,\n        "99.0": 985.0\n      }\n    }\n  }\n}\n\n\n百分位的统计也可以指定 percents 参数指定百分位，如下：\n\nget latency/_search\n{\n  "size": 0,\n  "aggs" : {\n    "load_time_outlier" : {\n      "percentiles" : {\n        "field" : "load_time",\n        "percents": [60, 80, 95]\n      }\n    }\n  }\n}\n\n\n\n# percentiles ranks aggregation\n\npercentiles ranks aggregation 与 percentiles aggregation 统计恰恰相反，就是想看当前数值处在什么范围内（百分位）， 假如你查一下当前值 500 和 600 所处的百分位，发现是 90.01 和 100，那么说明有 90.01 % 的数值都在 500 以内，100 % 的数值在 600 以内。\n\nget latency/_search\n{\n  "size": 0,\n    "aggs" : {\n      "load_time_ranks" : {\n        "percentile_ranks" : {\n          "field" : "load_time",\n          "values" : [500, 600]\n        }\n      }\n  }\n}\n\n\n同样 load_time 字段必须是数字类型。\n\n返回结果大概类似如下：\n\n{\n  ...\n  "aggregations": {\n    "load_time_ranks": {\n      "values" : {\n        "500.0": 90.01,\n        "600.0": 100.0\n      }\n    }\n  }\n}\n\n\n可以设置 keyed 参数为 true，将对应的 values 作为桶 key 一起返回，默认是 false。\n\nget latency/_search\n{\n  "size": 0,\n  "aggs": {\n    "load_time_ranks": {\n      "percentile_ranks": {\n        "field": "load_time",\n        "values": [500, 600],\n        "keyed": true\n      }\n    }\n  }\n}\n\n\n返回结果如下：\n\n{\n  ...\n  "aggregations": {\n    "load_time_ranks": {\n      "values": [\n        {\n          "key": 500.0,\n          "value": 90.01\n        },\n        {\n          "key": 600.0,\n          "value": 100.0\n        }\n      ]\n    }\n  }\n}\n\n\n\n# 桶聚合\n\nbucket 可以理解为一个桶，它会遍历文档中的内容，凡是符合某一要求的就放入一个桶中，分桶相当于 sql 中的 group by。从另外一个角度，可以将指标聚合看成单桶聚合，即把所有文档放到一个桶中，而桶聚合是多桶型聚合，它根据相应的条件进行分组。\n\n种类                                      描述/场景\n词项聚合（terms aggregation）                 用于分组聚合，让用户得知文档中每个词项的频率，它返回每个词项出现的次数。\n差异词项聚合（significant terms aggregation）   它会返回某个词项在整个索引中和在查询结果中的词频差异，这有助于我们发现搜索场景中有意义的词。\n过滤器聚合（filter aggregation）               指定过滤器匹配的所有文档到单个桶（bucket），通常这将用于将当前聚合上下文缩小到一组特定的文档。\n多过滤器聚合（filters aggregation）             指定多个过滤器匹配所有文档到多个桶（bucket）。\n范围聚合（range aggregation）                 范围聚合，用于反映数据的分布情况。\n日期范围聚合（date range aggregation）          专门用于日期类型的范围聚合。\nip 范围聚合（ip range aggregation）           用于对 ip 类型数据范围聚合。\n直方图聚合（histogram aggregation）            可能是数值，或者日期型，和范围聚集类似。\n时间直方图聚合（date histogram aggregation）     时间直方图聚合，常用于按照日期对文档进行统计并绘制条形图。\n空值聚合（missing aggregation）               空值聚合，可以把文档集中所有缺失字段的文档分到一个桶中。\n地理点范围聚合（geo distance aggregation）       用于对地理点（geo point）做范围统计。\n\n\n# terms aggregation\n\nterms aggregation 用于词项的分组聚合。最为经典的用例是获取 x 中最频繁（top frequent）的项目，其中 x 是文档中的某个字段，如用户的名称、标签或分类。由于 terms 聚集统计的是每个词条，而不是整个字段值，因此通常需要在一个非分析型的字段上运行这种聚集。原因是, 你期望“big data”作为词组统计，而不是“big”单独统计一次，“data”再单独统计一次。\n\n用户可以使用 terms 聚集，从分析型字段（如内容）中抽取最为频繁的词条。还可以使用这种信息来生成一个单词云。\n\n{\n  "aggs": {\n    "profit_terms": {\n      "terms": { // terms 聚合 关键字\n        "field": "profit",\n        ......\n      }\n    }\n  }\n}\n\n\n在 terms 分桶的基础上，还可以对每个桶进行指标统计，也可以基于一些指标或字段值进行排序。示例如下：\n\n{\n  "aggs": {\n    "item_terms": {\n      "terms": {\n        "field": "item_id",\n        "size": 1000,\n        "order":[{\n          "gmv_stat": "desc"\n        },{\n          "gmv_180d": "desc"\n        }]\n      },\n      "aggs": {\n        "gmv_stat": {\n          "sum": {\n            "field": "gmv"\n          }\n        },\n        "gmv_180d": {\n          "sum": {\n            "script": "doc[\'gmv_90d\'].value*2"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n返回的结果如下：\n\n{\n  ...\n  "aggregations": {\n    "hospital_id_agg": {\n      "doc_count_error_upper_bound": 0,\n      "sum_other_doc_count": 260,\n      "buckets": [\n        {\n          "key": 23388,\n          "doc_count": 18,\n          "gmv_stat": {\n            "value": 176220\n          },\n          "gmv_180d": {\n            "value": 89732\n          }\n        },\n        {\n          "key": 96117,\n          "doc_count": 16,\n          "gmv_stat": {\n            "value": 129306\n          },\n          "gmv_180d": {\n            "value": 56988\n          }\n        },\n        ...\n      ]\n    }\n  }\n}\n\n\n默认情况下返回按文档计数从高到低的前 10 个分组，可以通过 size 参数指定返回的分组数。\n\n\n# filter aggregation\n\nfilter aggregation 是过滤器聚合，可以把符合过滤器中的条件的文档分到一个桶中，即是单分组聚合。\n\n{\n  "aggs": {\n    "age_terms": {\n      "filter": {"match":{"gender":"f"}},\n      "aggs": {\n        "avg_age": {\n          "avg": {\n            "field": "age"\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n# filters aggregation\n\nfilters aggregation 是多过滤器聚合，可以把符合多个过滤条件的文档分到不同的桶中，即每个分组关联一个过滤条件，并收集所有满足自身过滤条件的文档。\n\n{\n  "size": 0,\n  "aggs": {\n    "messages": {\n      "filters": {\n        "filters": {\n          "errors": { "match": { "body": "error" } },\n          "warnings": { "match": { "body": "warning" } }\n        }\n      }\n    }\n  }\n}\n\n\n在这个例子里，我们分析日志信息。聚合会创建两个关于日志数据的分组，一个收集包含错误信息的文档，另一个收集包含告警信息的文档。而且每个分组会按月份划分。\n\n{\n  ...\n  "aggregations": {\n    "messages": {\n      "buckets": {\n        "errors": {\n          "doc_count": 1\n        },\n        "warnings": {\n          "doc_count": 2\n        }\n      }\n    }\n  }\n}\n\n\n\n# range aggregation\n\nrange aggregation 范围聚合是一个基于多组值来源的聚合，可以让用户定义一系列范围，每个范围代表一个分组。在聚合执行的过程中，从每个文档提取出来的值都会检查每个分组的范围，并且使相关的文档落入分组中。注意，范围聚合的每个范围内包含 from 值但是排除 to 值。\n\n{\n  "aggs": {\n    "age_range": {\n      "range": {\n        "field": "age",\n          "ranges": [{\n            "to": 25\n          },\n          {\n            "from": 25,\n            "to": 35\n          },\n          {\n            "from": 35\n          }]\n        },\n        "aggs": {\n          "bmax": {\n            "max": {\n              "field": "balance"\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n\n返回结果如下：\n\n{\n  ...\n  "aggregations": {\n    "age_range": {\n      "buckets": [{\n        "key": "*-25.0",\n        "to": 25,\n        "doc_count": 225,\n        "bmax": {\n          "value": 49587\n        }\n      },\n      {\n        "key": "25.0-35.0",\n        "from": 25,\n        "to": 35,\n        "doc_count": 485,\n        "bmax": {\n          "value": 49795\n        }\n      },\n      {\n        "key": "35.0-*",\n        "from": 35,\n        "doc_count": 290,\n        "bmax": {\n          "value": 49989\n        }\n      }]\n    }\n  }\n}\n\n\n\n# 参考资料\n\n * elasticsearch 教程',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 分析器",frontmatter:{title:"Elasticsearch 分析器",date:"2022-02-22T21:01:01.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","分词"],permalink:"/pages/a5a001/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/09.Elasticsearch%E5%88%86%E6%9E%90%E5%99%A8.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/09.Elasticsearch分析器.md",key:"v-665de124",path:"/pages/a5a001/",headers:[{level:2,title:"字符过滤器（Character Filters）",slug:"字符过滤器-character-filters",normalizedTitle:"字符过滤器（character filters）",charIndex:164},{level:3,title:"HTML strip character filter",slug:"html-strip-character-filter",normalizedTitle:"html strip character filter",charIndex:661},{level:3,title:"Mapping character filter",slug:"mapping-character-filter",normalizedTitle:"mapping character filter",charIndex:897},{level:3,title:"Pattern Replace character filter",slug:"pattern-replace-character-filter",normalizedTitle:"pattern replace character filter",charIndex:1731},{level:2,title:"分词器（Tokenizer）",slug:"分词器-tokenizer",normalizedTitle:"分词器（tokenizer）",charIndex:189},{level:3,title:"elasticsearch-plugin 使用",slug:"elasticsearch-plugin-使用",normalizedTitle:"elasticsearch-plugin 使用",charIndex:2553},{level:3,title:"elasticsearch-analysis-ik 安装",slug:"elasticsearch-analysis-ik-安装",normalizedTitle:"elasticsearch-analysis-ik 安装",charIndex:3173},{level:3,title:"elasticsearch-analysis-ik 使用",slug:"elasticsearch-analysis-ik-使用",normalizedTitle:"elasticsearch-analysis-ik 使用",charIndex:3597},{level:4,title:"索引 mapping 设置",slug:"索引-mapping-设置",normalizedTitle:"索引 mapping 设置",charIndex:3771},{level:4,title:"查看 ik 分词结果",slug:"查看-ik-分词结果",normalizedTitle:"查看 ik 分词结果",charIndex:4534},{level:4,title:"elasticsearch-analysis-ik 自定义词典",slug:"elasticsearch-analysis-ik-自定义词典",normalizedTitle:"elasticsearch-analysis-ik 自定义词典",charIndex:5201},{level:2,title:"词元过滤器（Token Filters）",slug:"词元过滤器-token-filters",normalizedTitle:"词元过滤器（token filters）",charIndex:204},{level:3,title:"同义词",slug:"同义词",normalizedTitle:"同义词",charIndex:6126},{level:4,title:"同义词（synonym）配置语法",slug:"同义词-synonym-配置语法",normalizedTitle:"同义词（synonym）配置语法",charIndex:6882},{level:4,title:"同义词文档格式",slug:"同义词文档格式",normalizedTitle:"同义词文档格式",charIndex:7382},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:7772}],headersStr:"字符过滤器（Character Filters） HTML strip character filter Mapping character filter Pattern Replace character filter 分词器（Tokenizer） elasticsearch-plugin 使用 elasticsearch-analysis-ik 安装 elasticsearch-analysis-ik 使用 索引 mapping 设置 查看 ik 分词结果 elasticsearch-analysis-ik 自定义词典 词元过滤器（Token Filters） 同义词 同义词（synonym）配置语法 同义词文档格式 参考资料",content:'# Elasticsearch 分析器\n\n文本分析是把全文本转换为一系列单词（term/token）的过程，也叫分词。在 Elasticsearch 中，分词是通过 analyzer（分析器）来完成的，不管是索引还是搜索，都需要使用 analyzer（分析器）。分析器，分为内置分析器和自定义的分析器。\n\n分析器可以进一步细分为字符过滤器（Character Filters）、分词器（Tokenizer）和词元过滤器（Token Filters）三部分。它的执行顺序如下：\n\ncharacter filters -> tokenizer -> token filters\n\n\n# 字符过滤器（Character Filters）\n\ncharacter filter 的输入是原始的文本 text，如果配置了多个，它会按照配置的顺序执行，目前 ES 自带的 character filter 主要有如下 3 类：\n\n 1. html strip character filter：从文本中剥离 HTML 元素，并用其解码值替换 HTML 实体（如，将 ＆amp; 替换为 ＆）。\n 2. mapping character filter：自定义一个 map 映射，可以进行一些自定义的替换，如常用的大写变小写也可以在该环节设置。\n 3. pattern replace character filter：使用 java 正则表达式来匹配应替换为指定替换字符串的字符，此外，替换字符串可以引用正则表达式中的捕获组。\n\n\n# HTML strip character filter\n\nHTML strip 如下示例：\n\nGET /_analyze\n{\n  "tokenizer": "keyword",\n  "char_filter": [\n    "html_strip"\n  ],\n  "text": "<p>I&apos;m so <b>happy</b>!</p>"\n}\n\n\n经过 html_strip 字符过滤器处理后，输出如下：\n\n[ \\nI\'m so happy!\\n ]\n\n\n\n# Mapping character filter\n\nMapping character filter 接收键和值映射（key => value）作为配置参数，每当在预处理过程中遇到与键值映射中的键相同的字符串时，就会使用该键对应的值去替换它。\n\n原始文本中的字符串和键值映射中的键的匹配是贪心的，在对给定的文本进行预处理过程中如果配置的键值映射存在包含关系，会优先匹配最长键。同样也可以用空字符串进行替换。\n\nmapping char_filter 不像 html_strip 那样拆箱即可用，必须先进行配置才能使用，它有两个属性可以配置：\n\n参数名称            参数说明\nmappings        一组映射，每个元素的格式为 key => value。\nmappings_path   一个相对或者绝对的文件路径，指向一个每行包含一个 key =>value 映射的 UTF-8 编码文本映射文件。\n\nmapping char_filter 示例如下：\n\nGET /_analyze\n{\n  "tokenizer": "keyword",\n  "char_filter": [\n    {\n      "type": "mapping",\n      "mappings": [\n        "٠ => 0",\n        "١ => 1",\n        "٢ => 2",\n        "٣ => 3",\n        "٤ => 4",\n        "٥ => 5",\n        "٦ => 6",\n        "٧ => 7",\n        "٨ => 8",\n        "٩ => 9"\n      ]\n    }\n  ],\n  "text": "My license plate is ٢٥٠١٥"\n}\n\n\n分析结果如下：\n\n[ My license plate is 25015 ]\n\n\n\n# Pattern Replace character filter\n\nPattern Replace character filter 支持如下三个参数：\n\n参数名称          参数说明\npattern       必填参数，一个 java 的正则表达式。\nreplacement   替换字符串，可以使用 $1 ... $9 语法来引用捕获组。\nflags         Java 正则表达式的标志，具体参考 java 的 java.util.regex.Pattern 类的标志属性。\n\n如将输入的 text 中大于一个的空格都转变为一个空格，在 settings 时，配置示例如下：\n\n"char_filter": {\n  "multi_space_2_one": {\n    "pattern": "[ ]+",\n    "type": "pattern_replace",\n    "replacement": " "\n  },\n  ...\n}\n\n\n\n# 分词器（Tokenizer）\n\ntokenizer 即分词器，也是 analyzer 最重要的组件，它对文本进行分词；一个 analyzer 必需且只可包含一个 tokenizer。\n\nES 自带默认的分词器是 standard tokenizer，标准分词器提供基于语法的分词（基于 Unicode 文本分割算法），并且适用于大多数语言。\n\n此外有很多第三方的分词插件，如中文分词界最经典的 ik 分词器，它对应的 tokenizer 分为 ik_smart 和 ik_max_word，一个是智能分词（针对搜索侧），一个是全切分词（针对索引侧）。\n\nES 默认提供的分词器 standard 对中文分词不优化，效果差，一般会安装第三方中文分词插件，通常首先 elasticsearch-analysis-ik 插件，它其实是 ik 针对的 ES 的定制版。\n\n\n# elasticsearch-plugin 使用\n\n在安装 elasticsearch-analysis-ik 第三方之前，我们首先要了解 es 的插件管理工具 elasticsearch-plugin 的使用。\n\n现在的 elasticsearch 安装完后，在安装目录的 bin 目录下会存在 elasticsearch-plugin 命令工具，用它来对 es 插件进行管理。\n\nbin/elasticsearch-plugin\n\n\n其实该命令的是软连接，原始路径是：\n\nlibexec/bin/elasticsearch-plugin\n\n\n再进一步看脚本代码，你会发现，它是通过 elasticsearch-cli 执行 libexec/lib/tools/plugin-cli/elasticsearch-plugin-cli-x.x.x.jar。\n\n但一般使用者了解 elasticsearch-plugin 命令使用就可：\n\n#  安装指定的插件到当前 ES 节点中\nelasticsearch-plugin install {plugin_url}\n\n#  显示当前 ES 节点已经安装的插件列表\nelasticsearch-plugin list\n\n#  删除已安装的插件\nelasticsearch-plugin remove {plugin_name}\n\n\n> 在安装插件时，要保证安装的插件与 ES 版本一致。\n\n\n# elasticsearch-analysis-ik 安装\n\n在确定要安装的 ik 版本之后，执行如下命令：\n\n./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v{X.X.X}/elasticsearch-analysis-ik-{X.X.X}.zip\n\n\n执行完安装命令后，我们会发现在 plugins 中多了 analysis-ik 目录，这里面主要存放的是源码 jar 包，此外，在 config 文件里也多了 analysis-ik 目录，里面主要是 ik 相关的配置，如 IKAnalyzer.cfg.xml 配置、词典文件等。\n\n#  两个新增目录路径\nlibexec/plugins/analysis-ik/\nlibexec/config/analysis-ik/\n\n\n\n# elasticsearch-analysis-ik 使用\n\nES 5.X 版本开始安装完的 elasticsearch-analysis-ik 提供了两个分词器，分别对应名称是 ik_max_word 和 ik_smart，ik_max_word 是索引侧的分词器，走全切模式，ik_smart 是搜索侧的分词器，走智能分词，属于搜索模式。\n\n# 索引 mapping 设置\n\n安装完 elasticsearch-analysis-ik 后，我们可以指定索引及指定字段设置可用的分析器（analyzer），示例如下：\n\n{\n  "qa": {\n    "mappings": {\n      "qa": {\n        "_all": {\n          "enabled": false\n        },\n        "properties": {\n          "question": {\n            "type": "text",\n            "store": true,\n            "similarity": "BM25",\n            "analyzer": "ik_max_word",\n            "search_analyzer": "ik_smart"\n          },\n          "answer": {\n            "type": "text",\n            "store": false,\n            "similarity": "BM25",\n            "analyzer": "ik_max_word",\n            "search_analyzer": "ik_smart"\n          },\n          ...\n        }\n      }\n    }\n  }\n}\n\n\n如上示例中，analyzer 指定 ik_max_word，即索引侧使用 ik 全切模式，search_analyzer 设置 ik_smart，即搜索侧使用 ik 智能分词模式。\n\n# 查看 ik 分词结果\n\nes 提供了查看分词结果的 api analyze，具体示例如下：\n\nGET {index}/_analyze\n{\n  "analyzer" : "ik_smart",\n  "text" : "es 中文分词器安装"\n}\n\n\n输出如下：\n\n{\n  "tokens": [\n    {\n      "token": "es",\n      "start_offset": 0,\n      "end_offset": 2,\n      "type": "CN_WORD",\n      "position": 0\n    },\n    {\n      "token": "中文",\n      "start_offset": 3,\n      "end_offset": 5,\n      "type": "CN_WORD",\n      "position": 1\n    },\n    {\n      "token": "分词器",\n      "start_offset": 5,\n      "end_offset": 8,\n      "type": "CN_WORD",\n      "position": 2\n    },\n    {\n      "token": "安装",\n      "start_offset": 8,\n      "end_offset": 10,\n      "type": "CN_WORD",\n      "position": 3\n    }\n  ]\n}\n\n\n# elasticsearch-analysis-ik 自定义词典\n\nelasticsearch-analysis-ik 本质是 ik 分词器，使用者根据实际需求可以扩展自定义的词典，具体主要分为如下 2 大类，每类又分为本地配置和远程配置 2 种：\n\n 1. 自定义扩展词典；\n 2. 自定义扩展停用词典；\n\nelasticsearch-analysis-ik 配置文件为 IKAnalyzer.cfg.xml，它位于 libexec/config/analysis-ik 目录下，具体配置结构如下：\n\n<?xml version="1.0" encoding="UTF-8"?>\n<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">\n<properties>\n\t<comment>IK Analyzer 扩展配置</comment>\n\t\x3c!--用户可以在这里配置自己的扩展字典 --\x3e\n\t<entry key="ext_dict"></entry>\n\t \x3c!--用户可以在这里配置自己的扩展停止词字典--\x3e\n\t<entry key="ext_stopwords"></entry>\n\t\x3c!--用户可以在这里配置远程扩展字典 --\x3e\n\t\x3c!-- <entry key="remote_ext_dict">words_location</entry> --\x3e\n\t\x3c!--用户可以在这里配置远程扩展停止词字典--\x3e\n\t\x3c!-- <entry key="remote_ext_stopwords">words_location</entry> --\x3e\n</properties>\n\n\n> 当然，如果开发者认为 ik 默认的词表有问题，也可以进行调整，文件都在 libexec/config/analysis-ik 下，如 main.dic 为主词典，stopword.dic 为停用词表。\n\n\n# 词元过滤器（Token Filters）\n\ntoken filters 叫词元过滤器，或词项过滤器，对 tokenizer 分出的词进行过滤处理。常用的有转小写、停用词处理、同义词处理等等。一个 analyzer 可包含 0 个或多个词项过滤器，按配置顺序进行过滤。\n\n以同义词过滤器的使用示例，具体如下：\n\nPUT /test_index\n{\n  "settings": {\n    "index": {\n      "analysis": {\n        "analyzer": {\n          "synonym": {\n            "tokenizer": "standard",\n            "filter": [ "my_stop", "synonym" ]\n          }\n        },\n        "filter": {\n          "my_stop": {\n            "type": "stop",\n            "stopwords": [ "bar" ]\n          },\n          "synonym": {\n            "type": "synonym",\n            "lenient": true,\n            "synonyms": [ "foo, bar => baz" ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n# 同义词\n\nElasticsearch 同义词通过专有的同义词过滤器（synonym token filter）来进行工作，它允许在分析（analysis）过程中方便地处理同义词，一般是通过配置文件配置同义词。此外，同义词可以再建索引时（index-time synonyms）或者检索时（search-time synonyms）使用。\n\n# 同义词（synonym）配置语法\n\n如上例子所示，es 同义词配置的 filter 语法具体如下选项：\n\n * type：指定 synonym，表示同义词 filter；\n\n * synonyms_path：指定同义词配置文件路径；\n\n * expand：该参数决定映射行为的模式，默认为 true，表示扩展模式，具体示例如下：\n   \n   * 当 expand == true 时，\n     \n     ipod, i-pod, i pod\n     \n     \n     等价于：\n     \n     ipod, i-pod, i pod => ipod, i-pod, i pod\n     \n     \n     当 expand == false 时，\n     \n     ipod, i-pod, i pod\n     \n     \n     仅映射第一个单词，等价于：\n     \n     ipod, i-pod, i pod => ipod\n     \n\n * lenient：如果值为 true 时，遇到那些无法解析的同义词规则时，忽略异常。默认为 false。\n\n# 同义词文档格式\n\nelasticsearch 的同义词有如下两种形式：\n\n * 单向同义词：\n   \n   ipod, i-pod, i pod => ipod\n   \n\n * 双向同义词：\n   \n   马铃薯, 土豆, potato\n   \n\n单向同义词不管索引还是检索时，箭头左侧的词都会映射成箭头右侧的词；\n\n双向同义词是索引时，都建立同义词的倒排索引，检索时，同义词之间都会进行倒排索引的匹配。\n\n> 同义词的文档化时，需要注意的是，同一个词在不同的同义词关系中出现时，其它同义词之间不具有传递性，这点需要注意。\n\n假设如上示例中，如果“马铃薯”和其它两个同义词分成两行写：\n\n马铃薯,土豆\n马铃薯,potato\n\n\n此时，elasticsearch 中不会将“土豆”和“potato”视为同义词关系，所以多个同义词要写在一起，这往往是开发中经常容易疏忽的点。\n\n\n# 参考资料\n\n * Elasticsearch 教程',normalizedContent:'# elasticsearch 分析器\n\n文本分析是把全文本转换为一系列单词（term/token）的过程，也叫分词。在 elasticsearch 中，分词是通过 analyzer（分析器）来完成的，不管是索引还是搜索，都需要使用 analyzer（分析器）。分析器，分为内置分析器和自定义的分析器。\n\n分析器可以进一步细分为字符过滤器（character filters）、分词器（tokenizer）和词元过滤器（token filters）三部分。它的执行顺序如下：\n\ncharacter filters -> tokenizer -> token filters\n\n\n# 字符过滤器（character filters）\n\ncharacter filter 的输入是原始的文本 text，如果配置了多个，它会按照配置的顺序执行，目前 es 自带的 character filter 主要有如下 3 类：\n\n 1. html strip character filter：从文本中剥离 html 元素，并用其解码值替换 html 实体（如，将 ＆amp; 替换为 ＆）。\n 2. mapping character filter：自定义一个 map 映射，可以进行一些自定义的替换，如常用的大写变小写也可以在该环节设置。\n 3. pattern replace character filter：使用 java 正则表达式来匹配应替换为指定替换字符串的字符，此外，替换字符串可以引用正则表达式中的捕获组。\n\n\n# html strip character filter\n\nhtml strip 如下示例：\n\nget /_analyze\n{\n  "tokenizer": "keyword",\n  "char_filter": [\n    "html_strip"\n  ],\n  "text": "<p>i&apos;m so <b>happy</b>!</p>"\n}\n\n\n经过 html_strip 字符过滤器处理后，输出如下：\n\n[ \\ni\'m so happy!\\n ]\n\n\n\n# mapping character filter\n\nmapping character filter 接收键和值映射（key => value）作为配置参数，每当在预处理过程中遇到与键值映射中的键相同的字符串时，就会使用该键对应的值去替换它。\n\n原始文本中的字符串和键值映射中的键的匹配是贪心的，在对给定的文本进行预处理过程中如果配置的键值映射存在包含关系，会优先匹配最长键。同样也可以用空字符串进行替换。\n\nmapping char_filter 不像 html_strip 那样拆箱即可用，必须先进行配置才能使用，它有两个属性可以配置：\n\n参数名称            参数说明\nmappings        一组映射，每个元素的格式为 key => value。\nmappings_path   一个相对或者绝对的文件路径，指向一个每行包含一个 key =>value 映射的 utf-8 编码文本映射文件。\n\nmapping char_filter 示例如下：\n\nget /_analyze\n{\n  "tokenizer": "keyword",\n  "char_filter": [\n    {\n      "type": "mapping",\n      "mappings": [\n        "٠ => 0",\n        "١ => 1",\n        "٢ => 2",\n        "٣ => 3",\n        "٤ => 4",\n        "٥ => 5",\n        "٦ => 6",\n        "٧ => 7",\n        "٨ => 8",\n        "٩ => 9"\n      ]\n    }\n  ],\n  "text": "my license plate is ٢٥٠١٥"\n}\n\n\n分析结果如下：\n\n[ my license plate is 25015 ]\n\n\n\n# pattern replace character filter\n\npattern replace character filter 支持如下三个参数：\n\n参数名称          参数说明\npattern       必填参数，一个 java 的正则表达式。\nreplacement   替换字符串，可以使用 $1 ... $9 语法来引用捕获组。\nflags         java 正则表达式的标志，具体参考 java 的 java.util.regex.pattern 类的标志属性。\n\n如将输入的 text 中大于一个的空格都转变为一个空格，在 settings 时，配置示例如下：\n\n"char_filter": {\n  "multi_space_2_one": {\n    "pattern": "[ ]+",\n    "type": "pattern_replace",\n    "replacement": " "\n  },\n  ...\n}\n\n\n\n# 分词器（tokenizer）\n\ntokenizer 即分词器，也是 analyzer 最重要的组件，它对文本进行分词；一个 analyzer 必需且只可包含一个 tokenizer。\n\nes 自带默认的分词器是 standard tokenizer，标准分词器提供基于语法的分词（基于 unicode 文本分割算法），并且适用于大多数语言。\n\n此外有很多第三方的分词插件，如中文分词界最经典的 ik 分词器，它对应的 tokenizer 分为 ik_smart 和 ik_max_word，一个是智能分词（针对搜索侧），一个是全切分词（针对索引侧）。\n\nes 默认提供的分词器 standard 对中文分词不优化，效果差，一般会安装第三方中文分词插件，通常首先 elasticsearch-analysis-ik 插件，它其实是 ik 针对的 es 的定制版。\n\n\n# elasticsearch-plugin 使用\n\n在安装 elasticsearch-analysis-ik 第三方之前，我们首先要了解 es 的插件管理工具 elasticsearch-plugin 的使用。\n\n现在的 elasticsearch 安装完后，在安装目录的 bin 目录下会存在 elasticsearch-plugin 命令工具，用它来对 es 插件进行管理。\n\nbin/elasticsearch-plugin\n\n\n其实该命令的是软连接，原始路径是：\n\nlibexec/bin/elasticsearch-plugin\n\n\n再进一步看脚本代码，你会发现，它是通过 elasticsearch-cli 执行 libexec/lib/tools/plugin-cli/elasticsearch-plugin-cli-x.x.x.jar。\n\n但一般使用者了解 elasticsearch-plugin 命令使用就可：\n\n#  安装指定的插件到当前 es 节点中\nelasticsearch-plugin install {plugin_url}\n\n#  显示当前 es 节点已经安装的插件列表\nelasticsearch-plugin list\n\n#  删除已安装的插件\nelasticsearch-plugin remove {plugin_name}\n\n\n> 在安装插件时，要保证安装的插件与 es 版本一致。\n\n\n# elasticsearch-analysis-ik 安装\n\n在确定要安装的 ik 版本之后，执行如下命令：\n\n./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v{x.x.x}/elasticsearch-analysis-ik-{x.x.x}.zip\n\n\n执行完安装命令后，我们会发现在 plugins 中多了 analysis-ik 目录，这里面主要存放的是源码 jar 包，此外，在 config 文件里也多了 analysis-ik 目录，里面主要是 ik 相关的配置，如 ikanalyzer.cfg.xml 配置、词典文件等。\n\n#  两个新增目录路径\nlibexec/plugins/analysis-ik/\nlibexec/config/analysis-ik/\n\n\n\n# elasticsearch-analysis-ik 使用\n\nes 5.x 版本开始安装完的 elasticsearch-analysis-ik 提供了两个分词器，分别对应名称是 ik_max_word 和 ik_smart，ik_max_word 是索引侧的分词器，走全切模式，ik_smart 是搜索侧的分词器，走智能分词，属于搜索模式。\n\n# 索引 mapping 设置\n\n安装完 elasticsearch-analysis-ik 后，我们可以指定索引及指定字段设置可用的分析器（analyzer），示例如下：\n\n{\n  "qa": {\n    "mappings": {\n      "qa": {\n        "_all": {\n          "enabled": false\n        },\n        "properties": {\n          "question": {\n            "type": "text",\n            "store": true,\n            "similarity": "bm25",\n            "analyzer": "ik_max_word",\n            "search_analyzer": "ik_smart"\n          },\n          "answer": {\n            "type": "text",\n            "store": false,\n            "similarity": "bm25",\n            "analyzer": "ik_max_word",\n            "search_analyzer": "ik_smart"\n          },\n          ...\n        }\n      }\n    }\n  }\n}\n\n\n如上示例中，analyzer 指定 ik_max_word，即索引侧使用 ik 全切模式，search_analyzer 设置 ik_smart，即搜索侧使用 ik 智能分词模式。\n\n# 查看 ik 分词结果\n\nes 提供了查看分词结果的 api analyze，具体示例如下：\n\nget {index}/_analyze\n{\n  "analyzer" : "ik_smart",\n  "text" : "es 中文分词器安装"\n}\n\n\n输出如下：\n\n{\n  "tokens": [\n    {\n      "token": "es",\n      "start_offset": 0,\n      "end_offset": 2,\n      "type": "cn_word",\n      "position": 0\n    },\n    {\n      "token": "中文",\n      "start_offset": 3,\n      "end_offset": 5,\n      "type": "cn_word",\n      "position": 1\n    },\n    {\n      "token": "分词器",\n      "start_offset": 5,\n      "end_offset": 8,\n      "type": "cn_word",\n      "position": 2\n    },\n    {\n      "token": "安装",\n      "start_offset": 8,\n      "end_offset": 10,\n      "type": "cn_word",\n      "position": 3\n    }\n  ]\n}\n\n\n# elasticsearch-analysis-ik 自定义词典\n\nelasticsearch-analysis-ik 本质是 ik 分词器，使用者根据实际需求可以扩展自定义的词典，具体主要分为如下 2 大类，每类又分为本地配置和远程配置 2 种：\n\n 1. 自定义扩展词典；\n 2. 自定义扩展停用词典；\n\nelasticsearch-analysis-ik 配置文件为 ikanalyzer.cfg.xml，它位于 libexec/config/analysis-ik 目录下，具体配置结构如下：\n\n<?xml version="1.0" encoding="utf-8"?>\n<!doctype properties system "http://java.sun.com/dtd/properties.dtd">\n<properties>\n\t<comment>ik analyzer 扩展配置</comment>\n\t\x3c!--用户可以在这里配置自己的扩展字典 --\x3e\n\t<entry key="ext_dict"></entry>\n\t \x3c!--用户可以在这里配置自己的扩展停止词字典--\x3e\n\t<entry key="ext_stopwords"></entry>\n\t\x3c!--用户可以在这里配置远程扩展字典 --\x3e\n\t\x3c!-- <entry key="remote_ext_dict">words_location</entry> --\x3e\n\t\x3c!--用户可以在这里配置远程扩展停止词字典--\x3e\n\t\x3c!-- <entry key="remote_ext_stopwords">words_location</entry> --\x3e\n</properties>\n\n\n> 当然，如果开发者认为 ik 默认的词表有问题，也可以进行调整，文件都在 libexec/config/analysis-ik 下，如 main.dic 为主词典，stopword.dic 为停用词表。\n\n\n# 词元过滤器（token filters）\n\ntoken filters 叫词元过滤器，或词项过滤器，对 tokenizer 分出的词进行过滤处理。常用的有转小写、停用词处理、同义词处理等等。一个 analyzer 可包含 0 个或多个词项过滤器，按配置顺序进行过滤。\n\n以同义词过滤器的使用示例，具体如下：\n\nput /test_index\n{\n  "settings": {\n    "index": {\n      "analysis": {\n        "analyzer": {\n          "synonym": {\n            "tokenizer": "standard",\n            "filter": [ "my_stop", "synonym" ]\n          }\n        },\n        "filter": {\n          "my_stop": {\n            "type": "stop",\n            "stopwords": [ "bar" ]\n          },\n          "synonym": {\n            "type": "synonym",\n            "lenient": true,\n            "synonyms": [ "foo, bar => baz" ]\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n# 同义词\n\nelasticsearch 同义词通过专有的同义词过滤器（synonym token filter）来进行工作，它允许在分析（analysis）过程中方便地处理同义词，一般是通过配置文件配置同义词。此外，同义词可以再建索引时（index-time synonyms）或者检索时（search-time synonyms）使用。\n\n# 同义词（synonym）配置语法\n\n如上例子所示，es 同义词配置的 filter 语法具体如下选项：\n\n * type：指定 synonym，表示同义词 filter；\n\n * synonyms_path：指定同义词配置文件路径；\n\n * expand：该参数决定映射行为的模式，默认为 true，表示扩展模式，具体示例如下：\n   \n   * 当 expand == true 时，\n     \n     ipod, i-pod, i pod\n     \n     \n     等价于：\n     \n     ipod, i-pod, i pod => ipod, i-pod, i pod\n     \n     \n     当 expand == false 时，\n     \n     ipod, i-pod, i pod\n     \n     \n     仅映射第一个单词，等价于：\n     \n     ipod, i-pod, i pod => ipod\n     \n\n * lenient：如果值为 true 时，遇到那些无法解析的同义词规则时，忽略异常。默认为 false。\n\n# 同义词文档格式\n\nelasticsearch 的同义词有如下两种形式：\n\n * 单向同义词：\n   \n   ipod, i-pod, i pod => ipod\n   \n\n * 双向同义词：\n   \n   马铃薯, 土豆, potato\n   \n\n单向同义词不管索引还是检索时，箭头左侧的词都会映射成箭头右侧的词；\n\n双向同义词是索引时，都建立同义词的倒排索引，检索时，同义词之间都会进行倒排索引的匹配。\n\n> 同义词的文档化时，需要注意的是，同一个词在不同的同义词关系中出现时，其它同义词之间不具有传递性，这点需要注意。\n\n假设如上示例中，如果“马铃薯”和其它两个同义词分成两行写：\n\n马铃薯,土豆\n马铃薯,potato\n\n\n此时，elasticsearch 中不会将“土豆”和“potato”视为同义词关系，所以多个同义词要写在一起，这往往是开发中经常容易疏忽的点。\n\n\n# 参考资料\n\n * elasticsearch 教程',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 性能优化",frontmatter:{title:"Elasticsearch 性能优化",date:"2022-01-21T19:54:43.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","性能"],permalink:"/pages/2d95ce/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/10.Elasticsearch%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/10.Elasticsearch性能优化.md",key:"v-9a8c12f8",path:"/pages/2d95ce/",headers:[{level:2,title:"硬件配置优化",slug:"硬件配置优化",normalizedTitle:"硬件配置优化",charIndex:189},{level:3,title:"CPU 配置",slug:"cpu-配置",normalizedTitle:"cpu 配置",charIndex:329},{level:3,title:"内存配置",slug:"内存配置",normalizedTitle:"内存配置",charIndex:587},{level:4,title:"内存分配",slug:"内存分配",normalizedTitle:"内存分配",charIndex:1039},{level:4,title:"禁止 swap",slug:"禁止-swap",normalizedTitle:"禁止 swap",charIndex:1520},{level:4,title:"GC 设置",slug:"gc-设置",normalizedTitle:"gc 设置",charIndex:1642},{level:3,title:"磁盘",slug:"磁盘",normalizedTitle:"磁盘",charIndex:700},{level:2,title:"索引优化设置",slug:"索引优化设置",normalizedTitle:"索引优化设置",charIndex:196},{level:3,title:"批量提交",slug:"批量提交",normalizedTitle:"批量提交",charIndex:2838},{level:3,title:"增加 Refresh 时间间隔",slug:"增加-refresh-时间间隔",normalizedTitle:"增加 refresh 时间间隔",charIndex:3208},{level:3,title:"修改 indexbuffersize 的设置",slug:"修改-index-buffer-size-的设置",normalizedTitle:"修改 indexbuffersize 的设置",charIndex:null},{level:3,title:"修改 translog 相关的设置",slug:"修改-translog-相关的设置",normalizedTitle:"修改 translog 相关的设置",charIndex:4084},{level:3,title:"注意 _id 字段的使用",slug:"注意-id-字段的使用",normalizedTitle:"注意 _id 字段的使用",charIndex:4320},{level:3,title:"注意 _all 字段及 _source 字段的使用",slug:"注意-all-字段及-source-字段的使用",normalizedTitle:"注意 _all 字段及 _source 字段的使用",charIndex:4413},{level:3,title:"合理的配置使用 index 属性",slug:"合理的配置使用-index-属性",normalizedTitle:"合理的配置使用 index 属性",charIndex:4607},{level:3,title:"减少副本数量",slug:"减少副本数量",normalizedTitle:"减少副本数量",charIndex:4741},{level:2,title:"查询方面优化",slug:"查询方面优化",normalizedTitle:"查询方面优化",charIndex:203},{level:3,title:"路由优化",slug:"路由优化",normalizedTitle:"路由优化",charIndex:4998},{level:4,title:"不带 routing 查询",slug:"不带-routing-查询",normalizedTitle:"不带 routing 查询",charIndex:5162},{level:4,title:"带 routing 查询",slug:"带-routing-查询",normalizedTitle:"带 routing 查询",charIndex:5163},{level:3,title:"Filter VS Query",slug:"filter-vs-query",normalizedTitle:"filter vs query",charIndex:5436},{level:3,title:"深度翻页",slug:"深度翻页",normalizedTitle:"深度翻页",charIndex:5624},{level:3,title:"脚本（script）合理使用",slug:"脚本-script-合理使用",normalizedTitle:"脚本（script）合理使用",charIndex:6012},{level:2,title:"数据结构优化",slug:"数据结构优化",normalizedTitle:"数据结构优化",charIndex:210},{level:3,title:"尽量减少不需要的字段",slug:"尽量减少不需要的字段",normalizedTitle:"尽量减少不需要的字段",charIndex:6303},{level:3,title:"Nested Object vs Parent/Child",slug:"nested-object-vs-parent-child",normalizedTitle:"nested object vs parent/child",charIndex:6654},{level:3,title:"选择静态映射，非必需时，禁止动态映射",slug:"选择静态映射-非必需时-禁止动态映射",normalizedTitle:"选择静态映射，非必需时，禁止动态映射",charIndex:7211},{level:2,title:"集群架构设计",slug:"集群架构设计",normalizedTitle:"集群架构设计",charIndex:7441},{level:3,title:"主节点、数据节点和协调节点分离",slug:"主节点、数据节点和协调节点分离",normalizedTitle:"主节点、数据节点和协调节点分离",charIndex:7488},{level:4,title:"主（master）节点",slug:"主-master-节点",normalizedTitle:"主（master）节点",charIndex:7670},{level:4,title:"数据（data）节点",slug:"数据-data-节点",normalizedTitle:"数据（data）节点",charIndex:8056},{level:4,title:"协调（coordinating）节点",slug:"协调-coordinating-节点",normalizedTitle:"协调（coordinating）节点",charIndex:9019},{level:3,title:"关闭 data 节点服务器中的 http 功能",slug:"关闭-data-节点服务器中的-http-功能",normalizedTitle:"关闭 data 节点服务器中的 http 功能",charIndex:9727},{level:3,title:"一台服务器上最好只部署一个 node",slug:"一台服务器上最好只部署一个-node",normalizedTitle:"一台服务器上最好只部署一个 node",charIndex:10002},{level:3,title:"集群分片设置",slug:"集群分片设置",normalizedTitle:"集群分片设置",charIndex:10126},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:10528}],headersStr:"硬件配置优化 CPU 配置 内存配置 内存分配 禁止 swap GC 设置 磁盘 索引优化设置 批量提交 增加 Refresh 时间间隔 修改 indexbuffersize 的设置 修改 translog 相关的设置 注意 _id 字段的使用 注意 _all 字段及 _source 字段的使用 合理的配置使用 index 属性 减少副本数量 查询方面优化 路由优化 不带 routing 查询 带 routing 查询 Filter VS Query 深度翻页 脚本（script）合理使用 数据结构优化 尽量减少不需要的字段 Nested Object vs Parent/Child 选择静态映射，非必需时，禁止动态映射 集群架构设计 主节点、数据节点和协调节点分离 主（master）节点 数据（data）节点 协调（coordinating）节点 关闭 data 节点服务器中的 http 功能 一台服务器上最好只部署一个 node 集群分片设置 参考资料",content:"# Elasticsearch 性能优化\n\nElasticsearch 是当前流行的企业级搜索引擎，设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。作为一个开箱即用的产品，在生产环境上线之后，我们其实不一定能确保其的性能和稳定性。如何根据实际情况提高服务的性能，其实有很多技巧。这章我们分享从实战经验中总结出来的 elasticsearch 性能优化，主要从硬件配置优化、索引优化设置、查询方面优化、数据结构优化、集群架构优化等方面讲解。\n\n\n# 硬件配置优化\n\n升级硬件设备配置一直都是提高服务能力最快速有效的手段，在系统层面能够影响应用性能的一般包括三个因素：CPU、内存和 IO，可以从这三方面进行 ES 的性能优化工作。\n\n\n# CPU 配置\n\n一般说来，CPU 繁忙的原因有以下几个：\n\n 1. 线程中有无限空循环、无阻塞、正则匹配或者单纯的计算；\n 2. 发生了频繁的 GC；\n 3. 多线程的上下文切换；\n\n大多数 Elasticsearch 部署往往对 CPU 要求不高。因此，相对其它资源，具体配置多少个（CPU）不是那么关键。你应该选择具有多个内核的现代处理器，常见的集群使用 2 到 8 个核的机器。如果你要在更快的 CPUs 和更多的核数之间选择，选择更多的核数更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。\n\n\n# 内存配置\n\n如果有一种资源是最先被耗尽的，它可能是内存。排序和聚合都很耗内存，所以有足够的堆空间来应付它们是很重要的。即使堆空间是比较小的时候，也能为操作系统文件缓存提供额外的内存。因为 Lucene 使用的许多数据结构是基于磁盘的格式，Elasticsearch 利用操作系统缓存能产生很大效果。\n\n64 GB 内存的机器是非常理想的，但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反（你最终需要很多很多的小机器），大于 64 GB 的机器也会有问题。\n\n由于 ES 构建基于 lucene，而 lucene 设计强大之处在于 lucene 能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能。lucene 的索引文件 segements 是存储在单文件中的，并且不可变，对于 OS 来说，能够很友好地将索引文件保持在 cache 中，以便快速访问；因此，我们很有必要将一半的物理内存留给 lucene；另一半的物理内存留给 ES（JVM heap）。\n\n# 内存分配\n\n当机器内存小于 64G 时，遵循通用的原则，50% 给 ES，50% 留给 lucene。\n\n当机器内存大于 64G 时，遵循以下原则：\n\n * 如果主要的使用场景是全文检索，那么建议给 ES Heap 分配 4~32G 的内存即可；其它内存留给操作系统，供 lucene 使用（segments cache），以提供更快的查询性能。\n * 如果主要的使用场景是聚合或排序，并且大多数是 numerics，dates，geo_points 以及 not_analyzed 的字符类型，建议分配给 ES Heap 分配 4~32G 的内存即可，其它内存留给操作系统，供 lucene 使用，提供快速的基于文档的聚类、排序性能。\n * 如果使用场景是聚合或排序，并且都是基于 analyzed 字符数据，这时需要更多的 heap size，建议机器上运行多 ES 实例，每个实例保持不超过 50% 的 ES heap 设置（但不超过 32 G，堆内存设置 32 G 以下时，JVM 使用对象指标压缩技巧节省空间），50% 以上留给 lucene。\n\n# 禁止 swap\n\n禁止 swap，一旦允许内存与磁盘的交换，会引起致命的性能问题。可以通过在 elasticsearch.yml 中 bootstrap.memory_lock: true，以保持 JVM 锁定内存，保证 ES 的性能。\n\n# GC 设置\n\n保持 GC 的现有设置，默认设置为：Concurrent-Mark and Sweep（CMS），别换成 G1 GC，因为目前 G1 还有很多 BUG。\n\n保持线程池的现有设置，目前 ES 的线程池较 1.X 有了较多优化设置，保持现状即可；默认线程池大小等于 CPU 核心数。如果一定要改，按公式 ( ( CPU 核心数 * 3 ) / 2 ) + 1 设置；不能超过 CPU 核心数的 2 倍；但是不建议修改默认配置，否则会对 CPU 造成硬伤。\n\n\n# 磁盘\n\n硬盘对所有的集群都很重要，对大量写入的集群更是加倍重要（例如那些存储日志数据的）。硬盘是服务器上最慢的子系统，这意味着那些写入量很大的集群很容易让硬盘饱和，使得它成为集群的瓶颈。\n\n在经济压力能承受的范围下，尽量使用固态硬盘（SSD）。固态硬盘相比于任何旋转介质（机械硬盘，磁带等），无论随机写还是顺序写，都会对 IO 有较大的提升。\n\n> 如果你正在使用 SSDs，确保你的系统 I/O 调度程序是配置正确的。当你向硬盘写数据，I/O 调度程序决定何时把数据实际发送到硬盘。大多数默认 *nix 发行版下的调度程序都叫做 cfq（完全公平队列）。\n> \n> 调度程序分配时间片到每个进程。并且优化这些到硬盘的众多队列的传递。但它是为旋转介质优化的：机械硬盘的固有特性意味着它写入数据到基于物理布局的硬盘会更高效。\n> \n> 这对 SSD 来说是低效的，尽管这里没有涉及到机械硬盘。但是，deadline 或者 noop 应该被使用。deadline 调度程序基于写入等待时间进行优化，noop 只是一个简单的 FIFO 队列。\n> \n> 这个简单的更改可以带来显著的影响。仅仅是使用正确的调度程序，我们看到了 500 倍的写入能力提升。\n\n如果你使用旋转介质（如机械硬盘），尝试获取尽可能快的硬盘（高性能服务器硬盘，15k RPM 驱动器）。\n\n使用 RAID0 是提高硬盘速度的有效途径，对机械硬盘和 SSD 来说都是如此。没有必要使用镜像或其它 RAID 变体，因为 Elasticsearch 在自身层面通过副本，已经提供了备份的功能，所以不需要利用磁盘的备份功能，同时如果使用磁盘备份功能的话，对写入速度有较大的影响。\n\n最后，避免使用网络附加存储（NAS）。人们常声称他们的 NAS 解决方案比本地驱动器更快更可靠。除却这些声称，我们从没看到 NAS 能配得上它的大肆宣传。NAS 常常很慢，显露出更大的延时和更宽的平均延时方差，而且它是单点故障的。\n\n\n# 索引优化设置\n\n索引优化主要是在 Elasticsearch 的插入层面优化，Elasticsearch 本身索引速度其实还是蛮快的，具体数据，我们可以参考官方的 benchmark 数据。我们可以根据不同的需求，针对索引优化。\n\n\n# 批量提交\n\n当有大量数据提交的时候，建议采用批量提交（Bulk 操作）；此外使用 bulk 请求时，每个请求不超过几十 M，因为太大会导致内存使用过大。\n\n比如在做 ELK 过程中，Logstash indexer 提交数据到 Elasticsearch 中，batch size 就可以作为一个优化功能点。但是优化 size 大小需要根据文档大小和服务器性能而定。\n\n像 Logstash 中提交文档大小超过 20MB，Logstash 会将一个批量请求切分为多个批量请求。\n\n如果在提交过程中，遇到 EsRejectedExecutionException 异常的话，则说明集群的索引性能已经达到极限了。这种情况，要么提高服务器集群的资源，要么根据业务规则，减少数据收集速度，比如只收集 Warn、Error 级别以上的日志。\n\n\n# 增加 Refresh 时间间隔\n\n为了提高索引性能，Elasticsearch 在写入数据的时候，采用延迟写入的策略，即数据先写到内存中，当超过默认 1 秒（index.refresh_interval）会进行一次写入操作，就是将内存中 segment 数据刷新到磁盘中，此时我们才能将数据搜索出来，所以这就是为什么 Elasticsearch 提供的是近实时搜索功能，而不是实时搜索功能。\n\n如果我们的系统对数据延迟要求不高的话，我们可以通过延长 refresh 时间间隔，可以有效地减少 segment 合并压力，提高索引速度。比如在做全链路跟踪的过程中，我们就将 index.refresh_interval 设置为 30s，减少 refresh 次数。再如，在进行全量索引时，可以将 refresh 次数临时关闭，即 index.refresh_interval 设置为-1，数据导入成功后再打开到正常模式，比如 30s。\n\n> 在加载大量数据时候可以暂时不用 refresh 和 repliccas，index.refresh_interval 设置为-1，index.number_of_replicas 设置为 0。\n\n\n# 修改 index_buffer_size 的设置\n\n索引缓冲的设置可以控制多少内存分配给索引进程。这是一个全局配置，会应用于一个节点上所有不同的分片上。\n\nindices.memory.index_buffer_size: 10%\nindices.memory.min_index_buffer_size: 48mb\n\n\nindices.memory.index_buffer_size 接受一个百分比或者一个表示字节大小的值。默认是 10%，意味着分配给节点的总内存的 10%用来做索引缓冲的大小。这个数值被分到不同的分片（shards）上。如果设置的是百分比，还可以设置 min_index_buffer_size （默认 48mb）和 max_index_buffer_size（默认没有上限）。\n\n\n# 修改 translog 相关的设置\n\n一是控制数据从内存到硬盘的操作频率，以减少硬盘 IO。可将 sync_interval 的时间设置大一些。默认为 5s。\n\nindex.translog.sync_interval: 5s\n\n\n也可以控制 tranlog 数据块的大小，达到 threshold 大小时，才会 flush 到 lucene 索引文件。默认为 512m。\n\nindex.translog.flush_threshold_size: 512mb\n\n\n\n# 注意 _id 字段的使用\n\n_id 字段的使用，应尽可能避免自定义 _id，以避免针对 ID 的版本管理；建议使用 ES 的默认 ID 生成策略或使用数字类型 ID 做为主键。\n\n\n# 注意 _all 字段及 _source 字段的使用\n\n**_**all 字段及 _source 字段的使用，应该注意场景和需要，_all 字段包含了所有的索引字段，方便做全文检索，如果无此需求，可以禁用；_source 存储了原始的 document 内容，如果没有获取原始文档数据的需求，可通过设置 includes、excludes 属性来定义放入 _source 的字段。\n\n\n# 合理的配置使用 index 属性\n\n合理的配置使用 index 属性，analyzed 和 not_analyzed，根据业务需求来控制字段是否分词或不分词。只有 groupby 需求的字段，配置时就设置成 not_analyzed，以提高查询或聚类的效率。\n\n\n# 减少副本数量\n\nElasticsearch 默认副本数量为 3 个，虽然这样会提高集群的可用性，增加搜索的并发数，但是同时也会影响写入索引的效率。\n\n在索引过程中，需要把更新的文档发到副本节点上，等副本节点生效后在进行返回结束。使用 Elasticsearch 做业务搜索的时候，建议副本数目还是设置为 3 个，但是像内部 ELK 日志系统、分布式跟踪系统中，完全可以将副本数目设置为 1 个。\n\n\n# 查询方面优化\n\nElasticsearch 作为业务搜索的近实时查询时，查询效率的优化显得尤为重要。\n\n\n# 路由优化\n\n当我们查询文档的时候，Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？它其实是通过下面这个公式来计算出来的。\n\nshard = hash(routing) % number_of_primary_shards\n\n\nrouting 默认值是文档的 id，也可以采用自定义值，比如用户 ID。\n\n# 不带 routing 查询\n\n在查询的时候因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤：\n\n 1. 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。\n 2. 聚合：协调节点搜集到每个分片上查询结果，再将查询的结果进行排序，之后给用户返回结果。\n\n# 带 routing 查询\n\n查询的时候，可以直接根据 routing 信息定位到某个分配查询，不需要查询所有的分配，经过协调节点排序。\n\n向上面自定义的用户查询，如果 routing 设置为 userid 的话，就可以直接查询出数据来，效率提升很多。\n\n\n# Filter VS Query\n\n尽可能使用过滤器上下文（Filter）替代查询上下文（Query）\n\n * Query：此文档与此查询子句的匹配程度如何？\n * Filter：此文档和查询子句匹配吗？\n\nElasticsearch 针对 Filter 查询只需要回答「是」或者「否」，不需要像 Query 查询一样计算相关性分数，同时 Filter 结果可以缓存。\n\n\n# 深度翻页\n\n在使用 Elasticsearch 过程中，应尽量避免大翻页的出现。\n\n正常翻页查询都是从 from 开始 size 条数据，这样就需要在每个分片中查询打分排名在前面的 from+size 条数据。协同节点收集每个分配的前 from+size 条数据。协同节点一共会受到 N*(from+size) 条数据，然后进行排序，再将其中 from 到 from+size 条数据返回出去。如果 from 或者 size 很大的话，导致参加排序的数量会同步扩大很多，最终会导致 CPU 资源消耗增大。\n\n可以通过使用 Elasticsearch scroll 和 scroll-scan 高效滚动的方式来解决这样的问题。\n\n也可以结合实际业务特点，文档 id 大小如果和文档创建时间是一致有序的，可以以文档 id 作为分页的偏移量，并将其作为分页查询的一个条件。\n\n\n# 脚本（script）合理使用\n\n我们知道脚本使用主要有 3 种形式，内联动态编译方式、_script 索引库中存储和文件脚本存储的形式；一般脚本的使用场景是粗排，尽量用第二种方式先将脚本存储在 _script 索引库中，起到提前编译，然后通过引用脚本 id，并结合 params 参数使用，即可以达到模型（逻辑）和数据进行了分离，同时又便于脚本模块的扩展与维护。具体 ES 脚本的深入内容请参考 Elasticsearch 脚本模块的详解。\n\n\n# 数据结构优化\n\n基于 Elasticsearch 的使用场景，文档数据结构尽量和使用场景进行结合，去掉没用及不合理的数据。\n\n\n# 尽量减少不需要的字段\n\n如果 Elasticsearch 用于业务搜索服务，一些不需要用于搜索的字段最好不存到 ES 中，这样即节省空间，同时在相同的数据量下，也能提高搜索性能。\n\n避免使用动态值作字段，动态递增的 mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。控制索引的字段数量、mapping 深度、索引字段的类型，对于 ES 的性能优化是重中之重。\n\n以下是 ES 关于字段数、mapping 深度的一些默认设置：\n\nindex.mapping.nested_objects.limit: 10000\nindex.mapping.total_fields.limit: 1000\nindex.mapping.depth.limit: 20\n\n\n\n# Nested Object vs Parent/Child\n\n尽量避免使用 nested 或 parent/child 的字段，能不用就不用；nested query 慢，parent/child query 更慢，比 nested query 慢上百倍；因此能在 mapping 设计阶段搞定的（大宽表设计或采用比较 smart 的数据结构），就不要用父子关系的 mapping。\n\n如果一定要使用 nested fields，保证 nested fields 字段不能过多，目前 ES 默认限制是 50。因为针对 1 个 document，每一个 nested field，都会生成一个独立的 document，这将使 doc 数量剧增，影响查询效率，尤其是 JOIN 的效率。\n\nindex.mapping.nested_fields.limit: 50\n\n\n对比   NESTED OBJECT        PARENT/CHILD\n优点   文档存储在一起，因此读取性高       父子文档可以独立更新，互不影响\n缺点   更新父文档或子文档时需要更新整个文档   为了维护 join 关系，需要占用部分内存，读取性能较差\n场景   子文档偶尔更新，查询频繁         子文档更新频繁\n\n\n# 选择静态映射，非必需时，禁止动态映射\n\n尽量避免使用动态映射，这样有可能会导致集群崩溃，此外，动态映射有可能会带来不可控制的数据类型，进而有可能导致在查询端出现相关异常，影响业务。\n\n此外，Elasticsearch 作为搜索引擎时，主要承载 query 的匹配和排序的功能，那数据的存储类型基于这两种功能的用途分为两类，一是需要匹配的字段，用来建立倒排索引对 query 匹配用，另一类字段是用做粗排用到的特征字段，如 ctr、点击数、评论数等等。\n\n\n# 集群架构设计\n\n合理的部署 Elasticsearch 有助于提高服务的整体可用性。\n\n\n# 主节点、数据节点和协调节点分离\n\nElasticsearch 集群在架构拓朴时，采用主节点、数据节点和负载均衡节点分离的架构，在 5.x 版本以后，又可将数据节点再细分为“Hot-Warm”的架构模式。\n\nElasticsearch 的配置文件中有 2 个参数，node.master 和 node.data。这两个参数搭配使用时，能够帮助提供服务器性能。\n\n# 主（master）节点\n\n配置 node.master:true 和 node.data:false，该 node 服务器只作为一个主节点，但不存储任何索引数据。我们推荐每个集群运行 3 个专用的 master 节点来提供最好的弹性。使用时，你还需要将 discovery.zen.minimum_master_nodes setting 参数设置为 2，以免出现脑裂（split-brain）的情况。用 3 个专用的 master 节点，专门负责处理集群的管理以及加强状态的整体稳定性。因为这 3 个 master 节点不包含数据也不会实际参与搜索以及索引操作，在 JVM 上它们不用做相同的事，例如繁重的索引或者耗时，资源耗费很大的搜索。因此不太可能会因为垃圾回收而导致停顿。因此，master 节点的 CPU，内存以及磁盘配置可以比 data 节点少很多的。\n\n# 数据（data）节点\n\n配置 node.master:false 和 node.data:true，该 node 服务器只作为一个数据节点，只用于存储索引数据，使该 node 服务器功能单一，只用于数据存储和数据查询，降低其资源消耗率。\n\n在 Elasticsearch 5.x 版本之后，data 节点又可再细分为“Hot-Warm”架构，即分为热节点（hot node）和暖节点（warm node）。\n\nhot 节点：\n\nhot 节点主要是索引节点（写节点），同时会保存近期的一些频繁被查询的索引。由于进行索引非常耗费 CPU 和 IO，即属于 IO 和 CPU 密集型操作，建议使用 SSD 的磁盘类型，保持良好的写性能；我们推荐部署最小化的 3 个 hot 节点来保证高可用性。根据近期需要收集以及查询的数据量，可以增加服务器数量来获得想要的性能。\n\n将节点设置为 hot 类型需要 elasticsearch.yml 如下配置：\n\nnode.attr.box_type: hot\n\n\n如果是针对指定的 index 操作，可以通过 settings 设置 index.routing.allocation.require.box_type: hot 将索引写入 hot 节点。\n\nwarm 节点：\n\n这种类型的节点是为了处理大量的，而且不经常访问的只读索引而设计的。由于这些索引是只读的，warm 节点倾向于挂载大量磁盘（普通磁盘）来替代 SSD。内存、CPU 的配置跟 hot 节点保持一致即可；节点数量一般也是大于等于 3 个。\n\n将节点设置为 warm 类型需要 elasticsearch.yml 如下配置：\n\nnode.attr.box_type: warm\n\n\n同时，也可以在 elasticsearch.yml 中设置 index.codec:best_compression 保证 warm 节点的压缩配置。\n\n当索引不再被频繁查询时，可通过 index.routing.allocation.require.box_type:warm，将索引标记为 warm，从而保证索引不写入 hot 节点，以便将 SSD 磁盘资源用在刀刃上。一旦设置这个属性，ES 会自动将索引合并到 warm 节点。\n\n# 协调（coordinating）节点\n\n协调节点用于做分布式里的协调，将各分片或节点返回的数据整合后返回。该节点不会被选作主节点，也不会存储任何索引数据。该服务器主要用于查询负载均衡。在查询的时候，通常会涉及到从多个 node 服务器上查询数据，并将请求分发到多个指定的 node 服务器，并对各个 node 服务器返回的结果进行一个汇总处理，最终返回给客户端。在 ES 集群中，所有的节点都有可能是协调节点，但是，可以通过设置 node.master、node.data、node.ingest 都为 false 来设置专门的协调节点。需要较好的 CPU 和较高的内存。\n\n * node.master:false 和 node.data:true，该 node 服务器只作为一个数据节点，只用于存储索引数据，使该 node 服务器功能单一，只用于数据存储和数据查询，降低其资源消耗率。\n * node.master:true 和 node.data:false，该 node 服务器只作为一个主节点，但不存储任何索引数据，该 node 服务器将使用自身空闲的资源，来协调各种创建索引请求或者查询请求，并将这些请求合理分发到相关的 node 服务器上。\n * node.master:false 和 node.data:false，该 node 服务器即不会被选作主节点，也不会存储任何索引数据。该服务器主要用于查询负载均衡。在查询的时候，通常会涉及到从多个 node 服务器上查询数据，并将请求分发到多个指定的 node 服务器，并对各个 node 服务器返回的结果进行一个汇总处理，最终返回给客户端。\n\n\n# 关闭 data 节点服务器中的 http 功能\n\n针对 Elasticsearch 集群中的所有数据节点，不用开启 http 服务。将其中的配置参数这样设置，http.enabled:false，同时也不要安装 head, bigdesk, marvel 等监控插件，这样保证 data 节点服务器只需处理创建/更新/删除/查询索引数据等操作。\n\nhttp 功能可以在非数据节点服务器上开启，上述相关的监控插件也安装到这些服务器上，用于监控 Elasticsearch 集群状态等数据信息。这样做一来出于数据安全考虑，二来出于服务性能考虑。\n\n\n# 一台服务器上最好只部署一个 node\n\n一台物理服务器上可以启动多个 node 服务器节点（通过设置不同的启动 port），但一台服务器上的 CPU、内存、硬盘等资源毕竟有限，从服务器性能考虑，不建议一台服务器上启动多个 node 节点。\n\n\n# 集群分片设置\n\nES 一旦创建好索引后，就无法调整分片的设置，而在 ES 中，一个分片实际上对应一个 lucene 索引，而 lucene 索引的读写会占用很多的系统资源，因此，分片数不能设置过大；所以，在创建索引时，合理配置分片数是非常重要的。一般来说，我们遵循一些原则：\n\n 1. 控制每个分片占用的硬盘容量不超过 ES 的最大 JVM 的堆空间设置（一般设置不超过 32 G，参考上面的 JVM 内存设置原则），因此，如果索引的总容量在 500 G 左右，那分片大小在 16 个左右即可；当然，最好同时考虑原则 2。\n 2. 考虑一下 node 数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了 1 个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以，一般都设置分片数不超过节点数的 3 倍。\n\n\n# 参考资料\n\n * Elasticsearch 教程",normalizedContent:"# elasticsearch 性能优化\n\nelasticsearch 是当前流行的企业级搜索引擎，设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。作为一个开箱即用的产品，在生产环境上线之后，我们其实不一定能确保其的性能和稳定性。如何根据实际情况提高服务的性能，其实有很多技巧。这章我们分享从实战经验中总结出来的 elasticsearch 性能优化，主要从硬件配置优化、索引优化设置、查询方面优化、数据结构优化、集群架构优化等方面讲解。\n\n\n# 硬件配置优化\n\n升级硬件设备配置一直都是提高服务能力最快速有效的手段，在系统层面能够影响应用性能的一般包括三个因素：cpu、内存和 io，可以从这三方面进行 es 的性能优化工作。\n\n\n# cpu 配置\n\n一般说来，cpu 繁忙的原因有以下几个：\n\n 1. 线程中有无限空循环、无阻塞、正则匹配或者单纯的计算；\n 2. 发生了频繁的 gc；\n 3. 多线程的上下文切换；\n\n大多数 elasticsearch 部署往往对 cpu 要求不高。因此，相对其它资源，具体配置多少个（cpu）不是那么关键。你应该选择具有多个内核的现代处理器，常见的集群使用 2 到 8 个核的机器。如果你要在更快的 cpus 和更多的核数之间选择，选择更多的核数更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。\n\n\n# 内存配置\n\n如果有一种资源是最先被耗尽的，它可能是内存。排序和聚合都很耗内存，所以有足够的堆空间来应付它们是很重要的。即使堆空间是比较小的时候，也能为操作系统文件缓存提供额外的内存。因为 lucene 使用的许多数据结构是基于磁盘的格式，elasticsearch 利用操作系统缓存能产生很大效果。\n\n64 gb 内存的机器是非常理想的，但是 32 gb 和 16 gb 机器也是很常见的。少于 8 gb 会适得其反（你最终需要很多很多的小机器），大于 64 gb 的机器也会有问题。\n\n由于 es 构建基于 lucene，而 lucene 设计强大之处在于 lucene 能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能。lucene 的索引文件 segements 是存储在单文件中的，并且不可变，对于 os 来说，能够很友好地将索引文件保持在 cache 中，以便快速访问；因此，我们很有必要将一半的物理内存留给 lucene；另一半的物理内存留给 es（jvm heap）。\n\n# 内存分配\n\n当机器内存小于 64g 时，遵循通用的原则，50% 给 es，50% 留给 lucene。\n\n当机器内存大于 64g 时，遵循以下原则：\n\n * 如果主要的使用场景是全文检索，那么建议给 es heap 分配 4~32g 的内存即可；其它内存留给操作系统，供 lucene 使用（segments cache），以提供更快的查询性能。\n * 如果主要的使用场景是聚合或排序，并且大多数是 numerics，dates，geo_points 以及 not_analyzed 的字符类型，建议分配给 es heap 分配 4~32g 的内存即可，其它内存留给操作系统，供 lucene 使用，提供快速的基于文档的聚类、排序性能。\n * 如果使用场景是聚合或排序，并且都是基于 analyzed 字符数据，这时需要更多的 heap size，建议机器上运行多 es 实例，每个实例保持不超过 50% 的 es heap 设置（但不超过 32 g，堆内存设置 32 g 以下时，jvm 使用对象指标压缩技巧节省空间），50% 以上留给 lucene。\n\n# 禁止 swap\n\n禁止 swap，一旦允许内存与磁盘的交换，会引起致命的性能问题。可以通过在 elasticsearch.yml 中 bootstrap.memory_lock: true，以保持 jvm 锁定内存，保证 es 的性能。\n\n# gc 设置\n\n保持 gc 的现有设置，默认设置为：concurrent-mark and sweep（cms），别换成 g1 gc，因为目前 g1 还有很多 bug。\n\n保持线程池的现有设置，目前 es 的线程池较 1.x 有了较多优化设置，保持现状即可；默认线程池大小等于 cpu 核心数。如果一定要改，按公式 ( ( cpu 核心数 * 3 ) / 2 ) + 1 设置；不能超过 cpu 核心数的 2 倍；但是不建议修改默认配置，否则会对 cpu 造成硬伤。\n\n\n# 磁盘\n\n硬盘对所有的集群都很重要，对大量写入的集群更是加倍重要（例如那些存储日志数据的）。硬盘是服务器上最慢的子系统，这意味着那些写入量很大的集群很容易让硬盘饱和，使得它成为集群的瓶颈。\n\n在经济压力能承受的范围下，尽量使用固态硬盘（ssd）。固态硬盘相比于任何旋转介质（机械硬盘，磁带等），无论随机写还是顺序写，都会对 io 有较大的提升。\n\n> 如果你正在使用 ssds，确保你的系统 i/o 调度程序是配置正确的。当你向硬盘写数据，i/o 调度程序决定何时把数据实际发送到硬盘。大多数默认 *nix 发行版下的调度程序都叫做 cfq（完全公平队列）。\n> \n> 调度程序分配时间片到每个进程。并且优化这些到硬盘的众多队列的传递。但它是为旋转介质优化的：机械硬盘的固有特性意味着它写入数据到基于物理布局的硬盘会更高效。\n> \n> 这对 ssd 来说是低效的，尽管这里没有涉及到机械硬盘。但是，deadline 或者 noop 应该被使用。deadline 调度程序基于写入等待时间进行优化，noop 只是一个简单的 fifo 队列。\n> \n> 这个简单的更改可以带来显著的影响。仅仅是使用正确的调度程序，我们看到了 500 倍的写入能力提升。\n\n如果你使用旋转介质（如机械硬盘），尝试获取尽可能快的硬盘（高性能服务器硬盘，15k rpm 驱动器）。\n\n使用 raid0 是提高硬盘速度的有效途径，对机械硬盘和 ssd 来说都是如此。没有必要使用镜像或其它 raid 变体，因为 elasticsearch 在自身层面通过副本，已经提供了备份的功能，所以不需要利用磁盘的备份功能，同时如果使用磁盘备份功能的话，对写入速度有较大的影响。\n\n最后，避免使用网络附加存储（nas）。人们常声称他们的 nas 解决方案比本地驱动器更快更可靠。除却这些声称，我们从没看到 nas 能配得上它的大肆宣传。nas 常常很慢，显露出更大的延时和更宽的平均延时方差，而且它是单点故障的。\n\n\n# 索引优化设置\n\n索引优化主要是在 elasticsearch 的插入层面优化，elasticsearch 本身索引速度其实还是蛮快的，具体数据，我们可以参考官方的 benchmark 数据。我们可以根据不同的需求，针对索引优化。\n\n\n# 批量提交\n\n当有大量数据提交的时候，建议采用批量提交（bulk 操作）；此外使用 bulk 请求时，每个请求不超过几十 m，因为太大会导致内存使用过大。\n\n比如在做 elk 过程中，logstash indexer 提交数据到 elasticsearch 中，batch size 就可以作为一个优化功能点。但是优化 size 大小需要根据文档大小和服务器性能而定。\n\n像 logstash 中提交文档大小超过 20mb，logstash 会将一个批量请求切分为多个批量请求。\n\n如果在提交过程中，遇到 esrejectedexecutionexception 异常的话，则说明集群的索引性能已经达到极限了。这种情况，要么提高服务器集群的资源，要么根据业务规则，减少数据收集速度，比如只收集 warn、error 级别以上的日志。\n\n\n# 增加 refresh 时间间隔\n\n为了提高索引性能，elasticsearch 在写入数据的时候，采用延迟写入的策略，即数据先写到内存中，当超过默认 1 秒（index.refresh_interval）会进行一次写入操作，就是将内存中 segment 数据刷新到磁盘中，此时我们才能将数据搜索出来，所以这就是为什么 elasticsearch 提供的是近实时搜索功能，而不是实时搜索功能。\n\n如果我们的系统对数据延迟要求不高的话，我们可以通过延长 refresh 时间间隔，可以有效地减少 segment 合并压力，提高索引速度。比如在做全链路跟踪的过程中，我们就将 index.refresh_interval 设置为 30s，减少 refresh 次数。再如，在进行全量索引时，可以将 refresh 次数临时关闭，即 index.refresh_interval 设置为-1，数据导入成功后再打开到正常模式，比如 30s。\n\n> 在加载大量数据时候可以暂时不用 refresh 和 repliccas，index.refresh_interval 设置为-1，index.number_of_replicas 设置为 0。\n\n\n# 修改 index_buffer_size 的设置\n\n索引缓冲的设置可以控制多少内存分配给索引进程。这是一个全局配置，会应用于一个节点上所有不同的分片上。\n\nindices.memory.index_buffer_size: 10%\nindices.memory.min_index_buffer_size: 48mb\n\n\nindices.memory.index_buffer_size 接受一个百分比或者一个表示字节大小的值。默认是 10%，意味着分配给节点的总内存的 10%用来做索引缓冲的大小。这个数值被分到不同的分片（shards）上。如果设置的是百分比，还可以设置 min_index_buffer_size （默认 48mb）和 max_index_buffer_size（默认没有上限）。\n\n\n# 修改 translog 相关的设置\n\n一是控制数据从内存到硬盘的操作频率，以减少硬盘 io。可将 sync_interval 的时间设置大一些。默认为 5s。\n\nindex.translog.sync_interval: 5s\n\n\n也可以控制 tranlog 数据块的大小，达到 threshold 大小时，才会 flush 到 lucene 索引文件。默认为 512m。\n\nindex.translog.flush_threshold_size: 512mb\n\n\n\n# 注意 _id 字段的使用\n\n_id 字段的使用，应尽可能避免自定义 _id，以避免针对 id 的版本管理；建议使用 es 的默认 id 生成策略或使用数字类型 id 做为主键。\n\n\n# 注意 _all 字段及 _source 字段的使用\n\n**_**all 字段及 _source 字段的使用，应该注意场景和需要，_all 字段包含了所有的索引字段，方便做全文检索，如果无此需求，可以禁用；_source 存储了原始的 document 内容，如果没有获取原始文档数据的需求，可通过设置 includes、excludes 属性来定义放入 _source 的字段。\n\n\n# 合理的配置使用 index 属性\n\n合理的配置使用 index 属性，analyzed 和 not_analyzed，根据业务需求来控制字段是否分词或不分词。只有 groupby 需求的字段，配置时就设置成 not_analyzed，以提高查询或聚类的效率。\n\n\n# 减少副本数量\n\nelasticsearch 默认副本数量为 3 个，虽然这样会提高集群的可用性，增加搜索的并发数，但是同时也会影响写入索引的效率。\n\n在索引过程中，需要把更新的文档发到副本节点上，等副本节点生效后在进行返回结束。使用 elasticsearch 做业务搜索的时候，建议副本数目还是设置为 3 个，但是像内部 elk 日志系统、分布式跟踪系统中，完全可以将副本数目设置为 1 个。\n\n\n# 查询方面优化\n\nelasticsearch 作为业务搜索的近实时查询时，查询效率的优化显得尤为重要。\n\n\n# 路由优化\n\n当我们查询文档的时候，elasticsearch 如何知道一个文档应该存放到哪个分片中呢？它其实是通过下面这个公式来计算出来的。\n\nshard = hash(routing) % number_of_primary_shards\n\n\nrouting 默认值是文档的 id，也可以采用自定义值，比如用户 id。\n\n# 不带 routing 查询\n\n在查询的时候因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤：\n\n 1. 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。\n 2. 聚合：协调节点搜集到每个分片上查询结果，再将查询的结果进行排序，之后给用户返回结果。\n\n# 带 routing 查询\n\n查询的时候，可以直接根据 routing 信息定位到某个分配查询，不需要查询所有的分配，经过协调节点排序。\n\n向上面自定义的用户查询，如果 routing 设置为 userid 的话，就可以直接查询出数据来，效率提升很多。\n\n\n# filter vs query\n\n尽可能使用过滤器上下文（filter）替代查询上下文（query）\n\n * query：此文档与此查询子句的匹配程度如何？\n * filter：此文档和查询子句匹配吗？\n\nelasticsearch 针对 filter 查询只需要回答「是」或者「否」，不需要像 query 查询一样计算相关性分数，同时 filter 结果可以缓存。\n\n\n# 深度翻页\n\n在使用 elasticsearch 过程中，应尽量避免大翻页的出现。\n\n正常翻页查询都是从 from 开始 size 条数据，这样就需要在每个分片中查询打分排名在前面的 from+size 条数据。协同节点收集每个分配的前 from+size 条数据。协同节点一共会受到 n*(from+size) 条数据，然后进行排序，再将其中 from 到 from+size 条数据返回出去。如果 from 或者 size 很大的话，导致参加排序的数量会同步扩大很多，最终会导致 cpu 资源消耗增大。\n\n可以通过使用 elasticsearch scroll 和 scroll-scan 高效滚动的方式来解决这样的问题。\n\n也可以结合实际业务特点，文档 id 大小如果和文档创建时间是一致有序的，可以以文档 id 作为分页的偏移量，并将其作为分页查询的一个条件。\n\n\n# 脚本（script）合理使用\n\n我们知道脚本使用主要有 3 种形式，内联动态编译方式、_script 索引库中存储和文件脚本存储的形式；一般脚本的使用场景是粗排，尽量用第二种方式先将脚本存储在 _script 索引库中，起到提前编译，然后通过引用脚本 id，并结合 params 参数使用，即可以达到模型（逻辑）和数据进行了分离，同时又便于脚本模块的扩展与维护。具体 es 脚本的深入内容请参考 elasticsearch 脚本模块的详解。\n\n\n# 数据结构优化\n\n基于 elasticsearch 的使用场景，文档数据结构尽量和使用场景进行结合，去掉没用及不合理的数据。\n\n\n# 尽量减少不需要的字段\n\n如果 elasticsearch 用于业务搜索服务，一些不需要用于搜索的字段最好不存到 es 中，这样即节省空间，同时在相同的数据量下，也能提高搜索性能。\n\n避免使用动态值作字段，动态递增的 mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。控制索引的字段数量、mapping 深度、索引字段的类型，对于 es 的性能优化是重中之重。\n\n以下是 es 关于字段数、mapping 深度的一些默认设置：\n\nindex.mapping.nested_objects.limit: 10000\nindex.mapping.total_fields.limit: 1000\nindex.mapping.depth.limit: 20\n\n\n\n# nested object vs parent/child\n\n尽量避免使用 nested 或 parent/child 的字段，能不用就不用；nested query 慢，parent/child query 更慢，比 nested query 慢上百倍；因此能在 mapping 设计阶段搞定的（大宽表设计或采用比较 smart 的数据结构），就不要用父子关系的 mapping。\n\n如果一定要使用 nested fields，保证 nested fields 字段不能过多，目前 es 默认限制是 50。因为针对 1 个 document，每一个 nested field，都会生成一个独立的 document，这将使 doc 数量剧增，影响查询效率，尤其是 join 的效率。\n\nindex.mapping.nested_fields.limit: 50\n\n\n对比   nested object        parent/child\n优点   文档存储在一起，因此读取性高       父子文档可以独立更新，互不影响\n缺点   更新父文档或子文档时需要更新整个文档   为了维护 join 关系，需要占用部分内存，读取性能较差\n场景   子文档偶尔更新，查询频繁         子文档更新频繁\n\n\n# 选择静态映射，非必需时，禁止动态映射\n\n尽量避免使用动态映射，这样有可能会导致集群崩溃，此外，动态映射有可能会带来不可控制的数据类型，进而有可能导致在查询端出现相关异常，影响业务。\n\n此外，elasticsearch 作为搜索引擎时，主要承载 query 的匹配和排序的功能，那数据的存储类型基于这两种功能的用途分为两类，一是需要匹配的字段，用来建立倒排索引对 query 匹配用，另一类字段是用做粗排用到的特征字段，如 ctr、点击数、评论数等等。\n\n\n# 集群架构设计\n\n合理的部署 elasticsearch 有助于提高服务的整体可用性。\n\n\n# 主节点、数据节点和协调节点分离\n\nelasticsearch 集群在架构拓朴时，采用主节点、数据节点和负载均衡节点分离的架构，在 5.x 版本以后，又可将数据节点再细分为“hot-warm”的架构模式。\n\nelasticsearch 的配置文件中有 2 个参数，node.master 和 node.data。这两个参数搭配使用时，能够帮助提供服务器性能。\n\n# 主（master）节点\n\n配置 node.master:true 和 node.data:false，该 node 服务器只作为一个主节点，但不存储任何索引数据。我们推荐每个集群运行 3 个专用的 master 节点来提供最好的弹性。使用时，你还需要将 discovery.zen.minimum_master_nodes setting 参数设置为 2，以免出现脑裂（split-brain）的情况。用 3 个专用的 master 节点，专门负责处理集群的管理以及加强状态的整体稳定性。因为这 3 个 master 节点不包含数据也不会实际参与搜索以及索引操作，在 jvm 上它们不用做相同的事，例如繁重的索引或者耗时，资源耗费很大的搜索。因此不太可能会因为垃圾回收而导致停顿。因此，master 节点的 cpu，内存以及磁盘配置可以比 data 节点少很多的。\n\n# 数据（data）节点\n\n配置 node.master:false 和 node.data:true，该 node 服务器只作为一个数据节点，只用于存储索引数据，使该 node 服务器功能单一，只用于数据存储和数据查询，降低其资源消耗率。\n\n在 elasticsearch 5.x 版本之后，data 节点又可再细分为“hot-warm”架构，即分为热节点（hot node）和暖节点（warm node）。\n\nhot 节点：\n\nhot 节点主要是索引节点（写节点），同时会保存近期的一些频繁被查询的索引。由于进行索引非常耗费 cpu 和 io，即属于 io 和 cpu 密集型操作，建议使用 ssd 的磁盘类型，保持良好的写性能；我们推荐部署最小化的 3 个 hot 节点来保证高可用性。根据近期需要收集以及查询的数据量，可以增加服务器数量来获得想要的性能。\n\n将节点设置为 hot 类型需要 elasticsearch.yml 如下配置：\n\nnode.attr.box_type: hot\n\n\n如果是针对指定的 index 操作，可以通过 settings 设置 index.routing.allocation.require.box_type: hot 将索引写入 hot 节点。\n\nwarm 节点：\n\n这种类型的节点是为了处理大量的，而且不经常访问的只读索引而设计的。由于这些索引是只读的，warm 节点倾向于挂载大量磁盘（普通磁盘）来替代 ssd。内存、cpu 的配置跟 hot 节点保持一致即可；节点数量一般也是大于等于 3 个。\n\n将节点设置为 warm 类型需要 elasticsearch.yml 如下配置：\n\nnode.attr.box_type: warm\n\n\n同时，也可以在 elasticsearch.yml 中设置 index.codec:best_compression 保证 warm 节点的压缩配置。\n\n当索引不再被频繁查询时，可通过 index.routing.allocation.require.box_type:warm，将索引标记为 warm，从而保证索引不写入 hot 节点，以便将 ssd 磁盘资源用在刀刃上。一旦设置这个属性，es 会自动将索引合并到 warm 节点。\n\n# 协调（coordinating）节点\n\n协调节点用于做分布式里的协调，将各分片或节点返回的数据整合后返回。该节点不会被选作主节点，也不会存储任何索引数据。该服务器主要用于查询负载均衡。在查询的时候，通常会涉及到从多个 node 服务器上查询数据，并将请求分发到多个指定的 node 服务器，并对各个 node 服务器返回的结果进行一个汇总处理，最终返回给客户端。在 es 集群中，所有的节点都有可能是协调节点，但是，可以通过设置 node.master、node.data、node.ingest 都为 false 来设置专门的协调节点。需要较好的 cpu 和较高的内存。\n\n * node.master:false 和 node.data:true，该 node 服务器只作为一个数据节点，只用于存储索引数据，使该 node 服务器功能单一，只用于数据存储和数据查询，降低其资源消耗率。\n * node.master:true 和 node.data:false，该 node 服务器只作为一个主节点，但不存储任何索引数据，该 node 服务器将使用自身空闲的资源，来协调各种创建索引请求或者查询请求，并将这些请求合理分发到相关的 node 服务器上。\n * node.master:false 和 node.data:false，该 node 服务器即不会被选作主节点，也不会存储任何索引数据。该服务器主要用于查询负载均衡。在查询的时候，通常会涉及到从多个 node 服务器上查询数据，并将请求分发到多个指定的 node 服务器，并对各个 node 服务器返回的结果进行一个汇总处理，最终返回给客户端。\n\n\n# 关闭 data 节点服务器中的 http 功能\n\n针对 elasticsearch 集群中的所有数据节点，不用开启 http 服务。将其中的配置参数这样设置，http.enabled:false，同时也不要安装 head, bigdesk, marvel 等监控插件，这样保证 data 节点服务器只需处理创建/更新/删除/查询索引数据等操作。\n\nhttp 功能可以在非数据节点服务器上开启，上述相关的监控插件也安装到这些服务器上，用于监控 elasticsearch 集群状态等数据信息。这样做一来出于数据安全考虑，二来出于服务性能考虑。\n\n\n# 一台服务器上最好只部署一个 node\n\n一台物理服务器上可以启动多个 node 服务器节点（通过设置不同的启动 port），但一台服务器上的 cpu、内存、硬盘等资源毕竟有限，从服务器性能考虑，不建议一台服务器上启动多个 node 节点。\n\n\n# 集群分片设置\n\nes 一旦创建好索引后，就无法调整分片的设置，而在 es 中，一个分片实际上对应一个 lucene 索引，而 lucene 索引的读写会占用很多的系统资源，因此，分片数不能设置过大；所以，在创建索引时，合理配置分片数是非常重要的。一般来说，我们遵循一些原则：\n\n 1. 控制每个分片占用的硬盘容量不超过 es 的最大 jvm 的堆空间设置（一般设置不超过 32 g，参考上面的 jvm 内存设置原则），因此，如果索引的总容量在 500 g 左右，那分片大小在 16 个左右即可；当然，最好同时考虑原则 2。\n 2. 考虑一下 node 数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了 1 个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以，一般都设置分片数不超过节点数的 3 倍。\n\n\n# 参考资料\n\n * elasticsearch 教程",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch Rest API",frontmatter:{title:"Elasticsearch Rest API",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","API"],permalink:"/pages/4b1907/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/11.ElasticsearchRestApi.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/11.ElasticsearchRestApi.md",key:"v-3c818a28",path:"/pages/4b1907/",headers:[{level:2,title:"ElasticSearch Rest API 语法格式",slug:"elasticsearch-rest-api-语法格式",normalizedTitle:"elasticsearch rest api 语法格式",charIndex:311},{level:2,title:"索引 API",slug:"索引-api",normalizedTitle:"索引 api",charIndex:1025},{level:3,title:"创建索引",slug:"创建索引",normalizedTitle:"创建索引",charIndex:1073},{level:3,title:"删除索引",slug:"删除索引",normalizedTitle:"删除索引",charIndex:1637},{level:3,title:"查看索引",slug:"查看索引",normalizedTitle:"查看索引",charIndex:1750},{level:3,title:"索引别名",slug:"索引别名",normalizedTitle:"索引别名",charIndex:2231},{level:3,title:"打开/关闭索引",slug:"打开-关闭索引",normalizedTitle:"打开/关闭索引",charIndex:2852},{level:2,title:"文档",slug:"文档",normalizedTitle:"文档",charIndex:285},{level:3,title:"创建文档",slug:"创建文档",normalizedTitle:"创建文档",charIndex:5603},{level:4,title:"指定 ID",slug:"指定-id",normalizedTitle:"指定 id",charIndex:3389},{level:4,title:"自动生成 ID",slug:"自动生成-id",normalizedTitle:"自动生成 id",charIndex:5774},{level:3,title:"删除文档",slug:"删除文档",normalizedTitle:"删除文档",charIndex:3894},{level:3,title:"更新文档",slug:"更新文档",normalizedTitle:"更新文档",charIndex:5996},{level:4,title:"先删除，再写入",slug:"先删除-再写入",normalizedTitle:"先删除，再写入",charIndex:6004},{level:4,title:"在原文档上增加字段",slug:"在原文档上增加字段",normalizedTitle:"在原文档上增加字段",charIndex:3726},{level:3,title:"查询文档",slug:"查询文档",normalizedTitle:"查询文档",charIndex:2709},{level:4,title:"指定 ID 查询",slug:"指定-id-查询",normalizedTitle:"指定 id 查询",charIndex:6253},{level:4,title:"查询所有记录",slug:"查询所有记录",normalizedTitle:"查询所有记录",charIndex:6614},{level:3,title:"全文搜索",slug:"全文搜索",normalizedTitle:"全文搜索",charIndex:7610},{level:3,title:"逻辑运算",slug:"逻辑运算",normalizedTitle:"逻辑运算",charIndex:8778},{level:3,title:"批量执行",slug:"批量执行",normalizedTitle:"批量执行",charIndex:9183},{level:3,title:"批量读取",slug:"批量读取",normalizedTitle:"批量读取",charIndex:9631},{level:3,title:"批量查询",slug:"批量查询",normalizedTitle:"批量查询",charIndex:10441},{level:3,title:"URI Search 查询语义",slug:"uri-search-查询语义",normalizedTitle:"uri search 查询语义",charIndex:10619},{level:4,title:"Term 和 Phrase",slug:"term-和-phrase",normalizedTitle:"term 和 phrase",charIndex:10989},{level:4,title:"分组与引号",slug:"分组与引号",normalizedTitle:"分组与引号",charIndex:11247},{level:4,title:"AND、OR、NOT 或者 &&、||、!",slug:"and、or、not-或者-、-、",normalizedTitle:"and、or、not 或者 &amp;&amp;、||、!",charIndex:null},{level:4,title:"范围查询",slug:"范围查询",normalizedTitle:"范围查询",charIndex:11508},{level:4,title:"算数符号",slug:"算数符号",normalizedTitle:"算数符号",charIndex:11729},{level:4,title:"通配符查询",slug:"通配符查询",normalizedTitle:"通配符查询",charIndex:11983},{level:4,title:"正则表达式",slug:"正则表达式",normalizedTitle:"正则表达式",charIndex:12139},{level:4,title:"模糊匹配与近似查询",slug:"模糊匹配与近似查询",normalizedTitle:"模糊匹配与近似查询",charIndex:12162},{level:3,title:"Request Body & DSL",slug:"request-body-dsl",normalizedTitle:"request body &amp; dsl",charIndex:null},{level:4,title:"分页",slug:"分页",normalizedTitle:"分页",charIndex:10860},{level:4,title:"排序",slug:"排序",normalizedTitle:"排序",charIndex:2055},{level:4,title:"_source 过滤",slug:"source-过滤",normalizedTitle:"_source 过滤",charIndex:12989},{level:4,title:"脚本字段",slug:"脚本字段",normalizedTitle:"脚本字段",charIndex:13303},{level:4,title:"使用查询表达式 - Match",slug:"使用查询表达式-match",normalizedTitle:"使用查询表达式 - match",charIndex:13626},{level:4,title:"短语搜索 - Match Phrase",slug:"短语搜索-match-phrase",normalizedTitle:"短语搜索 - match phrase",charIndex:13891},{level:2,title:"集群 API",slug:"集群-api",normalizedTitle:"集群 api",charIndex:14045},{level:3,title:"集群健康 API",slug:"集群健康-api",normalizedTitle:"集群健康 api",charIndex:15339},{level:3,title:"集群状态 API",slug:"集群状态-api",normalizedTitle:"集群状态 api",charIndex:15547},{level:2,title:"节点 API",slug:"节点-api",normalizedTitle:"节点 api",charIndex:15608},{level:2,title:"分片 API",slug:"分片-api",normalizedTitle:"分片 api",charIndex:15753},{level:2,title:"监控 API",slug:"监控-api",normalizedTitle:"监控 api",charIndex:16013},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:1035}],headersStr:"ElasticSearch Rest API 语法格式 索引 API 创建索引 删除索引 查看索引 索引别名 打开/关闭索引 文档 创建文档 指定 ID 自动生成 ID 删除文档 更新文档 先删除，再写入 在原文档上增加字段 查询文档 指定 ID 查询 查询所有记录 全文搜索 逻辑运算 批量执行 批量读取 批量查询 URI Search 查询语义 Term 和 Phrase 分组与引号 AND、OR、NOT 或者 &&、||、! 范围查询 算数符号 通配符查询 正则表达式 模糊匹配与近似查询 Request Body & DSL 分页 排序 _source 过滤 脚本字段 使用查询表达式 - Match 短语搜索 - Match Phrase 集群 API 集群健康 API 集群状态 API 节点 API 分片 API 监控 API 参考资料",content:'# ElasticSearch Rest API\n\n> Elasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n> \n> Elasticsearch 基于搜索库 Lucene 开发。ElasticSearch 隐藏了 Lucene 的复杂性，提供了简单易用的 REST API / Java API 接口（另外还有其他语言的 API 接口）。\n> \n> 以下简称 ES。\n> \n> REST API 最详尽的文档应该参考：ES 官方 REST API\n\n\n# ElasticSearch Rest API 语法格式\n\n向 Elasticsearch 发出的请求的组成部分与其它普通的 HTTP 请求是一样的：\n\ncurl -X<VERB> \'<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>\' -d \'<BODY>\'\n\n\n * VERB：HTTP 方法，支持：GET, POST, PUT, HEAD, DELETE\n * PROTOCOL：http 或者 https 协议（只有在 Elasticsearch 前面有 https 代理的时候可用）\n * HOST：Elasticsearch 集群中的任何一个节点的主机名，如果是在本地的节点，那么就叫 localhost\n * PORT：Elasticsearch HTTP 服务所在的端口，默认为 9200 PATH API 路径（例如_count 将返回集群中文档的数量），\n * PATH：可以包含多个组件，例如 _cluster/stats 或者 _nodes/stats/jvm\n * QUERY_STRING：一些可选的查询请求参数，例如?pretty 参数将使请求返回更加美观易读的 JSON 数据\n * BODY：一个 JSON 格式的请求主体（如果请求需要的话）\n\nElasticSearch Rest API 分为两种：\n\n * URI Search：在 URL 中使用查询参数\n * Request Body Search：基于 JSON 格式的、更加完备的 DSL\n\nURI Search 示例：\n\n\n\nRequest Body Search 示例：\n\n\n\n\n# 索引 API\n\n> 参考资料：Elasticsearch 官方之 cat 索引 API\n\n\n# 创建索引\n\n新建 Index，可以直接向 ES 服务器发出 PUT 请求。\n\n语法格式：\n\nPUT /my_index\n{\n    "settings": { ... any settings ... },\n    "mappings": {\n        "type_one": { ... any mappings ... },\n        "type_two": { ... any mappings ... },\n        ...\n    }\n}\n\n\n示例：\n\nPUT /user\n{\n  "settings": {\n    "index": {\n      "number_of_shards": 3,\n      "number_of_replicas": 2\n    }\n  }\n}\n\n\n服务器返回一个 JSON 对象，里面的 acknowledged 字段表示操作成功。\n\n{"acknowledged":true,"shards_acknowledged":true,"index":"user"}\n\n\n如果你想禁止自动创建索引，可以通过在 config/elasticsearch.yml 的每个节点下添加下面的配置：\n\naction.auto_create_index: false\n\n\n\n# 删除索引\n\n然后，我们可以通过发送 DELETE 请求，删除这个 Index。\n\nDELETE /user\n\n\n删除多个索引\n\nDELETE /index_one,index_two\nDELETE /index_*\n\n\n\n# 查看索引\n\n可以通过 GET 请求查看索引信息\n\n# 查看索引相关信息\nGET kibana_sample_data_ecommerce\n\n# 查看索引的文档总数\nGET kibana_sample_data_ecommerce/_count\n\n# 查看前10条文档，了解文档格式\nGET kibana_sample_data_ecommerce/_search\n\n# _cat indices API\n# 查看indices\nGET /_cat/indices/kibana*?v&s=index\n\n# 查看状态为绿的索引\nGET /_cat/indices?v&health=green\n\n# 按照文档个数排序\nGET /_cat/indices?v&s=docs.count:desc\n\n# 查看具体的字段\nGET /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt\n\n# 查看索引占用的内存\nGET /_cat/indices?v&h=i,tm&s=tm:desc\n\n\n\n# 索引别名\n\nES 的索引别名就是给一个索引或者多个索引起的另一个名字，典型的应用场景是针对索引使用的平滑切换。\n\n首先，创建索引 my_index，然后将别名 my_alias 指向它，示例如下：\n\nPUT /my_index\nPUT /my_index/_alias/my_alias\n\n\n也可以通过如下形式：\n\nPOST /_aliases\n{\n  "actions": [\n    { "add": { "index": "my_index", "alias": "my_alias" }}\n  ]\n}\n\n\n也可以在一次请求中增加别名和移除别名混合使用：\n\nPOST /_aliases\n{\n  "actions": [\n    { "remove": { "index": "my_index", "alias": "my_alias" }}\n    { "add": { "index": "my_index_v2", "alias": "my_alias" }}\n  ]\n}\n\n\n> 需要注意的是，如果别名与索引是一对一的，使用别名索引文档或者查询文档是可以的，但是如果别名和索引是一对多的，使用别名会发生错误，因为 ES 不知道把文档写入哪个索引中去或者从哪个索引中读取文档。\n\nES 索引别名有个典型的应用场景是平滑切换，更多细节可以查看 Elasticsearch（ES）索引零停机（无需重启）无缝平滑切换的方法。\n\n\n# 打开/关闭索引\n\n通过在 POST 中添加 _close 或 _open 可以打开、关闭索引。\n\n打开索引\n\n# 打开索引\nPOST kibana_sample_data_ecommerce/_open\n# 关闭索引\nPOST kibana_sample_data_ecommerce/_close\n\n\n\n# 文档\n\n############Create Document############\n#create document. 自动生成 _id\nPOST users/_doc\n{\n\t"user" : "Mike",\n    "post_date" : "2019-04-15T14:12:12",\n    "message" : "trying out Kibana"\n}\n\n#create document. 指定Id。如果id已经存在，报错\nPUT users/_doc/1?op_type=create\n{\n    "user" : "Jack",\n    "post_date" : "2019-05-15T14:12:12",\n    "message" : "trying out Elasticsearch"\n}\n\n#create document. 指定 ID 如果已经存在，就报错\nPUT users/_create/1\n{\n     "user" : "Jack",\n    "post_date" : "2019-05-15T14:12:12",\n    "message" : "trying out Elasticsearch"\n}\n\n### Get Document by ID\n#Get the document by ID\nGET users/_doc/1\n\n\n###  Index & Update\n#Update 指定 ID  (先删除，在写入)\nGET users/_doc/1\n\nPUT users/_doc/1\n{\n\t"user" : "Mike"\n\n}\n\n\n#GET users/_doc/1\n#在原文档上增加字段\nPOST users/_update/1/\n{\n    "doc":{\n        "post_date" : "2019-05-15T14:12:12",\n        "message" : "trying out Elasticsearch"\n    }\n}\n\n\n\n### Delete by Id\n# 删除文档\nDELETE users/_doc/1\n\n\n### Bulk 操作\n#执行两次，查看每次的结果\n\n#执行第1次\nPOST _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n\n#执行第2次\nPOST _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n### mget 操作\nGET /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1"\n        },\n        {\n            "_index" : "test",\n            "_id" : "2"\n        }\n    ]\n}\n\n\n#URI中指定index\nGET /test/_mget\n{\n    "docs" : [\n        {\n\n            "_id" : "1"\n        },\n        {\n\n            "_id" : "2"\n        }\n    ]\n}\n\n\nGET /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1",\n            "_source" : false\n        },\n        {\n            "_index" : "test",\n            "_id" : "2",\n            "_source" : ["field3", "field4"]\n        },\n        {\n            "_index" : "test",\n            "_id" : "3",\n            "_source" : {\n                "include": ["user"],\n                "exclude": ["user.location"]\n            }\n        }\n    ]\n}\n\n### msearch 操作\nPOST kibana_sample_data_ecommerce/_msearch\n{}\n{"query" : {"match_all" : {}},"size":1}\n{"index" : "kibana_sample_data_flights"}\n{"query" : {"match_all" : {}},"size":2}\n\n\n### 清除测试数据\n#清除数据\nDELETE users\nDELETE test\nDELETE test2\n\n\n\n# 创建文档\n\n# 指定 ID\n\n语法格式：\n\nPUT /_index/_type/_create/_id\n\n\n示例：\n\nPUT /user/_doc/_create/1\n{\n  "user": "张三",\n  "title": "工程师",\n  "desc": "数据库管理"\n}\n\n\n> 注意：指定 Id，如果 id 已经存在，则报错\n\n# 自动生成 ID\n\n新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。\n\n语法格式：\n\nPOST /_index/_type\n\n\n示例：\n\nPOST /user/_doc\n{\n  "user": "张三",\n  "title": "工程师",\n  "desc": "超级管理员"\n}\n\n\n\n# 删除文档\n\n语法格式：\n\nDELETE /_index/_doc/_id\n\n\n示例：\n\nDELETE /user/_doc/1\n\n\n\n# 更新文档\n\n# 先删除，再写入\n\n语法格式：\n\nPUT /_index/_type/_id\n\n\n示例：\n\nPUT /user/_doc/1\n{\n  "user": "李四",\n  "title": "工程师",\n  "desc": "超级管理员"\n}\n\n\n# 在原文档上增加字段\n\n语法格式：\n\nPOST /_index/_update/_id\n\n\n示例：\n\nPOST /user/_update/1\n{\n    "doc":{\n        "age" : "30"\n    }\n}\n\n\n\n# 查询文档\n\n# 指定 ID 查询\n\n语法格式：\n\nGET /_index/_type/_id\n\n\n示例：\n\nGET /user/_doc/1\n\n\n结果：\n\n{\n  "_index": "user",\n  "_type": "_doc",\n  "_id": "1",\n  "_version": 1,\n  "_seq_no": 536248,\n  "_primary_term": 2,\n  "found": true,\n  "_source": {\n    "user": "张三",\n    "title": "工程师",\n    "desc": "数据库管理"\n  }\n}\n\n\n返回的数据中，found 字段表示查询成功，_source 字段返回原始记录。\n\n如果 id 不正确，就查不到数据，found 字段就是 false\n\n# 查询所有记录\n\n使用 GET 方法，直接请求 /index/type/_search，就会返回所有记录。\n\n$ curl \'localhost:9200/user/admin/_search?pretty\'\n{\n  "took" : 1,\n  "timed_out" : false,\n  "_shards" : {\n    "total" : 3,\n    "successful" : 3,\n    "skipped" : 0,\n    "failed" : 0\n  },\n  "hits" : {\n    "total" : 2,\n    "max_score" : 1.0,\n    "hits" : [\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "WWuoDG8BHwECs7SiYn93",\n        "_score" : 1.0,\n        "_source" : {\n          "user" : "李四",\n          "title" : "工程师",\n          "desc" : "系统管理"\n        }\n      },\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "1",\n        "_score" : 1.0,\n        "_source" : {\n          "user" : "张三",\n          "title" : "工程师",\n          "desc" : "超级管理员"\n        }\n      }\n    ]\n  }\n}\n\n\n上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。\n\n * total：返回记录数，本例是 2 条。\n * max_score：最高的匹配程度，本例是1.0。\n * hits：返回的记录组成的数组。\n\n返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。\n\n\n# 全文搜索\n\nES 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。\n\n$ curl -H \'Content-Type: application/json\' \'localhost:9200/user/admin/_search?pretty\'  -d \'\n{\n"query" : { "match" : { "desc" : "管理" }}\n}\'\n\n\n上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含"软件"这个词。返回结果如下。\n\n{\n  "took" : 2,\n  "timed_out" : false,\n  "_shards" : {\n    "total" : 3,\n    "successful" : 3,\n    "skipped" : 0,\n    "failed" : 0\n  },\n  "hits" : {\n    "total" : 2,\n    "max_score" : 0.38200712,\n    "hits" : [\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "WWuoDG8BHwECs7SiYn93",\n        "_score" : 0.38200712,\n        "_source" : {\n          "user" : "李四",\n          "title" : "工程师",\n          "desc" : "系统管理"\n        }\n      },\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "1",\n        "_score" : 0.3487891,\n        "_source" : {\n          "user" : "张三",\n          "title" : "工程师",\n          "desc" : "超级管理员"\n        }\n      }\n    ]\n  }\n}\n\n\nElastic 默认一次返回 10 条结果，可以通过size字段改变这个设置，还可以通过from字段，指定位移。\n\n$ curl \'localhost:9200/user/admin/_search\'  -d \'\n{\n  "query" : { "match" : { "desc" : "管理" }},\n  "from": 1,\n  "size": 1\n}\'\n\n\n上面代码指定，从位置 1 开始（默认是从位置 0 开始），只返回一条结果。\n\n\n# 逻辑运算\n\n如果有多个搜索关键字， Elastic 认为它们是or关系。\n\n$ curl \'localhost:9200/user/admin/_search\'  -d \'\n{\n"query" : { "match" : { "desc" : "软件 系统" }}\n}\'\n\n\n上面代码搜索的是软件 or 系统。\n\n如果要执行多个关键词的and搜索，必须使用布尔查询。\n\n$ curl -H \'Content-Type: application/json\' \'localhost:9200/user/admin/_search?pretty\'  -d \'\n{\n "query": {\n  "bool": {\n   "must": [\n    { "match": { "desc": "管理" } },\n    { "match": { "desc": "超级" } }\n   ]\n  }\n }\n}\'\n\n\n\n# 批量执行\n\n支持在一次 API 调用中，对不同的索引进行操作\n\n支持四种类型操作\n\n * index\n * create\n * update\n * delete\n\n操作中单条操作失败，并不会影响其他操作。\n\n返回结果包括了每一条操作执行的结果。\n\nPOST _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n\n> 说明：上面的示例如果执行多次，执行结果都不一样。\n\n\n# 批量读取\n\n读多个索引\n\nGET /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1"\n        },\n        {\n            "_index" : "test",\n            "_id" : "2"\n        }\n    ]\n}\n\n\n读一个索引\n\nGET /test/_mget\n{\n    "docs" : [\n        {\n\n            "_id" : "1"\n        },\n        {\n\n            "_id" : "2"\n        }\n    ]\n}\n\nGET /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1",\n            "_source" : false\n        },\n        {\n            "_index" : "test",\n            "_id" : "2",\n            "_source" : ["field3", "field4"]\n        },\n        {\n            "_index" : "test",\n            "_id" : "3",\n            "_source" : {\n                "include": ["user"],\n                "exclude": ["user.location"]\n            }\n        }\n    ]\n}\n\n\n\n# 批量查询\n\nPOST kibana_sample_data_ecommerce/_msearch\n{}\n{"query" : {"match_all" : {}},"size":1}\n{"index" : "kibana_sample_data_flights"}\n{"query" : {"match_all" : {}},"size":2}\n\n\n\n# URI Search 查询语义\n\nElasticsearch URI Search 遵循 QueryString 查询语义，其形式如下：\n\nGET /movies/_search?q=2012&df=title&sort=year:desc&from=0&size=10&timeout=1s\n{\n\t"profile": true\n}\n\n\n * q 指定查询语句，使用 QueryString 语义\n * df 默认字段，不指定时\n * sort 排序：from 和 size 用于分页\n * profile 可以查看查询时如何被执行的\n\nGET /movies/_search?q=title:2012&sort=year:desc&from=0&size=10&timeout=1s\n{\n\t"profile":"true"\n}\n\n\n# Term 和 Phrase\n\nBeautiful Mind 等效于 Beautiful OR Mind\n\n"Beautiful Mind" 等效于 Beautiful AND Mind\n\n# Term 查询\nGET /movies/_search?q=title:Beautiful Mind\n{\n\t"profile":"true"\n}\n\n# 使用引号，Phrase 查询\nGET /movies/_search?q=title:"Beautiful Mind"\n{\n\t"profile":"true"\n}\n\n\n# 分组与引号\n\ntitle:(Beautiful AND Mind)\n\ntitle="Beautiful Mind"\n\n# AND、OR、NOT 或者 &&、||、!\n\n> 注意：AND、OR、NOT 必须大写\n\n# 布尔操作符\nGET /movies/_search?q=title:(Beautiful AND Mind)\n{\n\t"profile":"true"\n}\n\nGET /movies/_search?q=title:(Beautiful NOT Mind)\n{\n\t"profile":"true"\n}\n\n\n# 范围查询\n\n * [] 表示闭区间\n * {} 表示开区间\n\n示例：\n\n# 范围查询 ,区间写法\nGET /movies/_search?q=title:beautiful AND year:{2010 TO 2018%7D\n{\n\t"profile":"true"\n}\n\nGET /movies/_search?q=title:beautiful AND year:[* TO 2018]\n{\n\t"profile":"true"\n}\n\n\n# 算数符号\n\n# 2010 年以后的记录\nGET /movies/_search?q=year:>2010\n{\n\t"profile":"true"\n}\n\n# 2010 年到 2018 年的记录\nGET /movies/_search?q=year:(>2010 && <=2018)\n{\n\t"profile":"true"\n}\n\n# 2010 年到 2018 年的记录\nGET /movies/_search?q=year:(+>2010 +<=2018)\n{\n\t"profile":"true"\n}\n\n\n# 通配符查询\n\n * ? 代表 1 个字符\n * * 代表 0 或多个字符\n\n示例：\n\nGET /movies/_search?q=title:mi?d\n{\n\t"profile":"true"\n}\n\nGET /movies/_search?q=title:b*\n{\n\t"profile":"true"\n}\n\n\n# 正则表达式\n\ntitle:[bt]oy\n\n# 模糊匹配与近似查询\n\n示例：\n\n# 相似度在 1 个字符以内\nGET /movies/_search?q=title:beautifl~1\n{\n\t"profile":"true"\n}\n\n# 相似度在 2 个字符以内\nGET /movies/_search?q=title:"Lord Rings"~2\n{\n\t"profile":"true"\n}\n\n\n\n# Request Body & DSL\n\nElasticsearch 除了 URI Search 查询方式，还支持将查询语句通过 Http Request Body 发起查询。\n\nGET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n\t"profile":"true",\n\t"query": {\n\t  "match_all": {}\n\t}\n}\n\n\n# 分页\n\nGET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "from": 0,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n# 排序\n\n最好在数字型或日期型字段上排序\n\n因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值\n\nGET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "sort": [\n    {\n      "order_date": "desc"\n    }\n  ],\n  "from": 1,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n# _source 过滤\n\n如果 _source 没有存储，那就只返回匹配的文档的元数据\n\n_source 支持使用通配符，如：_source["name*", "desc*"]\n\n示例：\n\nGET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "_source": [\n    "order_date",\n    "category.keyword"\n  ],\n  "from": 1,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n# 脚本字段\n\nGET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "script_fields": {\n    "new_field": {\n      "script": {\n        "lang": "painless",\n        "source":"doc[\'order_date\'].value+\' hello\'"\n      }\n    }\n  },\n  "from": 1,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n\n# 使用查询表达式 - Match\n\nPOST movies/_search\n{\n  "query": {\n    "match": {\n      "title": "last christmas"\n    }\n  }\n}\n\nPOST movies/_search\n{\n  "query": {\n    "match": {\n      "title": {\n        "query": "last christmas",\n        "operator": "and"\n      }\n    }\n  }\n}\n\n\n\n# 短语搜索 - Match Phrase\n\nPOST movies/_search\n{\n  "query": {\n    "match_phrase": {\n      "title":{\n        "query": "last christmas"\n\n      }\n    }\n  }\n}\n\n\n\n# 集群 API\n\n> Elasticsearch 官方之 Cluster API\n\n一些集群级别的 API 可能会在节点的子集上运行，这些节点可以用节点过滤器指定。例如，任务管理、节点统计和节点信息 API 都可以报告来自一组过滤节点而不是所有节点的结果。\n\n节点过滤器以逗号分隔的单个过滤器列表的形式编写，每个过滤器从所选子集中添加或删除节点。每个过滤器可以是以下之一：\n\n * _all：将所有节点添加到子集\n * _local：将本地节点添加到子集\n * _master：将当前主节点添加到子集\n * 根据节点 ID 或节点名将匹配节点添加到子集\n * 根据 IP 地址或主机名将匹配节点添加到子集\n * 使用通配符，将节点名、地址名或主机名匹配的节点添加到子集\n * master:true, data:true, ingest:true, voting_only:true, ml:true 或 coordinating_only:true, 分别意味着将所有主节点、所有数据节点、所有摄取节点、所有仅投票节点、所有机器学习节点和所有协调节点添加到子集中。\n * master:false, data:false, ingest:false, voting_only:true, ml:false 或 coordinating_only:false, 分别意味着将所有主节点、所有数据节点、所有摄取节点、所有仅投票节点、所有机器学习节点和所有协调节点排除在子集外。\n * 配对模式，使用 * 通配符，格式为 attrname:attrvalue，将所有具有自定义节点属性的节点添加到子集中，其名称和值与相应的模式匹配。自定义节点属性是通过 node.attr.attrname: attrvalue 形式在配置文件中设置的。\n\n# 如果没有给出过滤器，默认是查询所有节点\nGET /_nodes\n# 查询所有节点\nGET /_nodes/_all\n# 查询本地节点\nGET /_nodes/_local\n# 查询主节点\nGET /_nodes/_master\n# 根据名称查询节点（支持通配符）\nGET /_nodes/node_name_goes_here\nGET /_nodes/node_name_goes_*\n# 根据地址查询节点（支持通配符）\nGET /_nodes/10.0.0.3,10.0.0.4\nGET /_nodes/10.0.0.*\n# 根据规则查询节点\nGET /_nodes/_all,master:false\nGET /_nodes/data:true,ingest:true\nGET /_nodes/coordinating_only:true\nGET /_nodes/master:true,voting_only:false\n# 根据自定义属性查询节点（如：查询配置文件中含 node.attr.rack:2 属性的节点）\nGET /_nodes/rack:2\nGET /_nodes/ra*:2\nGET /_nodes/ra*:2*\n\n\n\n# 集群健康 API\n\nGET /_cluster/health\nGET /_cluster/health?level=shards\nGET /_cluster/health/kibana_sample_data_ecommerce,kibana_sample_data_flights\nGET /_cluster/health/kibana_sample_data_flights?level=shards\n\n\n\n# 集群状态 API\n\n集群状态 API 返回表示整个集群状态的元数据。\n\nGET /_cluster/state\n\n\n\n# 节点 API\n\n> Elasticsearch 官方之 cat Nodes API——返回有关集群节点的信息。\n\n# 查看默认的字段\nGET /_cat/nodes?v=true\n# 查看指定的字段\nGET /_cat/nodes?v=true&h=id,ip,port,v,m\n\n\n\n# 分片 API\n\n> Elasticsearch 官方之 cat Shards API——shards 命令是哪些节点包含哪些分片的详细视图。它会告诉你它是主还是副本、文档数量、它在磁盘上占用的字节数以及它所在的节点。\n\n# 查看默认的字段\nGET /_cat/shards\n# 根据名称查询分片（支持通配符）\nGET /_cat/shards/my-index-*\n# 查看指定的字段\nGET /_cat/shards?h=index,shard,prirep,state,unassigned.reason\n\n\n\n# 监控 API\n\nElasticsearch 中集群相关的健康、统计等相关的信息都是围绕着 cat API 进行的。\n\n通过 GET 请求发送 cat，下面列出了所有可用的 API：\n\nGET /_cat\n\n=^.^=\n/_cat/allocation\n/_cat/shards\n/_cat/shards/{index}\n/_cat/master\n/_cat/nodes\n/_cat/tasks\n/_cat/indices\n/_cat/indices/{index}\n/_cat/segments\n/_cat/segments/{index}\n/_cat/count\n/_cat/count/{index}\n/_cat/recovery\n/_cat/recovery/{index}\n/_cat/health\n/_cat/pending_tasks\n/_cat/aliases\n/_cat/aliases/{alias}\n/_cat/thread_pool\n/_cat/thread_pool/{thread_pools}\n/_cat/plugins\n/_cat/fielddata\n/_cat/fielddata/{fields}\n/_cat/nodeattrs\n/_cat/repositories\n/_cat/snapshots/{repository}\n/_cat/templates\n\n\n\n# 参考资料\n\n * 官方\n   * Elasticsearch 官网\n   * Elasticsearch Github\n   * Elasticsearch 官方文档',normalizedContent:'# elasticsearch rest api\n\n> elasticsearch 是一个分布式、restful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 elastic stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n> \n> elasticsearch 基于搜索库 lucene 开发。elasticsearch 隐藏了 lucene 的复杂性，提供了简单易用的 rest api / java api 接口（另外还有其他语言的 api 接口）。\n> \n> 以下简称 es。\n> \n> rest api 最详尽的文档应该参考：es 官方 rest api\n\n\n# elasticsearch rest api 语法格式\n\n向 elasticsearch 发出的请求的组成部分与其它普通的 http 请求是一样的：\n\ncurl -x<verb> \'<protocol>://<host>:<port>/<path>?<query_string>\' -d \'<body>\'\n\n\n * verb：http 方法，支持：get, post, put, head, delete\n * protocol：http 或者 https 协议（只有在 elasticsearch 前面有 https 代理的时候可用）\n * host：elasticsearch 集群中的任何一个节点的主机名，如果是在本地的节点，那么就叫 localhost\n * port：elasticsearch http 服务所在的端口，默认为 9200 path api 路径（例如_count 将返回集群中文档的数量），\n * path：可以包含多个组件，例如 _cluster/stats 或者 _nodes/stats/jvm\n * query_string：一些可选的查询请求参数，例如?pretty 参数将使请求返回更加美观易读的 json 数据\n * body：一个 json 格式的请求主体（如果请求需要的话）\n\nelasticsearch rest api 分为两种：\n\n * uri search：在 url 中使用查询参数\n * request body search：基于 json 格式的、更加完备的 dsl\n\nuri search 示例：\n\n\n\nrequest body search 示例：\n\n\n\n\n# 索引 api\n\n> 参考资料：elasticsearch 官方之 cat 索引 api\n\n\n# 创建索引\n\n新建 index，可以直接向 es 服务器发出 put 请求。\n\n语法格式：\n\nput /my_index\n{\n    "settings": { ... any settings ... },\n    "mappings": {\n        "type_one": { ... any mappings ... },\n        "type_two": { ... any mappings ... },\n        ...\n    }\n}\n\n\n示例：\n\nput /user\n{\n  "settings": {\n    "index": {\n      "number_of_shards": 3,\n      "number_of_replicas": 2\n    }\n  }\n}\n\n\n服务器返回一个 json 对象，里面的 acknowledged 字段表示操作成功。\n\n{"acknowledged":true,"shards_acknowledged":true,"index":"user"}\n\n\n如果你想禁止自动创建索引，可以通过在 config/elasticsearch.yml 的每个节点下添加下面的配置：\n\naction.auto_create_index: false\n\n\n\n# 删除索引\n\n然后，我们可以通过发送 delete 请求，删除这个 index。\n\ndelete /user\n\n\n删除多个索引\n\ndelete /index_one,index_two\ndelete /index_*\n\n\n\n# 查看索引\n\n可以通过 get 请求查看索引信息\n\n# 查看索引相关信息\nget kibana_sample_data_ecommerce\n\n# 查看索引的文档总数\nget kibana_sample_data_ecommerce/_count\n\n# 查看前10条文档，了解文档格式\nget kibana_sample_data_ecommerce/_search\n\n# _cat indices api\n# 查看indices\nget /_cat/indices/kibana*?v&s=index\n\n# 查看状态为绿的索引\nget /_cat/indices?v&health=green\n\n# 按照文档个数排序\nget /_cat/indices?v&s=docs.count:desc\n\n# 查看具体的字段\nget /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt\n\n# 查看索引占用的内存\nget /_cat/indices?v&h=i,tm&s=tm:desc\n\n\n\n# 索引别名\n\nes 的索引别名就是给一个索引或者多个索引起的另一个名字，典型的应用场景是针对索引使用的平滑切换。\n\n首先，创建索引 my_index，然后将别名 my_alias 指向它，示例如下：\n\nput /my_index\nput /my_index/_alias/my_alias\n\n\n也可以通过如下形式：\n\npost /_aliases\n{\n  "actions": [\n    { "add": { "index": "my_index", "alias": "my_alias" }}\n  ]\n}\n\n\n也可以在一次请求中增加别名和移除别名混合使用：\n\npost /_aliases\n{\n  "actions": [\n    { "remove": { "index": "my_index", "alias": "my_alias" }}\n    { "add": { "index": "my_index_v2", "alias": "my_alias" }}\n  ]\n}\n\n\n> 需要注意的是，如果别名与索引是一对一的，使用别名索引文档或者查询文档是可以的，但是如果别名和索引是一对多的，使用别名会发生错误，因为 es 不知道把文档写入哪个索引中去或者从哪个索引中读取文档。\n\nes 索引别名有个典型的应用场景是平滑切换，更多细节可以查看 elasticsearch（es）索引零停机（无需重启）无缝平滑切换的方法。\n\n\n# 打开/关闭索引\n\n通过在 post 中添加 _close 或 _open 可以打开、关闭索引。\n\n打开索引\n\n# 打开索引\npost kibana_sample_data_ecommerce/_open\n# 关闭索引\npost kibana_sample_data_ecommerce/_close\n\n\n\n# 文档\n\n############create document############\n#create document. 自动生成 _id\npost users/_doc\n{\n\t"user" : "mike",\n    "post_date" : "2019-04-15t14:12:12",\n    "message" : "trying out kibana"\n}\n\n#create document. 指定id。如果id已经存在，报错\nput users/_doc/1?op_type=create\n{\n    "user" : "jack",\n    "post_date" : "2019-05-15t14:12:12",\n    "message" : "trying out elasticsearch"\n}\n\n#create document. 指定 id 如果已经存在，就报错\nput users/_create/1\n{\n     "user" : "jack",\n    "post_date" : "2019-05-15t14:12:12",\n    "message" : "trying out elasticsearch"\n}\n\n### get document by id\n#get the document by id\nget users/_doc/1\n\n\n###  index & update\n#update 指定 id  (先删除，在写入)\nget users/_doc/1\n\nput users/_doc/1\n{\n\t"user" : "mike"\n\n}\n\n\n#get users/_doc/1\n#在原文档上增加字段\npost users/_update/1/\n{\n    "doc":{\n        "post_date" : "2019-05-15t14:12:12",\n        "message" : "trying out elasticsearch"\n    }\n}\n\n\n\n### delete by id\n# 删除文档\ndelete users/_doc/1\n\n\n### bulk 操作\n#执行两次，查看每次的结果\n\n#执行第1次\npost _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n\n#执行第2次\npost _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n### mget 操作\nget /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1"\n        },\n        {\n            "_index" : "test",\n            "_id" : "2"\n        }\n    ]\n}\n\n\n#uri中指定index\nget /test/_mget\n{\n    "docs" : [\n        {\n\n            "_id" : "1"\n        },\n        {\n\n            "_id" : "2"\n        }\n    ]\n}\n\n\nget /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1",\n            "_source" : false\n        },\n        {\n            "_index" : "test",\n            "_id" : "2",\n            "_source" : ["field3", "field4"]\n        },\n        {\n            "_index" : "test",\n            "_id" : "3",\n            "_source" : {\n                "include": ["user"],\n                "exclude": ["user.location"]\n            }\n        }\n    ]\n}\n\n### msearch 操作\npost kibana_sample_data_ecommerce/_msearch\n{}\n{"query" : {"match_all" : {}},"size":1}\n{"index" : "kibana_sample_data_flights"}\n{"query" : {"match_all" : {}},"size":2}\n\n\n### 清除测试数据\n#清除数据\ndelete users\ndelete test\ndelete test2\n\n\n\n# 创建文档\n\n# 指定 id\n\n语法格式：\n\nput /_index/_type/_create/_id\n\n\n示例：\n\nput /user/_doc/_create/1\n{\n  "user": "张三",\n  "title": "工程师",\n  "desc": "数据库管理"\n}\n\n\n> 注意：指定 id，如果 id 已经存在，则报错\n\n# 自动生成 id\n\n新增记录的时候，也可以不指定 id，这时要改成 post 请求。\n\n语法格式：\n\npost /_index/_type\n\n\n示例：\n\npost /user/_doc\n{\n  "user": "张三",\n  "title": "工程师",\n  "desc": "超级管理员"\n}\n\n\n\n# 删除文档\n\n语法格式：\n\ndelete /_index/_doc/_id\n\n\n示例：\n\ndelete /user/_doc/1\n\n\n\n# 更新文档\n\n# 先删除，再写入\n\n语法格式：\n\nput /_index/_type/_id\n\n\n示例：\n\nput /user/_doc/1\n{\n  "user": "李四",\n  "title": "工程师",\n  "desc": "超级管理员"\n}\n\n\n# 在原文档上增加字段\n\n语法格式：\n\npost /_index/_update/_id\n\n\n示例：\n\npost /user/_update/1\n{\n    "doc":{\n        "age" : "30"\n    }\n}\n\n\n\n# 查询文档\n\n# 指定 id 查询\n\n语法格式：\n\nget /_index/_type/_id\n\n\n示例：\n\nget /user/_doc/1\n\n\n结果：\n\n{\n  "_index": "user",\n  "_type": "_doc",\n  "_id": "1",\n  "_version": 1,\n  "_seq_no": 536248,\n  "_primary_term": 2,\n  "found": true,\n  "_source": {\n    "user": "张三",\n    "title": "工程师",\n    "desc": "数据库管理"\n  }\n}\n\n\n返回的数据中，found 字段表示查询成功，_source 字段返回原始记录。\n\n如果 id 不正确，就查不到数据，found 字段就是 false\n\n# 查询所有记录\n\n使用 get 方法，直接请求 /index/type/_search，就会返回所有记录。\n\n$ curl \'localhost:9200/user/admin/_search?pretty\'\n{\n  "took" : 1,\n  "timed_out" : false,\n  "_shards" : {\n    "total" : 3,\n    "successful" : 3,\n    "skipped" : 0,\n    "failed" : 0\n  },\n  "hits" : {\n    "total" : 2,\n    "max_score" : 1.0,\n    "hits" : [\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "wwuodg8bhwecs7siyn93",\n        "_score" : 1.0,\n        "_source" : {\n          "user" : "李四",\n          "title" : "工程师",\n          "desc" : "系统管理"\n        }\n      },\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "1",\n        "_score" : 1.0,\n        "_source" : {\n          "user" : "张三",\n          "title" : "工程师",\n          "desc" : "超级管理员"\n        }\n      }\n    ]\n  }\n}\n\n\n上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。\n\n * total：返回记录数，本例是 2 条。\n * max_score：最高的匹配程度，本例是1.0。\n * hits：返回的记录组成的数组。\n\n返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。\n\n\n# 全文搜索\n\nes 的查询非常特别，使用自己的查询语法，要求 get 请求带有数据体。\n\n$ curl -h \'content-type: application/json\' \'localhost:9200/user/admin/_search?pretty\'  -d \'\n{\n"query" : { "match" : { "desc" : "管理" }}\n}\'\n\n\n上面代码使用 match 查询，指定的匹配条件是desc字段里面包含"软件"这个词。返回结果如下。\n\n{\n  "took" : 2,\n  "timed_out" : false,\n  "_shards" : {\n    "total" : 3,\n    "successful" : 3,\n    "skipped" : 0,\n    "failed" : 0\n  },\n  "hits" : {\n    "total" : 2,\n    "max_score" : 0.38200712,\n    "hits" : [\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "wwuodg8bhwecs7siyn93",\n        "_score" : 0.38200712,\n        "_source" : {\n          "user" : "李四",\n          "title" : "工程师",\n          "desc" : "系统管理"\n        }\n      },\n      {\n        "_index" : "user",\n        "_type" : "admin",\n        "_id" : "1",\n        "_score" : 0.3487891,\n        "_source" : {\n          "user" : "张三",\n          "title" : "工程师",\n          "desc" : "超级管理员"\n        }\n      }\n    ]\n  }\n}\n\n\nelastic 默认一次返回 10 条结果，可以通过size字段改变这个设置，还可以通过from字段，指定位移。\n\n$ curl \'localhost:9200/user/admin/_search\'  -d \'\n{\n  "query" : { "match" : { "desc" : "管理" }},\n  "from": 1,\n  "size": 1\n}\'\n\n\n上面代码指定，从位置 1 开始（默认是从位置 0 开始），只返回一条结果。\n\n\n# 逻辑运算\n\n如果有多个搜索关键字， elastic 认为它们是or关系。\n\n$ curl \'localhost:9200/user/admin/_search\'  -d \'\n{\n"query" : { "match" : { "desc" : "软件 系统" }}\n}\'\n\n\n上面代码搜索的是软件 or 系统。\n\n如果要执行多个关键词的and搜索，必须使用布尔查询。\n\n$ curl -h \'content-type: application/json\' \'localhost:9200/user/admin/_search?pretty\'  -d \'\n{\n "query": {\n  "bool": {\n   "must": [\n    { "match": { "desc": "管理" } },\n    { "match": { "desc": "超级" } }\n   ]\n  }\n }\n}\'\n\n\n\n# 批量执行\n\n支持在一次 api 调用中，对不同的索引进行操作\n\n支持四种类型操作\n\n * index\n * create\n * update\n * delete\n\n操作中单条操作失败，并不会影响其他操作。\n\n返回结果包括了每一条操作执行的结果。\n\npost _bulk\n{ "index" : { "_index" : "test", "_id" : "1" } }\n{ "field1" : "value1" }\n{ "delete" : { "_index" : "test", "_id" : "2" } }\n{ "create" : { "_index" : "test2", "_id" : "3" } }\n{ "field1" : "value3" }\n{ "update" : {"_id" : "1", "_index" : "test"} }\n{ "doc" : {"field2" : "value2"} }\n\n\n> 说明：上面的示例如果执行多次，执行结果都不一样。\n\n\n# 批量读取\n\n读多个索引\n\nget /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1"\n        },\n        {\n            "_index" : "test",\n            "_id" : "2"\n        }\n    ]\n}\n\n\n读一个索引\n\nget /test/_mget\n{\n    "docs" : [\n        {\n\n            "_id" : "1"\n        },\n        {\n\n            "_id" : "2"\n        }\n    ]\n}\n\nget /_mget\n{\n    "docs" : [\n        {\n            "_index" : "test",\n            "_id" : "1",\n            "_source" : false\n        },\n        {\n            "_index" : "test",\n            "_id" : "2",\n            "_source" : ["field3", "field4"]\n        },\n        {\n            "_index" : "test",\n            "_id" : "3",\n            "_source" : {\n                "include": ["user"],\n                "exclude": ["user.location"]\n            }\n        }\n    ]\n}\n\n\n\n# 批量查询\n\npost kibana_sample_data_ecommerce/_msearch\n{}\n{"query" : {"match_all" : {}},"size":1}\n{"index" : "kibana_sample_data_flights"}\n{"query" : {"match_all" : {}},"size":2}\n\n\n\n# uri search 查询语义\n\nelasticsearch uri search 遵循 querystring 查询语义，其形式如下：\n\nget /movies/_search?q=2012&df=title&sort=year:desc&from=0&size=10&timeout=1s\n{\n\t"profile": true\n}\n\n\n * q 指定查询语句，使用 querystring 语义\n * df 默认字段，不指定时\n * sort 排序：from 和 size 用于分页\n * profile 可以查看查询时如何被执行的\n\nget /movies/_search?q=title:2012&sort=year:desc&from=0&size=10&timeout=1s\n{\n\t"profile":"true"\n}\n\n\n# term 和 phrase\n\nbeautiful mind 等效于 beautiful or mind\n\n"beautiful mind" 等效于 beautiful and mind\n\n# term 查询\nget /movies/_search?q=title:beautiful mind\n{\n\t"profile":"true"\n}\n\n# 使用引号，phrase 查询\nget /movies/_search?q=title:"beautiful mind"\n{\n\t"profile":"true"\n}\n\n\n# 分组与引号\n\ntitle:(beautiful and mind)\n\ntitle="beautiful mind"\n\n# and、or、not 或者 &&、||、!\n\n> 注意：and、or、not 必须大写\n\n# 布尔操作符\nget /movies/_search?q=title:(beautiful and mind)\n{\n\t"profile":"true"\n}\n\nget /movies/_search?q=title:(beautiful not mind)\n{\n\t"profile":"true"\n}\n\n\n# 范围查询\n\n * [] 表示闭区间\n * {} 表示开区间\n\n示例：\n\n# 范围查询 ,区间写法\nget /movies/_search?q=title:beautiful and year:{2010 to 2018%7d\n{\n\t"profile":"true"\n}\n\nget /movies/_search?q=title:beautiful and year:[* to 2018]\n{\n\t"profile":"true"\n}\n\n\n# 算数符号\n\n# 2010 年以后的记录\nget /movies/_search?q=year:>2010\n{\n\t"profile":"true"\n}\n\n# 2010 年到 2018 年的记录\nget /movies/_search?q=year:(>2010 && <=2018)\n{\n\t"profile":"true"\n}\n\n# 2010 年到 2018 年的记录\nget /movies/_search?q=year:(+>2010 +<=2018)\n{\n\t"profile":"true"\n}\n\n\n# 通配符查询\n\n * ? 代表 1 个字符\n * * 代表 0 或多个字符\n\n示例：\n\nget /movies/_search?q=title:mi?d\n{\n\t"profile":"true"\n}\n\nget /movies/_search?q=title:b*\n{\n\t"profile":"true"\n}\n\n\n# 正则表达式\n\ntitle:[bt]oy\n\n# 模糊匹配与近似查询\n\n示例：\n\n# 相似度在 1 个字符以内\nget /movies/_search?q=title:beautifl~1\n{\n\t"profile":"true"\n}\n\n# 相似度在 2 个字符以内\nget /movies/_search?q=title:"lord rings"~2\n{\n\t"profile":"true"\n}\n\n\n\n# request body & dsl\n\nelasticsearch 除了 uri search 查询方式，还支持将查询语句通过 http request body 发起查询。\n\nget /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n\t"profile":"true",\n\t"query": {\n\t  "match_all": {}\n\t}\n}\n\n\n# 分页\n\nget /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "from": 0,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n# 排序\n\n最好在数字型或日期型字段上排序\n\n因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值\n\nget /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "sort": [\n    {\n      "order_date": "desc"\n    }\n  ],\n  "from": 1,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n# _source 过滤\n\n如果 _source 没有存储，那就只返回匹配的文档的元数据\n\n_source 支持使用通配符，如：_source["name*", "desc*"]\n\n示例：\n\nget /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "_source": [\n    "order_date",\n    "category.keyword"\n  ],\n  "from": 1,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n# 脚本字段\n\nget /kibana_sample_data_ecommerce/_search?ignore_unavailable=true\n{\n  "profile": "true",\n  "script_fields": {\n    "new_field": {\n      "script": {\n        "lang": "painless",\n        "source":"doc[\'order_date\'].value+\' hello\'"\n      }\n    }\n  },\n  "from": 1,\n  "size": 10,\n  "query": {\n    "match_all": {}\n  }\n}\n\n\n\n# 使用查询表达式 - match\n\npost movies/_search\n{\n  "query": {\n    "match": {\n      "title": "last christmas"\n    }\n  }\n}\n\npost movies/_search\n{\n  "query": {\n    "match": {\n      "title": {\n        "query": "last christmas",\n        "operator": "and"\n      }\n    }\n  }\n}\n\n\n\n# 短语搜索 - match phrase\n\npost movies/_search\n{\n  "query": {\n    "match_phrase": {\n      "title":{\n        "query": "last christmas"\n\n      }\n    }\n  }\n}\n\n\n\n# 集群 api\n\n> elasticsearch 官方之 cluster api\n\n一些集群级别的 api 可能会在节点的子集上运行，这些节点可以用节点过滤器指定。例如，任务管理、节点统计和节点信息 api 都可以报告来自一组过滤节点而不是所有节点的结果。\n\n节点过滤器以逗号分隔的单个过滤器列表的形式编写，每个过滤器从所选子集中添加或删除节点。每个过滤器可以是以下之一：\n\n * _all：将所有节点添加到子集\n * _local：将本地节点添加到子集\n * _master：将当前主节点添加到子集\n * 根据节点 id 或节点名将匹配节点添加到子集\n * 根据 ip 地址或主机名将匹配节点添加到子集\n * 使用通配符，将节点名、地址名或主机名匹配的节点添加到子集\n * master:true, data:true, ingest:true, voting_only:true, ml:true 或 coordinating_only:true, 分别意味着将所有主节点、所有数据节点、所有摄取节点、所有仅投票节点、所有机器学习节点和所有协调节点添加到子集中。\n * master:false, data:false, ingest:false, voting_only:true, ml:false 或 coordinating_only:false, 分别意味着将所有主节点、所有数据节点、所有摄取节点、所有仅投票节点、所有机器学习节点和所有协调节点排除在子集外。\n * 配对模式，使用 * 通配符，格式为 attrname:attrvalue，将所有具有自定义节点属性的节点添加到子集中，其名称和值与相应的模式匹配。自定义节点属性是通过 node.attr.attrname: attrvalue 形式在配置文件中设置的。\n\n# 如果没有给出过滤器，默认是查询所有节点\nget /_nodes\n# 查询所有节点\nget /_nodes/_all\n# 查询本地节点\nget /_nodes/_local\n# 查询主节点\nget /_nodes/_master\n# 根据名称查询节点（支持通配符）\nget /_nodes/node_name_goes_here\nget /_nodes/node_name_goes_*\n# 根据地址查询节点（支持通配符）\nget /_nodes/10.0.0.3,10.0.0.4\nget /_nodes/10.0.0.*\n# 根据规则查询节点\nget /_nodes/_all,master:false\nget /_nodes/data:true,ingest:true\nget /_nodes/coordinating_only:true\nget /_nodes/master:true,voting_only:false\n# 根据自定义属性查询节点（如：查询配置文件中含 node.attr.rack:2 属性的节点）\nget /_nodes/rack:2\nget /_nodes/ra*:2\nget /_nodes/ra*:2*\n\n\n\n# 集群健康 api\n\nget /_cluster/health\nget /_cluster/health?level=shards\nget /_cluster/health/kibana_sample_data_ecommerce,kibana_sample_data_flights\nget /_cluster/health/kibana_sample_data_flights?level=shards\n\n\n\n# 集群状态 api\n\n集群状态 api 返回表示整个集群状态的元数据。\n\nget /_cluster/state\n\n\n\n# 节点 api\n\n> elasticsearch 官方之 cat nodes api——返回有关集群节点的信息。\n\n# 查看默认的字段\nget /_cat/nodes?v=true\n# 查看指定的字段\nget /_cat/nodes?v=true&h=id,ip,port,v,m\n\n\n\n# 分片 api\n\n> elasticsearch 官方之 cat shards api——shards 命令是哪些节点包含哪些分片的详细视图。它会告诉你它是主还是副本、文档数量、它在磁盘上占用的字节数以及它所在的节点。\n\n# 查看默认的字段\nget /_cat/shards\n# 根据名称查询分片（支持通配符）\nget /_cat/shards/my-index-*\n# 查看指定的字段\nget /_cat/shards?h=index,shard,prirep,state,unassigned.reason\n\n\n\n# 监控 api\n\nelasticsearch 中集群相关的健康、统计等相关的信息都是围绕着 cat api 进行的。\n\n通过 get 请求发送 cat，下面列出了所有可用的 api：\n\nget /_cat\n\n=^.^=\n/_cat/allocation\n/_cat/shards\n/_cat/shards/{index}\n/_cat/master\n/_cat/nodes\n/_cat/tasks\n/_cat/indices\n/_cat/indices/{index}\n/_cat/segments\n/_cat/segments/{index}\n/_cat/count\n/_cat/count/{index}\n/_cat/recovery\n/_cat/recovery/{index}\n/_cat/health\n/_cat/pending_tasks\n/_cat/aliases\n/_cat/aliases/{alias}\n/_cat/thread_pool\n/_cat/thread_pool/{thread_pools}\n/_cat/plugins\n/_cat/fielddata\n/_cat/fielddata/{fields}\n/_cat/nodeattrs\n/_cat/repositories\n/_cat/snapshots/{repository}\n/_cat/templates\n\n\n\n# 参考资料\n\n * 官方\n   * elasticsearch 官网\n   * elasticsearch github\n   * elasticsearch 官方文档',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"ElasticSearch Java API 之 High Level REST Client",frontmatter:{title:"ElasticSearch Java API 之 High Level REST Client",date:"2022-03-01T18:55:46.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","API"],permalink:"/pages/201e43/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/12.ElasticsearchHighLevelRestJavaApi.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/12.ElasticsearchHighLevelRestJavaApi.md",key:"v-7c0bbdd0",path:"/pages/201e43/",headers:[{level:2,title:"快速开始",slug:"快速开始",normalizedTitle:"快速开始",charIndex:131},{level:3,title:"引入依赖",slug:"引入依赖",normalizedTitle:"引入依赖",charIndex:140},{level:3,title:"创建连接和关闭",slug:"创建连接和关闭",normalizedTitle:"创建连接和关闭",charIndex:342},{level:2,title:"索引 API",slug:"索引-api",normalizedTitle:"索引 api",charIndex:585},{level:3,title:"测试准备",slug:"测试准备",normalizedTitle:"测试准备",charIndex:596},{level:3,title:"创建索引",slug:"创建索引",normalizedTitle:"创建索引",charIndex:1315},{level:3,title:"删除索引",slug:"删除索引",normalizedTitle:"删除索引",charIndex:1875},{level:3,title:"判断索引是否存在",slug:"判断索引是否存在",normalizedTitle:"判断索引是否存在",charIndex:2132},{level:2,title:"文档 API",slug:"文档-api",normalizedTitle:"文档 api",charIndex:2471},{level:3,title:"文档测试准备",slug:"文档测试准备",normalizedTitle:"文档测试准备",charIndex:2482},{level:3,title:"创建文档",slug:"创建文档",normalizedTitle:"创建文档",charIndex:4414},{level:3,title:"删除文档",slug:"删除文档",normalizedTitle:"删除文档",charIndex:5179},{level:3,title:"更新文档",slug:"更新文档",normalizedTitle:"更新文档",charIndex:5765},{level:3,title:"查看文档",slug:"查看文档",normalizedTitle:"查看文档",charIndex:6591},{level:3,title:"获取匹配条件的记录总数",slug:"获取匹配条件的记录总数",normalizedTitle:"获取匹配条件的记录总数",charIndex:7095},{level:3,title:"分页查询",slug:"分页查询",normalizedTitle:"分页查询",charIndex:7651},{level:3,title:"条件查询",slug:"条件查询",normalizedTitle:"条件查询",charIndex:8602},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:9509}],headersStr:"快速开始 引入依赖 创建连接和关闭 索引 API 测试准备 创建索引 删除索引 判断索引是否存在 文档 API 文档测试准备 创建文档 删除文档 更新文档 查看文档 获取匹配条件的记录总数 分页查询 条件查询 参考资料",content:'# ElasticSearch Java API 之 High Level REST Client\n\n> Elasticsearch 官方的 High Level REST Client 在 7.1.5.0 版本废弃。所以本文中的 API 不推荐使用。\n\n\n# 快速开始\n\n\n# 引入依赖\n\n在 pom.xml 中引入以下依赖：\n\n<dependency>\n    <groupId>org.elasticsearch.client</groupId>\n    <artifactId>elasticsearch-rest-high-level-client</artifactId>\n    <version>7.17.1</version>\n</dependency>\n\n\n\n# 创建连接和关闭\n\n// 创建连接\nRestHighLevelClient client = new RestHighLevelClient(\n        RestClient.builder(\n                new HttpHost("localhost", 9200, "http"),\n                new HttpHost("localhost", 9201, "http")));\n\n// 关闭\nclient.close();\n\n\n\n# 索引 API\n\n\n# 测试准备\n\npublic static final String INDEX = "mytest";\npublic static final String INDEX_ALIAS = "mytest_alias";\n/**\n * {@link User} 的 mapping 结构（json形式）\n */\npublic static final String MAPPING_JSON =\n  "{\\n" + "  \\"properties\\": {\\n" + "    \\"_class\\": {\\n" + "      \\"type\\": \\"keyword\\",\\n"\n  + "      \\"index\\": false,\\n" + "      \\"doc_values\\": false\\n" + "    },\\n" + "    \\"description\\": {\\n"\n  + "      \\"type\\": \\"text\\",\\n" + "      \\"fielddata\\": true\\n" + "    },\\n" + "    \\"enabled\\": {\\n"\n  + "      \\"type\\": \\"boolean\\"\\n" + "    },\\n" + "    \\"name\\": {\\n" + "      \\"type\\": \\"text\\",\\n"\n  + "      \\"fielddata\\": true\\n" + "    }\\n" + "  }\\n" + "}";\n\n@Autowired\nprivate RestHighLevelClient client;\n\n\n\n# 创建索引\n\n// 创建索引\nCreateIndexRequest createIndexRequest = new CreateIndexRequest(INDEX);\n\n  // 设置索引的 settings\n  createIndexRequest.settings(\n  Settings.builder().put("index.number_of_shards", 3).put("index.number_of_replicas", 2));\n\n  // 设置索引的 mapping\n  createIndexRequest.mapping(MAPPING_JSON, XContentType.JSON);\n\n  // 设置索引的别名\n  createIndexRequest.alias(new Alias(INDEX_ALIAS));\n\n  AcknowledgedResponse createIndexResponse = client.indices().create(createIndexRequest, RequestOptions.DEFAULT);\n  Assertions.assertTrue(createIndexResponse.isAcknowledged());\n\n\n\n# 删除索引\n\n// 删除索引\nDeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(INDEX);\n  AcknowledgedResponse deleteResponse = client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT);\n  Assertions.assertTrue(deleteResponse.isAcknowledged());\n\n\n\n# 判断索引是否存在\n\nGetIndexRequest getIndexRequest = new GetIndexRequest(INDEX);\n  Assertions.assertTrue(client.indices().exists(getIndexRequest, RequestOptions.DEFAULT));\n  GetIndexRequest getIndexAliasRequest = new GetIndexRequest(INDEX_ALIAS);\n  Assertions.assertTrue(client.indices().exists(getIndexAliasRequest, RequestOptions.DEFAULT));\n\n\n\n# 文档 API\n\n\n# 文档测试准备\n\npublic static final String INDEX = "mytest";\npublic static final String INDEX_ALIAS = "mytest_alias";\n/**\n * {@link User} 的 mapping 结构（json形式）\n */\npublic static final String MAPPING_JSON =\n  "{\\n" + "  \\"properties\\": {\\n" + "    \\"_class\\": {\\n" + "      \\"type\\": \\"keyword\\",\\n"\n  + "      \\"index\\": false,\\n" + "      \\"doc_values\\": false\\n" + "    },\\n" + "    \\"description\\": {\\n"\n  + "      \\"type\\": \\"text\\",\\n" + "      \\"fielddata\\": true\\n" + "    },\\n" + "    \\"enabled\\": {\\n"\n  + "      \\"type\\": \\"boolean\\"\\n" + "    },\\n" + "    \\"name\\": {\\n" + "      \\"type\\": \\"text\\",\\n"\n  + "      \\"fielddata\\": true\\n" + "    }\\n" + "  }\\n" + "}";\n\n@Autowired\nprivate RestHighLevelClient client;\n\n@BeforeEach\npublic void init() throws IOException {\n\n  // 创建索引\n  CreateIndexRequest createIndexRequest = new CreateIndexRequest(INDEX);\n\n  // 设置索引的 settings\n  createIndexRequest.settings(\n  Settings.builder().put("index.number_of_shards", 3).put("index.number_of_replicas", 2));\n\n  // 设置索引的 mapping\n  createIndexRequest.mapping(MAPPING_JSON, XContentType.JSON);\n\n  // 设置索引的别名\n  createIndexRequest.alias(new Alias(INDEX_ALIAS));\n\n  AcknowledgedResponse response = client.indices().create(createIndexRequest, RequestOptions.DEFAULT);\n  Assertions.assertTrue(response.isAcknowledged());\n\n  // 判断索引是否存在\n  GetIndexRequest getIndexRequest = new GetIndexRequest(INDEX_ALIAS);\n  Assertions.assertTrue(client.indices().exists(getIndexRequest, RequestOptions.DEFAULT));\n  GetIndexRequest getIndexAliasRequest = new GetIndexRequest(INDEX_ALIAS);\n  Assertions.assertTrue(client.indices().exists(getIndexAliasRequest, RequestOptions.DEFAULT));\n  }\n\n@AfterEach\npublic void destroy() throws IOException {\n  // 删除索引\n  DeleteIndexRequest request = new DeleteIndexRequest(INDEX);\n  AcknowledgedResponse response = client.indices().delete(request, RequestOptions.DEFAULT);\n  Assertions.assertTrue(response.isAcknowledged());\n  }\n\n\n\n# 创建文档\n\nRestHighLevelClient Api 使用 IndexRequest 来构建创建文档的请求参数。\n\n【示例】创建 id 为 1 的文档\n\nIndexRequest request = new IndexRequest("product");\n  request.id("1");\n  Product product = new Product();\n  product.setName("机器人");\n  product.setDescription("人工智能机器人");\n  product.setEnabled(true);\n  String jsonString = JSONUtil.toJsonStr(product);\n  request.source(jsonString, XContentType.JSON);\n\n\n同步执行\n\nIndexResponse indexResponse = client.index(request, RequestOptions.DEFAULT);\n\n\n异步执行\n\n// 异步执行\nclient.indexAsync(request, RequestOptions.DEFAULT, new ActionListener<IndexResponse>() {\n@Override\npublic void onResponse(IndexResponse indexResponse) {\n  System.out.println(indexResponse);\n  }\n\n@Override\npublic void onFailure(Exception e) {\n  System.out.println("执行失败");\n  }\n  });\n\n\n\n# 删除文档\n\nRestHighLevelClient Api 使用 DeleteRequest 来构建删除文档的请求参数。\n\n【示例】删除 id 为 1 的文档\n\nDeleteRequest deleteRequest = new DeleteRequest(INDEX_ALIAS, "1");\n\n\n同步执行\n\nDeleteResponse deleteResponse = client.delete(deleteRequest, RequestOptions.DEFAULT);\n  System.out.println(deleteResponse);\n\n\n异步执行\n\nclient.deleteAsync(deleteRequest, RequestOptions.DEFAULT, new ActionListener<DeleteResponse>() {\n@Override\npublic void onResponse(DeleteResponse deleteResponse) {\n  System.out.println(deleteResponse);\n  }\n\n@Override\npublic void onFailure(Exception e) {\n  System.out.println("执行失败");\n  }\n  });\n\n\n\n# 更新文档\n\nRestHighLevelClient Api 使用 UpdateRequest 来构建更新文档的请求参数。\n\n【示例】更新 id 为 1 的文档\n\nUpdateRequest updateRequest = new UpdateRequest(INDEX_ALIAS, "1");\n  Product product3 = new Product();\n  product3.setName("扫地机器人");\n  product3.setDescription("人工智能扫地机器人");\n  product3.setEnabled(true);\n  String jsonString2 = JSONUtil.toJsonStr(product3);\n  updateRequest.doc(jsonString2, XContentType.JSON);\n\n\n同步执行\n\nUpdateResponse updateResponse = client.update(updateRequest, RequestOptions.DEFAULT);\n  System.out.println(updateResponse);\n\n\n异步执行\n\nclient.updateAsync(updateRequest, RequestOptions.DEFAULT, new ActionListener<UpdateResponse>() {\n@Override\npublic void onResponse(UpdateResponse updateResponse) {\n  System.out.println(updateResponse);\n  }\n\n@Override\npublic void onFailure(Exception e) {\n  System.out.println("执行失败");\n  }\n  });\n\n\n\n# 查看文档\n\nRestHighLevelClient Api 使用 GetRequest 来构建查看文档的请求参数。\n\n【示例】查看 id 为 1 的文档\n\nGetRequest getRequest = new GetRequest(INDEX_ALIAS, "1");\n\n\n同步执行\n\nGetResponse getResponse = client.get(getRequest, RequestOptions.DEFAULT);\n\n\n异步执行\n\nclient.getAsync(getRequest, RequestOptions.DEFAULT, new ActionListener<GetResponse>() {\n@Override\npublic void onResponse(GetResponse getResponse) {\n  System.out.println(getResponse);\n  }\n\n@Override\npublic void onFailure(Exception e) {\n  System.out.println("执行失败");\n  }\n});\n\n\n\n# 获取匹配条件的记录总数\n\n@Test\n@DisplayName("获取匹配条件的记录总数")\npublic void count() throws IOException {\n    SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n    sourceBuilder.query(QueryBuilders.matchPhraseQuery("customer_gender", "MALE"));\n    sourceBuilder.trackTotalHits(true);\n\n    CountRequest countRequest = new CountRequest(INDEX);\n    countRequest.source(sourceBuilder);\n\n    CountResponse countResponse = client.count(countRequest, RequestOptions.DEFAULT);\n    long count = countResponse.getCount();\n    System.out.println("命中记录数：" + count);\n}\n\n\n\n# 分页查询\n\n@ParameterizedTest\n@ValueSource(ints = {0, 1, 2, 3})\n@DisplayName("分页查询测试")\npublic void pageTest(int page) throws IOException {\n\n    int size = 10;\n    int offset = page * size;\n    SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n    sourceBuilder.query(QueryBuilders.matchPhraseQuery("customer_gender", "MALE"));\n    sourceBuilder.from(offset);\n    sourceBuilder.size(size);\n    sourceBuilder.trackTotalHits(true);\n\n    SearchRequest searchRequest = new SearchRequest(INDEX);\n    searchRequest.source(sourceBuilder);\n    SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHit[] hits = response.getHits().getHits();\n    for (SearchHit hit : hits) {\n        KibanaSampleDataEcommerceBean bean =\n            BeanUtil.mapToBean(hit.getSourceAsMap(), KibanaSampleDataEcommerceBean.class, true,\n                               CopyOptions.create());\n        System.out.println(bean);\n    }\n}\n\n\n\n# 条件查询\n\n@Test\n@DisplayName("条件查询")\npublic void matchPhraseQuery() throws IOException {\n    SearchRequest searchRequest = new SearchRequest(INDEX);\n    SearchSourceBuilder sourceBuilder = new SearchSourceBuilder();\n\n    BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery();\n    boolQueryBuilder.must(QueryBuilders.matchPhraseQuery("customer_last_name", "Jensen"));\n    sourceBuilder.query(boolQueryBuilder);\n    sourceBuilder.trackTotalHits(true);\n    searchRequest.source(sourceBuilder);\n    SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n    SearchHit[] hits = response.getHits().getHits();\n    for (SearchHit hit : hits) {\n        KibanaSampleDataEcommerceBean bean =\n            BeanUtil.mapToBean(hit.getSourceAsMap(), KibanaSampleDataEcommerceBean.class, true,\n                               CopyOptions.create());\n        System.out.println(bean);\n    }\n}\n\n\n\n# 参考资料\n\n * 官方\n   * Java High Level REST Client',normalizedContent:'# elasticsearch java api 之 high level rest client\n\n> elasticsearch 官方的 high level rest client 在 7.1.5.0 版本废弃。所以本文中的 api 不推荐使用。\n\n\n# 快速开始\n\n\n# 引入依赖\n\n在 pom.xml 中引入以下依赖：\n\n<dependency>\n    <groupid>org.elasticsearch.client</groupid>\n    <artifactid>elasticsearch-rest-high-level-client</artifactid>\n    <version>7.17.1</version>\n</dependency>\n\n\n\n# 创建连接和关闭\n\n// 创建连接\nresthighlevelclient client = new resthighlevelclient(\n        restclient.builder(\n                new httphost("localhost", 9200, "http"),\n                new httphost("localhost", 9201, "http")));\n\n// 关闭\nclient.close();\n\n\n\n# 索引 api\n\n\n# 测试准备\n\npublic static final string index = "mytest";\npublic static final string index_alias = "mytest_alias";\n/**\n * {@link user} 的 mapping 结构（json形式）\n */\npublic static final string mapping_json =\n  "{\\n" + "  \\"properties\\": {\\n" + "    \\"_class\\": {\\n" + "      \\"type\\": \\"keyword\\",\\n"\n  + "      \\"index\\": false,\\n" + "      \\"doc_values\\": false\\n" + "    },\\n" + "    \\"description\\": {\\n"\n  + "      \\"type\\": \\"text\\",\\n" + "      \\"fielddata\\": true\\n" + "    },\\n" + "    \\"enabled\\": {\\n"\n  + "      \\"type\\": \\"boolean\\"\\n" + "    },\\n" + "    \\"name\\": {\\n" + "      \\"type\\": \\"text\\",\\n"\n  + "      \\"fielddata\\": true\\n" + "    }\\n" + "  }\\n" + "}";\n\n@autowired\nprivate resthighlevelclient client;\n\n\n\n# 创建索引\n\n// 创建索引\ncreateindexrequest createindexrequest = new createindexrequest(index);\n\n  // 设置索引的 settings\n  createindexrequest.settings(\n  settings.builder().put("index.number_of_shards", 3).put("index.number_of_replicas", 2));\n\n  // 设置索引的 mapping\n  createindexrequest.mapping(mapping_json, xcontenttype.json);\n\n  // 设置索引的别名\n  createindexrequest.alias(new alias(index_alias));\n\n  acknowledgedresponse createindexresponse = client.indices().create(createindexrequest, requestoptions.default);\n  assertions.asserttrue(createindexresponse.isacknowledged());\n\n\n\n# 删除索引\n\n// 删除索引\ndeleteindexrequest deleteindexrequest = new deleteindexrequest(index);\n  acknowledgedresponse deleteresponse = client.indices().delete(deleteindexrequest, requestoptions.default);\n  assertions.asserttrue(deleteresponse.isacknowledged());\n\n\n\n# 判断索引是否存在\n\ngetindexrequest getindexrequest = new getindexrequest(index);\n  assertions.asserttrue(client.indices().exists(getindexrequest, requestoptions.default));\n  getindexrequest getindexaliasrequest = new getindexrequest(index_alias);\n  assertions.asserttrue(client.indices().exists(getindexaliasrequest, requestoptions.default));\n\n\n\n# 文档 api\n\n\n# 文档测试准备\n\npublic static final string index = "mytest";\npublic static final string index_alias = "mytest_alias";\n/**\n * {@link user} 的 mapping 结构（json形式）\n */\npublic static final string mapping_json =\n  "{\\n" + "  \\"properties\\": {\\n" + "    \\"_class\\": {\\n" + "      \\"type\\": \\"keyword\\",\\n"\n  + "      \\"index\\": false,\\n" + "      \\"doc_values\\": false\\n" + "    },\\n" + "    \\"description\\": {\\n"\n  + "      \\"type\\": \\"text\\",\\n" + "      \\"fielddata\\": true\\n" + "    },\\n" + "    \\"enabled\\": {\\n"\n  + "      \\"type\\": \\"boolean\\"\\n" + "    },\\n" + "    \\"name\\": {\\n" + "      \\"type\\": \\"text\\",\\n"\n  + "      \\"fielddata\\": true\\n" + "    }\\n" + "  }\\n" + "}";\n\n@autowired\nprivate resthighlevelclient client;\n\n@beforeeach\npublic void init() throws ioexception {\n\n  // 创建索引\n  createindexrequest createindexrequest = new createindexrequest(index);\n\n  // 设置索引的 settings\n  createindexrequest.settings(\n  settings.builder().put("index.number_of_shards", 3).put("index.number_of_replicas", 2));\n\n  // 设置索引的 mapping\n  createindexrequest.mapping(mapping_json, xcontenttype.json);\n\n  // 设置索引的别名\n  createindexrequest.alias(new alias(index_alias));\n\n  acknowledgedresponse response = client.indices().create(createindexrequest, requestoptions.default);\n  assertions.asserttrue(response.isacknowledged());\n\n  // 判断索引是否存在\n  getindexrequest getindexrequest = new getindexrequest(index_alias);\n  assertions.asserttrue(client.indices().exists(getindexrequest, requestoptions.default));\n  getindexrequest getindexaliasrequest = new getindexrequest(index_alias);\n  assertions.asserttrue(client.indices().exists(getindexaliasrequest, requestoptions.default));\n  }\n\n@aftereach\npublic void destroy() throws ioexception {\n  // 删除索引\n  deleteindexrequest request = new deleteindexrequest(index);\n  acknowledgedresponse response = client.indices().delete(request, requestoptions.default);\n  assertions.asserttrue(response.isacknowledged());\n  }\n\n\n\n# 创建文档\n\nresthighlevelclient api 使用 indexrequest 来构建创建文档的请求参数。\n\n【示例】创建 id 为 1 的文档\n\nindexrequest request = new indexrequest("product");\n  request.id("1");\n  product product = new product();\n  product.setname("机器人");\n  product.setdescription("人工智能机器人");\n  product.setenabled(true);\n  string jsonstring = jsonutil.tojsonstr(product);\n  request.source(jsonstring, xcontenttype.json);\n\n\n同步执行\n\nindexresponse indexresponse = client.index(request, requestoptions.default);\n\n\n异步执行\n\n// 异步执行\nclient.indexasync(request, requestoptions.default, new actionlistener<indexresponse>() {\n@override\npublic void onresponse(indexresponse indexresponse) {\n  system.out.println(indexresponse);\n  }\n\n@override\npublic void onfailure(exception e) {\n  system.out.println("执行失败");\n  }\n  });\n\n\n\n# 删除文档\n\nresthighlevelclient api 使用 deleterequest 来构建删除文档的请求参数。\n\n【示例】删除 id 为 1 的文档\n\ndeleterequest deleterequest = new deleterequest(index_alias, "1");\n\n\n同步执行\n\ndeleteresponse deleteresponse = client.delete(deleterequest, requestoptions.default);\n  system.out.println(deleteresponse);\n\n\n异步执行\n\nclient.deleteasync(deleterequest, requestoptions.default, new actionlistener<deleteresponse>() {\n@override\npublic void onresponse(deleteresponse deleteresponse) {\n  system.out.println(deleteresponse);\n  }\n\n@override\npublic void onfailure(exception e) {\n  system.out.println("执行失败");\n  }\n  });\n\n\n\n# 更新文档\n\nresthighlevelclient api 使用 updaterequest 来构建更新文档的请求参数。\n\n【示例】更新 id 为 1 的文档\n\nupdaterequest updaterequest = new updaterequest(index_alias, "1");\n  product product3 = new product();\n  product3.setname("扫地机器人");\n  product3.setdescription("人工智能扫地机器人");\n  product3.setenabled(true);\n  string jsonstring2 = jsonutil.tojsonstr(product3);\n  updaterequest.doc(jsonstring2, xcontenttype.json);\n\n\n同步执行\n\nupdateresponse updateresponse = client.update(updaterequest, requestoptions.default);\n  system.out.println(updateresponse);\n\n\n异步执行\n\nclient.updateasync(updaterequest, requestoptions.default, new actionlistener<updateresponse>() {\n@override\npublic void onresponse(updateresponse updateresponse) {\n  system.out.println(updateresponse);\n  }\n\n@override\npublic void onfailure(exception e) {\n  system.out.println("执行失败");\n  }\n  });\n\n\n\n# 查看文档\n\nresthighlevelclient api 使用 getrequest 来构建查看文档的请求参数。\n\n【示例】查看 id 为 1 的文档\n\ngetrequest getrequest = new getrequest(index_alias, "1");\n\n\n同步执行\n\ngetresponse getresponse = client.get(getrequest, requestoptions.default);\n\n\n异步执行\n\nclient.getasync(getrequest, requestoptions.default, new actionlistener<getresponse>() {\n@override\npublic void onresponse(getresponse getresponse) {\n  system.out.println(getresponse);\n  }\n\n@override\npublic void onfailure(exception e) {\n  system.out.println("执行失败");\n  }\n});\n\n\n\n# 获取匹配条件的记录总数\n\n@test\n@displayname("获取匹配条件的记录总数")\npublic void count() throws ioexception {\n    searchsourcebuilder sourcebuilder = new searchsourcebuilder();\n    sourcebuilder.query(querybuilders.matchphrasequery("customer_gender", "male"));\n    sourcebuilder.tracktotalhits(true);\n\n    countrequest countrequest = new countrequest(index);\n    countrequest.source(sourcebuilder);\n\n    countresponse countresponse = client.count(countrequest, requestoptions.default);\n    long count = countresponse.getcount();\n    system.out.println("命中记录数：" + count);\n}\n\n\n\n# 分页查询\n\n@parameterizedtest\n@valuesource(ints = {0, 1, 2, 3})\n@displayname("分页查询测试")\npublic void pagetest(int page) throws ioexception {\n\n    int size = 10;\n    int offset = page * size;\n    searchsourcebuilder sourcebuilder = new searchsourcebuilder();\n    sourcebuilder.query(querybuilders.matchphrasequery("customer_gender", "male"));\n    sourcebuilder.from(offset);\n    sourcebuilder.size(size);\n    sourcebuilder.tracktotalhits(true);\n\n    searchrequest searchrequest = new searchrequest(index);\n    searchrequest.source(sourcebuilder);\n    searchresponse response = client.search(searchrequest, requestoptions.default);\n    searchhit[] hits = response.gethits().gethits();\n    for (searchhit hit : hits) {\n        kibanasampledataecommercebean bean =\n            beanutil.maptobean(hit.getsourceasmap(), kibanasampledataecommercebean.class, true,\n                               copyoptions.create());\n        system.out.println(bean);\n    }\n}\n\n\n\n# 条件查询\n\n@test\n@displayname("条件查询")\npublic void matchphrasequery() throws ioexception {\n    searchrequest searchrequest = new searchrequest(index);\n    searchsourcebuilder sourcebuilder = new searchsourcebuilder();\n\n    boolquerybuilder boolquerybuilder = querybuilders.boolquery();\n    boolquerybuilder.must(querybuilders.matchphrasequery("customer_last_name", "jensen"));\n    sourcebuilder.query(boolquerybuilder);\n    sourcebuilder.tracktotalhits(true);\n    searchrequest.source(sourcebuilder);\n    searchresponse response = client.search(searchrequest, requestoptions.default);\n    searchhit[] hits = response.gethits().gethits();\n    for (searchhit hit : hits) {\n        kibanasampledataecommercebean bean =\n            beanutil.maptobean(hit.getsourceasmap(), kibanasampledataecommercebean.class, true,\n                               copyoptions.create());\n        system.out.println(bean);\n    }\n}\n\n\n\n# 参考资料\n\n * 官方\n   * java high level rest client',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 集群和分片",frontmatter:{title:"Elasticsearch 集群和分片",date:"2022-03-01T20:52:25.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","集群","分片"],permalink:"/pages/9a2546/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/13.Elasticsearch%E9%9B%86%E7%BE%A4%E5%92%8C%E5%88%86%E7%89%87.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/13.Elasticsearch集群和分片.md",key:"v-2a07dc5d",path:"/pages/9a2546/",headers:[{level:2,title:"集群",slug:"集群",normalizedTitle:"集群",charIndex:16},{level:3,title:"空集群",slug:"空集群",normalizedTitle:"空集群",charIndex:33},{level:3,title:"集群健康",slug:"集群健康",normalizedTitle:"集群健康",charIndex:575},{level:3,title:"添加索引",slug:"添加索引",normalizedTitle:"添加索引",charIndex:1132},{level:3,title:"添加故障转移",slug:"添加故障转移",normalizedTitle:"添加故障转移",charIndex:2728},{level:3,title:"水平扩容",slug:"水平扩容",normalizedTitle:"水平扩容",charIndex:3827},{level:3,title:"更多的扩容",slug:"更多的扩容",normalizedTitle:"更多的扩容",charIndex:4183},{level:3,title:"应对故障",slug:"应对故障",normalizedTitle:"应对故障",charIndex:4742},{level:2,title:"分片",slug:"分片",normalizedTitle:"分片",charIndex:19},{level:3,title:"使文本可被搜索",slug:"使文本可被搜索",normalizedTitle:"使文本可被搜索",charIndex:5741},{level:3,title:"不变性",slug:"不变性",normalizedTitle:"不变性",charIndex:6595},{level:3,title:"动态更新索引",slug:"动态更新索引",normalizedTitle:"动态更新索引",charIndex:6997},{level:3,title:"删除和更新",slug:"删除和更新",normalizedTitle:"删除和更新",charIndex:8054},{level:3,title:"近实时搜索",slug:"近实时搜索",normalizedTitle:"近实时搜索",charIndex:8373},{level:3,title:"refresh API",slug:"refresh-api",normalizedTitle:"refresh api",charIndex:9028},{level:3,title:"持久化变更",slug:"持久化变更",normalizedTitle:"持久化变更",charIndex:9952},{level:3,title:"flush API",slug:"flush-api",normalizedTitle:"flush api",charIndex:11275},{level:3,title:"段合并",slug:"段合并",normalizedTitle:"段合并",charIndex:8337},{level:3,title:"optimize API",slug:"optimize-api",normalizedTitle:"optimize api",charIndex:5703},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:13800}],headersStr:"集群 空集群 集群健康 添加索引 添加故障转移 水平扩容 更多的扩容 应对故障 分片 使文本可被搜索 不变性 动态更新索引 删除和更新 近实时搜索 refresh API 持久化变更 flush API 段合并 optimize API 参考资料",content:'# Elasticsearch 集群和分片\n\n\n# 集群\n\n\n# 空集群\n\n如果我们启动了一个单独的节点，里面不包含任何的数据和索引，那我们的集群看起来就是一个包含空内容节点的集群。\n\nFigure 1. 包含空内容节点的集群\n\n\n\n图 1：只有一个空节点的集群\n\n一个运行中的 Elasticsearch 实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n\n当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。\n\n作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。\n\n\n# 集群健康\n\nElasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red 。\n\nGET /_cluster/health\n\n\n在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容：\n\n{\n  "cluster_name": "elasticsearch",\n  "status": "green",\n  "timed_out": false,\n  "number_of_nodes": 1,\n  "number_of_data_nodes": 1,\n  "active_primary_shards": 0,\n  "active_shards": 0,\n  "relocating_shards": 0,\n  "initializing_shards": 0,\n  "unassigned_shards": 0\n}\n\n\nstatus 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：\n\n * green：所有的主分片和副本分片都正常运行。\n * yellow：所有的主分片都正常运行，但不是所有的副本分片都正常运行。\n * red：有主分片没能正常运行。\n\n\n# 添加索引\n\n我们往 Elasticsearch 添加数据时需要用到 索引 —— 保存相关数据的地方。索引实际上是指向一个或者多个物理分片的逻辑命名空间 。\n\n一个 分片 是一个底层的 工作单元 ，它仅保存了全部数据中的一部分。现在我们只需知道一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。\n\nElasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。\n\n一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。\n\n> 技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。\n\n一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n\n在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。\n\n让我们在包含一个空节点的集群内创建名为 blogs 的索引。 索引在默认情况下会被分配 5 个主分片， 但是为了演示目的，我们将分配 3 个主分片和一份副本（每个主分片拥有一个副本分片）：\n\nPUT /blogs\n{\n   "settings" : {\n      "number_of_shards" : 3,\n      "number_of_replicas" : 1\n   }\n}\n\n\n我们的集群现在是 拥有一个索引的单节点集群。所有 3 个主分片都被分配在 Node 1 。\n\nFigure 2. 拥有一个索引的单节点集群\n\n\n\n如果我们现在查看集群健康，我们将看到如下内容：\n\n{\n  "cluster_name": "elasticsearch",\n  "status": "yellow",\n  "timed_out": false,\n  "number_of_nodes": 1,\n  "number_of_data_nodes": 1,\n  "active_primary_shards": 3,\n  "active_shards": 3,\n  "relocating_shards": 0,\n  "initializing_shards": 0,\n  "unassigned_shards": 3,\n  "delayed_unassigned_shards": 0,\n  "number_of_pending_tasks": 0,\n  "number_of_in_flight_fetch": 0,\n  "task_max_waiting_in_queue_millis": 0,\n  "active_shards_percent_as_number": 50\n}\n\n\n * 集群 status 值为 yellow\n * 没有被分配到任何节点的副本数\n\n集群的健康状况为 yellow 则表示全部 主 分片都正常运行（集群可以正常服务所有请求），但是 副本 分片没有全部处在正常状态。 实际上，所有 3 个副本分片都是 unassigned —— 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。\n\n当前我们的集群是正常运行的，但是在硬件故障时有丢失数据的风险。\n\n\n# 添加故障转移\n\n当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。\n\n> 为了测试第二个节点启动后的情况，你可以在同一个目录内，完全依照启动第一个节点的方式来启动一个新节点（参考安装并运行 Elasticsearch）。多个节点可以共享同一个目录。\n> \n> 当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的 cluster.name 配置，它就会自动发现集群并加入到其中。 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。\n\n如果启动了第二个节点，我们的集群将会拥有两个节点的集群——所有主分片和副本分片都已被分配。\n\nFigure 3. 拥有两个节点的集群——所有主分片和副本分片都已被分配\n\n\n\n当第二个节点加入到集群后，3 个 副本分片 将会分配到这个节点上——每个主分片对应一个副本分片。 这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。\n\n所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。\n\ncluster-health 现在展示的状态为 green ，这表示所有 6 个分片（包括 3 个主分片和 3 个副本分片）都在正常运行。\n\n{\n  "cluster_name": "elasticsearch",\n  "status": "green",\n  "timed_out": false,\n  "number_of_nodes": 2,\n  "number_of_data_nodes": 2,\n  "active_primary_shards": 3,\n  "active_shards": 6,\n  "relocating_shards": 0,\n  "initializing_shards": 0,\n  "unassigned_shards": 0,\n  "delayed_unassigned_shards": 0,\n  "number_of_pending_tasks": 0,\n  "number_of_in_flight_fetch": 0,\n  "task_max_waiting_in_queue_millis": 0,\n  "active_shards_percent_as_number": 100\n}\n\n\n * 集群 status 值为 green\n\n我们的集群现在不仅仅是正常运行的，并且还处于 始终可用 的状态。\n\n\n# 水平扩容\n\n怎样为我们的正在增长中的应用程序按需扩容呢？ 当启动了第三个节点，我们的集群将拥有三个节点的集群——为了分散负载而对分片进行重新分配。\n\nFigure 4. 拥有三个节点的集群——为了分散负载而对分片进行重新分配\n\n\n\nNode 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点，现在每个节点上都拥有 2 个分片，而不是之前的 3 个。 这表示每个节点的硬件资源（CPU, RAM, I/O）将被更少的分片所共享，每个分片的性能将会得到提升。\n\n分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有 6 个分片（3 个主分片和 3 个副本分片）的索引可以最大扩容到 6 个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。\n\n\n# 更多的扩容\n\n但是如果我们想要扩容超过 6 个节点怎么办呢？\n\n主分片的数目在索引创建时就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。\n\n在运行中的集群上是可以动态调整副本分片数目的，我们可以按需伸缩集群。让我们把副本数从默认的 1 增加到 2 ：\n\nPUT /blogs/_settings\n{\n   "number_of_replicas" : 2\n}\n\n\nblogs 索引现在拥有 9 个分片：3 个主分片和 6 个副本分片。 这意味着我们可以将集群扩容到 9 个节点，每个节点上一个分片。相比原来 3 个节点时，集群搜索性能可以提升 3 倍。\n\nFigure 5. 将参数 number_of_replicas 调大到 2\n\n\n\n> 当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。\n> \n> 但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去 2 个节点的情况下不丢失任何数据。\n\n\n# 应对故障\n\n我们之前说过 Elasticsearch 可以应对节点故障，接下来让我们尝试下这个功能。 如果我们关闭第一个节点，这时集群的状态为关闭了一个节点后的集群。\n\nFigure 6. 关闭了一个节点后的集群\n\n\n\n我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点： Node 2 。\n\n在我们关闭 Node 1 的同时也失去了主分片 1 和 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ：不是所有主分片都在正常工作。\n\n幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在 Node 2 和 Node 3 上对应的副本分片提升为主分片， 此时集群的状态将会为 yellow 。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。\n\n为什么我们集群状态是 yellow 而不是 green 呢？ 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应 2 份副本分片，而此时只存在一份副本分片。 所以集群不能为 green 的状态，不过我们不必过于担心：如果我们同样关闭了 Node 2 ，我们的程序 依然 可以保持在不丢任何数据的情况下运行，因为 Node 3 为每一个分片都保留着一份副本。\n\n如果我们重新启动 Node 1 ，集群可以将缺失的副本分片再次进行分配，那么集群的状态也将如 Figure 5. 将参数 number_of_replicas 调大到 2 所示。 如果 Node 1 依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。\n\n到目前为止，你应该对分片如何使得 Elasticsearch 进行水平扩容以及数据保障等知识有了一定了解。 接下来我们将讲述关于分片生命周期的更多细节。\n\n\n# 分片\n\n>  * 为什么搜索是 近 实时的？\n>  * 为什么文档的 CRUD (创建-读取-更新-删除) 操作是 实时 的?\n>  * Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据?\n>  * 为什么删除文档不会立刻释放空间？\n>  * refresh, flush, 和 optimize API 都做了什么, 你什么情况下应该使用他们？\n\n\n# 使文本可被搜索\n\n必须解决的第一个挑战是如何使文本可被搜索。 传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值(这里指单词)的能力。\n\n最好的支持 一个字段多个值 需求的数据结构是我们在 倒排索引 章节中介绍过的 倒排索引 。 倒排索引包含一个有序列表，列表包含所有文档出现过的不重复个体，或称为 词项 ，对于每一个词项，包含了它所有曾出现过文档的列表。\n\nTerm  | Doc 1 | Doc 2 | Doc 3 | ...\n------------------------------------\nbrown |   X   |       |  X    | ...\nfox   |   X   |   X   |  X    | ...\nquick |   X   |   X   |       | ...\nthe   |   X   |       |  X    | ...\n\n\n> 当讨论倒排索引时，我们会谈到 文档 标引，因为历史原因，倒排索引被用来对整个非结构化文本文档进行标引。 Elasticsearch 中的 文档 是有字段和值的结构化 JSON 文档。事实上，在 JSON 文档中， 每个被索引的字段都有自己的倒排索引。\n\n这个倒排索引相比特定词项出现过的文档列表，会包含更多其它信息。它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息允许 Elasticsearch 决定哪些词比其它词更重要，哪些文档比其它文档更重要，这些内容在 什么是相关性? 中有描述。\n\n为了能够实现预期功能，倒排索引需要知道集合中的 所有 文档，这是需要认识到的关键问题。\n\n早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。\n\n\n# 不变性\n\n倒排索引被写入磁盘后是 不可改变 的:它永远不会修改。 不变性有重要的价值：\n\n * 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。\n * 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。\n * 其它缓存(像 filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。\n * 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和 需要被缓存到内存的索引的使用量。\n\n当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。\n\n\n# 动态更新索引\n\n下一个需要被解决的问题是怎样在保留不变性的前提下实现倒排索引的更新？答案是: 用更多的索引。\n\n通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到—从最早的开始—查询完后再对结果进行合并。\n\nElasticsearch 基于 Lucene, 这个 java 库引入了 按段搜索 的概念。 每一 段 本身都是一个倒排索引， 但 索引 在 Lucene 中除表示所有 段 的集合外， 还增加了 提交点 的概念 — 一个列出了所有已知段的文件，就像在 Figure 16, “一个 Lucene 索引包含一个提交点和三个段” 中描绘的那样。 如 Figure 17, “一个在内存缓存中包含新文档的 Lucene 索引” 所示，新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段，如 Figure 18, “在一次提交后，一个新的段被添加到提交点而且缓存被清空。” 所示。\n\nFigure 16. 一个 Lucene 索引包含一个提交点和三个段\n\n\n\n> 被混淆的概念是，一个 Lucene 索引 我们在 Elasticsearch 称作 分片 。 一个 Elasticsearch 索引 是分片的集合。 当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后像 执行分布式检索 提到的那样，合并每个分片的结果到一个全局的结果集。\n\n逐段搜索会以如下流程进行工作：\n\n 1. 新文档被收集到内存索引缓存， 见 Figure 17, “一个在内存缓存中包含新文档的 Lucene 索引” 。\n 2. 不时地, 缓存被 提交 ：\n    * 一个新的段—一个追加的倒排索引—被写入磁盘。\n    * 一个新的包含新段名字的 提交点 被写入磁盘。\n    * 磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。\n 3. 新的段被开启，让它包含的文档可见以被搜索。\n 4. 内存缓存被清空，等待接收新的文档。\n\nFigure 17. 一个在内存缓存中包含新文档的 Lucene 索引\n\n\n\nFigure 18. 在一次提交后，一个新的段被添加到提交点而且缓存被清空。\n\n\n\n当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。\n\n\n# 删除和更新\n\n段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。\n\n当一个文档被 “删除” 时，它实际上只是在 .del 文件中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n\n文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。\n\n在 段合并 , 我们展示了一个被删除的文档是怎样被文件系统移除的。\n\n\n# 近实时搜索\n\n随着按段（per-segment）搜索的发展，一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。\n\n磁盘在这里成为了瓶颈。提交（Commiting）一个新的段到磁盘需要一个 fsync 来确保段被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。\n\n我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着 fsync 要从整个过程中被移除。\n\n在 Elasticsearch 和磁盘之间是文件系统缓存。 像之前描述的一样， 在内存索引缓冲区（ Figure 19, “在内存缓冲区中包含了新文档的 Lucene 索引” ）中的文档会被写入到一个新的段中（ Figure 20, “缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交” ）。 但是这里新段会被先写入到文件系统缓存—这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\nFigure 19. 在内存缓冲区中包含了新文档的 Lucene 索引\n\n\n\nLucene 允许新段被写入和打开—使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\nFigure 20. 缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n\n\n\n# refresh API\n\n在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用 refresh API 执行一次手动刷新:\n\nPOST /_refresh\nPOST /blogs/_refresh\n\n\n刷新（Refresh）所有的索引\n\n只刷新（Refresh） blogs 索引\n\n> 尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。\n\n并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件， 你可能想优化索引速度而不是近实时搜索， 可以通过设置 refresh_interval ， 降低每个索引的刷新频率：\n\nPUT /my_logs\n{\n  "settings": {\n    "refresh_interval": "30s"\n  }\n}\n\n\n> 每 30 秒刷新 my_logs 索引。\n\nrefresh_interval 可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来：\n\nPUT /my_logs/_settings\n{ "refresh_interval": -1 }\n\nPUT /my_logs/_settings\n{ "refresh_interval": "1s" }\n\n\n * 关闭自动刷新。\n\n * 每秒自动刷新。\n\n> refresh_interval 需要一个 持续时间 值， 例如 1s （1 秒） 或 2m （2 分钟）。 一个绝对值 1 表示的是 1 毫秒 --无疑会使你的集群陷入瘫痪。\n\n\n# 持久化变更\n\n如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 Elasticsearch 的可靠性，需要确保数据变化被持久化到磁盘。\n\n在 动态更新索引，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。Elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。\n\n即使通过每秒刷新（refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办？我们也不希望丢失掉这些数据。\n\nElasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录。通过 translog ，整个流程看起来是下面这样：\n\n一个文档被索引之后，就会被添加到内存缓冲区，并且 追加到了 translog ，正如 Figure 21, “新的文档被添加到内存缓冲区并且被追加到了事务日志” 描述的一样。\n\nFigure 21. 新的文档被添加到内存缓冲区并且被追加到了事务日志\n\n\n\n刷新（refresh）使分片处于 Figure 22, “刷新（refresh）完成后, 缓存被清空但是事务日志不会” 描述的状态，分片每秒被刷新（refresh）一次：\n\n * 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作。\n * 这个段被打开，使其可被搜索。\n * 内存缓冲区被清空。\n\nFigure 22. 刷新（refresh）完成后, 缓存被清空但是事务日志不会\n\n\n\n这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志（见 Figure 23, “事务日志不断积累文档” ）。\n\nFigure 23. 事务日志不断积累文档\n\n\n\n 1. 每隔一段时间—例如 translog 变得越来越大—索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行（见 Figure 24, “在刷新（flush）之后，段被全量提交，并且事务日志被清空” ）：\n    * 所有在内存缓冲区的文档都被写入一个新的段。\n    * 缓冲区被清空。\n    * 一个提交点被写入硬盘。\n    * 文件系统缓存通过 fsync 被刷新（flush）。\n    * 老的 translog 被删除。\n\ntranslog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。\n\ntranslog 也被用来提供实时 CRUD 。当你试着通过 ID 查询、更新、删除一个文档，它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。\n\nFigure 24. 在刷新（flush）之后，段被全量提交，并且事务日志被清空\n\n\n\n\n# flush API\n\n这个执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次 flush 。 分片每 30 分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新。请查看 translog 文档 来设置，它可以用来 控制这些阈值：\n\nflush API 可以被用来执行一个手工的刷新（flush）:\n\nPOST /blogs/_flush\nPOST /_flush?wait_for_ongoing\n\n\n * 刷新（flush） blogs 索引。\n * 刷新（flush）所有的索引并且并且等待所有刷新在返回前完成。\n\n你很少需要自己手动执行 flush 操作；通常情况下，自动刷新就足够了。\n\n这就是说，在重启节点或关闭索引之前执行 flush 有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快。\n\n> translog 的目的是保证操作不会丢失。这引出了这个问题： Translog 有多安全？\n> \n> 在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。最终， 基本上，这意味着在整个请求被 fsync 到主分片和复制分片的 translog 之前，你的客户端不会得到一个 200 OK 响应。\n> \n> 在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是 bulk 导入，它在一次请求中平摊了大量文档的开销）。\n> \n> 但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每 5 秒执行一次 fsync 。\n> \n> 这个行为可以通过设置 durability 参数为 async 来启用：\n> \n> PUT /my_index/_settings\n> {\n>     "index.translog.durability": "async",\n>     "index.translog.sync_interval": "5s"\n> }\n> \n> \n> 这个选项可以针对索引单独设置，并且可以动态进行修改。如果你决定使用异步 translog 的话，你需要 保证 在发生 crash 时，丢失掉 sync_interval 时间段的数据也无所谓。请在决定前知晓这个特性。\n> \n> 如果你不确定这个行为的后果，最好是使用默认的参数（ "index.translog.durability": "request" ）来避免数据丢失。\n\n\n# 段合并\n\n由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和 cpu 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。\n\nElasticsearch 通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。\n\n段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。\n\n启动段合并不需要你做任何事。进行索引和搜索时会自动进行。这个流程像在 Figure 25, “两个提交了的段和一个未提交的段正在被合并到一个更大的段” 中提到的一样工作：\n\n1、 当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。\n\n2、 合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。\n\nFigure 25. 两个提交了的段和一个未提交的段正在被合并到一个更大的段\n\n\n\nFigure 26, “一旦合并结束，老的段被删除” 说明合并完成时的活动：\n\n * 新的段被刷新（flush）到了磁盘。 ** 写入一个包含新段且排除旧的和较小的段的新提交点。\n * 新的段被打开用来搜索。\n * 老的段被删除。\n\nFigure 26. 一旦合并结束，老的段被删除\n\n\n\n合并大的段需要消耗大量的 I/O 和 CPU 资源，如果任其发展会影响搜索性能。Elasticsearch 在默认情况下会对合并流程进行资源限制，所以搜索仍然 有足够的资源很好地执行。\n\n\n# optimize API\n\noptimize API 大可看做是 强制合并 API。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。\n\n> optimize API 不应该 被用在一个活跃的索引————一个正积极更新的索引。后台合并流程已经可以很好地完成工作。 optimizing 会阻碍这个进程。不要干扰它！\n\n在特定情况下，使用 optimize API 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化。\n\n在这种情况下，使用 optimize 优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速：\n\nPOST /logstash-2014-10/_optimize?max_num_segments=1\n\n\n合并索引中的每个分片为一个单独的段\n\n> 请注意，使用 optimize API 触发段合并的操作不会受到任何资源上的限制。这可能会消耗掉你节点上全部的 I/O 资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 optimize，你需要先使用分片分配（查看 迁移旧索引）把索引移到一个安全的节点，再执行。\n\n\n# 参考资料\n\n * Elasticsearch 官方文档之 集群内的原理',normalizedContent:'# elasticsearch 集群和分片\n\n\n# 集群\n\n\n# 空集群\n\n如果我们启动了一个单独的节点，里面不包含任何的数据和索引，那我们的集群看起来就是一个包含空内容节点的集群。\n\nfigure 1. 包含空内容节点的集群\n\n\n\n图 1：只有一个空节点的集群\n\n一个运行中的 elasticsearch 实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。\n\n当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。\n\n作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 elasticsearch 对这一切的管理都是透明的。\n\n\n# 集群健康\n\nelasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red 。\n\nget /_cluster/health\n\n\n在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容：\n\n{\n  "cluster_name": "elasticsearch",\n  "status": "green",\n  "timed_out": false,\n  "number_of_nodes": 1,\n  "number_of_data_nodes": 1,\n  "active_primary_shards": 0,\n  "active_shards": 0,\n  "relocating_shards": 0,\n  "initializing_shards": 0,\n  "unassigned_shards": 0\n}\n\n\nstatus 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：\n\n * green：所有的主分片和副本分片都正常运行。\n * yellow：所有的主分片都正常运行，但不是所有的副本分片都正常运行。\n * red：有主分片没能正常运行。\n\n\n# 添加索引\n\n我们往 elasticsearch 添加数据时需要用到 索引 —— 保存相关数据的地方。索引实际上是指向一个或者多个物理分片的逻辑命名空间 。\n\n一个 分片 是一个底层的 工作单元 ，它仅保存了全部数据中的一部分。现在我们只需知道一个分片是一个 lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。\n\nelasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。\n\n一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。\n\n> 技术上来说，一个主分片最大能够存储 integer.max_value - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。\n\n一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。\n\n在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。\n\n让我们在包含一个空节点的集群内创建名为 blogs 的索引。 索引在默认情况下会被分配 5 个主分片， 但是为了演示目的，我们将分配 3 个主分片和一份副本（每个主分片拥有一个副本分片）：\n\nput /blogs\n{\n   "settings" : {\n      "number_of_shards" : 3,\n      "number_of_replicas" : 1\n   }\n}\n\n\n我们的集群现在是 拥有一个索引的单节点集群。所有 3 个主分片都被分配在 node 1 。\n\nfigure 2. 拥有一个索引的单节点集群\n\n\n\n如果我们现在查看集群健康，我们将看到如下内容：\n\n{\n  "cluster_name": "elasticsearch",\n  "status": "yellow",\n  "timed_out": false,\n  "number_of_nodes": 1,\n  "number_of_data_nodes": 1,\n  "active_primary_shards": 3,\n  "active_shards": 3,\n  "relocating_shards": 0,\n  "initializing_shards": 0,\n  "unassigned_shards": 3,\n  "delayed_unassigned_shards": 0,\n  "number_of_pending_tasks": 0,\n  "number_of_in_flight_fetch": 0,\n  "task_max_waiting_in_queue_millis": 0,\n  "active_shards_percent_as_number": 50\n}\n\n\n * 集群 status 值为 yellow\n * 没有被分配到任何节点的副本数\n\n集群的健康状况为 yellow 则表示全部 主 分片都正常运行（集群可以正常服务所有请求），但是 副本 分片没有全部处在正常状态。 实际上，所有 3 个副本分片都是 unassigned —— 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。\n\n当前我们的集群是正常运行的，但是在硬件故障时有丢失数据的风险。\n\n\n# 添加故障转移\n\n当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。\n\n> 为了测试第二个节点启动后的情况，你可以在同一个目录内，完全依照启动第一个节点的方式来启动一个新节点（参考安装并运行 elasticsearch）。多个节点可以共享同一个目录。\n> \n> 当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的 cluster.name 配置，它就会自动发现集群并加入到其中。 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。\n\n如果启动了第二个节点，我们的集群将会拥有两个节点的集群——所有主分片和副本分片都已被分配。\n\nfigure 3. 拥有两个节点的集群——所有主分片和副本分片都已被分配\n\n\n\n当第二个节点加入到集群后，3 个 副本分片 将会分配到这个节点上——每个主分片对应一个副本分片。 这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。\n\n所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。\n\ncluster-health 现在展示的状态为 green ，这表示所有 6 个分片（包括 3 个主分片和 3 个副本分片）都在正常运行。\n\n{\n  "cluster_name": "elasticsearch",\n  "status": "green",\n  "timed_out": false,\n  "number_of_nodes": 2,\n  "number_of_data_nodes": 2,\n  "active_primary_shards": 3,\n  "active_shards": 6,\n  "relocating_shards": 0,\n  "initializing_shards": 0,\n  "unassigned_shards": 0,\n  "delayed_unassigned_shards": 0,\n  "number_of_pending_tasks": 0,\n  "number_of_in_flight_fetch": 0,\n  "task_max_waiting_in_queue_millis": 0,\n  "active_shards_percent_as_number": 100\n}\n\n\n * 集群 status 值为 green\n\n我们的集群现在不仅仅是正常运行的，并且还处于 始终可用 的状态。\n\n\n# 水平扩容\n\n怎样为我们的正在增长中的应用程序按需扩容呢？ 当启动了第三个节点，我们的集群将拥有三个节点的集群——为了分散负载而对分片进行重新分配。\n\nfigure 4. 拥有三个节点的集群——为了分散负载而对分片进行重新分配\n\n\n\nnode 1 和 node 2 上各有一个分片被迁移到了新的 node 3 节点，现在每个节点上都拥有 2 个分片，而不是之前的 3 个。 这表示每个节点的硬件资源（cpu, ram, i/o）将被更少的分片所共享，每个分片的性能将会得到提升。\n\n分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有 6 个分片（3 个主分片和 3 个副本分片）的索引可以最大扩容到 6 个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。\n\n\n# 更多的扩容\n\n但是如果我们想要扩容超过 6 个节点怎么办呢？\n\n主分片的数目在索引创建时就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。\n\n在运行中的集群上是可以动态调整副本分片数目的，我们可以按需伸缩集群。让我们把副本数从默认的 1 增加到 2 ：\n\nput /blogs/_settings\n{\n   "number_of_replicas" : 2\n}\n\n\nblogs 索引现在拥有 9 个分片：3 个主分片和 6 个副本分片。 这意味着我们可以将集群扩容到 9 个节点，每个节点上一个分片。相比原来 3 个节点时，集群搜索性能可以提升 3 倍。\n\nfigure 5. 将参数 number_of_replicas 调大到 2\n\n\n\n> 当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。\n> \n> 但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去 2 个节点的情况下不丢失任何数据。\n\n\n# 应对故障\n\n我们之前说过 elasticsearch 可以应对节点故障，接下来让我们尝试下这个功能。 如果我们关闭第一个节点，这时集群的状态为关闭了一个节点后的集群。\n\nfigure 6. 关闭了一个节点后的集群\n\n\n\n我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点： node 2 。\n\n在我们关闭 node 1 的同时也失去了主分片 1 和 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ：不是所有主分片都在正常工作。\n\n幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在 node 2 和 node 3 上对应的副本分片提升为主分片， 此时集群的状态将会为 yellow 。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。\n\n为什么我们集群状态是 yellow 而不是 green 呢？ 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应 2 份副本分片，而此时只存在一份副本分片。 所以集群不能为 green 的状态，不过我们不必过于担心：如果我们同样关闭了 node 2 ，我们的程序 依然 可以保持在不丢任何数据的情况下运行，因为 node 3 为每一个分片都保留着一份副本。\n\n如果我们重新启动 node 1 ，集群可以将缺失的副本分片再次进行分配，那么集群的状态也将如 figure 5. 将参数 number_of_replicas 调大到 2 所示。 如果 node 1 依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。\n\n到目前为止，你应该对分片如何使得 elasticsearch 进行水平扩容以及数据保障等知识有了一定了解。 接下来我们将讲述关于分片生命周期的更多细节。\n\n\n# 分片\n\n>  * 为什么搜索是 近 实时的？\n>  * 为什么文档的 crud (创建-读取-更新-删除) 操作是 实时 的?\n>  * elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据?\n>  * 为什么删除文档不会立刻释放空间？\n>  * refresh, flush, 和 optimize api 都做了什么, 你什么情况下应该使用他们？\n\n\n# 使文本可被搜索\n\n必须解决的第一个挑战是如何使文本可被搜索。 传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值(这里指单词)的能力。\n\n最好的支持 一个字段多个值 需求的数据结构是我们在 倒排索引 章节中介绍过的 倒排索引 。 倒排索引包含一个有序列表，列表包含所有文档出现过的不重复个体，或称为 词项 ，对于每一个词项，包含了它所有曾出现过文档的列表。\n\nterm  | doc 1 | doc 2 | doc 3 | ...\n------------------------------------\nbrown |   x   |       |  x    | ...\nfox   |   x   |   x   |  x    | ...\nquick |   x   |   x   |       | ...\nthe   |   x   |       |  x    | ...\n\n\n> 当讨论倒排索引时，我们会谈到 文档 标引，因为历史原因，倒排索引被用来对整个非结构化文本文档进行标引。 elasticsearch 中的 文档 是有字段和值的结构化 json 文档。事实上，在 json 文档中， 每个被索引的字段都有自己的倒排索引。\n\n这个倒排索引相比特定词项出现过的文档列表，会包含更多其它信息。它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息允许 elasticsearch 决定哪些词比其它词更重要，哪些文档比其它文档更重要，这些内容在 什么是相关性? 中有描述。\n\n为了能够实现预期功能，倒排索引需要知道集合中的 所有 文档，这是需要认识到的关键问题。\n\n早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。\n\n\n# 不变性\n\n倒排索引被写入磁盘后是 不可改变 的:它永远不会修改。 不变性有重要的价值：\n\n * 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。\n * 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。\n * 其它缓存(像 filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。\n * 写入单个大的倒排索引允许数据被压缩，减少磁盘 i/o 和 需要被缓存到内存的索引的使用量。\n\n当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。\n\n\n# 动态更新索引\n\n下一个需要被解决的问题是怎样在保留不变性的前提下实现倒排索引的更新？答案是: 用更多的索引。\n\n通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到—从最早的开始—查询完后再对结果进行合并。\n\nelasticsearch 基于 lucene, 这个 java 库引入了 按段搜索 的概念。 每一 段 本身都是一个倒排索引， 但 索引 在 lucene 中除表示所有 段 的集合外， 还增加了 提交点 的概念 — 一个列出了所有已知段的文件，就像在 figure 16, “一个 lucene 索引包含一个提交点和三个段” 中描绘的那样。 如 figure 17, “一个在内存缓存中包含新文档的 lucene 索引” 所示，新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段，如 figure 18, “在一次提交后，一个新的段被添加到提交点而且缓存被清空。” 所示。\n\nfigure 16. 一个 lucene 索引包含一个提交点和三个段\n\n\n\n> 被混淆的概念是，一个 lucene 索引 我们在 elasticsearch 称作 分片 。 一个 elasticsearch 索引 是分片的集合。 当 elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(lucene 索引)，然后像 执行分布式检索 提到的那样，合并每个分片的结果到一个全局的结果集。\n\n逐段搜索会以如下流程进行工作：\n\n 1. 新文档被收集到内存索引缓存， 见 figure 17, “一个在内存缓存中包含新文档的 lucene 索引” 。\n 2. 不时地, 缓存被 提交 ：\n    * 一个新的段—一个追加的倒排索引—被写入磁盘。\n    * 一个新的包含新段名字的 提交点 被写入磁盘。\n    * 磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。\n 3. 新的段被开启，让它包含的文档可见以被搜索。\n 4. 内存缓存被清空，等待接收新的文档。\n\nfigure 17. 一个在内存缓存中包含新文档的 lucene 索引\n\n\n\nfigure 18. 在一次提交后，一个新的段被添加到提交点而且缓存被清空。\n\n\n\n当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。\n\n\n# 删除和更新\n\n段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。\n\n当一个文档被 “删除” 时，它实际上只是在 .del 文件中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n\n文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。\n\n在 段合并 , 我们展示了一个被删除的文档是怎样被文件系统移除的。\n\n\n# 近实时搜索\n\n随着按段（per-segment）搜索的发展，一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。\n\n磁盘在这里成为了瓶颈。提交（commiting）一个新的段到磁盘需要一个 fsync 来确保段被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。\n\n我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着 fsync 要从整个过程中被移除。\n\n在 elasticsearch 和磁盘之间是文件系统缓存。 像之前描述的一样， 在内存索引缓冲区（ figure 19, “在内存缓冲区中包含了新文档的 lucene 索引” ）中的文档会被写入到一个新的段中（ figure 20, “缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交” ）。 但是这里新段会被先写入到文件系统缓存—这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。\n\nfigure 19. 在内存缓冲区中包含了新文档的 lucene 索引\n\n\n\nlucene 允许新段被写入和打开—使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。\n\nfigure 20. 缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交\n\n\n\n\n# refresh api\n\n在 elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n\n这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用 refresh api 执行一次手动刷新:\n\npost /_refresh\npost /blogs/_refresh\n\n\n刷新（refresh）所有的索引\n\n只刷新（refresh） blogs 索引\n\n> 尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 elasticsearch 的近实时的性质，并接受它的不足。\n\n并不是所有的情况都需要每秒刷新。可能你正在使用 elasticsearch 索引大量的日志文件， 你可能想优化索引速度而不是近实时搜索， 可以通过设置 refresh_interval ， 降低每个索引的刷新频率：\n\nput /my_logs\n{\n  "settings": {\n    "refresh_interval": "30s"\n  }\n}\n\n\n> 每 30 秒刷新 my_logs 索引。\n\nrefresh_interval 可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来：\n\nput /my_logs/_settings\n{ "refresh_interval": -1 }\n\nput /my_logs/_settings\n{ "refresh_interval": "1s" }\n\n\n * 关闭自动刷新。\n\n * 每秒自动刷新。\n\n> refresh_interval 需要一个 持续时间 值， 例如 1s （1 秒） 或 2m （2 分钟）。 一个绝对值 1 表示的是 1 毫秒 --无疑会使你的集群陷入瘫痪。\n\n\n# 持久化变更\n\n如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 elasticsearch 的可靠性，需要确保数据变化被持久化到磁盘。\n\n在 动态更新索引，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。\n\n即使通过每秒刷新（refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办？我们也不希望丢失掉这些数据。\n\nelasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 elasticsearch 进行操作时均进行了日志记录。通过 translog ，整个流程看起来是下面这样：\n\n一个文档被索引之后，就会被添加到内存缓冲区，并且 追加到了 translog ，正如 figure 21, “新的文档被添加到内存缓冲区并且被追加到了事务日志” 描述的一样。\n\nfigure 21. 新的文档被添加到内存缓冲区并且被追加到了事务日志\n\n\n\n刷新（refresh）使分片处于 figure 22, “刷新（refresh）完成后, 缓存被清空但是事务日志不会” 描述的状态，分片每秒被刷新（refresh）一次：\n\n * 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作。\n * 这个段被打开，使其可被搜索。\n * 内存缓冲区被清空。\n\nfigure 22. 刷新（refresh）完成后, 缓存被清空但是事务日志不会\n\n\n\n这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志（见 figure 23, “事务日志不断积累文档” ）。\n\nfigure 23. 事务日志不断积累文档\n\n\n\n 1. 每隔一段时间—例如 translog 变得越来越大—索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行（见 figure 24, “在刷新（flush）之后，段被全量提交，并且事务日志被清空” ）：\n    * 所有在内存缓冲区的文档都被写入一个新的段。\n    * 缓冲区被清空。\n    * 一个提交点被写入硬盘。\n    * 文件系统缓存通过 fsync 被刷新（flush）。\n    * 老的 translog 被删除。\n\ntranslog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 elasticsearch 启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。\n\ntranslog 也被用来提供实时 crud 。当你试着通过 id 查询、更新、删除一个文档，它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。\n\nfigure 24. 在刷新（flush）之后，段被全量提交，并且事务日志被清空\n\n\n\n\n# flush api\n\n这个执行一个提交并且截断 translog 的行为在 elasticsearch 被称作一次 flush 。 分片每 30 分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新。请查看 translog 文档 来设置，它可以用来 控制这些阈值：\n\nflush api 可以被用来执行一个手工的刷新（flush）:\n\npost /blogs/_flush\npost /_flush?wait_for_ongoing\n\n\n * 刷新（flush） blogs 索引。\n * 刷新（flush）所有的索引并且并且等待所有刷新在返回前完成。\n\n你很少需要自己手动执行 flush 操作；通常情况下，自动刷新就足够了。\n\n这就是说，在重启节点或关闭索引之前执行 flush 有益于你的索引。当 elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快。\n\n> translog 的目的是保证操作不会丢失。这引出了这个问题： translog 有多安全？\n> \n> 在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。最终， 基本上，这意味着在整个请求被 fsync 到主分片和复制分片的 translog 之前，你的客户端不会得到一个 200 ok 响应。\n> \n> 在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是 bulk 导入，它在一次请求中平摊了大量文档的开销）。\n> \n> 但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每 5 秒执行一次 fsync 。\n> \n> 这个行为可以通过设置 durability 参数为 async 来启用：\n> \n> put /my_index/_settings\n> {\n>     "index.translog.durability": "async",\n>     "index.translog.sync_interval": "5s"\n> }\n> \n> \n> 这个选项可以针对索引单独设置，并且可以动态进行修改。如果你决定使用异步 translog 的话，你需要 保证 在发生 crash 时，丢失掉 sync_interval 时间段的数据也无所谓。请在决定前知晓这个特性。\n> \n> 如果你不确定这个行为的后果，最好是使用默认的参数（ "index.translog.durability": "request" ）来避免数据丢失。\n\n\n# 段合并\n\n由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和 cpu 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。\n\nelasticsearch 通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。\n\n段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。\n\n启动段合并不需要你做任何事。进行索引和搜索时会自动进行。这个流程像在 figure 25, “两个提交了的段和一个未提交的段正在被合并到一个更大的段” 中提到的一样工作：\n\n1、 当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。\n\n2、 合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。\n\nfigure 25. 两个提交了的段和一个未提交的段正在被合并到一个更大的段\n\n\n\nfigure 26, “一旦合并结束，老的段被删除” 说明合并完成时的活动：\n\n * 新的段被刷新（flush）到了磁盘。 ** 写入一个包含新段且排除旧的和较小的段的新提交点。\n * 新的段被打开用来搜索。\n * 老的段被删除。\n\nfigure 26. 一旦合并结束，老的段被删除\n\n\n\n合并大的段需要消耗大量的 i/o 和 cpu 资源，如果任其发展会影响搜索性能。elasticsearch 在默认情况下会对合并流程进行资源限制，所以搜索仍然 有足够的资源很好地执行。\n\n\n# optimize api\n\noptimize api 大可看做是 强制合并 api。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。\n\n> optimize api 不应该 被用在一个活跃的索引————一个正积极更新的索引。后台合并流程已经可以很好地完成工作。 optimizing 会阻碍这个进程。不要干扰它！\n\n在特定情况下，使用 optimize api 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化。\n\n在这种情况下，使用 optimize 优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速：\n\npost /logstash-2014-10/_optimize?max_num_segments=1\n\n\n合并索引中的每个分片为一个单独的段\n\n> 请注意，使用 optimize api 触发段合并的操作不会受到任何资源上的限制。这可能会消耗掉你节点上全部的 i/o 资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 optimize，你需要先使用分片分配（查看 迁移旧索引）把索引移到一个安全的节点，再执行。\n\n\n# 参考资料\n\n * elasticsearch 官方文档之 集群内的原理',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 运维",frontmatter:{title:"Elasticsearch 运维",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch","运维"],permalink:"/pages/fdaf15/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/20.Elasticsearch%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/20.Elasticsearch运维.md",key:"v-717638d6",path:"/pages/fdaf15/",headers:[{level:2,title:"Elasticsearch 安装",slug:"elasticsearch-安装",normalizedTitle:"elasticsearch 安装",charIndex:135},{level:2,title:"Elasticsearch 集群规划",slug:"elasticsearch-集群规划",normalizedTitle:"elasticsearch 集群规划",charIndex:336},{level:2,title:"Elasticsearch 配置",slug:"elasticsearch-配置",normalizedTitle:"elasticsearch 配置",charIndex:720},{level:2,title:"Elasticsearch FAQ",slug:"elasticsearch-faq",normalizedTitle:"elasticsearch faq",charIndex:3240},{level:3,title:"elasticsearch 不允许以 root 权限来运行",slug:"elasticsearch-不允许以-root-权限来运行",normalizedTitle:"elasticsearch 不允许以 root 权限来运行",charIndex:3262},{level:3,title:"vm.maxmapcount 不低于 262144",slug:"vm-max-map-count-不低于-262144",normalizedTitle:"vm.maxmapcount 不低于 262144",charIndex:null},{level:3,title:"nofile 不低于 65536",slug:"nofile-不低于-65536",normalizedTitle:"nofile 不低于 65536",charIndex:4161},{level:3,title:"nproc 不低于 2048",slug:"nproc-不低于-2048",normalizedTitle:"nproc 不低于 2048",charIndex:4511},{level:2,title:"参考资料",slug:"参考资料",normalizedTitle:"参考资料",charIndex:4827}],headersStr:"Elasticsearch 安装 Elasticsearch 集群规划 Elasticsearch 配置 Elasticsearch FAQ elasticsearch 不允许以 root 权限来运行 vm.maxmapcount 不低于 262144 nofile 不低于 65536 nproc 不低于 2048 参考资料",content:'# Elasticsearch 运维\n\n> Elasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n\n\n# Elasticsearch 安装\n\n> Elasticsearch 官方下载安装说明\n\n（1）下载解压\n\n访问 官方下载地址 ，选择需要的版本，下载解压到本地。\n\n（2）运行\n\n运行 bin/elasticsearch (Windows 系统上运行 bin\\elasticsearch.bat )\n\n（3）访问\n\n执行 curl http://localhost:9200/ 测试服务是否启动\n\n\n# Elasticsearch 集群规划\n\nElasticSearch 集群需要根据业务实际情况去合理规划。\n\n需要考虑的问题点：\n\n * 集群部署几个节点？\n * 有多少个索引？\n * 每个索引有多大数据量？\n * 每个索引有多少个分片？\n\n一个参考规划：\n\n * 3 台机器，每台机器是 6 核 64G 的。\n * 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。\n * 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。\n\n\n# Elasticsearch 配置\n\nES 的默认配置文件为 config/elasticsearch.yml\n\n基本配置说明如下：\n\ncluster.name: elasticsearch\n#配置es的集群名称，默认是elasticsearch，es会自动发现在同一网段下的es，如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群。\nnode.name: \'Franz Kafka\'\n#节点名，默认随机指定一个name列表中名字，该列表在es的jar包中config文件夹里name.txt文件中，其中有很多作者添加的有趣名字。\nnode.master: true\n#指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master。\nnode.data: true\n#指定该节点是否存储索引数据，默认为true。\nindex.number_of_shards: 5\n#设置默认索引分片个数，默认为5片。\nindex.number_of_replicas: 1\n#设置默认索引副本个数，默认为1个副本。\npath.conf: /path/to/conf\n#设置配置文件的存储路径，默认是es根目录下的config文件夹。\npath.data: /path/to/data\n#设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：\n#path.data: /path/to/data1,/path/to/data2\npath.work: /path/to/work\n#设置临时文件的存储路径，默认是es根目录下的work文件夹。\npath.logs: /path/to/logs\n#设置日志文件的存储路径，默认是es根目录下的logs文件夹\npath.plugins: /path/to/plugins\n#设置插件的存放路径，默认是es根目录下的plugins文件夹\nbootstrap.mlockall: true\n#设置为true来锁住内存。因为当jvm开始swapping时es的效率会降低，所以要保证它不swap，可以把#ES_MIN_MEM和ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。同时也要#允许elasticsearch的进程可以锁住内存，linux下可以通过`ulimit -l unlimited`命令。\nnetwork.bind_host: 192.168.0.1\n#设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0。\nnetwork.publish_host: 192.168.0.1\n#设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。\nnetwork.host: 192.168.0.1\n#这个参数是用来同时设置bind_host和publish_host上面两个参数。\ntransport.tcp.port: 9300\n#设置节点间交互的tcp端口，默认是9300。\ntransport.tcp.compress: true\n#设置是否压缩tcp传输时的数据，默认为false，不压缩。\nhttp.port: 9200\n#设置对外服务的http端口，默认为9200。\nhttp.max_content_length: 100mb\n#设置内容的最大容量，默认100mb\nhttp.enabled: false\n#是否使用http协议对外提供服务，默认为true，开启。\ngateway.type: local\n#gateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，hadoop的#HDFS，和amazon的s3服务器，其它文件系统的设置方法下次再详细说。\ngateway.recover_after_nodes: 1\n#设置集群中N个节点启动时进行数据恢复，默认为1。\ngateway.recover_after_time: 5m\n#设置初始化数据恢复进程的超时时间，默认是5分钟。\ngateway.expected_nodes: 2\n#设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复。\ncluster.routing.allocation.node_initial_primaries_recoveries: 4\n#初始化数据恢复时，并发恢复线程的个数，默认为4。\ncluster.routing.allocation.node_concurrent_recoveries: 2\n#添加删除节点或负载均衡时并发恢复线程的个数，默认为4。\nindices.recovery.max_size_per_sec: 0\n#设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。\nindices.recovery.concurrent_streams: 5\n#设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。\ndiscovery.zen.minimum_master_nodes: 1\n#设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4）\ndiscovery.zen.ping.timeout: 3s\n#设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。\ndiscovery.zen.ping.multicast.enabled: false\n#设置是否打开多播发现节点，默认是true。\ndiscovery.zen.ping.unicast.hosts: [\'host1\', \'host2:port\', \'host3[portX-portY]\']\n#设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。\n\n\n\n# Elasticsearch FAQ\n\n\n# elasticsearch 不允许以 root 权限来运行\n\n**问题：**在 Linux 环境中，elasticsearch 不允许以 root 权限来运行。\n\n如果以 root 身份运行 elasticsearch，会提示这样的错误：\n\ncan not run elasticsearch as root\n\n\n**解决方法：**使用非 root 权限账号运行 elasticsearch\n\n# 创建用户组\ngroupadd elk\n# 创建新用户，-g elk 设置其用户组为 elk，-p elk 设置其密码为 elk\nuseradd elk -g elk -p elk\n# 更改 /opt 文件夹及内部文件的所属用户及组为 elk:elk\nchown -R elk:elk /opt # 假设你的 elasticsearch 安装在 opt 目录下\n# 切换账号\nsu elk\n\n\n\n# vm.max_map_count 不低于 262144\n\n问题：vm.max_map_count 表示虚拟内存大小，它是一个内核参数。elasticsearch 默认要求 vm.max_map_count 不低于 262144。\n\nmax virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n\n\n解决方法：\n\n你可以执行以下命令，设置 vm.max_map_count ，但是重启后又会恢复为原值。\n\nsysctl -w vm.max_map_count=262144\n\n\n持久性的做法是在 /etc/sysctl.conf 文件中修改 vm.max_map_count 参数：\n\necho "vm.max_map_count=262144" > /etc/sysctl.conf\nsysctl -p\n\n\n> 注意\n> \n> 如果运行环境为 docker 容器，可能会限制执行 sysctl 来修改内核参数。\n> \n> 这种情况下，你只能选择直接修改宿主机上的参数了。\n\n\n# nofile 不低于 65536\n\n问题： nofile 表示进程允许打开的最大文件数。elasticsearch 进程要求可以打开的最大文件数不低于 65536。\n\nmax file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\n\n\n解决方法：\n\n在 /etc/security/limits.conf 文件中修改 nofile 参数：\n\necho "* soft nofile 65536" > /etc/security/limits.conf\necho "* hard nofile 131072" > /etc/security/limits.conf\n\n\n\n# nproc 不低于 2048\n\n问题： nproc 表示最大线程数。elasticsearch 要求最大线程数不低于 2048。\n\nmax number of threads [1024] for user [user] is too low, increase to at least [2048]\n\n\n解决方法：\n\n在 /etc/security/limits.conf 文件中修改 nproc 参数：\n\necho "* soft nproc 2048" > /etc/security/limits.conf\necho "* hard nproc 4096" > /etc/security/limits.conf\n\n\n\n# 参考资料\n\n * Elasticsearch 官方下载安装说明\n * Install Elasticsearch with RPM\n * Elasticsearch 使用积累',normalizedContent:'# elasticsearch 运维\n\n> elasticsearch 是一个分布式、restful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 elastic stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n\n\n# elasticsearch 安装\n\n> elasticsearch 官方下载安装说明\n\n（1）下载解压\n\n访问 官方下载地址 ，选择需要的版本，下载解压到本地。\n\n（2）运行\n\n运行 bin/elasticsearch (windows 系统上运行 bin\\elasticsearch.bat )\n\n（3）访问\n\n执行 curl http://localhost:9200/ 测试服务是否启动\n\n\n# elasticsearch 集群规划\n\nelasticsearch 集群需要根据业务实际情况去合理规划。\n\n需要考虑的问题点：\n\n * 集群部署几个节点？\n * 有多少个索引？\n * 每个索引有多大数据量？\n * 每个索引有多少个分片？\n\n一个参考规划：\n\n * 3 台机器，每台机器是 6 核 64g 的。\n * 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500mb，每月增量数据大概是 6 亿，15g。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100g 左右。\n * 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20g，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。\n\n\n# elasticsearch 配置\n\nes 的默认配置文件为 config/elasticsearch.yml\n\n基本配置说明如下：\n\ncluster.name: elasticsearch\n#配置es的集群名称，默认是elasticsearch，es会自动发现在同一网段下的es，如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群。\nnode.name: \'franz kafka\'\n#节点名，默认随机指定一个name列表中名字，该列表在es的jar包中config文件夹里name.txt文件中，其中有很多作者添加的有趣名字。\nnode.master: true\n#指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master。\nnode.data: true\n#指定该节点是否存储索引数据，默认为true。\nindex.number_of_shards: 5\n#设置默认索引分片个数，默认为5片。\nindex.number_of_replicas: 1\n#设置默认索引副本个数，默认为1个副本。\npath.conf: /path/to/conf\n#设置配置文件的存储路径，默认是es根目录下的config文件夹。\npath.data: /path/to/data\n#设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：\n#path.data: /path/to/data1,/path/to/data2\npath.work: /path/to/work\n#设置临时文件的存储路径，默认是es根目录下的work文件夹。\npath.logs: /path/to/logs\n#设置日志文件的存储路径，默认是es根目录下的logs文件夹\npath.plugins: /path/to/plugins\n#设置插件的存放路径，默认是es根目录下的plugins文件夹\nbootstrap.mlockall: true\n#设置为true来锁住内存。因为当jvm开始swapping时es的效率会降低，所以要保证它不swap，可以把#es_min_mem和es_max_mem两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。同时也要#允许elasticsearch的进程可以锁住内存，linux下可以通过`ulimit -l unlimited`命令。\nnetwork.bind_host: 192.168.0.1\n#设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0。\nnetwork.publish_host: 192.168.0.1\n#设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。\nnetwork.host: 192.168.0.1\n#这个参数是用来同时设置bind_host和publish_host上面两个参数。\ntransport.tcp.port: 9300\n#设置节点间交互的tcp端口，默认是9300。\ntransport.tcp.compress: true\n#设置是否压缩tcp传输时的数据，默认为false，不压缩。\nhttp.port: 9200\n#设置对外服务的http端口，默认为9200。\nhttp.max_content_length: 100mb\n#设置内容的最大容量，默认100mb\nhttp.enabled: false\n#是否使用http协议对外提供服务，默认为true，开启。\ngateway.type: local\n#gateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，hadoop的#hdfs，和amazon的s3服务器，其它文件系统的设置方法下次再详细说。\ngateway.recover_after_nodes: 1\n#设置集群中n个节点启动时进行数据恢复，默认为1。\ngateway.recover_after_time: 5m\n#设置初始化数据恢复进程的超时时间，默认是5分钟。\ngateway.expected_nodes: 2\n#设置这个集群中节点的数量，默认为2，一旦这n个节点启动，就会立即进行数据恢复。\ncluster.routing.allocation.node_initial_primaries_recoveries: 4\n#初始化数据恢复时，并发恢复线程的个数，默认为4。\ncluster.routing.allocation.node_concurrent_recoveries: 2\n#添加删除节点或负载均衡时并发恢复线程的个数，默认为4。\nindices.recovery.max_size_per_sec: 0\n#设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。\nindices.recovery.concurrent_streams: 5\n#设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。\ndiscovery.zen.minimum_master_nodes: 1\n#设置这个参数来保证集群中的节点可以知道其它n个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4）\ndiscovery.zen.ping.timeout: 3s\n#设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。\ndiscovery.zen.ping.multicast.enabled: false\n#设置是否打开多播发现节点，默认是true。\ndiscovery.zen.ping.unicast.hosts: [\'host1\', \'host2:port\', \'host3[portx-porty]\']\n#设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。\n\n\n\n# elasticsearch faq\n\n\n# elasticsearch 不允许以 root 权限来运行\n\n**问题：**在 linux 环境中，elasticsearch 不允许以 root 权限来运行。\n\n如果以 root 身份运行 elasticsearch，会提示这样的错误：\n\ncan not run elasticsearch as root\n\n\n**解决方法：**使用非 root 权限账号运行 elasticsearch\n\n# 创建用户组\ngroupadd elk\n# 创建新用户，-g elk 设置其用户组为 elk，-p elk 设置其密码为 elk\nuseradd elk -g elk -p elk\n# 更改 /opt 文件夹及内部文件的所属用户及组为 elk:elk\nchown -r elk:elk /opt # 假设你的 elasticsearch 安装在 opt 目录下\n# 切换账号\nsu elk\n\n\n\n# vm.max_map_count 不低于 262144\n\n问题：vm.max_map_count 表示虚拟内存大小，它是一个内核参数。elasticsearch 默认要求 vm.max_map_count 不低于 262144。\n\nmax virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]\n\n\n解决方法：\n\n你可以执行以下命令，设置 vm.max_map_count ，但是重启后又会恢复为原值。\n\nsysctl -w vm.max_map_count=262144\n\n\n持久性的做法是在 /etc/sysctl.conf 文件中修改 vm.max_map_count 参数：\n\necho "vm.max_map_count=262144" > /etc/sysctl.conf\nsysctl -p\n\n\n> 注意\n> \n> 如果运行环境为 docker 容器，可能会限制执行 sysctl 来修改内核参数。\n> \n> 这种情况下，你只能选择直接修改宿主机上的参数了。\n\n\n# nofile 不低于 65536\n\n问题： nofile 表示进程允许打开的最大文件数。elasticsearch 进程要求可以打开的最大文件数不低于 65536。\n\nmax file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]\n\n\n解决方法：\n\n在 /etc/security/limits.conf 文件中修改 nofile 参数：\n\necho "* soft nofile 65536" > /etc/security/limits.conf\necho "* hard nofile 131072" > /etc/security/limits.conf\n\n\n\n# nproc 不低于 2048\n\n问题： nproc 表示最大线程数。elasticsearch 要求最大线程数不低于 2048。\n\nmax number of threads [1024] for user [user] is too low, increase to at least [2048]\n\n\n解决方法：\n\n在 /etc/security/limits.conf 文件中修改 nproc 参数：\n\necho "* soft nproc 2048" > /etc/security/limits.conf\necho "* hard nproc 4096" > /etc/security/limits.conf\n\n\n\n# 参考资料\n\n * elasticsearch 官方下载安装说明\n * install elasticsearch with rpm\n * elasticsearch 使用积累',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elasticsearch 教程",frontmatter:{title:"Elasticsearch 教程",date:"2022-04-11T16:52:35.000Z",categories:["数据库","搜索引擎数据库","Elasticsearch"],tags:["数据库","搜索引擎数据库","Elasticsearch"],permalink:"/pages/74675e/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/01.Elasticsearch/",relativePath:"12.数据库/07.搜索引擎数据库/01.Elasticsearch/README.md",key:"v-2c1e440a",path:"/pages/74675e/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:128},{level:3,title:"Elasticsearch 面试总结 💯",slug:"elasticsearch-面试总结-💯",normalizedTitle:"elasticsearch 面试总结 💯",charIndex:138},{level:3,title:"Elasticsearch 快速入门",slug:"elasticsearch-快速入门",normalizedTitle:"elasticsearch 快速入门",charIndex:164},{level:3,title:"Elasticsearch 简介",slug:"elasticsearch-简介",normalizedTitle:"elasticsearch 简介",charIndex:187},{level:3,title:"Elasticsearch 索引管理",slug:"elasticsearch-索引管理",normalizedTitle:"elasticsearch 索引管理",charIndex:208},{level:3,title:"Elasticsearch 映射",slug:"elasticsearch-映射",normalizedTitle:"elasticsearch 映射",charIndex:231},{level:3,title:"Elasticsearch 查询",slug:"elasticsearch-查询",normalizedTitle:"elasticsearch 查询",charIndex:252},{level:3,title:"Elasticsearch 高亮",slug:"elasticsearch-高亮",normalizedTitle:"elasticsearch 高亮",charIndex:273},{level:3,title:"Elasticsearch 排序",slug:"elasticsearch-排序",normalizedTitle:"elasticsearch 排序",charIndex:294},{level:3,title:"Elasticsearch 聚合",slug:"elasticsearch-聚合",normalizedTitle:"elasticsearch 聚合",charIndex:315},{level:3,title:"Elasticsearch 分析器",slug:"elasticsearch-分析器",normalizedTitle:"elasticsearch 分析器",charIndex:336},{level:3,title:"Elasticsearch 性能优化",slug:"elasticsearch-性能优化",normalizedTitle:"elasticsearch 性能优化",charIndex:358},{level:3,title:"Elasticsearch Rest API",slug:"elasticsearch-rest-api",normalizedTitle:"elasticsearch rest api",charIndex:381},{level:3,title:"ElasticSearch Java API 之 High Level REST Client",slug:"elasticsearch-java-api-之-high-level-rest-client",normalizedTitle:"elasticsearch java api 之 high level rest client",charIndex:408},{level:3,title:"Elasticsearch 集群和分片",slug:"elasticsearch-集群和分片",normalizedTitle:"elasticsearch 集群和分片",charIndex:460},{level:3,title:"Elasticsearch 运维",slug:"elasticsearch-运维",normalizedTitle:"elasticsearch 运维",charIndex:484},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:505},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:1107}],headersStr:"📖 内容 Elasticsearch 面试总结 💯 Elasticsearch 快速入门 Elasticsearch 简介 Elasticsearch 索引管理 Elasticsearch 映射 Elasticsearch 查询 Elasticsearch 高亮 Elasticsearch 排序 Elasticsearch 聚合 Elasticsearch 分析器 Elasticsearch 性能优化 Elasticsearch Rest API ElasticSearch Java API 之 High Level REST Client Elasticsearch 集群和分片 Elasticsearch 运维 📚 资料 🚪 传送",content:"# Elasticsearch 教程\n\n> Elasticsearch 是一个基于 Lucene 的搜索和数据分析工具，它提供了一个分布式服务。Elasticsearch 是遵从 Apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n\n# 📖 内容\n\n\n# Elasticsearch 面试总结 💯\n\n\n# Elasticsearch 快速入门\n\n\n# Elasticsearch 简介\n\n\n# Elasticsearch 索引管理\n\n\n# Elasticsearch 映射\n\n\n# Elasticsearch 查询\n\n\n# Elasticsearch 高亮\n\n\n# Elasticsearch 排序\n\n\n# Elasticsearch 聚合\n\n\n# Elasticsearch 分析器\n\n\n# Elasticsearch 性能优化\n\n\n# Elasticsearch Rest API\n\n\n# ElasticSearch Java API 之 High Level REST Client\n\n\n# Elasticsearch 集群和分片\n\n\n# Elasticsearch 运维\n\n\n# 📚 资料\n\n * 官方\n   * Elasticsearch 官网\n   * Elasticsearch Github\n   * Elasticsearch 官方文档\n   * Elasticsearch: The Definitive Guide - ElasticSearch 官方学习资料\n * 书籍\n   * 《Elasticsearch 实战》\n * 教程\n   * ELK Stack 权威指南\n   * Elasticsearch 教程\n * 文章\n   * Elasticsearch+Logstash+Kibana 教程\n   * ELK（Elasticsearch、Logstash、Kibana）安装和配置\n   * 性能调优相关的工程实践\n     * Elasticsearch Performance Tuning Practice at eBay\n     * Elasticsearch at Kickstarter\n     * 9 tips on ElasticSearch configuration for high performance\n     * Elasticsearch In Production - Deployment Best Practices\n * 更多资源\n   * GitHub: Awesome ElasticSearch\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# elasticsearch 教程\n\n> elasticsearch 是一个基于 lucene 的搜索和数据分析工具，它提供了一个分布式服务。elasticsearch 是遵从 apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n\n# 📖 内容\n\n\n# elasticsearch 面试总结 💯\n\n\n# elasticsearch 快速入门\n\n\n# elasticsearch 简介\n\n\n# elasticsearch 索引管理\n\n\n# elasticsearch 映射\n\n\n# elasticsearch 查询\n\n\n# elasticsearch 高亮\n\n\n# elasticsearch 排序\n\n\n# elasticsearch 聚合\n\n\n# elasticsearch 分析器\n\n\n# elasticsearch 性能优化\n\n\n# elasticsearch rest api\n\n\n# elasticsearch java api 之 high level rest client\n\n\n# elasticsearch 集群和分片\n\n\n# elasticsearch 运维\n\n\n# 📚 资料\n\n * 官方\n   * elasticsearch 官网\n   * elasticsearch github\n   * elasticsearch 官方文档\n   * elasticsearch: the definitive guide - elasticsearch 官方学习资料\n * 书籍\n   * 《elasticsearch 实战》\n * 教程\n   * elk stack 权威指南\n   * elasticsearch 教程\n * 文章\n   * elasticsearch+logstash+kibana 教程\n   * elk（elasticsearch、logstash、kibana）安装和配置\n   * 性能调优相关的工程实践\n     * elasticsearch performance tuning practice at ebay\n     * elasticsearch at kickstarter\n     * 9 tips on elasticsearch configuration for high performance\n     * elasticsearch in production - deployment best practices\n * 更多资源\n   * github: awesome elasticsearch\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elastic 快速入门",frontmatter:{title:"Elastic 快速入门",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic"],permalink:"/pages/553160/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/01.Elastic%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/01.Elastic快速入门.md",key:"v-778f3185",path:"/pages/553160/",headers:[{level:2,title:"1. 简介",slug:"_1-简介",normalizedTitle:"1. 简介",charIndex:38},{level:3,title:"1.1. Elastic Stack 是什么",slug:"_1-1-elastic-stack-是什么",normalizedTitle:"1.1. elastic stack 是什么",charIndex:48},{level:3,title:"1.2. 为什么使用 Elastic Stack",slug:"_1-2-为什么使用-elastic-stack",normalizedTitle:"1.2. 为什么使用 elastic stack",charIndex:397},{level:3,title:"1.3. Elastic Stack 架构",slug:"_1-3-elastic-stack-架构",normalizedTitle:"1.3. elastic stack 架构",charIndex:563},{level:2,title:"2. ElasticSearch",slug:"_2-elasticsearch",normalizedTitle:"2. elasticsearch",charIndex:972},{level:3,title:"2.1. ElasticSearch 简介",slug:"_2-1-elasticsearch-简介",normalizedTitle:"2.1. elasticsearch 简介",charIndex:1105},{level:4,title:"2.1.1. 核心概念",slug:"_2-1-1-核心概念",normalizedTitle:"2.1.1. 核心概念",charIndex:1379},{level:3,title:"2.2. ElasticSearch 原理",slug:"_2-2-elasticsearch-原理",normalizedTitle:"2.2. elasticsearch 原理",charIndex:2042},{level:4,title:"2.2.1. ES 写数据过程",slug:"_2-2-1-es-写数据过程",normalizedTitle:"2.2.1. es 写数据过程",charIndex:2067},{level:4,title:"2.2.2. es 读数据过程",slug:"_2-2-2-es-读数据过程",normalizedTitle:"2.2.2. es 读数据过程",charIndex:2350},{level:4,title:"2.2.3. 写数据底层原理",slug:"_2-2-3-写数据底层原理",normalizedTitle:"2.2.3. 写数据底层原理",charIndex:2699},{level:4,title:"2.2.4. 删除/更新数据底层原理",slug:"_2-2-4-删除-更新数据底层原理",normalizedTitle:"2.2.4. 删除/更新数据底层原理",charIndex:4961},{level:4,title:"2.2.5. 底层 lucene",slug:"_2-2-5-底层-lucene",normalizedTitle:"2.2.5. 底层 lucene",charIndex:5407},{level:4,title:"2.2.6. 倒排索引",slug:"_2-2-6-倒排索引",normalizedTitle:"2.2.6. 倒排索引",charIndex:5586},{level:2,title:"3. Logstash",slug:"_3-logstash",normalizedTitle:"3. logstash",charIndex:6504},{level:3,title:"3.1. Logstash 简介",slug:"_3-1-logstash-简介",normalizedTitle:"3.1. logstash 简介",charIndex:6587},{level:3,title:"3.2. Logstash 原理",slug:"_3-2-logstash-原理",normalizedTitle:"3.2. logstash 原理",charIndex:6747},{level:2,title:"4. Beats",slug:"_4-beats",normalizedTitle:"4. beats",charIndex:7025},{level:3,title:"4.1. Filebeat 简介",slug:"_4-1-filebeat-简介",normalizedTitle:"4.1. filebeat 简介",charIndex:7332},{level:3,title:"4.2. Filebeat 原理",slug:"_4-2-filebeat-原理",normalizedTitle:"4.2. filebeat 原理",charIndex:7704},{level:2,title:"5. 运维",slug:"_5-运维",normalizedTitle:"5. 运维",charIndex:8111},{level:2,title:"6. 参考资料",slug:"_6-参考资料",normalizedTitle:"6. 参考资料",charIndex:8182}],headersStr:"1. 简介 1.1. Elastic Stack 是什么 1.2. 为什么使用 Elastic Stack 1.3. Elastic Stack 架构 2. ElasticSearch 2.1. ElasticSearch 简介 2.1.1. 核心概念 2.2. ElasticSearch 原理 2.2.1. ES 写数据过程 2.2.2. es 读数据过程 2.2.3. 写数据底层原理 2.2.4. 删除/更新数据底层原理 2.2.5. 底层 lucene 2.2.6. 倒排索引 3. Logstash 3.1. Logstash 简介 3.2. Logstash 原理 4. Beats 4.1. Filebeat 简介 4.2. Filebeat 原理 5. 运维 6. 参考资料",content:"# Elastic 快速入门\n\n> 开源协议：Apache 2.0\n\n\n# 1. 简介\n\n\n# 1.1. Elastic Stack 是什么\n\nElastic Stack 即 ELK Stack。\n\nELK 是指 Elastic 公司旗下三款产品 ElasticSearch 、Logstash 、Kibana 的首字母组合。\n\n * Elasticsearch 是一个搜索和分析引擎。\n * Logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 Elasticsearch 等“存储库”中。\n * Kibana 则可以让用户在 Elasticsearch 中使用图形和图表对数据进行可视化。\n\n而 Elastic Stack 是 ELK Stack 的更新换代产品，最新产品引入了轻量型的单一功能数据采集器，并把它们叫做 Beats。\n\n\n# 1.2. 为什么使用 Elastic Stack\n\n对于有一定规模的公司来说，通常会很多个应用，并部署在大量的服务器上。运维和开发人员常常需要通过查看日志来定位问题。如果应用是集群化部署，试想如果登录一台台服务器去查看日志，是多么费时费力。\n\n而通过 ELK 这套解决方案，可以同时实现日志收集、日志搜索和日志分析的功能。\n\n\n# 1.3. Elastic Stack 架构\n\n\n\n> 说明\n> \n> 以上是 Elastic Stack 的一个架构图。从图中可以清楚的看到数据流向。\n> \n>  * Beats 是单一用途的数据传输平台，它可以将多台机器的数据发送到 Logstash 或 ElasticSearch。但 Beats 并不是不可或缺的一环，所以本文中暂不介绍。\n> \n>  * Logstash 是一个动态数据收集管道。支持以 TCP/UDP/HTTP 多种方式收集数据（也可以接受 Beats 传输来的数据），并对数据做进一步丰富或提取字段处理。\n> \n>  * ElasticSearch 是一个基于 JSON 的分布式的搜索和分析引擎。作为 ELK 的核心，它集中存储数据。\n> \n>  * Kibana 是 ELK 的用户界面。它将收集的数据进行可视化展示（各种报表、图形化数据），并提供配置、管理 ELK 的界面。\n\n\n# 2. ElasticSearch\n\n> Elasticsearch 是一个分布式、RESTful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 Elastic Stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n\n\n# 2.1. ElasticSearch 简介\n\nElasticsearch 基于搜索库 Lucene 开发。ElasticSearch 隐藏了 Lucene 的复杂性，提供了简单易用的 REST API / Java API 接口（另外还有其他语言的 API 接口）。\n\nElasticSearch 可以视为一个文档存储，它将复杂数据结构序列化为 JSON 存储。\n\nElasticSearch 是近乎于实时的全文搜素，这是指：\n\n * 从写入数据到数据可以被搜索，存在较小的延迟（大概是 1s）\n * 基于 ES 执行搜索和分析可以达到秒级\n\n# 2.1.1. 核心概念\n\n * 索引（Index） 可以认为是文档（document）的优化集合。\n * 每个 文档（document） 都是字段（field）的集合。\n * 字段（field） 是包含数据的键值对。\n * 默认情况下，Elasticsearch 对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。\n * 每个索引里可以有一个或者多个类型（type）。类型（type） 是 index 的一个逻辑分类，\n * 当单台机器不足以存储大量数据时，Elasticsearch 可以将一个索引中的数据切分为多个 分片（shard） 。 分片（shard） 分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。\n * 任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 副本（replica）。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。\n\n\n# 2.2. ElasticSearch 原理\n\n# 2.2.1. ES 写数据过程\n\n * 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。\n * coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。\n * 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。\n * coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。\n\n\n\n# 2.2.2. es 读数据过程\n\n可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n\n * 客户端发送请求到任意一个 node，成为 coordinate node。\n * coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n * 接收请求的 node 返回 document 给 coordinate node。\n * coordinate node 返回 document 给客户端。\n\n# 2.2.3. 写数据底层原理\n\n\n\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。\n\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。\n\n为什么叫 es 是准实时的？ NRT，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\n\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n\n这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\n\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\n\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n\n实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n\n> 数据写入 segment file 之后，同时就建立好了倒排索引。\n\n# 2.2.4. 删除/更新数据底层原理\n\n如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。\n\n# 2.2.5. 底层 lucene\n\n简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n\n# 2.2.6. 倒排索引\n\n在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n\n那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n\n举个栗子。\n\n有以下文档：\n\nDOCID   DOC\n1       谷歌地图之父跳槽 Facebook\n2       谷歌地图之父加盟 Facebook\n3       谷歌地图创始人拉斯离开谷歌加盟 Facebook\n4       谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关\n5       谷歌地图之父拉斯加盟社交网站 Facebook\n\n对文档进行分词之后，得到以下倒排索引。\n\nWORDID   WORD       DOCIDS\n1        谷歌         1,2,3,4,5\n2        地图         1,2,3,4,5\n3        之父         1,2,4,5\n4        跳槽         1,4\n5        Facebook   1,2,3,4,5\n6        加盟         2,3,5\n7        创始人        3\n8        拉斯         3,5\n9        离开         3\n10       与          4\n..       ..         ..\n\n另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n\n要注意倒排索引的两个重要细节：\n\n * 倒排索引中的所有词项对应一个或多个文档；\n * 倒排索引中的词项根据字典顺序升序排列\n\n> 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n\n\n# 3. Logstash\n\n> Logstash 是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。\n\n\n# 3.1. Logstash 简介\n\nLogstash 可以传输和处理你的日志、事务或其他数据。\n\nLogstash 是 Elasticsearch 的最佳数据管道。\n\nLogstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制。Logstash 社区有超过 200 种可用插件。\n\n\n# 3.2. Logstash 原理\n\nLogstash 有两个必要元素：input 和 output ，一个可选元素：filter。\n\n这三个元素，分别代表 Logstash 事件处理的三个阶段：输入 > 过滤器 > 输出。\n\n\n\n * input - 负责从数据源采集数据。\n * filter - 将数据修改为你指定的格式或内容。\n * output - 将数据传输到目的地。\n\n在实际应用场景中，通常输入、输出、过滤器不止一个。Logstash 的这三个元素都使用插件式管理方式，用户可以根据应用需要，灵活的选用各阶段需要的插件，并组合使用。\n\n\n# 4. Beats\n\n> Beats 是安装在服务器上的数据中转代理。\n> \n> Beats 可以将数据直接传输到 Elasticsearch 或传输到 Logstash 。\n\n\n\nBeats 有多种类型，可以根据实际应用需要选择合适的类型。\n\n常用的类型有：\n\n * **Packetbeat：**网络数据包分析器，提供有关您的应用程序服务器之间交换的事务的信息。\n * **Filebeat：**从您的服务器发送日志文件。\n * **Metricbeat：**是一个服务器监视代理程序，它定期从服务器上运行的操作系统和服务收集指标。\n * **Winlogbeat：**提供 Windows 事件日志。\n\n\n# 4.1. Filebeat 简介\n\n> 由于本人仅接触过 Filebeat，所以本文只介绍 Beats 组件中的 Filebeat。\n\n相比 Logstash，FileBeat 更加轻量化。\n\n在任何环境下，应用程序都有停机的可能性。 Filebeat 读取并转发日志行，如果中断，则会记住所有事件恢复联机状态时所在位置。\n\nFilebeat 带有内部模块（auditd，Apache，Nginx，System 和 MySQL），可通过一个指定命令来简化通用日志格式的收集，解析和可视化。\n\nFileBeat 不会让你的管道超负荷。FileBeat 如果是向 Logstash 传输数据，当 Logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。\n\n\n\n\n# 4.2. Filebeat 原理\n\nFilebeat 有两个主要组件：\n\n * harvester：负责读取一个文件的内容。它会逐行读取文件内容，并将内容发送到输出目的地。\n * prospector：负责管理 harvester 并找到所有需要读取的文件源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。\n\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /var/log/*.log\n      - /var/path2/*.log\n\n\nFilebeat 保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。\n\nFilebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。\n\n\n# 5. 运维\n\n * ElasticSearch 运维\n * Logstash 运维\n * Kibana 运维\n * Beats 运维\n\n\n# 6. 参考资料\n\n * 官方资源\n   * Elasticsearch 官网\n   * Elasticsearch Github\n   * Elasticsearch 官方文档\n   * Logstash 官网\n   * Logstash Github\n   * Logstash 官方文档\n   * Kibana 官网\n   * Kibana Github\n   * Kibana 官方文档\n   * Beats 官网\n   * Beats Github\n   * Beats 官方文档\n * 文章\n   * 什么是 ELK Stack？\n   * https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/es-introduction.md\n   * es-write-query-search",normalizedContent:"# elastic 快速入门\n\n> 开源协议：apache 2.0\n\n\n# 1. 简介\n\n\n# 1.1. elastic stack 是什么\n\nelastic stack 即 elk stack。\n\nelk 是指 elastic 公司旗下三款产品 elasticsearch 、logstash 、kibana 的首字母组合。\n\n * elasticsearch 是一个搜索和分析引擎。\n * logstash 是服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到诸如 elasticsearch 等“存储库”中。\n * kibana 则可以让用户在 elasticsearch 中使用图形和图表对数据进行可视化。\n\n而 elastic stack 是 elk stack 的更新换代产品，最新产品引入了轻量型的单一功能数据采集器，并把它们叫做 beats。\n\n\n# 1.2. 为什么使用 elastic stack\n\n对于有一定规模的公司来说，通常会很多个应用，并部署在大量的服务器上。运维和开发人员常常需要通过查看日志来定位问题。如果应用是集群化部署，试想如果登录一台台服务器去查看日志，是多么费时费力。\n\n而通过 elk 这套解决方案，可以同时实现日志收集、日志搜索和日志分析的功能。\n\n\n# 1.3. elastic stack 架构\n\n\n\n> 说明\n> \n> 以上是 elastic stack 的一个架构图。从图中可以清楚的看到数据流向。\n> \n>  * beats 是单一用途的数据传输平台，它可以将多台机器的数据发送到 logstash 或 elasticsearch。但 beats 并不是不可或缺的一环，所以本文中暂不介绍。\n> \n>  * logstash 是一个动态数据收集管道。支持以 tcp/udp/http 多种方式收集数据（也可以接受 beats 传输来的数据），并对数据做进一步丰富或提取字段处理。\n> \n>  * elasticsearch 是一个基于 json 的分布式的搜索和分析引擎。作为 elk 的核心，它集中存储数据。\n> \n>  * kibana 是 elk 的用户界面。它将收集的数据进行可视化展示（各种报表、图形化数据），并提供配置、管理 elk 的界面。\n\n\n# 2. elasticsearch\n\n> elasticsearch 是一个分布式、restful 风格的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 作为 elastic stack 的核心，它集中存储您的数据，帮助您发现意料之中以及意料之外的情况。\n\n\n# 2.1. elasticsearch 简介\n\nelasticsearch 基于搜索库 lucene 开发。elasticsearch 隐藏了 lucene 的复杂性，提供了简单易用的 rest api / java api 接口（另外还有其他语言的 api 接口）。\n\nelasticsearch 可以视为一个文档存储，它将复杂数据结构序列化为 json 存储。\n\nelasticsearch 是近乎于实时的全文搜素，这是指：\n\n * 从写入数据到数据可以被搜索，存在较小的延迟（大概是 1s）\n * 基于 es 执行搜索和分析可以达到秒级\n\n# 2.1.1. 核心概念\n\n * 索引（index） 可以认为是文档（document）的优化集合。\n * 每个 文档（document） 都是字段（field）的集合。\n * 字段（field） 是包含数据的键值对。\n * 默认情况下，elasticsearch 对每个字段中的所有数据建立索引，并且每个索引字段都具有专用的优化数据结构。\n * 每个索引里可以有一个或者多个类型（type）。类型（type） 是 index 的一个逻辑分类，\n * 当单台机器不足以存储大量数据时，elasticsearch 可以将一个索引中的数据切分为多个 分片（shard） 。 分片（shard） 分布在多台服务器上存储。有了 shard 就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个 shard 都是一个 lucene index。\n * 任何一个服务器随时可能故障或宕机，此时 shard 可能就会丢失，因此可以为每个 shard 创建多个 副本（replica）。replica 可以在 shard 故障时提供备用服务，保证数据不丢失，多个 replica 还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认 5 个），replica shard（随时修改数量，默认 1 个），默认每个索引 10 个 shard，5 个 primary shard，5 个 replica shard，最小的高可用配置，是 2 台服务器。\n\n\n# 2.2. elasticsearch 原理\n\n# 2.2.1. es 写数据过程\n\n * 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。\n * coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。\n * 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。\n * coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。\n\n\n\n# 2.2.2. es 读数据过程\n\n可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。\n\n * 客户端发送请求到任意一个 node，成为 coordinate node。\n * coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。\n * 接收请求的 node 返回 document 给 coordinate node。\n * coordinate node 返回 document 给客户端。\n\n# 2.2.3. 写数据底层原理\n\n\n\n先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。\n\n如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh。\n\n每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。\n\n但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。\n\n操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。\n\n为什么叫 es 是准实时的？ nrt，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。\n\n重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。\n\ncommit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。\n\n这个 commit 操作叫做 flush。默认 30 分钟自动执行一次 flush，但如果 translog 过大，也会触发 flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。\n\ntranslog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。\n\ntranslog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。\n\n实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。\n\n总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。\n\n> 数据写入 segment file 之后，同时就建立好了倒排索引。\n\n# 2.2.4. 删除/更新数据底层原理\n\n如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。\n\n如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。\n\nbuffer 每 refresh 一次，就会产生一个 segment file，所以默认情况下是 1 秒钟一个 segment file，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point，标识所有新的 segment file，然后打开 segment file 供搜索使用，同时删除旧的 segment file。\n\n# 2.2.5. 底层 lucene\n\n简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。\n\n通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。\n\n# 2.2.6. 倒排索引\n\n在搜索引擎中，每个文档都有一个对应的文档 id，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。\n\n那么，倒排索引就是关键词到文档 id 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。\n\n举个栗子。\n\n有以下文档：\n\ndocid   doc\n1       谷歌地图之父跳槽 facebook\n2       谷歌地图之父加盟 facebook\n3       谷歌地图创始人拉斯离开谷歌加盟 facebook\n4       谷歌地图之父跳槽 facebook 与 wave 项目取消有关\n5       谷歌地图之父拉斯加盟社交网站 facebook\n\n对文档进行分词之后，得到以下倒排索引。\n\nwordid   word       docids\n1        谷歌         1,2,3,4,5\n2        地图         1,2,3,4,5\n3        之父         1,2,4,5\n4        跳槽         1,4\n5        facebook   1,2,3,4,5\n6        加盟         2,3,5\n7        创始人        3\n8        拉斯         3,5\n9        离开         3\n10       与          4\n..       ..         ..\n\n另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。\n\n那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 facebook，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。\n\n要注意倒排索引的两个重要细节：\n\n * 倒排索引中的所有词项对应一个或多个文档；\n * 倒排索引中的词项根据字典顺序升序排列\n\n> 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。\n\n\n# 3. logstash\n\n> logstash 是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。\n\n\n# 3.1. logstash 简介\n\nlogstash 可以传输和处理你的日志、事务或其他数据。\n\nlogstash 是 elasticsearch 的最佳数据管道。\n\nlogstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制。logstash 社区有超过 200 种可用插件。\n\n\n# 3.2. logstash 原理\n\nlogstash 有两个必要元素：input 和 output ，一个可选元素：filter。\n\n这三个元素，分别代表 logstash 事件处理的三个阶段：输入 > 过滤器 > 输出。\n\n\n\n * input - 负责从数据源采集数据。\n * filter - 将数据修改为你指定的格式或内容。\n * output - 将数据传输到目的地。\n\n在实际应用场景中，通常输入、输出、过滤器不止一个。logstash 的这三个元素都使用插件式管理方式，用户可以根据应用需要，灵活的选用各阶段需要的插件，并组合使用。\n\n\n# 4. beats\n\n> beats 是安装在服务器上的数据中转代理。\n> \n> beats 可以将数据直接传输到 elasticsearch 或传输到 logstash 。\n\n\n\nbeats 有多种类型，可以根据实际应用需要选择合适的类型。\n\n常用的类型有：\n\n * **packetbeat：**网络数据包分析器，提供有关您的应用程序服务器之间交换的事务的信息。\n * **filebeat：**从您的服务器发送日志文件。\n * **metricbeat：**是一个服务器监视代理程序，它定期从服务器上运行的操作系统和服务收集指标。\n * **winlogbeat：**提供 windows 事件日志。\n\n\n# 4.1. filebeat 简介\n\n> 由于本人仅接触过 filebeat，所以本文只介绍 beats 组件中的 filebeat。\n\n相比 logstash，filebeat 更加轻量化。\n\n在任何环境下，应用程序都有停机的可能性。 filebeat 读取并转发日志行，如果中断，则会记住所有事件恢复联机状态时所在位置。\n\nfilebeat 带有内部模块（auditd，apache，nginx，system 和 mysql），可通过一个指定命令来简化通用日志格式的收集，解析和可视化。\n\nfilebeat 不会让你的管道超负荷。filebeat 如果是向 logstash 传输数据，当 logstash 忙于处理数据，会通知 filebeat 放慢读取速度。一旦拥塞得到解决，filebeat 将恢复到原来的速度并继续传播。\n\n\n\n\n# 4.2. filebeat 原理\n\nfilebeat 有两个主要组件：\n\n * harvester：负责读取一个文件的内容。它会逐行读取文件内容，并将内容发送到输出目的地。\n * prospector：负责管理 harvester 并找到所有需要读取的文件源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。\n\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /var/log/*.log\n      - /var/path2/*.log\n\n\nfilebeat 保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。\n\nfilebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。\n\n\n# 5. 运维\n\n * elasticsearch 运维\n * logstash 运维\n * kibana 运维\n * beats 运维\n\n\n# 6. 参考资料\n\n * 官方资源\n   * elasticsearch 官网\n   * elasticsearch github\n   * elasticsearch 官方文档\n   * logstash 官网\n   * logstash github\n   * logstash 官方文档\n   * kibana 官网\n   * kibana github\n   * kibana 官方文档\n   * beats 官网\n   * beats github\n   * beats 官方文档\n * 文章\n   * 什么是 elk stack？\n   * https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/es-introduction.md\n   * es-write-query-search",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elastic 技术栈之 Filebeat",frontmatter:{title:"Elastic 技术栈之 Filebeat",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic","Filebeat"],permalink:"/pages/b7f079/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/02.Elastic%E6%8A%80%E6%9C%AF%E6%A0%88%E4%B9%8BFilebeat.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/02.Elastic技术栈之Filebeat.md",key:"v-ec0e4a94",path:"/pages/b7f079/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:28},{level:3,title:"FileBeat 的作用",slug:"filebeat-的作用",normalizedTitle:"filebeat 的作用",charIndex:421},{level:2,title:"安装",slug:"安装",normalizedTitle:"安装",charIndex:39},{level:2,title:"配置",slug:"配置",normalizedTitle:"配置",charIndex:963},{level:3,title:"配置文件",slug:"配置文件",normalizedTitle:"配置文件",charIndex:970},{level:3,title:"重要配置项",slug:"重要配置项",normalizedTitle:"重要配置项",charIndex:1346},{level:4,title:"filebeat.prospectors",slug:"filebeat-prospectors",normalizedTitle:"filebeat.prospectors",charIndex:1355},{level:4,title:"output.elasticsearch",slug:"output-elasticsearch",normalizedTitle:"output.elasticsearch",charIndex:1494},{level:4,title:"output.logstash",slug:"output-logstash",normalizedTitle:"output.logstash",charIndex:1646},{level:4,title:"setup.kibana",slug:"setup-kibana",normalizedTitle:"setup.kibana",charIndex:2432},{level:4,title:"setup.template.settings",slug:"setup-template-settings",normalizedTitle:"setup.template.settings",charIndex:2545},{level:4,title:"setup.dashboards",slug:"setup-dashboards",normalizedTitle:"setup.dashboards",charIndex:2925},{level:2,title:"命令",slug:"命令",normalizedTitle:"命令",charIndex:589},{level:2,title:"模块",slug:"模块",normalizedTitle:"模块",charIndex:543},{level:3,title:"运行模块的步骤",slug:"运行模块的步骤",normalizedTitle:"运行模块的步骤",charIndex:3647},{level:2,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:4094},{level:2,title:"资料",slug:"资料",normalizedTitle:"资料",charIndex:4482}],headersStr:"简介 FileBeat 的作用 安装 配置 配置文件 重要配置项 filebeat.prospectors output.elasticsearch output.logstash setup.kibana setup.template.settings setup.dashboards 命令 模块 运行模块的步骤 原理 资料",content:'# Elastic 技术栈之 Filebeat\n\n\n# 简介\n\nBeats 是安装在服务器上的数据中转代理。\n\nBeats 可以将数据直接传输到 Elasticsearch 或传输到 Logstash 。\n\n\n\nBeats 有多种类型，可以根据实际应用需要选择合适的类型。\n\n常用的类型有：\n\n * **Packetbeat：**网络数据包分析器，提供有关您的应用程序服务器之间交换的事务的信息。\n * **Filebeat：**从您的服务器发送日志文件。\n * **Metricbeat：**是一个服务器监视代理程序，它定期从服务器上运行的操作系统和服务收集指标。\n * **Winlogbeat：**提供 Windows 事件日志。\n\n> 参考\n> \n> 更多 Beats 类型可以参考：community-beats\n> \n> 说明\n> \n> 由于本人工作中只应用了 FileBeat，所以后面内容仅介绍 FileBeat 。\n\n\n# FileBeat 的作用\n\n相比 Logstash，FileBeat 更加轻量化。\n\n在任何环境下，应用程序都有停机的可能性。 Filebeat 读取并转发日志行，如果中断，则会记住所有事件恢复联机状态时所在位置。\n\nFilebeat 带有内部模块（auditd，Apache，Nginx，System 和 MySQL），可通过一个指定命令来简化通用日志格式的收集，解析和可视化。\n\nFileBeat 不会让你的管道超负荷。FileBeat 如果是向 Logstash 传输数据，当 Logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。\n\n\n\n\n# 安装\n\nUnix / Linux 系统建议使用下面方式安装，因为比较通用。\n\nwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gz\ntar -zxf filebeat-6.1.1-linux-x86_64.tar.gz\n\n\n> 参考\n> \n> 更多内容可以参考：filebeat-installation\n\n\n# 配置\n\n\n# 配置文件\n\n首先，需要知道的是：filebeat.yml 是 filebeat 的配置文件。配置文件的路径会因为你安装方式的不同而变化。\n\nBeat 所有系列产品的配置文件都基于 YAML 格式，FileBeat 当然也不例外。\n\nfilebeat.yml 部分配置示例：\n\nfilebeat:\n  prospectors:\n    - type: log\n      paths:\n        - /var/log/*.log\n      multiline:\n        pattern: \'^[\'\n        match: after\n\n\n> 参考\n> \n> 更多 filebeat 配置内容可以参考：配置 filebeat\n> \n> 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式\n\n\n# 重要配置项\n\n# filebeat.prospectors\n\n（文件监视器）用于指定需要关注的文件。\n\n示例\n\nfilebeat.prospectors:\n  - type: log\n    enabled: true\n    paths:\n      - /var/log/*.log\n\n\n# output.elasticsearch\n\n如果你希望使用 filebeat 直接向 elasticsearch 输出数据，需要配置 output.elasticsearch 。\n\n示例\n\noutput.elasticsearch:\n  hosts: [\'192.168.1.42:9200\']\n\n\n# output.logstash\n\n如果你希望使用 filebeat 向 logstash 输出数据，然后由 logstash 再向 elasticsearch 输出数据，需要配置 output.logstash。\n\n> 注意\n> \n> 相比于向 elasticsearch 输出数据，个人更推荐向 logstash 输出数据。\n> \n> 因为 logstash 和 filebeat 一起工作时，如果 logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。这样，可以减少管道超负荷的情况。\n\n示例\n\noutput.logstash:\n  hosts: [\'127.0.0.1:5044\']\n\n\n此外，还需要在 logstash 的配置文件（如 logstash.conf）中指定 beats input 插件：\n\ninput {\n  beats {\n    port => 5044 # 此端口需要与 filebeat.yml 中的端口相同\n  }\n}\n\n# The filter part of this file is commented out to indicate that it is\n# optional.\n# filter {\n#\n# }\n\noutput {\n  elasticsearch {\n    hosts => "localhost:9200"\n    manage_template => false\n    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"\n    document_type => "%{[@metadata][type]}"\n  }\n}\n\n\n# setup.kibana\n\n如果打算使用 Filebeat 提供的 Kibana 仪表板，需要配置 setup.kibana 。\n\n示例\n\nsetup.kibana:\n  host: \'localhost:5601\'\n\n\n# setup.template.settings\n\n在 Elasticsearch 中，索引模板用于定义设置和映射，以确定如何分析字段。\n\n在 Filebeat 中，setup.template.settings 用于配置索引模板。\n\nFilebeat 推荐的索引模板文件由 Filebeat 软件包安装。如果您接受 filebeat.yml 配置文件中的默认配置，Filebeat 在成功连接到 Elasticsearch 后自动加载模板。\n\n您可以通过在 Filebeat 配置文件中配置模板加载选项来禁用自动模板加载，或加载自己的模板。您还可以设置选项来更改索引和索引模板的名称。\n\n> 参考\n> \n> 更多内容可以参考：filebeat-template\n> \n> 说明\n> \n> 如无必要，使用 Filebeat 配置文件中的默认索引模板即可。\n\n# setup.dashboards\n\nFilebeat 附带了示例 Kibana 仪表板。在使用仪表板之前，您需要创建索引模式 filebeat- *，并将仪表板加载到 Kibana 中。为此，您可以运行 setup 命令或在 filebeat.yml 配置文件中配置仪表板加载。\n\n为了在 Kibana 中加载 Filebeat 的仪表盘，需要在 filebeat.yml 配置中启动开关：\n\nsetup.dashboards.enabled: true\n\n\n> 参考\n> \n> 更多内容可以参考：configuration-dashboards\n\n\n# 命令\n\nfilebeat 提供了一系列命令来完成各种功能。\n\n执行命令方式：\n\n./filebeat COMMAND\n\n\n> 参考\n> \n> 更多内容可以参考：command-line-options\n> \n> 说明\n> \n> 个人认为命令行没有必要一一掌握，因为绝大部分功能都可以通过配置来完成。且通过命令行指定功能这种方式要求每次输入同样参数，不利于固化启动方式。\n> \n> 最重要的当然是启动命令 run 了。\n> \n> 示例 指定配置文件启动\n> \n> ./filebeat run -e -c filebeat.yml -d "publish"\n> ./filebeat -e -c filebeat.yml -d "publish" # run 可以省略\n\n\n# 模块\n\nFilebeat 提供了一套预构建的模块，让您可以快速实施和部署日志监视解决方案，并附带示例仪表板和数据可视化。这些模块支持常见的日志格式，例如 Nginx，Apache2 和 MySQL 等。\n\n\n# 运行模块的步骤\n\n * 配置 elasticsearch 和 kibana\n\noutput.elasticsearch:\n  hosts: ["myEShost:9200"]\n  username: "elastic"\n  password: "elastic"\nsetup.kibana:\n  host: "mykibanahost:5601"\n  username: "elastic"\n  password: "elastic\n\n\n> username 和 password 是可选的，如果不需要认证则不填。\n\n * 初始化环境\n\n执行下面命令，filebeat 会加载推荐索引模板。\n\n./filebeat setup -e\n\n\n * 指定模块\n\n执行下面命令，指定希望加载的模块。\n\n./filebeat -e --modules system,nginx,mysql\n\n\n> 参考\n> \n> 更多内容可以参考： 配置 filebeat 模块 | filebeat 支持模块\n\n\n# 原理\n\nFilebeat 有两个主要组件：\n\nharvester：负责读取一个文件的内容。它会逐行读取文件内容，并将内容发送到输出目的地。\n\nprospector：负责管理 harvester 并找到所有需要读取的文件源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。\n\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /var/log/*.log\n      - /var/path2/*.log\n\n\nFilebeat 保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。\n\nFilebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。\n\n\n# 资料\n\nBeats 官方文档',normalizedContent:'# elastic 技术栈之 filebeat\n\n\n# 简介\n\nbeats 是安装在服务器上的数据中转代理。\n\nbeats 可以将数据直接传输到 elasticsearch 或传输到 logstash 。\n\n\n\nbeats 有多种类型，可以根据实际应用需要选择合适的类型。\n\n常用的类型有：\n\n * **packetbeat：**网络数据包分析器，提供有关您的应用程序服务器之间交换的事务的信息。\n * **filebeat：**从您的服务器发送日志文件。\n * **metricbeat：**是一个服务器监视代理程序，它定期从服务器上运行的操作系统和服务收集指标。\n * **winlogbeat：**提供 windows 事件日志。\n\n> 参考\n> \n> 更多 beats 类型可以参考：community-beats\n> \n> 说明\n> \n> 由于本人工作中只应用了 filebeat，所以后面内容仅介绍 filebeat 。\n\n\n# filebeat 的作用\n\n相比 logstash，filebeat 更加轻量化。\n\n在任何环境下，应用程序都有停机的可能性。 filebeat 读取并转发日志行，如果中断，则会记住所有事件恢复联机状态时所在位置。\n\nfilebeat 带有内部模块（auditd，apache，nginx，system 和 mysql），可通过一个指定命令来简化通用日志格式的收集，解析和可视化。\n\nfilebeat 不会让你的管道超负荷。filebeat 如果是向 logstash 传输数据，当 logstash 忙于处理数据，会通知 filebeat 放慢读取速度。一旦拥塞得到解决，filebeat 将恢复到原来的速度并继续传播。\n\n\n\n\n# 安装\n\nunix / linux 系统建议使用下面方式安装，因为比较通用。\n\nwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gz\ntar -zxf filebeat-6.1.1-linux-x86_64.tar.gz\n\n\n> 参考\n> \n> 更多内容可以参考：filebeat-installation\n\n\n# 配置\n\n\n# 配置文件\n\n首先，需要知道的是：filebeat.yml 是 filebeat 的配置文件。配置文件的路径会因为你安装方式的不同而变化。\n\nbeat 所有系列产品的配置文件都基于 yaml 格式，filebeat 当然也不例外。\n\nfilebeat.yml 部分配置示例：\n\nfilebeat:\n  prospectors:\n    - type: log\n      paths:\n        - /var/log/*.log\n      multiline:\n        pattern: \'^[\'\n        match: after\n\n\n> 参考\n> \n> 更多 filebeat 配置内容可以参考：配置 filebeat\n> \n> 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式\n\n\n# 重要配置项\n\n# filebeat.prospectors\n\n（文件监视器）用于指定需要关注的文件。\n\n示例\n\nfilebeat.prospectors:\n  - type: log\n    enabled: true\n    paths:\n      - /var/log/*.log\n\n\n# output.elasticsearch\n\n如果你希望使用 filebeat 直接向 elasticsearch 输出数据，需要配置 output.elasticsearch 。\n\n示例\n\noutput.elasticsearch:\n  hosts: [\'192.168.1.42:9200\']\n\n\n# output.logstash\n\n如果你希望使用 filebeat 向 logstash 输出数据，然后由 logstash 再向 elasticsearch 输出数据，需要配置 output.logstash。\n\n> 注意\n> \n> 相比于向 elasticsearch 输出数据，个人更推荐向 logstash 输出数据。\n> \n> 因为 logstash 和 filebeat 一起工作时，如果 logstash 忙于处理数据，会通知 filebeat 放慢读取速度。一旦拥塞得到解决，filebeat 将恢复到原来的速度并继续传播。这样，可以减少管道超负荷的情况。\n\n示例\n\noutput.logstash:\n  hosts: [\'127.0.0.1:5044\']\n\n\n此外，还需要在 logstash 的配置文件（如 logstash.conf）中指定 beats input 插件：\n\ninput {\n  beats {\n    port => 5044 # 此端口需要与 filebeat.yml 中的端口相同\n  }\n}\n\n# the filter part of this file is commented out to indicate that it is\n# optional.\n# filter {\n#\n# }\n\noutput {\n  elasticsearch {\n    hosts => "localhost:9200"\n    manage_template => false\n    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+yyyy.mm.dd}"\n    document_type => "%{[@metadata][type]}"\n  }\n}\n\n\n# setup.kibana\n\n如果打算使用 filebeat 提供的 kibana 仪表板，需要配置 setup.kibana 。\n\n示例\n\nsetup.kibana:\n  host: \'localhost:5601\'\n\n\n# setup.template.settings\n\n在 elasticsearch 中，索引模板用于定义设置和映射，以确定如何分析字段。\n\n在 filebeat 中，setup.template.settings 用于配置索引模板。\n\nfilebeat 推荐的索引模板文件由 filebeat 软件包安装。如果您接受 filebeat.yml 配置文件中的默认配置，filebeat 在成功连接到 elasticsearch 后自动加载模板。\n\n您可以通过在 filebeat 配置文件中配置模板加载选项来禁用自动模板加载，或加载自己的模板。您还可以设置选项来更改索引和索引模板的名称。\n\n> 参考\n> \n> 更多内容可以参考：filebeat-template\n> \n> 说明\n> \n> 如无必要，使用 filebeat 配置文件中的默认索引模板即可。\n\n# setup.dashboards\n\nfilebeat 附带了示例 kibana 仪表板。在使用仪表板之前，您需要创建索引模式 filebeat- *，并将仪表板加载到 kibana 中。为此，您可以运行 setup 命令或在 filebeat.yml 配置文件中配置仪表板加载。\n\n为了在 kibana 中加载 filebeat 的仪表盘，需要在 filebeat.yml 配置中启动开关：\n\nsetup.dashboards.enabled: true\n\n\n> 参考\n> \n> 更多内容可以参考：configuration-dashboards\n\n\n# 命令\n\nfilebeat 提供了一系列命令来完成各种功能。\n\n执行命令方式：\n\n./filebeat command\n\n\n> 参考\n> \n> 更多内容可以参考：command-line-options\n> \n> 说明\n> \n> 个人认为命令行没有必要一一掌握，因为绝大部分功能都可以通过配置来完成。且通过命令行指定功能这种方式要求每次输入同样参数，不利于固化启动方式。\n> \n> 最重要的当然是启动命令 run 了。\n> \n> 示例 指定配置文件启动\n> \n> ./filebeat run -e -c filebeat.yml -d "publish"\n> ./filebeat -e -c filebeat.yml -d "publish" # run 可以省略\n\n\n# 模块\n\nfilebeat 提供了一套预构建的模块，让您可以快速实施和部署日志监视解决方案，并附带示例仪表板和数据可视化。这些模块支持常见的日志格式，例如 nginx，apache2 和 mysql 等。\n\n\n# 运行模块的步骤\n\n * 配置 elasticsearch 和 kibana\n\noutput.elasticsearch:\n  hosts: ["myeshost:9200"]\n  username: "elastic"\n  password: "elastic"\nsetup.kibana:\n  host: "mykibanahost:5601"\n  username: "elastic"\n  password: "elastic\n\n\n> username 和 password 是可选的，如果不需要认证则不填。\n\n * 初始化环境\n\n执行下面命令，filebeat 会加载推荐索引模板。\n\n./filebeat setup -e\n\n\n * 指定模块\n\n执行下面命令，指定希望加载的模块。\n\n./filebeat -e --modules system,nginx,mysql\n\n\n> 参考\n> \n> 更多内容可以参考： 配置 filebeat 模块 | filebeat 支持模块\n\n\n# 原理\n\nfilebeat 有两个主要组件：\n\nharvester：负责读取一个文件的内容。它会逐行读取文件内容，并将内容发送到输出目的地。\n\nprospector：负责管理 harvester 并找到所有需要读取的文件源。比如类型是日志，prospector 就会遍历制定路径下的所有匹配要求的文件。\n\nfilebeat.prospectors:\n  - type: log\n    paths:\n      - /var/log/*.log\n      - /var/path2/*.log\n\n\nfilebeat 保持每个文件的状态，并经常刷新注册表文件中的磁盘状态。状态用于记住 harvester 正在读取的最后偏移量，并确保发送所有日志行。\n\nfilebeat 将每个事件的传递状态存储在注册表文件中。所以它能保证事件至少传递一次到配置的输出，没有数据丢失。\n\n\n# 资料\n\nbeats 官方文档',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Filebeat 运维",frontmatter:{title:"Filebeat 运维",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic","Filebeat"],permalink:"/pages/7c067f/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/03.Filebeat%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/03.Filebeat运维.md",key:"v-15d3f40c",path:"/pages/7c067f/",headers:[{level:2,title:"1. Filebeat 安装",slug:"_1-filebeat-安装",normalizedTitle:"1. filebeat 安装",charIndex:141},{level:3,title:"1.1. 环境要求",slug:"_1-1-环境要求",normalizedTitle:"1.1. 环境要求",charIndex:160},{level:3,title:"1.2. 安装步骤",slug:"_1-2-安装步骤",normalizedTitle:"1.2. 安装步骤",charIndex:198},{level:2,title:"2. Filebeat 配置",slug:"_2-filebeat-配置",normalizedTitle:"2. filebeat 配置",charIndex:421},{level:3,title:"2.1. 重要配置项",slug:"_2-1-重要配置项",normalizedTitle:"2.1. 重要配置项",charIndex:803},{level:4,title:"2.1.1. filebeat.prospectors",slug:"_2-1-1-filebeat-prospectors",normalizedTitle:"2.1.1. filebeat.prospectors",charIndex:958},{level:4,title:"2.1.2. output.elasticsearch",slug:"_2-1-2-output-elasticsearch",normalizedTitle:"2.1.2. output.elasticsearch",charIndex:1104},{level:4,title:"2.1.3. output.logstash",slug:"_2-1-3-output-logstash",normalizedTitle:"2.1.3. output.logstash",charIndex:1263},{level:4,title:"2.1.4. setup.kibana",slug:"_2-1-4-setup-kibana",normalizedTitle:"2.1.4. setup.kibana",charIndex:2056},{level:4,title:"2.1.5. setup.template.settings",slug:"_2-1-5-setup-template-settings",normalizedTitle:"2.1.5. setup.template.settings",charIndex:2176},{level:4,title:"2.1.6. setup.dashboards",slug:"_2-1-6-setup-dashboards",normalizedTitle:"2.1.6. setup.dashboards",charIndex:2563},{level:2,title:"3. Filebeat 命令",slug:"_3-filebeat-命令",normalizedTitle:"3. filebeat 命令",charIndex:2848},{level:2,title:"4. Filebeat 模块",slug:"_4-filebeat-模块",normalizedTitle:"4. filebeat 模块",charIndex:3198},{level:2,title:"5. 参考资料",slug:"_5-参考资料",normalizedTitle:"5. 参考资料",charIndex:3757}],headersStr:"1. Filebeat 安装 1.1. 环境要求 1.2. 安装步骤 2. Filebeat 配置 2.1. 重要配置项 2.1.1. filebeat.prospectors 2.1.2. output.elasticsearch 2.1.3. output.logstash 2.1.4. setup.kibana 2.1.5. setup.template.settings 2.1.6. setup.dashboards 3. Filebeat 命令 4. Filebeat 模块 5. 参考资料",content:'# Filebeat 运维\n\n> Beats 平台集合了多种单一用途数据采集器。它们从成百上千或成千上万台机器和系统向 Logstash 或 Elasticsearch 发送数据。\n> \n> 因为我只接触过 Filebeat，所有本文仅介绍 Filebeat 的日常运维。\n\n\n# 1. Filebeat 安装\n\n\n# 1.1. 环境要求\n\n> 版本：Elastic Stack 7.4\n\n\n# 1.2. 安装步骤\n\nUnix / Linux 系统建议使用下面方式安装，因为比较通用。\n\nwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gz\ntar -zxf filebeat-6.1.1-linux-x86_64.tar.gz\n\n\n> 更多内容可以参考：filebeat-installation\n\n\n# 2. Filebeat 配置\n\n> 首先，必须要知道的是：filebeat.yml 是 filebeat 的配置文件。其路径会因为你安装方式而有所不同。\n> \n> Beat 所有系列产品的配置文件都基于 YAML 格式，FileBeat 当然也不例外。\n> \n> 更多 filebeat 配置内容可以参考：配置 filebeat\n> \n> 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式\n\nfilebeat.yml 部分配置示例：\n\nfilebeat:\n  prospectors:\n    - type: log\n      paths:\n        - /var/log/*.log\n      multiline:\n        pattern: \'^[\'\n        match: after\n\n\n\n# 2.1. 重要配置项\n\n> 下面我将列举 Filebeat 的较为重要的配置项。\n> \n> 如果想了解更多配置信息，可以参考：\n> \n> 更多 filebeat 配置内容可以参考：配置 filebeat\n> \n> 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式\n\n# 2.1.1. filebeat.prospectors\n\n（文件监视器）用于指定需要关注的文件。\n\n示例\n\nfilebeat.prospectors:\n  - type: log\n    enabled: true\n    paths:\n      - /var/log/*.log\n\n\n# 2.1.2. output.elasticsearch\n\n如果你希望使用 filebeat 直接向 elasticsearch 输出数据，需要配置 output.elasticsearch 。\n\n示例\n\noutput.elasticsearch:\n  hosts: [\'192.168.1.42:9200\']\n\n\n# 2.1.3. output.logstash\n\n如果你希望使用 filebeat 向 logstash 输出数据，然后由 logstash 再向 elasticsearch 输出数据，需要配置 output.logstash。\n\n> 注意\n> \n> 相比于向 elasticsearch 输出数据，个人更推荐向 logstash 输出数据。\n> \n> 因为 logstash 和 filebeat 一起工作时，如果 logstash 忙于处理数据，会通知 FileBeat 放慢读取速度。一旦拥塞得到解决，FileBeat 将恢复到原来的速度并继续传播。这样，可以减少管道超负荷的情况。\n\n示例\n\noutput.logstash:\n  hosts: [\'127.0.0.1:5044\']\n\n\n此外，还需要在 logstash 的配置文件（如 logstash.conf）中指定 beats input 插件：\n\ninput {\n  beats {\n    port => 5044 # 此端口需要与 filebeat.yml 中的端口相同\n  }\n}\n\n# The filter part of this file is commented out to indicate that it is\n# optional.\n# filter {\n#\n# }\n\noutput {\n  elasticsearch {\n    hosts => "localhost:9200"\n    manage_template => false\n    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"\n    document_type => "%{[@metadata][type]}"\n  }\n}\n\n\n# 2.1.4. setup.kibana\n\n如果打算使用 Filebeat 提供的 Kibana 仪表板，需要配置 setup.kibana 。\n\n示例\n\nsetup.kibana:\n  host: \'localhost:5601\'\n\n\n# 2.1.5. setup.template.settings\n\n在 Elasticsearch 中，索引模板用于定义设置和映射，以确定如何分析字段。\n\n在 Filebeat 中，setup.template.settings 用于配置索引模板。\n\nFilebeat 推荐的索引模板文件由 Filebeat 软件包安装。如果您接受 filebeat.yml 配置文件中的默认配置，Filebeat 在成功连接到 Elasticsearch 后自动加载模板。\n\n您可以通过在 Filebeat 配置文件中配置模板加载选项来禁用自动模板加载，或加载自己的模板。您还可以设置选项来更改索引和索引模板的名称。\n\n> 参考\n> \n> 更多内容可以参考：filebeat-template\n> \n> 说明\n> \n> 如无必要，使用 Filebeat 配置文件中的默认索引模板即可。\n\n# 2.1.6. setup.dashboards\n\nFilebeat 附带了示例 Kibana 仪表板。在使用仪表板之前，您需要创建索引模式 filebeat- *，并将仪表板加载到 Kibana 中。为此，您可以运行 setup 命令或在 filebeat.yml 配置文件中配置仪表板加载。\n\n为了在 Kibana 中加载 Filebeat 的仪表盘，需要在 filebeat.yml 配置中启动开关：\n\nsetup.dashboards.enabled: true\n\n\n> 参考\n> \n> 更多内容可以参考：configuration-dashboards\n\n\n# 3. Filebeat 命令\n\nfilebeat 提供了一系列命令来完成各种功能。\n\n执行命令方式：\n\n./filebeat COMMAND\n\n\n> 参考\n> \n> 更多内容可以参考：command-line-options\n> \n> 说明\n> \n> 个人认为命令行没有必要一一掌握，因为绝大部分功能都可以通过配置来完成。且通过命令行指定功能这种方式要求每次输入同样参数，不利于固化启动方式。\n> \n> 最重要的当然是启动命令 run 了。\n> \n> 示例 指定配置文件启动\n> \n> ./filebeat run -e -c filebeat.yml -d "publish"\n> ./filebeat -e -c filebeat.yml -d "publish" # run 可以省略\n\n\n# 4. Filebeat 模块\n\n> Filebeat 和 Metricbeat 内部集成了一系列模块，用以简化常见日志格式（例如 NGINX、Apache 或诸如 Redis 或 Docker 等系统指标）的收集、解析和可视化过程。\n\n * 配置 elasticsearch 和 kibana\n\noutput.elasticsearch:\n  hosts: ["myEShost:9200"]\n  username: "elastic"\n  password: "elastic"\nsetup.kibana:\n  host: "mykibanahost:5601"\n  username: "elastic"\n  password: "elastic\n\n\n> username 和 password 是可选的，如果不需要认证则不填。\n\n * 初始化环境\n\n执行下面命令，filebeat 会加载推荐索引模板。\n\n./filebeat setup -e\n\n\n * 指定模块\n\n执行下面命令，指定希望加载的模块。\n\n./filebeat -e --modules system,nginx,mysql\n\n\n> 更多内容可以参考：\n> \n>  * 配置 filebeat 模块\n>  * filebeat 支持模块\n\n\n# 5. 参考资料\n\n * Beats 官网\n * Beats Github\n * Beats 官方文档',normalizedContent:'# filebeat 运维\n\n> beats 平台集合了多种单一用途数据采集器。它们从成百上千或成千上万台机器和系统向 logstash 或 elasticsearch 发送数据。\n> \n> 因为我只接触过 filebeat，所有本文仅介绍 filebeat 的日常运维。\n\n\n# 1. filebeat 安装\n\n\n# 1.1. 环境要求\n\n> 版本：elastic stack 7.4\n\n\n# 1.2. 安装步骤\n\nunix / linux 系统建议使用下面方式安装，因为比较通用。\n\nwget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.1.1-linux-x86_64.tar.gz\ntar -zxf filebeat-6.1.1-linux-x86_64.tar.gz\n\n\n> 更多内容可以参考：filebeat-installation\n\n\n# 2. filebeat 配置\n\n> 首先，必须要知道的是：filebeat.yml 是 filebeat 的配置文件。其路径会因为你安装方式而有所不同。\n> \n> beat 所有系列产品的配置文件都基于 yaml 格式，filebeat 当然也不例外。\n> \n> 更多 filebeat 配置内容可以参考：配置 filebeat\n> \n> 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式\n\nfilebeat.yml 部分配置示例：\n\nfilebeat:\n  prospectors:\n    - type: log\n      paths:\n        - /var/log/*.log\n      multiline:\n        pattern: \'^[\'\n        match: after\n\n\n\n# 2.1. 重要配置项\n\n> 下面我将列举 filebeat 的较为重要的配置项。\n> \n> 如果想了解更多配置信息，可以参考：\n> \n> 更多 filebeat 配置内容可以参考：配置 filebeat\n> \n> 更多 filebeat.yml 文件格式内容可以参考：filebeat.yml 文件格式\n\n# 2.1.1. filebeat.prospectors\n\n（文件监视器）用于指定需要关注的文件。\n\n示例\n\nfilebeat.prospectors:\n  - type: log\n    enabled: true\n    paths:\n      - /var/log/*.log\n\n\n# 2.1.2. output.elasticsearch\n\n如果你希望使用 filebeat 直接向 elasticsearch 输出数据，需要配置 output.elasticsearch 。\n\n示例\n\noutput.elasticsearch:\n  hosts: [\'192.168.1.42:9200\']\n\n\n# 2.1.3. output.logstash\n\n如果你希望使用 filebeat 向 logstash 输出数据，然后由 logstash 再向 elasticsearch 输出数据，需要配置 output.logstash。\n\n> 注意\n> \n> 相比于向 elasticsearch 输出数据，个人更推荐向 logstash 输出数据。\n> \n> 因为 logstash 和 filebeat 一起工作时，如果 logstash 忙于处理数据，会通知 filebeat 放慢读取速度。一旦拥塞得到解决，filebeat 将恢复到原来的速度并继续传播。这样，可以减少管道超负荷的情况。\n\n示例\n\noutput.logstash:\n  hosts: [\'127.0.0.1:5044\']\n\n\n此外，还需要在 logstash 的配置文件（如 logstash.conf）中指定 beats input 插件：\n\ninput {\n  beats {\n    port => 5044 # 此端口需要与 filebeat.yml 中的端口相同\n  }\n}\n\n# the filter part of this file is commented out to indicate that it is\n# optional.\n# filter {\n#\n# }\n\noutput {\n  elasticsearch {\n    hosts => "localhost:9200"\n    manage_template => false\n    index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+yyyy.mm.dd}"\n    document_type => "%{[@metadata][type]}"\n  }\n}\n\n\n# 2.1.4. setup.kibana\n\n如果打算使用 filebeat 提供的 kibana 仪表板，需要配置 setup.kibana 。\n\n示例\n\nsetup.kibana:\n  host: \'localhost:5601\'\n\n\n# 2.1.5. setup.template.settings\n\n在 elasticsearch 中，索引模板用于定义设置和映射，以确定如何分析字段。\n\n在 filebeat 中，setup.template.settings 用于配置索引模板。\n\nfilebeat 推荐的索引模板文件由 filebeat 软件包安装。如果您接受 filebeat.yml 配置文件中的默认配置，filebeat 在成功连接到 elasticsearch 后自动加载模板。\n\n您可以通过在 filebeat 配置文件中配置模板加载选项来禁用自动模板加载，或加载自己的模板。您还可以设置选项来更改索引和索引模板的名称。\n\n> 参考\n> \n> 更多内容可以参考：filebeat-template\n> \n> 说明\n> \n> 如无必要，使用 filebeat 配置文件中的默认索引模板即可。\n\n# 2.1.6. setup.dashboards\n\nfilebeat 附带了示例 kibana 仪表板。在使用仪表板之前，您需要创建索引模式 filebeat- *，并将仪表板加载到 kibana 中。为此，您可以运行 setup 命令或在 filebeat.yml 配置文件中配置仪表板加载。\n\n为了在 kibana 中加载 filebeat 的仪表盘，需要在 filebeat.yml 配置中启动开关：\n\nsetup.dashboards.enabled: true\n\n\n> 参考\n> \n> 更多内容可以参考：configuration-dashboards\n\n\n# 3. filebeat 命令\n\nfilebeat 提供了一系列命令来完成各种功能。\n\n执行命令方式：\n\n./filebeat command\n\n\n> 参考\n> \n> 更多内容可以参考：command-line-options\n> \n> 说明\n> \n> 个人认为命令行没有必要一一掌握，因为绝大部分功能都可以通过配置来完成。且通过命令行指定功能这种方式要求每次输入同样参数，不利于固化启动方式。\n> \n> 最重要的当然是启动命令 run 了。\n> \n> 示例 指定配置文件启动\n> \n> ./filebeat run -e -c filebeat.yml -d "publish"\n> ./filebeat -e -c filebeat.yml -d "publish" # run 可以省略\n\n\n# 4. filebeat 模块\n\n> filebeat 和 metricbeat 内部集成了一系列模块，用以简化常见日志格式（例如 nginx、apache 或诸如 redis 或 docker 等系统指标）的收集、解析和可视化过程。\n\n * 配置 elasticsearch 和 kibana\n\noutput.elasticsearch:\n  hosts: ["myeshost:9200"]\n  username: "elastic"\n  password: "elastic"\nsetup.kibana:\n  host: "mykibanahost:5601"\n  username: "elastic"\n  password: "elastic\n\n\n> username 和 password 是可选的，如果不需要认证则不填。\n\n * 初始化环境\n\n执行下面命令，filebeat 会加载推荐索引模板。\n\n./filebeat setup -e\n\n\n * 指定模块\n\n执行下面命令，指定希望加载的模块。\n\n./filebeat -e --modules system,nginx,mysql\n\n\n> 更多内容可以参考：\n> \n>  * 配置 filebeat 模块\n>  * filebeat 支持模块\n\n\n# 5. 参考资料\n\n * beats 官网\n * beats github\n * beats 官方文档',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elastic 技术栈之 Kibana",frontmatter:{title:"Elastic 技术栈之 Kibana",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic","Kibana"],permalink:"/pages/002159/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/04.Elastic%E6%8A%80%E6%9C%AF%E6%A0%88%E4%B9%8BKibana.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/04.Elastic技术栈之Kibana.md",key:"v-6550ec14",path:"/pages/002159/",headers:[{level:2,title:"Discover",slug:"discover",normalizedTitle:"discover",charIndex:26},{level:3,title:"查询语义",slug:"查询语义",normalizedTitle:"查询语义",charIndex:520},{level:4,title:"字段名称",slug:"字段名称",normalizedTitle:"字段名称",charIndex:311},{level:4,title:"通配符",slug:"通配符",normalizedTitle:"通配符",charIndex:1052},{level:4,title:"正则表达式",slug:"正则表达式",normalizedTitle:"正则表达式",charIndex:1463},{level:4,title:"模糊查询",slug:"模糊查询",normalizedTitle:"模糊查询",charIndex:1571},{level:4,title:"近似检索",slug:"近似检索",normalizedTitle:"近似检索",charIndex:1840},{level:4,title:"范围",slug:"范围",normalizedTitle:"范围",charIndex:2083},{level:4,title:"Boosting",slug:"boosting",normalizedTitle:"boosting",charIndex:2575},{level:4,title:"布尔操作",slug:"布尔操作",normalizedTitle:"布尔操作",charIndex:2745},{level:4,title:"分组",slug:"分组",normalizedTitle:"分组",charIndex:3162},{level:4,title:"保留字",slug:"保留字",normalizedTitle:"保留字",charIndex:3304},{level:4,title:"空查询",slug:"空查询",normalizedTitle:"空查询",charIndex:3492},{level:2,title:"Visualize",slug:"visualize",normalizedTitle:"visualize",charIndex:144},{level:3,title:"Pie",slug:"pie",normalizedTitle:"pie",charIndex:3712},{level:3,title:"Vertical Bar",slug:"vertical-bar",normalizedTitle:"vertical bar",charIndex:4083},{level:2,title:"Dashboard",slug:"dashboard",normalizedTitle:"dashboard",charIndex:4324}],headersStr:"Discover 查询语义 字段名称 通配符 正则表达式 模糊查询 近似检索 范围 Boosting 布尔操作 分组 保留字 空查询 Visualize Pie Vertical Bar Dashboard",content:'# Elastic 技术栈之 Kibana\n\n\n# Discover\n\n单击侧面导航栏中的 Discover ，可以显示 Kibana 的数据查询功能功能。\n\n\n\n在搜索栏中，您可以输入 Elasticsearch 查询条件来搜索您的数据。您可以在 Discover 页面中浏览结果并在 Visualize 页面中创建已保存搜索条件的可视化。\n\n当前索引模式显示在查询栏下方。索引模式确定提交查询时搜索哪些索引。要搜索一组不同的索引，请从下拉菜单中选择不同的模式。要添加索引模式（index pattern），请转至 Management/Kibana/Index Patterns 并单击 Add New。\n\n您可以使用字段名称和您感兴趣的值构建搜索。对于数字字段，可以使用比较运算符，如大于（>），小于（<）或等于（=）。您可以将元素与逻辑运算符 AND，OR 和 NOT 链接，全部使用大写。\n\n默认情况下，每个匹配文档都显示所有字段。要选择要显示的文档字段，请将鼠标悬停在“可用字段”列表上，然后单击要包含的每个字段旁边的添加按钮。例如，如果只添加 account_number，则显示将更改为包含五个帐号的简单列表：\n\n\n\n\n# 查询语义\n\nkibana 的搜索栏遵循 query-string-syntax 文档中所说明的查询语义。\n\n这里说明一些最基本的查询语义。\n\n查询字符串会被解析为一系列的术语和运算符。一个术语可以是一个单词（如：quick、brown）或用双引号包围的短语（如"quick brown"）。\n\n查询操作允许您自定义搜索 - 下面介绍了可用的选项。\n\n# 字段名称\n\n正如查询字符串查询中所述，将在搜索条件中搜索 default_field，但可以在查询语法中指定其他字段：\n\n例如：\n\n * 查询 status 字段中包含 active 关键字\n\nstatus:active\n\n\n * title 字段包含 quick 或 brown 关键字。如果您省略 OR 运算符，则将使用默认运算符\n\ntitle:(quick OR brown)\ntitle:(quick brown)\n\n\n * author 字段查找精确的短语 "john smith"，即精确查找。\n\nauthor:"John Smith"\n\n\n * 任意字段 book.title，book.content 或 book.date 都包含 quick 或 brown（注意我们需要如何使用 \\* 表示通配符）\n\nbook.\\*:(quick brown)\n\n\n * title 字段包含任意非 null 值\n\n_exists_:title\n\n\n# 通配符\n\nELK 提供了 ? 和 * 两个通配符。\n\n * ? 表示任意单个字符；\n * * 表示任意零个或多个字符。\n\nqu?ck bro*\n\n\n> 注意：通配符查询会使用大量的内存并且执行性能较为糟糕，所以请慎用。 > 提示：纯通配符 * 被写入 exsits 查询，从而提高了查询效率。因此，通配符 field：* 将匹配包含空值的文档，如：{“field”：“”}，但是如果字段丢失或显示将值置为 null 则不匹配，如：“field”：null} > 提示：在一个单词的开头（例如：*ing）使用通配符这种方式的查询量特别大，因为索引中的所有术语都需要检查，以防万一匹配。通过将 allow_leading_wildcard 设置为 false，可以禁用。\n\n# 正则表达式\n\n可以通过 / 将正则表达式包裹在查询字符串中进行查询\n\n例：\n\nname:/joh?n(ath[oa]n)/\n\n\n支持的正则表达式语义可以参考：Regular expression syntax\n\n# 模糊查询\n\n我们可以使用 ~ 运算符来进行模糊查询。\n\n例：\n\n假设我们实际想查询\n\nquick brown forks\n\n\n但是，由于拼写错误，我们的查询关键字变成如下情况，依然可以查到想要的结果。\n\nquikc\\~ brwn\\~ foks\\~\n\n\n这种模糊查询使用 Damerau-Levenshtein 距离来查找所有匹配最多两个更改的项。所谓的更改是指单个字符的插入，删除或替换，或者两个相邻字符的换位。\n\n默认编辑距离为 2，但编辑距离为 1 应足以捕捉所有人类拼写错误的 80％。它可以被指定为：\n\nquikc\\~1\n\n\n# 近似检索\n\n尽管短语查询（例如，john smith）期望所有的词条都是完全相同的顺序，但是近似查询允许指定的单词进一步分开或以不同的顺序排列。与模糊查询可以为单词中的字符指定最大编辑距离一样，近似搜索也允许我们指定短语中单词的最大编辑距离：\n\n例\n\n"fox quick"\\~5\n\n\n字段中的文本越接近查询字符串中指定的原始顺序，该文档就越被认为是相关的。当与上面的示例查询相比时，短语 "quick fox" 将被认为比 "quick brown fox" 更近似查询条件。\n\n# 范围\n\n可以为日期，数字或字符串字段指定范围。闭区间范围用方括号 [min TO max] 和开区间范围用花括号 {min TO max} 来指定。\n\n我们不妨来看一些示例。\n\n * 2012 年的所有日子\n\ndate:[2012-01-01 TO 2012-12-31]\n\n\n * 数字 1 到 5\n\ncount:[1 TO 5]\n\n\n * 在 alpha 和 omega 之间的标签，不包括 alpha 和 omega\n\ntag:{alpha TO omega}\n\n\n * 10 以上的数字\n\ncount:[10 TO *]\n\n\n * 2012 年以前的所有日期\n\ndate:{* TO 2012-01-01}\n\n\n此外，开区间和闭区间也可以组合使用\n\n * 数组 1 到 5，但不包括 5\n\ncount:[1 TO 5}\n\n\n一边无界的范围也可以使用以下语法：\n\nage:>10\nage:>=10\nage:<10\nage:<=10\n\n\n当然，你也可以使用 AND 运算符来得到连个查询结果的交集\n\nage:(>=10 AND <20)\nage:(+>=10 +<20)\n\n\n# Boosting\n\n使用操作符 ^ 使一个术语比另一个术语更相关。例如，如果我们想查找所有有关狐狸的文档，但我们对狐狸特别感兴趣：\n\nquick^2 fox\n\n\n默认提升值是 1，但可以是任何正浮点数。 0 到 1 之间的提升减少了相关性。\n\n增强也可以应用于短语或组：\n\n"john smith"^2   (foo bar)^4\n\n\n# 布尔操作\n\n默认情况下，只要一个词匹配，所有词都是可选的。搜索 foo bar baz 将查找包含 foo 或 bar 或 baz 中的一个或多个的任何文档。我们已经讨论了上面的default_operator，它允许你强制要求所有的项，但也有布尔运算符可以在查询字符串本身中使用，以提供更多的控制。\n\n首选的操作符是 +（此术语必须存在）和 - （此术语不得存在）。所有其他条款是可选的。例如，这个查询：\n\nquick brown +fox -news\n\n\n这条查询意味着：\n\n * fox 必须存在\n * news 必须不存在\n * quick 和 brown 是可有可无的\n\n熟悉的运算符 AND，OR 和 NOT（也写成 &&，|| 和 !）也被支持。然而，这些操作符有一定的优先级：NOT 优先于 AND，AND 优先于 OR。虽然 + 和 - 仅影响运算符右侧的术语，但 AND 和 OR 会影响左侧和右侧的术语。\n\n# 分组\n\n多个术语或子句可以用圆括号组合在一起，形成子查询\n\n(quick OR brown) AND fox\n\n\n可以使用组来定位特定的字段，或者增强子查询的结果：\n\nstatus:(active OR pending) title:(full text search)^2\n\n\n# 保留字\n\n如果你需要使用任何在你的查询本身中作为操作符的字符（而不是作为操作符），那么你应该用一个反斜杠来转义它们。例如，要搜索（1 + 1）= 2，您需要将查询写为 \\(1\\+1\\)\\=2\n\n保留字符是：+ - = && || > < ! ( ) { } [ ] ^ " ~ * ? : \\ /\n\n无法正确地转义这些特殊字符可能会导致语法错误，从而阻止您的查询运行。\n\n# 空查询\n\n如果查询字符串为空或仅包含空格，则查询将生成一个空的结果集。\n\n\n# Visualize\n\n要想使用可视化的方式展示您的数据，请单击侧面导航栏中的 Visualize。\n\nVisualize 工具使您能够以多种方式（如饼图、柱状图、曲线图、分布图等）查看数据。要开始使用，请点击蓝色的 Create a visualization 或 + 按钮。\n\n\n\n有许多可视化类型可供选择。\n\n\n\n下面，我们来看创建几个图标示例：\n\n\n# Pie\n\n您可以从保存的搜索中构建可视化文件，也可以输入新的搜索条件。要输入新的搜索条件，首先需要选择一个索引模式来指定要搜索的索引。\n\n默认搜索匹配所有文档。最初，一个“切片”包含整个饼图：\n\n\n\n要指定在图表中展示哪些数据，请使用 Elasticsearch 存储桶聚合。分组汇总只是将与您的搜索条件相匹配的文档分类到不同的分类中，也称为分组。\n\n为每个范围定义一个存储桶：\n\n 1. 单击 Split Slices。\n 2. 在 Aggregation 列表中选择 Terms。注意：这里的 Terms 是 Elk 采集数据时定义好的字段或标签。\n 3. 在 Field 列表中选择 level.keyword。\n 4. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 Save 按钮。\n\n\n# Vertical Bar\n\n我们在展示一下如何创建柱状图。\n\n 1. 点击蓝色的 Create a visualization 或 + 按钮。选择 Vertical Bar\n 2. 选择索引模式。由于您尚未定义任何 bucket ，因此您会看到一个大栏，显示与默认通配符查询匹配的文档总数。\n 3. 指定 Y 轴所代表的字段\n 4. 指定 X 轴所代表的字段\n 5. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 Save 按钮。\n\n\n# Dashboard\n\nDashboard 可以整合和共享 Visualize 集合。\n\n 1. 点击侧面导航栏中的 Dashboard。\n 2. 点击添加显示保存的可视化列表。\n 3. 点击之前保存的 Visualize，然后点击列表底部的小向上箭头关闭可视化列表。\n 4. 将鼠标悬停在可视化对象上会显示允许您编辑，移动，删除和调整可视化对象大小的容器控件。',normalizedContent:'# elastic 技术栈之 kibana\n\n\n# discover\n\n单击侧面导航栏中的 discover ，可以显示 kibana 的数据查询功能功能。\n\n\n\n在搜索栏中，您可以输入 elasticsearch 查询条件来搜索您的数据。您可以在 discover 页面中浏览结果并在 visualize 页面中创建已保存搜索条件的可视化。\n\n当前索引模式显示在查询栏下方。索引模式确定提交查询时搜索哪些索引。要搜索一组不同的索引，请从下拉菜单中选择不同的模式。要添加索引模式（index pattern），请转至 management/kibana/index patterns 并单击 add new。\n\n您可以使用字段名称和您感兴趣的值构建搜索。对于数字字段，可以使用比较运算符，如大于（>），小于（<）或等于（=）。您可以将元素与逻辑运算符 and，or 和 not 链接，全部使用大写。\n\n默认情况下，每个匹配文档都显示所有字段。要选择要显示的文档字段，请将鼠标悬停在“可用字段”列表上，然后单击要包含的每个字段旁边的添加按钮。例如，如果只添加 account_number，则显示将更改为包含五个帐号的简单列表：\n\n\n\n\n# 查询语义\n\nkibana 的搜索栏遵循 query-string-syntax 文档中所说明的查询语义。\n\n这里说明一些最基本的查询语义。\n\n查询字符串会被解析为一系列的术语和运算符。一个术语可以是一个单词（如：quick、brown）或用双引号包围的短语（如"quick brown"）。\n\n查询操作允许您自定义搜索 - 下面介绍了可用的选项。\n\n# 字段名称\n\n正如查询字符串查询中所述，将在搜索条件中搜索 default_field，但可以在查询语法中指定其他字段：\n\n例如：\n\n * 查询 status 字段中包含 active 关键字\n\nstatus:active\n\n\n * title 字段包含 quick 或 brown 关键字。如果您省略 or 运算符，则将使用默认运算符\n\ntitle:(quick or brown)\ntitle:(quick brown)\n\n\n * author 字段查找精确的短语 "john smith"，即精确查找。\n\nauthor:"john smith"\n\n\n * 任意字段 book.title，book.content 或 book.date 都包含 quick 或 brown（注意我们需要如何使用 \\* 表示通配符）\n\nbook.\\*:(quick brown)\n\n\n * title 字段包含任意非 null 值\n\n_exists_:title\n\n\n# 通配符\n\nelk 提供了 ? 和 * 两个通配符。\n\n * ? 表示任意单个字符；\n * * 表示任意零个或多个字符。\n\nqu?ck bro*\n\n\n> 注意：通配符查询会使用大量的内存并且执行性能较为糟糕，所以请慎用。 > 提示：纯通配符 * 被写入 exsits 查询，从而提高了查询效率。因此，通配符 field：* 将匹配包含空值的文档，如：{“field”：“”}，但是如果字段丢失或显示将值置为 null 则不匹配，如：“field”：null} > 提示：在一个单词的开头（例如：*ing）使用通配符这种方式的查询量特别大，因为索引中的所有术语都需要检查，以防万一匹配。通过将 allow_leading_wildcard 设置为 false，可以禁用。\n\n# 正则表达式\n\n可以通过 / 将正则表达式包裹在查询字符串中进行查询\n\n例：\n\nname:/joh?n(ath[oa]n)/\n\n\n支持的正则表达式语义可以参考：regular expression syntax\n\n# 模糊查询\n\n我们可以使用 ~ 运算符来进行模糊查询。\n\n例：\n\n假设我们实际想查询\n\nquick brown forks\n\n\n但是，由于拼写错误，我们的查询关键字变成如下情况，依然可以查到想要的结果。\n\nquikc\\~ brwn\\~ foks\\~\n\n\n这种模糊查询使用 damerau-levenshtein 距离来查找所有匹配最多两个更改的项。所谓的更改是指单个字符的插入，删除或替换，或者两个相邻字符的换位。\n\n默认编辑距离为 2，但编辑距离为 1 应足以捕捉所有人类拼写错误的 80％。它可以被指定为：\n\nquikc\\~1\n\n\n# 近似检索\n\n尽管短语查询（例如，john smith）期望所有的词条都是完全相同的顺序，但是近似查询允许指定的单词进一步分开或以不同的顺序排列。与模糊查询可以为单词中的字符指定最大编辑距离一样，近似搜索也允许我们指定短语中单词的最大编辑距离：\n\n例\n\n"fox quick"\\~5\n\n\n字段中的文本越接近查询字符串中指定的原始顺序，该文档就越被认为是相关的。当与上面的示例查询相比时，短语 "quick fox" 将被认为比 "quick brown fox" 更近似查询条件。\n\n# 范围\n\n可以为日期，数字或字符串字段指定范围。闭区间范围用方括号 [min to max] 和开区间范围用花括号 {min to max} 来指定。\n\n我们不妨来看一些示例。\n\n * 2012 年的所有日子\n\ndate:[2012-01-01 to 2012-12-31]\n\n\n * 数字 1 到 5\n\ncount:[1 to 5]\n\n\n * 在 alpha 和 omega 之间的标签，不包括 alpha 和 omega\n\ntag:{alpha to omega}\n\n\n * 10 以上的数字\n\ncount:[10 to *]\n\n\n * 2012 年以前的所有日期\n\ndate:{* to 2012-01-01}\n\n\n此外，开区间和闭区间也可以组合使用\n\n * 数组 1 到 5，但不包括 5\n\ncount:[1 to 5}\n\n\n一边无界的范围也可以使用以下语法：\n\nage:>10\nage:>=10\nage:<10\nage:<=10\n\n\n当然，你也可以使用 and 运算符来得到连个查询结果的交集\n\nage:(>=10 and <20)\nage:(+>=10 +<20)\n\n\n# boosting\n\n使用操作符 ^ 使一个术语比另一个术语更相关。例如，如果我们想查找所有有关狐狸的文档，但我们对狐狸特别感兴趣：\n\nquick^2 fox\n\n\n默认提升值是 1，但可以是任何正浮点数。 0 到 1 之间的提升减少了相关性。\n\n增强也可以应用于短语或组：\n\n"john smith"^2   (foo bar)^4\n\n\n# 布尔操作\n\n默认情况下，只要一个词匹配，所有词都是可选的。搜索 foo bar baz 将查找包含 foo 或 bar 或 baz 中的一个或多个的任何文档。我们已经讨论了上面的default_operator，它允许你强制要求所有的项，但也有布尔运算符可以在查询字符串本身中使用，以提供更多的控制。\n\n首选的操作符是 +（此术语必须存在）和 - （此术语不得存在）。所有其他条款是可选的。例如，这个查询：\n\nquick brown +fox -news\n\n\n这条查询意味着：\n\n * fox 必须存在\n * news 必须不存在\n * quick 和 brown 是可有可无的\n\n熟悉的运算符 and，or 和 not（也写成 &&，|| 和 !）也被支持。然而，这些操作符有一定的优先级：not 优先于 and，and 优先于 or。虽然 + 和 - 仅影响运算符右侧的术语，但 and 和 or 会影响左侧和右侧的术语。\n\n# 分组\n\n多个术语或子句可以用圆括号组合在一起，形成子查询\n\n(quick or brown) and fox\n\n\n可以使用组来定位特定的字段，或者增强子查询的结果：\n\nstatus:(active or pending) title:(full text search)^2\n\n\n# 保留字\n\n如果你需要使用任何在你的查询本身中作为操作符的字符（而不是作为操作符），那么你应该用一个反斜杠来转义它们。例如，要搜索（1 + 1）= 2，您需要将查询写为 \\(1\\+1\\)\\=2\n\n保留字符是：+ - = && || > < ! ( ) { } [ ] ^ " ~ * ? : \\ /\n\n无法正确地转义这些特殊字符可能会导致语法错误，从而阻止您的查询运行。\n\n# 空查询\n\n如果查询字符串为空或仅包含空格，则查询将生成一个空的结果集。\n\n\n# visualize\n\n要想使用可视化的方式展示您的数据，请单击侧面导航栏中的 visualize。\n\nvisualize 工具使您能够以多种方式（如饼图、柱状图、曲线图、分布图等）查看数据。要开始使用，请点击蓝色的 create a visualization 或 + 按钮。\n\n\n\n有许多可视化类型可供选择。\n\n\n\n下面，我们来看创建几个图标示例：\n\n\n# pie\n\n您可以从保存的搜索中构建可视化文件，也可以输入新的搜索条件。要输入新的搜索条件，首先需要选择一个索引模式来指定要搜索的索引。\n\n默认搜索匹配所有文档。最初，一个“切片”包含整个饼图：\n\n\n\n要指定在图表中展示哪些数据，请使用 elasticsearch 存储桶聚合。分组汇总只是将与您的搜索条件相匹配的文档分类到不同的分类中，也称为分组。\n\n为每个范围定义一个存储桶：\n\n 1. 单击 split slices。\n 2. 在 aggregation 列表中选择 terms。注意：这里的 terms 是 elk 采集数据时定义好的字段或标签。\n 3. 在 field 列表中选择 level.keyword。\n 4. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 save 按钮。\n\n\n# vertical bar\n\n我们在展示一下如何创建柱状图。\n\n 1. 点击蓝色的 create a visualization 或 + 按钮。选择 vertical bar\n 2. 选择索引模式。由于您尚未定义任何 bucket ，因此您会看到一个大栏，显示与默认通配符查询匹配的文档总数。\n 3. 指定 y 轴所代表的字段\n 4. 指定 x 轴所代表的字段\n 5. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 save 按钮。\n\n\n# dashboard\n\ndashboard 可以整合和共享 visualize 集合。\n\n 1. 点击侧面导航栏中的 dashboard。\n 2. 点击添加显示保存的可视化列表。\n 3. 点击之前保存的 visualize，然后点击列表底部的小向上箭头关闭可视化列表。\n 4. 将鼠标悬停在可视化对象上会显示允许您编辑，移动，删除和调整可视化对象大小的容器控件。',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Kibana 运维",frontmatter:{title:"Kibana 运维",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic","Kibana"],permalink:"/pages/fc47af/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/05.Kibana%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/05.Kibana运维.md",key:"v-db66b0d8",path:"/pages/fc47af/",headers:[{level:2,title:"1. 安装",slug:"_1-安装",normalizedTitle:"1. 安装",charIndex:128},{level:3,title:"1.1. 环境要求",slug:"_1-1-环境要求",normalizedTitle:"1.1. 环境要求",charIndex:138},{level:3,title:"1.2. 安装步骤",slug:"_1-2-安装步骤",normalizedTitle:"1.2. 安装步骤",charIndex:176},{level:2,title:"2. 使用",slug:"_2-使用",normalizedTitle:"2. 使用",charIndex:387},{level:3,title:"2.1. 检索",slug:"_2-1-检索",normalizedTitle:"2.1. 检索",charIndex:397},{level:4,title:"2.1.1. 字段名称",slug:"_2-1-1-字段名称",normalizedTitle:"2.1.1. 字段名称",charIndex:1062},{level:4,title:"2.1.2. 通配符",slug:"_2-1-2-通配符",normalizedTitle:"2.1.2. 通配符",charIndex:1498},{level:4,title:"2.1.3. 正则表达式",slug:"_2-1-3-正则表达式",normalizedTitle:"2.1.3. 正则表达式",charIndex:1842},{level:4,title:"2.1.4. 模糊查询",slug:"_2-1-4-模糊查询",normalizedTitle:"2.1.4. 模糊查询",charIndex:1957},{level:4,title:"2.1.5. 近似检索",slug:"_2-1-5-近似检索",normalizedTitle:"2.1.5. 近似检索",charIndex:2233},{level:4,title:"2.1.6. 范围",slug:"_2-1-6-范围",normalizedTitle:"2.1.6. 范围",charIndex:2483},{level:4,title:"2.1.7. Boosting",slug:"_2-1-7-boosting",normalizedTitle:"2.1.7. boosting",charIndex:2982},{level:4,title:"2.1.8. 布尔操作",slug:"_2-1-8-布尔操作",normalizedTitle:"2.1.8. 布尔操作",charIndex:3159},{level:4,title:"2.1.9. 分组",slug:"_2-1-9-分组",normalizedTitle:"2.1.9. 分组",charIndex:3583},{level:4,title:"2.1.10. 保留字",slug:"_2-1-10-保留字",normalizedTitle:"2.1.10. 保留字",charIndex:3732},{level:4,title:"2.1.11. 空查询",slug:"_2-1-11-空查询",normalizedTitle:"2.1.11. 空查询",charIndex:3928},{level:3,title:"2.2. 可视化",slug:"_2-2-可视化",normalizedTitle:"2.2. 可视化",charIndex:3976},{level:4,title:"2.2.1. Pie",slug:"_2-2-1-pie",normalizedTitle:"2.2.1. pie",charIndex:4159},{level:4,title:"2.2.2. Vertical Bar",slug:"_2-2-2-vertical-bar",normalizedTitle:"2.2.2. vertical bar",charIndex:4536},{level:3,title:"2.3. 报表",slug:"_2-3-报表",normalizedTitle:"2.3. 报表",charIndex:4784},{level:2,title:"3. FAQ",slug:"_3-faq",normalizedTitle:"3. faq",charIndex:4971},{level:3,title:"3.1. Kibana No Default Index Pattern Warning",slug:"_3-1-kibana-no-default-index-pattern-warning",normalizedTitle:"3.1. kibana no default index pattern warning",charIndex:4982},{level:2,title:"4. 参考资料",slug:"_4-参考资料",normalizedTitle:"4. 参考资料",charIndex:5318}],headersStr:"1. 安装 1.1. 环境要求 1.2. 安装步骤 2. 使用 2.1. 检索 2.1.1. 字段名称 2.1.2. 通配符 2.1.3. 正则表达式 2.1.4. 模糊查询 2.1.5. 近似检索 2.1.6. 范围 2.1.7. Boosting 2.1.8. 布尔操作 2.1.9. 分组 2.1.10. 保留字 2.1.11. 空查询 2.2. 可视化 2.2.1. Pie 2.2.2. Vertical Bar 2.3. 报表 3. FAQ 3.1. Kibana No Default Index Pattern Warning 4. 参考资料",content:'# Kibana 运维\n\n> 通过 Kibana，您可以对自己的 Elasticsearch 进行可视化，还可以在 Elastic Stack 中进行导航，这样您便可以进行各种操作了，从跟踪查询负载，到理解请求如何流经您的整个应用，都能轻松完成。\n\n\n# 1. 安装\n\n\n# 1.1. 环境要求\n\n> 版本：Elastic Stack 7.4\n\n\n# 1.2. 安装步骤\n\n安装步骤如下：\n\n 1. 在 kibana 官方下载地址下载所需版本包并解压到本地。\n 2. 修改 config/kibana.yml 配置文件，设置 elasticsearch.url 指向 Elasticsearch 实例。\n 3. 运行 bin/kibana （Windows 上运行 bin\\kibana.bat）\n 4. 在浏览器上访问 http://localhost:5601\n\n\n# 2. 使用\n\n\n# 2.1. 检索\n\n单击侧面导航栏中的 检索（Discover） ，可以显示 Kibana 的数据查询功能功能。\n\n\n\n在搜索栏中，您可以输入 Elasticsearch 查询条件来搜索您的数据。您可以在 Discover 页面中浏览结果并在 Visualize 页面中创建已保存搜索条件的可视化。\n\n当前索引模式显示在查询栏下方。索引模式确定提交查询时搜索哪些索引。要搜索一组不同的索引，请从下拉菜单中选择不同的模式。要添加索引模式（index pattern），请转至 Management/Kibana/Index Patterns 并单击 Add New。\n\n您可以使用字段名称和您感兴趣的值构建搜索。对于数字字段，可以使用比较运算符，如大于（>），小于（<）或等于（=）。您可以将元素与逻辑运算符 AND，OR 和 NOT 链接，全部使用大写。\n\n默认情况下，每个匹配文档都显示所有字段。要选择要显示的文档字段，请将鼠标悬停在“可用字段”列表上，然后单击要包含的每个字段旁边的添加按钮。例如，如果只添加 account_number，则显示将更改为包含五个帐号的简单列表：\n\n\n\nkibana 的搜索栏遵循 query-string-syntax 文档中所说明的查询语义。\n\n这里说明一些最基本的查询语义。\n\n查询字符串会被解析为一系列的术语和运算符。一个术语可以是一个单词（如：quick、brown）或用双引号包围的短语（如"quick brown"）。\n\n查询操作允许您自定义搜索 - 下面介绍了可用的选项。\n\n# 2.1.1. 字段名称\n\n正如查询字符串查询中所述，将在搜索条件中搜索 default_field，但可以在查询语法中指定其他字段：\n\n例如：\n\n * 查询 status 字段中包含 active 关键字\n\nstatus:active\n\n\n * title 字段包含 quick 或 brown 关键字。如果您省略 OR 运算符，则将使用默认运算符\n\ntitle:(quick OR brown)\ntitle:(quick brown)\n\n\n * author 字段查找精确的短语 "john smith"，即精确查找。\n\nauthor:"John Smith"\n\n\n * 任意字段 book.title，book.content 或 book.date 都包含 quick 或 brown（注意我们需要如何使用 \\* 表示通配符）\n\nbook.\\*:(quick brown)\n\n\n * title 字段包含任意非 null 值\n\n_exists_:title\n\n\n# 2.1.2. 通配符\n\nELK 提供了 ? 和 * 两个通配符。\n\n * ? 表示任意单个字符；\n * * 表示任意零个或多个字符。\n\nqu?ck bro*\n\n\n> 注意：通配符查询会使用大量的内存并且执行性能较为糟糕，所以请慎用。 > 提示：纯通配符 * 被写入 exsits 查询，从而提高了查询效率。因此，通配符 field：* 将匹配包含空值的文档，如：{“field”：“”}，但是如果字段丢失或显示将值置为 null 则不匹配，如：“field”：null} > 提示：在一个单词的开头（例如：*ing）使用通配符这种方式的查询量特别大，因为索引中的所有术语都需要检查，以防万一匹配。通过将 allow_leading_wildcard 设置为 false，可以禁用。\n\n# 2.1.3. 正则表达式\n\n可以通过 / 将正则表达式包裹在查询字符串中进行查询\n\n例：\n\nname:/joh?n(ath[oa]n)/\n\n\n支持的正则表达式语义可以参考：Regular expression syntax\n\n# 2.1.4. 模糊查询\n\n我们可以使用 ~ 运算符来进行模糊查询。\n\n例：\n\n假设我们实际想查询\n\nquick brown forks\n\n\n但是，由于拼写错误，我们的查询关键字变成如下情况，依然可以查到想要的结果。\n\nquikc\\~ brwn\\~ foks\\~\n\n\n这种模糊查询使用 Damerau-Levenshtein 距离来查找所有匹配最多两个更改的项。所谓的更改是指单个字符的插入，删除或替换，或者两个相邻字符的换位。\n\n默认编辑距离为 2，但编辑距离为 1 应足以捕捉所有人类拼写错误的 80％。它可以被指定为：\n\nquikc\\~1\n\n\n# 2.1.5. 近似检索\n\n尽管短语查询（例如，john smith）期望所有的词条都是完全相同的顺序，但是近似查询允许指定的单词进一步分开或以不同的顺序排列。与模糊查询可以为单词中的字符指定最大编辑距离一样，近似搜索也允许我们指定短语中单词的最大编辑距离：\n\n例\n\n"fox quick"\\~5\n\n\n字段中的文本越接近查询字符串中指定的原始顺序，该文档就越被认为是相关的。当与上面的示例查询相比时，短语 "quick fox" 将被认为比 "quick brown fox" 更近似查询条件。\n\n# 2.1.6. 范围\n\n可以为日期，数字或字符串字段指定范围。闭区间范围用方括号 [min TO max] 和开区间范围用花括号 {min TO max} 来指定。\n\n我们不妨来看一些示例。\n\n * 2012 年的所有日子\n\ndate:[2012-01-01 TO 2012-12-31]\n\n\n * 数字 1 到 5\n\ncount:[1 TO 5]\n\n\n * 在 alpha 和 omega 之间的标签，不包括 alpha 和 omega\n\ntag:{alpha TO omega}\n\n\n * 10 以上的数字\n\ncount:[10 TO *]\n\n\n * 2012 年以前的所有日期\n\ndate:{* TO 2012-01-01}\n\n\n此外，开区间和闭区间也可以组合使用\n\n * 数组 1 到 5，但不包括 5\n\ncount:[1 TO 5}\n\n\n一边无界的范围也可以使用以下语法：\n\nage:>10\nage:>=10\nage:<10\nage:<=10\n\n\n当然，你也可以使用 AND 运算符来得到连个查询结果的交集\n\nage:(>=10 AND <20)\nage:(+>=10 +<20)\n\n\n# 2.1.7. Boosting\n\n使用操作符 ^ 使一个术语比另一个术语更相关。例如，如果我们想查找所有有关狐狸的文档，但我们对狐狸特别感兴趣：\n\nquick^2 fox\n\n\n默认提升值是 1，但可以是任何正浮点数。 0 到 1 之间的提升减少了相关性。\n\n增强也可以应用于短语或组：\n\n"john smith"^2   (foo bar)^4\n\n\n# 2.1.8. 布尔操作\n\n默认情况下，只要一个词匹配，所有词都是可选的。搜索 foo bar baz 将查找包含 foo 或 bar 或 baz 中的一个或多个的任何文档。我们已经讨论了上面的default_operator，它允许你强制要求所有的项，但也有布尔运算符可以在查询字符串本身中使用，以提供更多的控制。\n\n首选的操作符是 +（此术语必须存在）和 - （此术语不得存在）。所有其他条款是可选的。例如，这个查询：\n\nquick brown +fox -news\n\n\n这条查询意味着：\n\n * fox 必须存在\n * news 必须不存在\n * quick 和 brown 是可有可无的\n\n熟悉的运算符 AND，OR 和 NOT（也写成 &&，|| 和 !）也被支持。然而，这些操作符有一定的优先级：NOT 优先于 AND，AND 优先于 OR。虽然 + 和 - 仅影响运算符右侧的术语，但 AND 和 OR 会影响左侧和右侧的术语。\n\n# 2.1.9. 分组\n\n多个术语或子句可以用圆括号组合在一起，形成子查询\n\n(quick OR brown) AND fox\n\n\n可以使用组来定位特定的字段，或者增强子查询的结果：\n\nstatus:(active OR pending) title:(full text search)^2\n\n\n# 2.1.10. 保留字\n\n如果你需要使用任何在你的查询本身中作为操作符的字符（而不是作为操作符），那么你应该用一个反斜杠来转义它们。例如，要搜索（1 + 1）= 2，您需要将查询写为 \\(1\\+1\\)\\=2\n\n保留字符是：+ - = && || > < ! ( ) { } [ ] ^ " ~ * ? : \\ /\n\n无法正确地转义这些特殊字符可能会导致语法错误，从而阻止您的查询运行。\n\n# 2.1.11. 空查询\n\n如果查询字符串为空或仅包含空格，则查询将生成一个空的结果集。\n\n\n# 2.2. 可视化\n\n要想使用可视化的方式展示您的数据，请单击侧面导航栏中的 可视化（Visualize）。\n\nVisualize 工具使您能够以多种方式（如饼图、柱状图、曲线图、分布图等）查看数据。要开始使用，请点击蓝色的 Create a visualization 或 + 按钮。\n\n\n\n有许多可视化类型可供选择。\n\n\n\n下面，我们来看创建几个图标示例：\n\n# 2.2.1. Pie\n\n您可以从保存的搜索中构建可视化文件，也可以输入新的搜索条件。要输入新的搜索条件，首先需要选择一个索引模式来指定要搜索的索引。\n\n默认搜索匹配所有文档。最初，一个“切片”包含整个饼图：\n\n\n\n要指定在图表中展示哪些数据，请使用 Elasticsearch 存储桶聚合。分组汇总只是将与您的搜索条件相匹配的文档分类到不同的分类中，也称为分组。\n\n为每个范围定义一个存储桶：\n\n 1. 单击 Split Slices。\n 2. 在 Aggregation 列表中选择 Terms。注意：这里的 Terms 是 Elk 采集数据时定义好的字段或标签。\n 3. 在 Field 列表中选择 level.keyword。\n 4. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 Save 按钮。\n\n# 2.2.2. Vertical Bar\n\n我们在展示一下如何创建柱状图。\n\n 1. 点击蓝色的 Create a visualization 或 + 按钮。选择 Vertical Bar\n 2. 选择索引模式。由于您尚未定义任何 bucket ，因此您会看到一个大栏，显示与默认通配符查询匹配的文档总数。\n 3. 指定 Y 轴所代表的字段\n 4. 指定 X 轴所代表的字段\n 5. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 Save 按钮。\n\n\n# 2.3. 报表\n\n报表（Dashboard） 可以整合和共享 Visualize 集合。\n\n 1. 点击侧面导航栏中的 Dashboard。\n 2. 点击添加显示保存的可视化列表。\n 3. 点击之前保存的 Visualize，然后点击列表底部的小向上箭头关闭可视化列表。\n 4. 将鼠标悬停在可视化对象上会显示允许您编辑，移动，删除和调整可视化对象大小的容器控件。\n\n\n# 3. FAQ\n\n\n# 3.1. Kibana No Default Index Pattern Warning\n\n**问题：**安装 ELK 后，访问 kibana 页面时，提示以下错误信息：\n\nWarning No default index pattern. You must select or create one to continue.\n...\nUnable to fetch mapping. Do you have indices matching the pattern?\n\n\n这就说明 logstash 没有把日志写入到 elasticsearch。\n\n解决方法：\n\n检查 logstash 与 elasticsearch 之间的通讯是否有问题，一般问题就出在这。\n\n\n# 4. 参考资料\n\n * Kibana 官网\n * Kibana Github\n * Kibana 官方文档',normalizedContent:'# kibana 运维\n\n> 通过 kibana，您可以对自己的 elasticsearch 进行可视化，还可以在 elastic stack 中进行导航，这样您便可以进行各种操作了，从跟踪查询负载，到理解请求如何流经您的整个应用，都能轻松完成。\n\n\n# 1. 安装\n\n\n# 1.1. 环境要求\n\n> 版本：elastic stack 7.4\n\n\n# 1.2. 安装步骤\n\n安装步骤如下：\n\n 1. 在 kibana 官方下载地址下载所需版本包并解压到本地。\n 2. 修改 config/kibana.yml 配置文件，设置 elasticsearch.url 指向 elasticsearch 实例。\n 3. 运行 bin/kibana （windows 上运行 bin\\kibana.bat）\n 4. 在浏览器上访问 http://localhost:5601\n\n\n# 2. 使用\n\n\n# 2.1. 检索\n\n单击侧面导航栏中的 检索（discover） ，可以显示 kibana 的数据查询功能功能。\n\n\n\n在搜索栏中，您可以输入 elasticsearch 查询条件来搜索您的数据。您可以在 discover 页面中浏览结果并在 visualize 页面中创建已保存搜索条件的可视化。\n\n当前索引模式显示在查询栏下方。索引模式确定提交查询时搜索哪些索引。要搜索一组不同的索引，请从下拉菜单中选择不同的模式。要添加索引模式（index pattern），请转至 management/kibana/index patterns 并单击 add new。\n\n您可以使用字段名称和您感兴趣的值构建搜索。对于数字字段，可以使用比较运算符，如大于（>），小于（<）或等于（=）。您可以将元素与逻辑运算符 and，or 和 not 链接，全部使用大写。\n\n默认情况下，每个匹配文档都显示所有字段。要选择要显示的文档字段，请将鼠标悬停在“可用字段”列表上，然后单击要包含的每个字段旁边的添加按钮。例如，如果只添加 account_number，则显示将更改为包含五个帐号的简单列表：\n\n\n\nkibana 的搜索栏遵循 query-string-syntax 文档中所说明的查询语义。\n\n这里说明一些最基本的查询语义。\n\n查询字符串会被解析为一系列的术语和运算符。一个术语可以是一个单词（如：quick、brown）或用双引号包围的短语（如"quick brown"）。\n\n查询操作允许您自定义搜索 - 下面介绍了可用的选项。\n\n# 2.1.1. 字段名称\n\n正如查询字符串查询中所述，将在搜索条件中搜索 default_field，但可以在查询语法中指定其他字段：\n\n例如：\n\n * 查询 status 字段中包含 active 关键字\n\nstatus:active\n\n\n * title 字段包含 quick 或 brown 关键字。如果您省略 or 运算符，则将使用默认运算符\n\ntitle:(quick or brown)\ntitle:(quick brown)\n\n\n * author 字段查找精确的短语 "john smith"，即精确查找。\n\nauthor:"john smith"\n\n\n * 任意字段 book.title，book.content 或 book.date 都包含 quick 或 brown（注意我们需要如何使用 \\* 表示通配符）\n\nbook.\\*:(quick brown)\n\n\n * title 字段包含任意非 null 值\n\n_exists_:title\n\n\n# 2.1.2. 通配符\n\nelk 提供了 ? 和 * 两个通配符。\n\n * ? 表示任意单个字符；\n * * 表示任意零个或多个字符。\n\nqu?ck bro*\n\n\n> 注意：通配符查询会使用大量的内存并且执行性能较为糟糕，所以请慎用。 > 提示：纯通配符 * 被写入 exsits 查询，从而提高了查询效率。因此，通配符 field：* 将匹配包含空值的文档，如：{“field”：“”}，但是如果字段丢失或显示将值置为 null 则不匹配，如：“field”：null} > 提示：在一个单词的开头（例如：*ing）使用通配符这种方式的查询量特别大，因为索引中的所有术语都需要检查，以防万一匹配。通过将 allow_leading_wildcard 设置为 false，可以禁用。\n\n# 2.1.3. 正则表达式\n\n可以通过 / 将正则表达式包裹在查询字符串中进行查询\n\n例：\n\nname:/joh?n(ath[oa]n)/\n\n\n支持的正则表达式语义可以参考：regular expression syntax\n\n# 2.1.4. 模糊查询\n\n我们可以使用 ~ 运算符来进行模糊查询。\n\n例：\n\n假设我们实际想查询\n\nquick brown forks\n\n\n但是，由于拼写错误，我们的查询关键字变成如下情况，依然可以查到想要的结果。\n\nquikc\\~ brwn\\~ foks\\~\n\n\n这种模糊查询使用 damerau-levenshtein 距离来查找所有匹配最多两个更改的项。所谓的更改是指单个字符的插入，删除或替换，或者两个相邻字符的换位。\n\n默认编辑距离为 2，但编辑距离为 1 应足以捕捉所有人类拼写错误的 80％。它可以被指定为：\n\nquikc\\~1\n\n\n# 2.1.5. 近似检索\n\n尽管短语查询（例如，john smith）期望所有的词条都是完全相同的顺序，但是近似查询允许指定的单词进一步分开或以不同的顺序排列。与模糊查询可以为单词中的字符指定最大编辑距离一样，近似搜索也允许我们指定短语中单词的最大编辑距离：\n\n例\n\n"fox quick"\\~5\n\n\n字段中的文本越接近查询字符串中指定的原始顺序，该文档就越被认为是相关的。当与上面的示例查询相比时，短语 "quick fox" 将被认为比 "quick brown fox" 更近似查询条件。\n\n# 2.1.6. 范围\n\n可以为日期，数字或字符串字段指定范围。闭区间范围用方括号 [min to max] 和开区间范围用花括号 {min to max} 来指定。\n\n我们不妨来看一些示例。\n\n * 2012 年的所有日子\n\ndate:[2012-01-01 to 2012-12-31]\n\n\n * 数字 1 到 5\n\ncount:[1 to 5]\n\n\n * 在 alpha 和 omega 之间的标签，不包括 alpha 和 omega\n\ntag:{alpha to omega}\n\n\n * 10 以上的数字\n\ncount:[10 to *]\n\n\n * 2012 年以前的所有日期\n\ndate:{* to 2012-01-01}\n\n\n此外，开区间和闭区间也可以组合使用\n\n * 数组 1 到 5，但不包括 5\n\ncount:[1 to 5}\n\n\n一边无界的范围也可以使用以下语法：\n\nage:>10\nage:>=10\nage:<10\nage:<=10\n\n\n当然，你也可以使用 and 运算符来得到连个查询结果的交集\n\nage:(>=10 and <20)\nage:(+>=10 +<20)\n\n\n# 2.1.7. boosting\n\n使用操作符 ^ 使一个术语比另一个术语更相关。例如，如果我们想查找所有有关狐狸的文档，但我们对狐狸特别感兴趣：\n\nquick^2 fox\n\n\n默认提升值是 1，但可以是任何正浮点数。 0 到 1 之间的提升减少了相关性。\n\n增强也可以应用于短语或组：\n\n"john smith"^2   (foo bar)^4\n\n\n# 2.1.8. 布尔操作\n\n默认情况下，只要一个词匹配，所有词都是可选的。搜索 foo bar baz 将查找包含 foo 或 bar 或 baz 中的一个或多个的任何文档。我们已经讨论了上面的default_operator，它允许你强制要求所有的项，但也有布尔运算符可以在查询字符串本身中使用，以提供更多的控制。\n\n首选的操作符是 +（此术语必须存在）和 - （此术语不得存在）。所有其他条款是可选的。例如，这个查询：\n\nquick brown +fox -news\n\n\n这条查询意味着：\n\n * fox 必须存在\n * news 必须不存在\n * quick 和 brown 是可有可无的\n\n熟悉的运算符 and，or 和 not（也写成 &&，|| 和 !）也被支持。然而，这些操作符有一定的优先级：not 优先于 and，and 优先于 or。虽然 + 和 - 仅影响运算符右侧的术语，但 and 和 or 会影响左侧和右侧的术语。\n\n# 2.1.9. 分组\n\n多个术语或子句可以用圆括号组合在一起，形成子查询\n\n(quick or brown) and fox\n\n\n可以使用组来定位特定的字段，或者增强子查询的结果：\n\nstatus:(active or pending) title:(full text search)^2\n\n\n# 2.1.10. 保留字\n\n如果你需要使用任何在你的查询本身中作为操作符的字符（而不是作为操作符），那么你应该用一个反斜杠来转义它们。例如，要搜索（1 + 1）= 2，您需要将查询写为 \\(1\\+1\\)\\=2\n\n保留字符是：+ - = && || > < ! ( ) { } [ ] ^ " ~ * ? : \\ /\n\n无法正确地转义这些特殊字符可能会导致语法错误，从而阻止您的查询运行。\n\n# 2.1.11. 空查询\n\n如果查询字符串为空或仅包含空格，则查询将生成一个空的结果集。\n\n\n# 2.2. 可视化\n\n要想使用可视化的方式展示您的数据，请单击侧面导航栏中的 可视化（visualize）。\n\nvisualize 工具使您能够以多种方式（如饼图、柱状图、曲线图、分布图等）查看数据。要开始使用，请点击蓝色的 create a visualization 或 + 按钮。\n\n\n\n有许多可视化类型可供选择。\n\n\n\n下面，我们来看创建几个图标示例：\n\n# 2.2.1. pie\n\n您可以从保存的搜索中构建可视化文件，也可以输入新的搜索条件。要输入新的搜索条件，首先需要选择一个索引模式来指定要搜索的索引。\n\n默认搜索匹配所有文档。最初，一个“切片”包含整个饼图：\n\n\n\n要指定在图表中展示哪些数据，请使用 elasticsearch 存储桶聚合。分组汇总只是将与您的搜索条件相匹配的文档分类到不同的分类中，也称为分组。\n\n为每个范围定义一个存储桶：\n\n 1. 单击 split slices。\n 2. 在 aggregation 列表中选择 terms。注意：这里的 terms 是 elk 采集数据时定义好的字段或标签。\n 3. 在 field 列表中选择 level.keyword。\n 4. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 save 按钮。\n\n# 2.2.2. vertical bar\n\n我们在展示一下如何创建柱状图。\n\n 1. 点击蓝色的 create a visualization 或 + 按钮。选择 vertical bar\n 2. 选择索引模式。由于您尚未定义任何 bucket ，因此您会看到一个大栏，显示与默认通配符查询匹配的文档总数。\n 3. 指定 y 轴所代表的字段\n 4. 指定 x 轴所代表的字段\n 5. 点击 按钮来更新图表。\n\n\n\n完成后，如果想要保存这个图表，可以点击页面最上方一栏中的 save 按钮。\n\n\n# 2.3. 报表\n\n报表（dashboard） 可以整合和共享 visualize 集合。\n\n 1. 点击侧面导航栏中的 dashboard。\n 2. 点击添加显示保存的可视化列表。\n 3. 点击之前保存的 visualize，然后点击列表底部的小向上箭头关闭可视化列表。\n 4. 将鼠标悬停在可视化对象上会显示允许您编辑，移动，删除和调整可视化对象大小的容器控件。\n\n\n# 3. faq\n\n\n# 3.1. kibana no default index pattern warning\n\n**问题：**安装 elk 后，访问 kibana 页面时，提示以下错误信息：\n\nwarning no default index pattern. you must select or create one to continue.\n...\nunable to fetch mapping. do you have indices matching the pattern?\n\n\n这就说明 logstash 没有把日志写入到 elasticsearch。\n\n解决方法：\n\n检查 logstash 与 elasticsearch 之间的通讯是否有问题，一般问题就出在这。\n\n\n# 4. 参考资料\n\n * kibana 官网\n * kibana github\n * kibana 官方文档',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elastic 技术栈之 Logstash",frontmatter:{title:"Elastic 技术栈之 Logstash",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic","Logstash"],permalink:"/pages/55ce99/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/06.Elastic%E6%8A%80%E6%9C%AF%E6%A0%88%E4%B9%8BLogstash.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/06.Elastic技术栈之Logstash.md",key:"v-8ad847d4",path:"/pages/55ce99/",headers:[{level:2,title:"简介",slug:"简介",normalizedTitle:"简介",charIndex:117},{level:3,title:"功能",slug:"功能",normalizedTitle:"功能",charIndex:155},{level:3,title:"工作原理",slug:"工作原理",normalizedTitle:"工作原理",charIndex:270},{level:2,title:"设置",slug:"设置",normalizedTitle:"设置",charIndex:548},{level:3,title:"设置文件",slug:"设置文件",normalizedTitle:"设置文件",charIndex:555},{level:3,title:"logstash.yml 设置项",slug:"logstash-yml-设置项",normalizedTitle:"logstash.yml 设置项",charIndex:801},{level:2,title:"启动",slug:"启动",normalizedTitle:"启动",charIndex:589},{level:3,title:"命令行",slug:"命令行",normalizedTitle:"命令行",charIndex:4126},{level:3,title:"配置文件",slug:"配置文件",normalizedTitle:"配置文件",charIndex:591},{level:4,title:"配置文件结构",slug:"配置文件结构",normalizedTitle:"配置文件结构",charIndex:4665},{level:4,title:"插件配置",slug:"插件配置",normalizedTitle:"插件配置",charIndex:3062},{level:4,title:"值类型",slug:"值类型",normalizedTitle:"值类型",charIndex:5145},{level:2,title:"插件",slug:"插件",normalizedTitle:"插件",charIndex:204},{level:3,title:"input",slug:"input",normalizedTitle:"input",charIndex:293},{level:4,title:"常用 input 插件",slug:"常用-input-插件",normalizedTitle:"常用 input 插件",charIndex:5971},{level:3,title:"filter",slug:"filter",normalizedTitle:"filter",charIndex:316},{level:4,title:"常用 filter 插件",slug:"常用-filter-插件",normalizedTitle:"常用 filter 插件",charIndex:6332},{level:3,title:"output",slug:"output",normalizedTitle:"output",charIndex:301},{level:4,title:"常用 output 插件",slug:"常用-output-插件",normalizedTitle:"常用 output 插件",charIndex:6679},{level:3,title:"codec",slug:"codec",normalizedTitle:"codec",charIndex:5592},{level:4,title:"常用 codec 插件",slug:"常用-codec-插件",normalizedTitle:"常用 codec 插件",charIndex:7002},{level:2,title:"实战",slug:"实战",normalizedTitle:"实战",charIndex:7125},{level:3,title:"传输控制台数据",slug:"传输控制台数据",normalizedTitle:"传输控制台数据",charIndex:7180},{level:3,title:"传输 logback 日志",slug:"传输-logback-日志",normalizedTitle:"传输 logback 日志",charIndex:7560},{level:4,title:"TCP 应用",slug:"tcp-应用",normalizedTitle:"tcp 应用",charIndex:7867},{level:4,title:"UDP 应用",slug:"udp-应用",normalizedTitle:"udp 应用",charIndex:9396},{level:3,title:"传输文件",slug:"传输文件",normalizedTitle:"传输文件",charIndex:10224},{level:2,title:"小技巧",slug:"小技巧",normalizedTitle:"小技巧",charIndex:10926},{level:3,title:"启动、终止应用",slug:"启动、终止应用",normalizedTitle:"启动、终止应用",charIndex:10934},{level:2,title:"资料",slug:"资料",normalizedTitle:"资料",charIndex:11385},{level:2,title:"推荐阅读",slug:"推荐阅读",normalizedTitle:"推荐阅读",charIndex:11499}],headersStr:"简介 功能 工作原理 设置 设置文件 logstash.yml 设置项 启动 命令行 配置文件 配置文件结构 插件配置 值类型 插件 input 常用 input 插件 filter 常用 filter 插件 output 常用 output 插件 codec 常用 codec 插件 实战 传输控制台数据 传输 logback 日志 TCP 应用 UDP 应用 传输文件 小技巧 启动、终止应用 资料 推荐阅读",content:'# Elastic 技术栈之 Logstash\n\n> 本文是 Elastic 技术栈（ELK）的 Logstash 应用。\n> \n> 如果不了解 Elastic 的安装、配置、部署，可以参考：Elastic 技术栈之快速入门\n\n\n# 简介\n\nLogstash 可以传输和处理你的日志、事务或其他数据。\n\n\n# 功能\n\nLogstash 是 Elasticsearch 的最佳数据管道。\n\nLogstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制。Logstash 社区有超过 200 种可用插件。\n\n\n# 工作原理\n\nLogstash 有两个必要元素：input 和 output ，一个可选元素：filter。\n\n这三个元素，分别代表 Logstash 事件处理的三个阶段：输入 > 过滤器 > 输出。\n\n\n\n * input 负责从数据源采集数据。\n * filter 将数据修改为你指定的格式或内容。\n * output 将数据传输到目的地。\n\n在实际应用场景中，通常输入、输出、过滤器不止一个。Logstash 的这三个元素都使用插件式管理方式，用户可以根据应用需要，灵活的选用各阶段需要的插件，并组合使用。\n\n后面将对插件展开讲解，暂且不表。\n\n\n# 设置\n\n\n# 设置文件\n\n * logstash.yml：logstash 的默认启动配置文件\n * jvm.options：logstash 的 JVM 配置文件。\n * startup.options (Linux)：包含系统安装脚本在 /usr/share/logstash/bin 中使用的选项为您的系统构建适当的启动脚本。安装 Logstash 软件包时，系统安装脚本将在安装过程结束时执行，并使用 startup.options 中指定的设置来设置用户，组，服务名称和服务描述等选项。\n\n\n# logstash.yml 设置项\n\n节选部分设置项，更多项请参考：https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html\n\n参数                         描述                                                                                           默认值\nnode.name                  节点名                                                                                          机器的主机名\npath.data                  Logstash 及其插件用于任何持久性需求的目录。                                                                   LOGSTASH_HOME/data\npipeline.workers           同时执行管道的过滤器和输出阶段的工作任务数量。如果发现事件正在备份，或 CPU                                                      Number of the host’s CPU cores\n                           未饱和，请考虑增加此数字以更好地利用机器处理能力。\npipeline.batch.size        尝试执行过滤器和输出之前，单个工作线程从输入收集的最大事件数量。较大的批量处理大小一般来说效率更高，但是以增加的内存开销为代价。您可能必须通过设置                    125\n                           LS_HEAP_SIZE 变量来有效使用该选项来增加 JVM 堆大小。\npipeline.batch.delay       创建管道事件批处理时，在将一个尺寸过小的批次发送给管道工作任务之前，等待每个事件需要多长时间（毫秒）。                                          5\npipeline.unsafe_shutdown   如果设置为 true，则即使在内存中仍存在 inflight 事件时，也会强制 Logstash                                             false\n                           在关闭期间退出。默认情况下，Logstash\n                           将拒绝退出，直到所有接收到的事件都被推送到输出。启用此选项可能会导致关机期间数据丢失。\npath.config                主管道的 Logstash 配置路径。如果您指定一个目录或通配符，配置文件将按字母顺序从目录中读取。                                           Platform-specific. See [dir-layout].\nconfig.string              包含用于主管道的管道配置的字符串。使用与配置文件相同的语法。                                                               None\nconfig.test_and_exit       设置为 true 时，检查配置是否有效，然后退出。请注意，使用此设置不会检查 grok 模式的正确性。 Logstash                                 false\n                           可以从目录中读取多个配置文件。如果将此设置与 log.level：debug 结合使用，则 Logstash\n                           将记录组合的配置文件，并注掉其源文件的配置块。\nconfig.reload.automatic    设置为 true 时，定期检查配置是否已更改，并在配置更改时重新加载配置。这也可以通过 SIGHUP 信号手动触发。                                   false\nconfig.reload.interval     Logstash 检查配置文件更改的时间间隔。                                                                      3s\nconfig.debug               设置为 true                                                                                     false\n                           时，将完全编译的配置显示为调试日志消息。您还必须设置log.level：debug。警告：日志消息将包括任何传递给插件配置作为明文的“密码”选项，并可能导致明文密码出现在您的日志！\nconfig.support_escapes     当设置为 true 时，带引号的字符串将处理转义字符。                                                                  false\nmodules                    配置时，模块必须处于上表所述的嵌套 YAML 结构中。                                                                  None\nhttp.host                  绑定地址                                                                                         "127.0.0.1"\nhttp.port                  绑定端口                                                                                         9600\nlog.level                  日志级别。有效选项：fatal > error > warn > info > debug > trace                                        info\nlog.format                 日志格式。json （JSON 格式）或 plain （原对象）                                                             plain\npath.logs                  Logstash 自身日志的存储路径                                                                           LOGSTASH_HOME/logs\npath.plugins               在哪里可以找到自定义的插件。您可以多次指定此设置以包含多个路径。                                                             \n\n\n# 启动\n\n\n# 命令行\n\n通过命令行启动 logstash 的方式如下：\n\nbin/logstash [options]\n\n\n其中 options 是您可以指定用于控制 Logstash 执行的命令行标志。\n\n在命令行上设置的任何标志都会覆盖 Logstash 设置文件（logstash.yml）中的相应设置，但设置文件本身不会更改。\n\n> 注\n> \n> 虽然可以通过指定命令行参数的方式，来控制 logstash 的运行方式，但显然这么做很麻烦。\n> \n> 建议通过指定配置文件的方式，来控制 logstash 运行，启动命令如下：\n> \n> bin/logstash -f logstash.conf\n> \n> \n> 若想了解更多的命令行参数细节，请参考：https://www.elastic.co/guide/en/logstash/current/running-logstash-command-line.html\n\n\n# 配置文件\n\n上节，我们了解到，logstash 可以执行 bin/logstash -f logstash.conf ，按照配置文件中的参数去覆盖默认设置文件（logstash.yml）中的设置。\n\n这节，我们就来学习一下这个配置文件如何配置参数。\n\n# 配置文件结构\n\n在工作原理一节中，我们已经知道了 Logstash 主要有三个工作阶段 input 、filter、output。而 logstash 配置文件文件结构也与之相对应：\n\ninput {}\n\nfilter {}\n\noutput {}\n\n\n> 每个部分都包含一个或多个插件的配置选项。如果指定了多个过滤器，则会按照它们在配置文件中的显示顺序应用它们。\n\n# 插件配置\n\n插件的配置由插件名称和插件的一个设置块组成。\n\n下面的例子中配置了两个输入文件配置：\n\ninput {\n  file {\n    path => "/var/log/messages"\n    type => "syslog"\n  }\n\n  file {\n    path => "/var/log/apache/access.log"\n    type => "apache"\n  }\n}\n\n\n您可以配置的设置因插件类型而异。你可以参考： Input Plugins, Output Plugins, Filter Plugins, 和 Codec Plugins 。\n\n# 值类型\n\n一个插件可以要求设置的值是一个特定的类型，比如布尔值，列表或哈希值。以下值类型受支持。\n\n * Array\n\n  users => [ {id => 1, name => bob}, {id => 2, name => jane} ]\n\n\n * Lists\n\npath => [\'/var/log/messages\', \'/var/log/*.log\']\nuris => [\'http://elastic.co\', \'http://example.net\']\n\n\n * Boolean\n\nssl_enable => true\n\n\n * Bytes\n\n  my_bytes => "1113"   # 1113 bytes\n  my_bytes => "10MiB"  # 10485760 bytes\n  my_bytes => "100kib" # 102400 bytes\n  my_bytes => "180 mb" # 180000000 bytes\n\n\n * Codec\n\ncodec => \'json\'\n\n\n * Hash\n\nmatch => {\n  "field1" => "value1"\n  "field2" => "value2"\n  ...\n}\n\n\n * Number\n\nport => 33\n\n\n * Password\n\nmy_password => \'password\'\n\n\n * URI\n\nmy_uri => \'http://foo:bar@example.net\'\n\n\n * Path\n\nmy_path => \'/tmp/logstash\'\n\n\n * String\n\n * 转义字符\n\n\n# 插件\n\n\n# input\n\n> Logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。\n\n# 常用 input 插件\n\n * file：从文件系统上的文件读取，就像 UNIX 命令 tail -0F 一样\n * **syslog：**在众所周知的端口 514 上侦听系统日志消息，并根据 RFC3164 格式进行解析\n * **redis：**从 redis 服务器读取，使用 redis 通道和 redis 列表。 Redis 经常用作集中式 Logstash 安装中的“代理”，它将来自远程 Logstash“托运人”的 Logstash 事件排队。\n * **beats：**处理由 Filebeat 发送的事件。\n\n更多详情请见：Input Plugins\n\n\n# filter\n\n> 过滤器是 Logstash 管道中的中间处理设备。如果符合特定条件，您可以将条件过滤器组合在一起，对事件执行操作。\n\n# 常用 filter 插件\n\n * **grok：**解析和结构任意文本。 Grok 目前是 Logstash 中将非结构化日志数据解析为结构化和可查询的最佳方法。\n * **mutate：**对事件字段执行一般转换。您可以重命名，删除，替换和修改事件中的字段。\n * **drop：**完全放弃一个事件，例如调试事件。\n * **clone：**制作一个事件的副本，可能会添加或删除字段。\n * **geoip：**添加有关 IP 地址的地理位置的信息（也可以在 Kibana 中显示惊人的图表！）\n\n更多详情请见：Filter Plugins\n\n\n# output\n\n> 输出是 Logstash 管道的最后阶段。一个事件可以通过多个输出，但是一旦所有输出处理完成，事件就完成了执行。\n\n# 常用 output 插件\n\n * **elasticsearch：**将事件数据发送给 Elasticsearch（推荐模式）。\n * **file：**将事件数据写入文件或磁盘。\n * **graphite：**将事件数据发送给 graphite（一个流行的开源工具，存储和绘制指标。 http://graphite.readthedocs.io/en/latest/）。\n * **statsd：**将事件数据发送到 statsd （这是一种侦听统计数据的服务，如计数器和定时器，通过 UDP 发送并将聚合发送到一个或多个可插入的后端服务）。\n\n更多详情请见：Output Plugins\n\n\n# codec\n\n用于格式化对应的内容。\n\n# 常用 codec 插件\n\n * **json：**以 JSON 格式对数据进行编码或解码。\n * **multiline：**将多行文本事件（如 java 异常和堆栈跟踪消息）合并为单个事件。\n\n更多插件请见：Codec Plugins\n\n\n# 实战\n\n前面的内容都是对 Logstash 的介绍和原理说明。接下来，我们来实战一些常见的应用场景。\n\n\n# 传输控制台数据\n\n> stdin input 插件从标准输入读取事件。这是最简单的 input 插件，一般用于测试场景。\n\n应用\n\n（1）创建 logstash-input-stdin.conf ：\n\ninput { stdin { } }\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：\n\nbin/logstash -f logstash-input-stdin.conf\n\n\n\n# 传输 logback 日志\n\n> elk 默认使用的 Java 日志工具是 log4j2 ，并不支持 logback 和 log4j。\n> \n> 想使用 logback + logstash ，可以使用 logstash-logback-encoder 。logstash-logback-encoder 提供了 UDP / TCP / 异步方式来传输日志数据到 logstash。\n> \n> 如果你使用的是 log4j ，也不是不可以用这种方式，只要引入桥接 jar 包即可。如果你对 log4j 、logback ，或是桥接 jar 包不太了解，可以参考我的这篇博文：细说 Java 主流日志工具库 。\n\n# TCP 应用\n\nlogstash 配置\n\n（1）创建 logstash-input-tcp.conf ：\n\ninput {\ntcp {\n  port => 9251\n  codec => json_lines\n  mode => server\n}\n}\noutput {\n elasticsearch { hosts => ["localhost:9200"] }\n stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置\n\n（1）在 Java 应用的 pom.xml 中引入 jar 包：\n\n<dependency>\n <groupId>net.logstash.logback</groupId>\n <artifactId>logstash-logback-encoder</artifactId>\n <version>4.11</version>\n</dependency>\n\n\x3c!-- logback 依赖包 --\x3e\n<dependency>\n <groupId>ch.qos.logback</groupId>\n <artifactId>logback-core</artifactId>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupId>ch.qos.logback</groupId>\n <artifactId>logback-classic</artifactId>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupId>ch.qos.logback</groupId>\n <artifactId>logback-access</artifactId>\n <version>1.2.3</version>\n</dependency>\n\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="ELK-TCP" class="net.logstash.logback.appender.LogstashTcpSocketAppender">\n \x3c!--\n destination 是 logstash 服务的 host:port，\n 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash\n --\x3e\n <destination>192.168.28.32:9251</destination>\n <encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder"/>\n</appender>\n<logger name="io.github.dunwu.spring" level="TRACE" additivity="false">\n <appender-ref ref="ELK-TCP" />\n</logger>\n\n\n（3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 Java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n# UDP 应用\n\nUDP 和 TCP 的使用方式大同小异。\n\nlogstash 配置\n\n（1）创建 logstash-input-udp.conf ：\n\ninput {\nudp {\n  port => 9250\n  codec => json\n}\n}\noutput {\n elasticsearch { hosts => ["localhost:9200"] }\n stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置\n\n（1）在 Java 应用的 pom.xml 中引入 jar 包：\n\n与 TCP 应用 一节中的引入依赖包完全相同。\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="ELK-UDP" class="net.logstash.logback.appender.LogstashSocketAppender">\n  <host>192.168.28.32</host>\n  <port>9250</port>\n</appender>\n<logger name="io.github.dunwu.spring" level="TRACE" additivity="false">\n  <appender-ref ref="ELK-UDP" />\n</logger>\n\n\n（3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 Java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n\n# 传输文件\n\n> 在 Java Web 领域，需要用到一些重要的工具，例如 Tomcat 、Nginx 、Mysql 等。这些不属于业务应用，但是它们的日志数据对于定位问题、分析统计同样很重要。这时无法使用 logback 方式将它们的日志传输到 logstash。\n> \n> 如何采集这些日志文件呢？别急，你可以使用 logstash 的 file input 插件。\n> \n> 需要注意的是，传输文件这种方式，必须在日志所在的机器上部署 logstash 。\n\n应用\n\nlogstash 配置\n\n（1）创建 logstash-input-file.conf ：\n\ninput {\n file {\n  path => ["/var/log/nginx/access.log"]\n  type => "nginx-access-log"\n  start_position => "beginning"\n }\n}\n\noutput {\n if [type] == "nginx-access-log" {\n  elasticsearch {\n   hosts => ["localhost:9200"]\n   index => "nginx-access-log"\n  }\n }\n}\n\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-file.conf\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html\n\n\n# 小技巧\n\n\n# 启动、终止应用\n\n如果你的 logstash 每次都是通过指定配置文件方式启动。不妨建立一个启动脚本。\n\n# cd xxx 进入 logstash 安装目录下的 bin 目录\nlogstash -f logstash.conf\n\n\n如果你的 logstash 运行在 linux 系统下，不妨使用 nohup 来启动一个守护进程。这样做的好处在于，即使关闭终端，应用仍会运行。\n\n创建 startup.sh\n\nnohup ./logstash -f logstash.conf >> nohup.out 2>&1 &\n\n\n终止应用没有什么好方法，你只能使用 ps -ef | grep logstash ，查出进程，将其 kill 。不过，我们可以写一个脚本来干这件事：\n\n创建 shutdown.sh\n\n脚本不多解释，请自行领会作用。\n\nPID=`ps -ef | grep logstash | awk \'{ print $2}\' | head -n 1`\nkill -9 ${PID}\n\n\n\n# 资料\n\n * Logstash 官方文档\n * logstash-logback-encoder\n * ELK Stack 权威指南\n * ELK（Elasticsearch、Logstash、Kibana）安装和配置\n\n\n# 推荐阅读\n\n * Elastic 技术栈\n * JavaStack',normalizedContent:'# elastic 技术栈之 logstash\n\n> 本文是 elastic 技术栈（elk）的 logstash 应用。\n> \n> 如果不了解 elastic 的安装、配置、部署，可以参考：elastic 技术栈之快速入门\n\n\n# 简介\n\nlogstash 可以传输和处理你的日志、事务或其他数据。\n\n\n# 功能\n\nlogstash 是 elasticsearch 的最佳数据管道。\n\nlogstash 是插件式管理模式，在输入、过滤、输出以及编码过程中都可以使用插件进行定制。logstash 社区有超过 200 种可用插件。\n\n\n# 工作原理\n\nlogstash 有两个必要元素：input 和 output ，一个可选元素：filter。\n\n这三个元素，分别代表 logstash 事件处理的三个阶段：输入 > 过滤器 > 输出。\n\n\n\n * input 负责从数据源采集数据。\n * filter 将数据修改为你指定的格式或内容。\n * output 将数据传输到目的地。\n\n在实际应用场景中，通常输入、输出、过滤器不止一个。logstash 的这三个元素都使用插件式管理方式，用户可以根据应用需要，灵活的选用各阶段需要的插件，并组合使用。\n\n后面将对插件展开讲解，暂且不表。\n\n\n# 设置\n\n\n# 设置文件\n\n * logstash.yml：logstash 的默认启动配置文件\n * jvm.options：logstash 的 jvm 配置文件。\n * startup.options (linux)：包含系统安装脚本在 /usr/share/logstash/bin 中使用的选项为您的系统构建适当的启动脚本。安装 logstash 软件包时，系统安装脚本将在安装过程结束时执行，并使用 startup.options 中指定的设置来设置用户，组，服务名称和服务描述等选项。\n\n\n# logstash.yml 设置项\n\n节选部分设置项，更多项请参考：https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html\n\n参数                         描述                                                                                           默认值\nnode.name                  节点名                                                                                          机器的主机名\npath.data                  logstash 及其插件用于任何持久性需求的目录。                                                                   logstash_home/data\npipeline.workers           同时执行管道的过滤器和输出阶段的工作任务数量。如果发现事件正在备份，或 cpu                                                      number of the host’s cpu cores\n                           未饱和，请考虑增加此数字以更好地利用机器处理能力。\npipeline.batch.size        尝试执行过滤器和输出之前，单个工作线程从输入收集的最大事件数量。较大的批量处理大小一般来说效率更高，但是以增加的内存开销为代价。您可能必须通过设置                    125\n                           ls_heap_size 变量来有效使用该选项来增加 jvm 堆大小。\npipeline.batch.delay       创建管道事件批处理时，在将一个尺寸过小的批次发送给管道工作任务之前，等待每个事件需要多长时间（毫秒）。                                          5\npipeline.unsafe_shutdown   如果设置为 true，则即使在内存中仍存在 inflight 事件时，也会强制 logstash                                             false\n                           在关闭期间退出。默认情况下，logstash\n                           将拒绝退出，直到所有接收到的事件都被推送到输出。启用此选项可能会导致关机期间数据丢失。\npath.config                主管道的 logstash 配置路径。如果您指定一个目录或通配符，配置文件将按字母顺序从目录中读取。                                           platform-specific. see [dir-layout].\nconfig.string              包含用于主管道的管道配置的字符串。使用与配置文件相同的语法。                                                               none\nconfig.test_and_exit       设置为 true 时，检查配置是否有效，然后退出。请注意，使用此设置不会检查 grok 模式的正确性。 logstash                                 false\n                           可以从目录中读取多个配置文件。如果将此设置与 log.level：debug 结合使用，则 logstash\n                           将记录组合的配置文件，并注掉其源文件的配置块。\nconfig.reload.automatic    设置为 true 时，定期检查配置是否已更改，并在配置更改时重新加载配置。这也可以通过 sighup 信号手动触发。                                   false\nconfig.reload.interval     logstash 检查配置文件更改的时间间隔。                                                                      3s\nconfig.debug               设置为 true                                                                                     false\n                           时，将完全编译的配置显示为调试日志消息。您还必须设置log.level：debug。警告：日志消息将包括任何传递给插件配置作为明文的“密码”选项，并可能导致明文密码出现在您的日志！\nconfig.support_escapes     当设置为 true 时，带引号的字符串将处理转义字符。                                                                  false\nmodules                    配置时，模块必须处于上表所述的嵌套 yaml 结构中。                                                                  none\nhttp.host                  绑定地址                                                                                         "127.0.0.1"\nhttp.port                  绑定端口                                                                                         9600\nlog.level                  日志级别。有效选项：fatal > error > warn > info > debug > trace                                        info\nlog.format                 日志格式。json （json 格式）或 plain （原对象）                                                             plain\npath.logs                  logstash 自身日志的存储路径                                                                           logstash_home/logs\npath.plugins               在哪里可以找到自定义的插件。您可以多次指定此设置以包含多个路径。                                                             \n\n\n# 启动\n\n\n# 命令行\n\n通过命令行启动 logstash 的方式如下：\n\nbin/logstash [options]\n\n\n其中 options 是您可以指定用于控制 logstash 执行的命令行标志。\n\n在命令行上设置的任何标志都会覆盖 logstash 设置文件（logstash.yml）中的相应设置，但设置文件本身不会更改。\n\n> 注\n> \n> 虽然可以通过指定命令行参数的方式，来控制 logstash 的运行方式，但显然这么做很麻烦。\n> \n> 建议通过指定配置文件的方式，来控制 logstash 运行，启动命令如下：\n> \n> bin/logstash -f logstash.conf\n> \n> \n> 若想了解更多的命令行参数细节，请参考：https://www.elastic.co/guide/en/logstash/current/running-logstash-command-line.html\n\n\n# 配置文件\n\n上节，我们了解到，logstash 可以执行 bin/logstash -f logstash.conf ，按照配置文件中的参数去覆盖默认设置文件（logstash.yml）中的设置。\n\n这节，我们就来学习一下这个配置文件如何配置参数。\n\n# 配置文件结构\n\n在工作原理一节中，我们已经知道了 logstash 主要有三个工作阶段 input 、filter、output。而 logstash 配置文件文件结构也与之相对应：\n\ninput {}\n\nfilter {}\n\noutput {}\n\n\n> 每个部分都包含一个或多个插件的配置选项。如果指定了多个过滤器，则会按照它们在配置文件中的显示顺序应用它们。\n\n# 插件配置\n\n插件的配置由插件名称和插件的一个设置块组成。\n\n下面的例子中配置了两个输入文件配置：\n\ninput {\n  file {\n    path => "/var/log/messages"\n    type => "syslog"\n  }\n\n  file {\n    path => "/var/log/apache/access.log"\n    type => "apache"\n  }\n}\n\n\n您可以配置的设置因插件类型而异。你可以参考： input plugins, output plugins, filter plugins, 和 codec plugins 。\n\n# 值类型\n\n一个插件可以要求设置的值是一个特定的类型，比如布尔值，列表或哈希值。以下值类型受支持。\n\n * array\n\n  users => [ {id => 1, name => bob}, {id => 2, name => jane} ]\n\n\n * lists\n\npath => [\'/var/log/messages\', \'/var/log/*.log\']\nuris => [\'http://elastic.co\', \'http://example.net\']\n\n\n * boolean\n\nssl_enable => true\n\n\n * bytes\n\n  my_bytes => "1113"   # 1113 bytes\n  my_bytes => "10mib"  # 10485760 bytes\n  my_bytes => "100kib" # 102400 bytes\n  my_bytes => "180 mb" # 180000000 bytes\n\n\n * codec\n\ncodec => \'json\'\n\n\n * hash\n\nmatch => {\n  "field1" => "value1"\n  "field2" => "value2"\n  ...\n}\n\n\n * number\n\nport => 33\n\n\n * password\n\nmy_password => \'password\'\n\n\n * uri\n\nmy_uri => \'http://foo:bar@example.net\'\n\n\n * path\n\nmy_path => \'/tmp/logstash\'\n\n\n * string\n\n * 转义字符\n\n\n# 插件\n\n\n# input\n\n> logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。能够以连续的流式传输方式，轻松地从您的日志、指标、web 应用、数据存储以及各种 aws 服务采集数据。\n\n# 常用 input 插件\n\n * file：从文件系统上的文件读取，就像 unix 命令 tail -0f 一样\n * **syslog：**在众所周知的端口 514 上侦听系统日志消息，并根据 rfc3164 格式进行解析\n * **redis：**从 redis 服务器读取，使用 redis 通道和 redis 列表。 redis 经常用作集中式 logstash 安装中的“代理”，它将来自远程 logstash“托运人”的 logstash 事件排队。\n * **beats：**处理由 filebeat 发送的事件。\n\n更多详情请见：input plugins\n\n\n# filter\n\n> 过滤器是 logstash 管道中的中间处理设备。如果符合特定条件，您可以将条件过滤器组合在一起，对事件执行操作。\n\n# 常用 filter 插件\n\n * **grok：**解析和结构任意文本。 grok 目前是 logstash 中将非结构化日志数据解析为结构化和可查询的最佳方法。\n * **mutate：**对事件字段执行一般转换。您可以重命名，删除，替换和修改事件中的字段。\n * **drop：**完全放弃一个事件，例如调试事件。\n * **clone：**制作一个事件的副本，可能会添加或删除字段。\n * **geoip：**添加有关 ip 地址的地理位置的信息（也可以在 kibana 中显示惊人的图表！）\n\n更多详情请见：filter plugins\n\n\n# output\n\n> 输出是 logstash 管道的最后阶段。一个事件可以通过多个输出，但是一旦所有输出处理完成，事件就完成了执行。\n\n# 常用 output 插件\n\n * **elasticsearch：**将事件数据发送给 elasticsearch（推荐模式）。\n * **file：**将事件数据写入文件或磁盘。\n * **graphite：**将事件数据发送给 graphite（一个流行的开源工具，存储和绘制指标。 http://graphite.readthedocs.io/en/latest/）。\n * **statsd：**将事件数据发送到 statsd （这是一种侦听统计数据的服务，如计数器和定时器，通过 udp 发送并将聚合发送到一个或多个可插入的后端服务）。\n\n更多详情请见：output plugins\n\n\n# codec\n\n用于格式化对应的内容。\n\n# 常用 codec 插件\n\n * **json：**以 json 格式对数据进行编码或解码。\n * **multiline：**将多行文本事件（如 java 异常和堆栈跟踪消息）合并为单个事件。\n\n更多插件请见：codec plugins\n\n\n# 实战\n\n前面的内容都是对 logstash 的介绍和原理说明。接下来，我们来实战一些常见的应用场景。\n\n\n# 传输控制台数据\n\n> stdin input 插件从标准输入读取事件。这是最简单的 input 插件，一般用于测试场景。\n\n应用\n\n（1）创建 logstash-input-stdin.conf ：\n\ninput { stdin { } }\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：\n\nbin/logstash -f logstash-input-stdin.conf\n\n\n\n# 传输 logback 日志\n\n> elk 默认使用的 java 日志工具是 log4j2 ，并不支持 logback 和 log4j。\n> \n> 想使用 logback + logstash ，可以使用 logstash-logback-encoder 。logstash-logback-encoder 提供了 udp / tcp / 异步方式来传输日志数据到 logstash。\n> \n> 如果你使用的是 log4j ，也不是不可以用这种方式，只要引入桥接 jar 包即可。如果你对 log4j 、logback ，或是桥接 jar 包不太了解，可以参考我的这篇博文：细说 java 主流日志工具库 。\n\n# tcp 应用\n\nlogstash 配置\n\n（1）创建 logstash-input-tcp.conf ：\n\ninput {\ntcp {\n  port => 9251\n  codec => json_lines\n  mode => server\n}\n}\noutput {\n elasticsearch { hosts => ["localhost:9200"] }\n stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置\n\n（1）在 java 应用的 pom.xml 中引入 jar 包：\n\n<dependency>\n <groupid>net.logstash.logback</groupid>\n <artifactid>logstash-logback-encoder</artifactid>\n <version>4.11</version>\n</dependency>\n\n\x3c!-- logback 依赖包 --\x3e\n<dependency>\n <groupid>ch.qos.logback</groupid>\n <artifactid>logback-core</artifactid>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupid>ch.qos.logback</groupid>\n <artifactid>logback-classic</artifactid>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupid>ch.qos.logback</groupid>\n <artifactid>logback-access</artifactid>\n <version>1.2.3</version>\n</dependency>\n\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="elk-tcp" class="net.logstash.logback.appender.logstashtcpsocketappender">\n \x3c!--\n destination 是 logstash 服务的 host:port，\n 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash\n --\x3e\n <destination>192.168.28.32:9251</destination>\n <encoder charset="utf-8" class="net.logstash.logback.encoder.logstashencoder"/>\n</appender>\n<logger name="io.github.dunwu.spring" level="trace" additivity="false">\n <appender-ref ref="elk-tcp" />\n</logger>\n\n\n（3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n# udp 应用\n\nudp 和 tcp 的使用方式大同小异。\n\nlogstash 配置\n\n（1）创建 logstash-input-udp.conf ：\n\ninput {\nudp {\n  port => 9250\n  codec => json\n}\n}\noutput {\n elasticsearch { hosts => ["localhost:9200"] }\n stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置\n\n（1）在 java 应用的 pom.xml 中引入 jar 包：\n\n与 tcp 应用 一节中的引入依赖包完全相同。\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="elk-udp" class="net.logstash.logback.appender.logstashsocketappender">\n  <host>192.168.28.32</host>\n  <port>9250</port>\n</appender>\n<logger name="io.github.dunwu.spring" level="trace" additivity="false">\n  <appender-ref ref="elk-udp" />\n</logger>\n\n\n（3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n\n# 传输文件\n\n> 在 java web 领域，需要用到一些重要的工具，例如 tomcat 、nginx 、mysql 等。这些不属于业务应用，但是它们的日志数据对于定位问题、分析统计同样很重要。这时无法使用 logback 方式将它们的日志传输到 logstash。\n> \n> 如何采集这些日志文件呢？别急，你可以使用 logstash 的 file input 插件。\n> \n> 需要注意的是，传输文件这种方式，必须在日志所在的机器上部署 logstash 。\n\n应用\n\nlogstash 配置\n\n（1）创建 logstash-input-file.conf ：\n\ninput {\n file {\n  path => ["/var/log/nginx/access.log"]\n  type => "nginx-access-log"\n  start_position => "beginning"\n }\n}\n\noutput {\n if [type] == "nginx-access-log" {\n  elasticsearch {\n   hosts => ["localhost:9200"]\n   index => "nginx-access-log"\n  }\n }\n}\n\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-file.conf\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html\n\n\n# 小技巧\n\n\n# 启动、终止应用\n\n如果你的 logstash 每次都是通过指定配置文件方式启动。不妨建立一个启动脚本。\n\n# cd xxx 进入 logstash 安装目录下的 bin 目录\nlogstash -f logstash.conf\n\n\n如果你的 logstash 运行在 linux 系统下，不妨使用 nohup 来启动一个守护进程。这样做的好处在于，即使关闭终端，应用仍会运行。\n\n创建 startup.sh\n\nnohup ./logstash -f logstash.conf >> nohup.out 2>&1 &\n\n\n终止应用没有什么好方法，你只能使用 ps -ef | grep logstash ，查出进程，将其 kill 。不过，我们可以写一个脚本来干这件事：\n\n创建 shutdown.sh\n\n脚本不多解释，请自行领会作用。\n\npid=`ps -ef | grep logstash | awk \'{ print $2}\' | head -n 1`\nkill -9 ${pid}\n\n\n\n# 资料\n\n * logstash 官方文档\n * logstash-logback-encoder\n * elk stack 权威指南\n * elk（elasticsearch、logstash、kibana）安装和配置\n\n\n# 推荐阅读\n\n * elastic 技术栈\n * javastack',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Logstash 运维",frontmatter:{title:"Logstash 运维",date:"2020-06-16T07:10:44.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic","Logstash"],permalink:"/pages/92df30/"},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/07.Logstash%E8%BF%90%E7%BB%B4.html",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/07.Logstash运维.md",key:"v-2afcea96",path:"/pages/92df30/",headers:[{level:2,title:"1. 安装",slug:"_1-安装",normalizedTitle:"1. 安装",charIndex:85},{level:3,title:"1.1. 安装步骤",slug:"_1-1-安装步骤",normalizedTitle:"1.1. 安装步骤",charIndex:95},{level:2,title:"2. 配置",slug:"_2-配置",normalizedTitle:"2. 配置",charIndex:404},{level:3,title:"2.1. 设置文件",slug:"_2-1-设置文件",normalizedTitle:"2.1. 设置文件",charIndex:414},{level:3,title:"2.2. logstash.yml 设置项",slug:"_2-2-logstash-yml-设置项",normalizedTitle:"2.2. logstash.yml 设置项",charIndex:665},{level:2,title:"3. 启动",slug:"_3-启动",normalizedTitle:"3. 启动",charIndex:3988},{level:3,title:"3.1. 命令行",slug:"_3-1-命令行",normalizedTitle:"3.1. 命令行",charIndex:3998},{level:3,title:"3.2. 配置文件",slug:"_3-2-配置文件",normalizedTitle:"3.2. 配置文件",charIndex:4414},{level:4,title:"3.2.1. 配置文件结构",slug:"_3-2-1-配置文件结构",normalizedTitle:"3.2.1. 配置文件结构",charIndex:4547},{level:4,title:"3.2.2. 插件配置",slug:"_3-2-2-插件配置",normalizedTitle:"3.2.2. 插件配置",charIndex:4739},{level:4,title:"3.2.3. 值类型",slug:"_3-2-3-值类型",normalizedTitle:"3.2.3. 值类型",charIndex:5041},{level:2,title:"4. 插件",slug:"_4-插件",normalizedTitle:"4. 插件",charIndex:5784},{level:3,title:"4.1. input",slug:"_4-1-input",normalizedTitle:"4.1. input",charIndex:5794},{level:4,title:"4.1.1. 常用 input 插件",slug:"_4-1-1-常用-input-插件",normalizedTitle:"4.1.1. 常用 input 插件",charIndex:5902},{level:3,title:"4.2. filter",slug:"_4-2-filter",normalizedTitle:"4.2. filter",charIndex:6199},{level:4,title:"4.2.1. 常用 filter 插件",slug:"_4-2-1-常用-filter-插件",normalizedTitle:"4.2.1. 常用 filter 插件",charIndex:6275},{level:3,title:"4.3. output",slug:"_4-3-output",normalizedTitle:"4.3. output",charIndex:6563},{level:4,title:"4.3.1. 常用 output 插件",slug:"_4-3-1-常用-output-插件",normalizedTitle:"4.3.1. 常用 output 插件",charIndex:6638},{level:3,title:"4.4. codec",slug:"_4-4-codec",normalizedTitle:"4.4. codec",charIndex:6946},{level:4,title:"4.4.1. 常用 codec 插件",slug:"_4-4-1-常用-codec-插件",normalizedTitle:"4.4.1. 常用 codec 插件",charIndex:6973},{level:2,title:"5. 实战",slug:"_5-实战",normalizedTitle:"5. 实战",charIndex:7103},{level:3,title:"5.1. 传输控制台数据",slug:"_5-1-传输控制台数据",normalizedTitle:"5.1. 传输控制台数据",charIndex:7161},{level:3,title:"5.2. 传输 logback 日志",slug:"_5-2-传输-logback-日志",normalizedTitle:"5.2. 传输 logback 日志",charIndex:7546},{level:4,title:"5.2.1. TCP 应用",slug:"_5-2-1-tcp-应用",normalizedTitle:"5.2.1. tcp 应用",charIndex:7858},{level:4,title:"5.2.2. UDP 应用",slug:"_5-2-2-udp-应用",normalizedTitle:"5.2.2. udp 应用",charIndex:9613},{level:3,title:"5.3. 传输文件",slug:"_5-3-传输文件",normalizedTitle:"5.3. 传输文件",charIndex:10450},{level:2,title:"6. 小技巧",slug:"_6-小技巧",normalizedTitle:"6. 小技巧",charIndex:11157},{level:3,title:"6.1. 启动、终止应用",slug:"_6-1-启动、终止应用",normalizedTitle:"6.1. 启动、终止应用",charIndex:11168},{level:2,title:"7. 参考资料",slug:"_7-参考资料",normalizedTitle:"7. 参考资料",charIndex:11626}],headersStr:"1. 安装 1.1. 安装步骤 2. 配置 2.1. 设置文件 2.2. logstash.yml 设置项 3. 启动 3.1. 命令行 3.2. 配置文件 3.2.1. 配置文件结构 3.2.2. 插件配置 3.2.3. 值类型 4. 插件 4.1. input 4.1.1. 常用 input 插件 4.2. filter 4.2.1. 常用 filter 插件 4.3. output 4.3.1. 常用 output 插件 4.4. codec 4.4.1. 常用 codec 插件 5. 实战 5.1. 传输控制台数据 5.2. 传输 logback 日志 5.2.1. TCP 应用 5.2.2. UDP 应用 5.3. 传输文件 6. 小技巧 6.1. 启动、终止应用 7. 参考资料",content:'# Logstash 运维\n\n> Logstash 是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。\n\n\n# 1. 安装\n\n\n# 1.1. 安装步骤\n\n安装步骤如下：\n\n（1）在 logstash 官方下载地址下载所需版本包并解压到本地。\n\n（2）添加一个 logstash.conf 文件，指定要使用的插件以及每个插件的设置。举个简单的例子：\n\ninput { stdin { } }\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n（3）运行 bin/logstash -f logstash.conf （Windows 上运行bin/logstash.bat -f logstash.conf）\n\n\n# 2. 配置\n\n\n# 2.1. 设置文件\n\n * logstash.yml：logstash 的默认启动配置文件\n * jvm.options：logstash 的 JVM 配置文件。\n * startup.options (Linux)：包含系统安装脚本在 /usr/share/logstash/bin 中使用的选项为您的系统构建适当的启动脚本。安装 Logstash 软件包时，系统安装脚本将在安装过程结束时执行，并使用 startup.options 中指定的设置来设置用户，组，服务名称和服务描述等选项。\n\n\n# 2.2. logstash.yml 设置项\n\n节选部分设置项，更多项请参考：https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html\n\n参数                         描述                                                                                           默认值\nnode.name                  节点名                                                                                          机器的主机名\npath.data                  Logstash 及其插件用于任何持久性需求的目录。                                                                   LOGSTASH_HOME/data\npipeline.workers           同时执行管道的过滤器和输出阶段的工作任务数量。如果发现事件正在备份，或 CPU                                                      Number of the host’s CPU cores\n                           未饱和，请考虑增加此数字以更好地利用机器处理能力。\npipeline.batch.size        尝试执行过滤器和输出之前，单个工作线程从输入收集的最大事件数量。较大的批量处理大小一般来说效率更高，但是以增加的内存开销为代价。您可能必须通过设置                    125\n                           LS_HEAP_SIZE 变量来有效使用该选项来增加 JVM 堆大小。\npipeline.batch.delay       创建管道事件批处理时，在将一个尺寸过小的批次发送给管道工作任务之前，等待每个事件需要多长时间（毫秒）。                                          5\npipeline.unsafe_shutdown   如果设置为 true，则即使在内存中仍存在 inflight 事件时，也会强制 Logstash                                             false\n                           在关闭期间退出。默认情况下，Logstash\n                           将拒绝退出，直到所有接收到的事件都被推送到输出。启用此选项可能会导致关机期间数据丢失。\npath.config                主管道的 Logstash 配置路径。如果您指定一个目录或通配符，配置文件将按字母顺序从目录中读取。                                           Platform-specific. See [dir-layout].\nconfig.string              包含用于主管道的管道配置的字符串。使用与配置文件相同的语法。                                                               None\nconfig.test_and_exit       设置为 true 时，检查配置是否有效，然后退出。请注意，使用此设置不会检查 grok 模式的正确性。 Logstash                                 false\n                           可以从目录中读取多个配置文件。如果将此设置与 log.level：debug 结合使用，则 Logstash\n                           将记录组合的配置文件，并注掉其源文件的配置块。\nconfig.reload.automatic    设置为 true 时，定期检查配置是否已更改，并在配置更改时重新加载配置。这也可以通过 SIGHUP 信号手动触发。                                   false\nconfig.reload.interval     Logstash 检查配置文件更改的时间间隔。                                                                      3s\nconfig.debug               设置为 true                                                                                     false\n                           时，将完全编译的配置显示为调试日志消息。您还必须设置log.level：debug。警告：日志消息将包括任何传递给插件配置作为明文的“密码”选项，并可能导致明文密码出现在您的日志！\nconfig.support_escapes     当设置为 true 时，带引号的字符串将处理转义字符。                                                                  false\nmodules                    配置时，模块必须处于上表所述的嵌套 YAML 结构中。                                                                  None\nhttp.host                  绑定地址                                                                                         "127.0.0.1"\nhttp.port                  绑定端口                                                                                         9600\nlog.level                  日志级别。有效选项：fatal > error > warn > info > debug > trace                                        info\nlog.format                 日志格式。json （JSON 格式）或 plain （原对象）                                                             plain\npath.logs                  Logstash 自身日志的存储路径                                                                           LOGSTASH_HOME/logs\npath.plugins               在哪里可以找到自定义的插件。您可以多次指定此设置以包含多个路径。                                                             \n\n\n# 3. 启动\n\n\n# 3.1. 命令行\n\n通过命令行启动 logstash 的方式如下：\n\nbin/logstash [options]\n\n\n其中 options 是您可以指定用于控制 Logstash 执行的命令行标志。\n\n在命令行上设置的任何标志都会覆盖 Logstash 设置文件（logstash.yml）中的相应设置，但设置文件本身不会更改。\n\n> 注\n> \n> 虽然可以通过指定命令行参数的方式，来控制 logstash 的运行方式，但显然这么做很麻烦。\n> \n> 建议通过指定配置文件的方式，来控制 logstash 运行，启动命令如下：\n> \n> bin/logstash -f logstash.conf\n> \n> \n> 若想了解更多的命令行参数细节，请参考：https://www.elastic.co/guide/en/logstash/current/running-logstash-command-line.html\n\n\n# 3.2. 配置文件\n\n上节，我们了解到，logstash 可以执行 bin/logstash -f logstash.conf ，按照配置文件中的参数去覆盖默认设置文件（logstash.yml）中的设置。\n\n这节，我们就来学习一下这个配置文件如何配置参数。\n\n# 3.2.1. 配置文件结构\n\n在工作原理一节中，我们已经知道了 Logstash 主要有三个工作阶段 input 、filter、output。而 logstash 配置文件文件结构也与之相对应：\n\ninput {}\n\nfilter {}\n\noutput {}\n\n\n> 每个部分都包含一个或多个插件的配置选项。如果指定了多个过滤器，则会按照它们在配置文件中的显示顺序应用它们。\n\n# 3.2.2. 插件配置\n\n插件的配置由插件名称和插件的一个设置块组成。\n\n下面的例子中配置了两个输入文件配置：\n\ninput {\n  file {\n    path => "/var/log/messages"\n    type => "syslog"\n  }\n\n  file {\n    path => "/var/log/apache/access.log"\n    type => "apache"\n  }\n}\n\n\n您可以配置的设置因插件类型而异。你可以参考： Input Plugins, Output Plugins, Filter Plugins, 和 Codec Plugins 。\n\n# 3.2.3. 值类型\n\n一个插件可以要求设置的值是一个特定的类型，比如布尔值，列表或哈希值。以下值类型受支持。\n\n * Array\n\n  users => [ {id => 1, name => bob}, {id => 2, name => jane} ]\n\n\n * Lists\n\n  path => [ "/var/log/messages", "/var/log/*.log" ]\n  uris => [ "http://elastic.co", "http://example.net" ]\n\n\n * Boolean\n\n  ssl_enable => true\n\n\n * Bytes\n\n  my_bytes => "1113"   # 1113 bytes\n  my_bytes => "10MiB"  # 10485760 bytes\n  my_bytes => "100kib" # 102400 bytes\n  my_bytes => "180 mb" # 180000000 bytes\n\n\n * Codec\n\n  codec => "json"\n\n\n * Hash\n\nmatch => {\n  "field1" => "value1"\n  "field2" => "value2"\n  ...\n}\n\n\n * Number\n\n  port => 33\n\n\n * Password\n\n  my_password => "password"\n\n\n * URI\n\n  my_uri => "http://foo:bar@example.net"\n\n\n * Path\n\n  my_path => "/tmp/logstash"\n\n\n * String\n\n * 转义字符\n\n\n# 4. 插件\n\n\n# 4.1. input\n\n> Logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。能够以连续的流式传输方式，轻松地从您的日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。\n\n# 4.1.1. 常用 input 插件\n\n * file：从文件系统上的文件读取，就像 UNIX 命令 tail -0F 一样\n * **syslog：**在众所周知的端口 514 上侦听系统日志消息，并根据 RFC3164 格式进行解析\n * **redis：**从 redis 服务器读取，使用 redis 通道和 redis 列表。 Redis 经常用作集中式 Logstash 安装中的“代理”，它将来自远程 Logstash“托运人”的 Logstash 事件排队。\n * **beats：**处理由 Filebeat 发送的事件。\n\n更多详情请见：Input Plugins\n\n\n# 4.2. filter\n\n> 过滤器是 Logstash 管道中的中间处理设备。如果符合特定条件，您可以将条件过滤器组合在一起，对事件执行操作。\n\n# 4.2.1. 常用 filter 插件\n\n * **grok：**解析和结构任意文本。 Grok 目前是 Logstash 中将非结构化日志数据解析为结构化和可查询的最佳方法。\n\n * **mutate：**对事件字段执行一般转换。您可以重命名，删除，替换和修改事件中的字段。\n\n * **drop：**完全放弃一个事件，例如调试事件。\n\n * **clone：**制作一个事件的副本，可能会添加或删除字段。\n\n * **geoip：**添加有关 IP 地址的地理位置的信息（也可以在 Kibana 中显示惊人的图表！）\n\n更多详情请见：Filter Plugins\n\n\n# 4.3. output\n\n> 输出是 Logstash 管道的最后阶段。一个事件可以通过多个输出，但是一旦所有输出处理完成，事件就完成了执行。\n\n# 4.3.1. 常用 output 插件\n\n * **elasticsearch：**将事件数据发送给 Elasticsearch（推荐模式）。\n * **file：**将事件数据写入文件或磁盘。\n * **graphite：**将事件数据发送给 graphite（一个流行的开源工具，存储和绘制指标。 http://graphite.readthedocs.io/en/latest/）。\n * **statsd：**将事件数据发送到 statsd （这是一种侦听统计数据的服务，如计数器和定时器，通过 UDP 发送并将聚合发送到一个或多个可插入的后端服务）。\n\n更多详情请见：Output Plugins\n\n\n# 4.4. codec\n\n用于格式化对应的内容。\n\n# 4.4.1. 常用 codec 插件\n\n * **json：**以 JSON 格式对数据进行编码或解码。\n * **multiline：**将多行文本事件（如 java 异常和堆栈跟踪消息）合并为单个事件。\n\n更多插件请见：Codec Plugins\n\n\n# 5. 实战\n\n前面的内容都是对 Logstash 的介绍和原理说明。接下来，我们来实战一些常见的应用场景。\n\n\n# 5.1. 传输控制台数据\n\n> stdin input 插件从标准输入读取事件。这是最简单的 input 插件，一般用于测试场景。\n\n应用\n\n（1）创建 logstash-input-stdin.conf ：\n\ninput { stdin { } }\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：\n\nbin/logstash -f logstash-input-stdin.conf\n\n\n\n# 5.2. 传输 logback 日志\n\n> elk 默认使用的 Java 日志工具是 log4j2 ，并不支持 logback 和 log4j。\n> \n> 想使用 logback + logstash ，可以使用 logstash-logback-encoder 。logstash-logback-encoder 提供了 UDP / TCP / 异步方式来传输日志数据到 logstash。\n> \n> 如果你使用的是 log4j ，也不是不可以用这种方式，只要引入桥接 jar 包即可。如果你对 log4j 、logback ，或是桥接 jar 包不太了解，可以参考我的这篇博文：细说 Java 主流日志工具库 。\n\n# 5.2.1. TCP 应用\n\nlogstash 配置：\n\n（1）创建 logstash-input-tcp.conf ：\n\ninput {\n  # stdin { }\n  tcp {\n    # host:port就是上面appender中的 destination，\n # 这里其实把logstash作为服务，开启9250端口接收logback发出的消息\n    host => "127.0.0.1" port => 9250 mode => "server" tags => ["tags"] codec => json_lines\n  }\n}\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置：\n\n（1）在 Java 应用的 pom.xml 中引入 jar 包：\n\n<dependency>\n <groupId>net.logstash.logback</groupId>\n <artifactId>logstash-logback-encoder</artifactId>\n <version>4.11</version>\n</dependency>\n\n\x3c!-- logback 依赖包 --\x3e\n<dependency>\n <groupId>ch.qos.logback</groupId>\n <artifactId>logback-core</artifactId>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupId>ch.qos.logback</groupId>\n <artifactId>logback-classic</artifactId>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupId>ch.qos.logback</groupId>\n <artifactId>logback-access</artifactId>\n <version>1.2.3</version>\n</dependency>\n\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="ELK-TCP" class="net.logstash.logback.appender.LogstashTcpSocketAppender">\n \x3c!--\n destination 是 logstash 服务的 host:port，\n 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash\n --\x3e\n <destination>192.168.28.32:9251</destination>\n <encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder"/>\n</appender>\n<logger name="io.github.dunwu.spring" level="TRACE" additivity="false">\n <appender-ref ref="ELK-TCP" />\n</logger>\n\n\n大功告成，此后，io.github.dunwu.spring 包中的 TRACE 及以上级别的日志信息都会被定向输出到 logstash 服务。\n\n\n\n接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 Java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n# 5.2.2. UDP 应用\n\nUDP 和 TCP 的使用方式大同小异。\n\nlogstash 配置：\n\n（1）创建 logstash-input-udp.conf ：\n\ninput {\nudp {\n  port => 9250\n  codec => json\n}\n}\noutput {\n elasticsearch { hosts => ["localhost:9200"] }\n stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置：\n\n（1）在 Java 应用的 pom.xml 中引入 jar 包：\n\n与 TCP 应用 一节中的引入依赖包完全相同。\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="ELK-UDP" class="net.logstash.logback.appender.LogstashSocketAppender">\n  <host>192.168.28.32</host>\n  <port>9250</port>\n</appender>\n<logger name="io.github.dunwu.spring" level="TRACE" additivity="false">\n  <appender-ref ref="ELK-UDP" />\n</logger>\n\n\n（3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 Java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n\n# 5.3. 传输文件\n\n> 在 Java Web 领域，需要用到一些重要的工具，例如 Tomcat 、Nginx 、Mysql 等。这些不属于业务应用，但是它们的日志数据对于定位问题、分析统计同样很重要。这时无法使用 logback 方式将它们的日志传输到 logstash。\n> \n> 如何采集这些日志文件呢？别急，你可以使用 logstash 的 file input 插件。\n> \n> 需要注意的是，传输文件这种方式，必须在日志所在的机器上部署 logstash 。\n\n应用\n\nlogstash 配置\n\n（1）创建 logstash-input-file.conf ：\n\ninput {\n file {\n  path => ["/var/log/nginx/access.log"]\n  type => "nginx-access-log"\n  start_position => "beginning"\n }\n}\n\noutput {\n if [type] == "nginx-access-log" {\n  elasticsearch {\n   hosts => ["localhost:9200"]\n   index => "nginx-access-log"\n  }\n }\n}\n\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-file.conf\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html\n\n\n# 6. 小技巧\n\n\n# 6.1. 启动、终止应用\n\n如果你的 logstash 每次都是通过指定配置文件方式启动。不妨建立一个启动脚本。\n\n# cd xxx 进入 logstash 安装目录下的 bin 目录\nlogstash -f logstash.conf\n\n\n如果你的 logstash 运行在 linux 系统下，不妨使用 nohup 来启动一个守护进程。这样做的好处在于，即使关闭终端，应用仍会运行。\n\n创建 startup.sh：\n\nnohup ./logstash -f logstash.conf >> nohup.out 2>&1 &\n\n\n终止应用没有什么好方法，你只能使用 ps -ef | grep logstash ，查出进程，将其 kill 。不过，我们可以写一个脚本来干这件事：\n\n创建 shutdown.sh：\n\n脚本不多解释，请自行领会作用。\n\nPID=`ps -ef | grep logstash | awk \'{ print $2}\' | head -n 1`\nkill -9 ${PID}\n\n\n\n# 7. 参考资料\n\n * Logstash 官网\n * Logstash Github\n * Logstash 官方文档\n * logstash-logback-encoder\n * ELK Stack 权威指南\n * ELK（Elasticsearch、Logstash、Kibana）安装和配置',normalizedContent:'# logstash 运维\n\n> logstash 是开源的服务器端数据处理管道，能够同时从多个来源采集数据，转换数据，然后将数据发送到您最喜欢的“存储库”中。\n\n\n# 1. 安装\n\n\n# 1.1. 安装步骤\n\n安装步骤如下：\n\n（1）在 logstash 官方下载地址下载所需版本包并解压到本地。\n\n（2）添加一个 logstash.conf 文件，指定要使用的插件以及每个插件的设置。举个简单的例子：\n\ninput { stdin { } }\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n（3）运行 bin/logstash -f logstash.conf （windows 上运行bin/logstash.bat -f logstash.conf）\n\n\n# 2. 配置\n\n\n# 2.1. 设置文件\n\n * logstash.yml：logstash 的默认启动配置文件\n * jvm.options：logstash 的 jvm 配置文件。\n * startup.options (linux)：包含系统安装脚本在 /usr/share/logstash/bin 中使用的选项为您的系统构建适当的启动脚本。安装 logstash 软件包时，系统安装脚本将在安装过程结束时执行，并使用 startup.options 中指定的设置来设置用户，组，服务名称和服务描述等选项。\n\n\n# 2.2. logstash.yml 设置项\n\n节选部分设置项，更多项请参考：https://www.elastic.co/guide/en/logstash/current/logstash-settings-file.html\n\n参数                         描述                                                                                           默认值\nnode.name                  节点名                                                                                          机器的主机名\npath.data                  logstash 及其插件用于任何持久性需求的目录。                                                                   logstash_home/data\npipeline.workers           同时执行管道的过滤器和输出阶段的工作任务数量。如果发现事件正在备份，或 cpu                                                      number of the host’s cpu cores\n                           未饱和，请考虑增加此数字以更好地利用机器处理能力。\npipeline.batch.size        尝试执行过滤器和输出之前，单个工作线程从输入收集的最大事件数量。较大的批量处理大小一般来说效率更高，但是以增加的内存开销为代价。您可能必须通过设置                    125\n                           ls_heap_size 变量来有效使用该选项来增加 jvm 堆大小。\npipeline.batch.delay       创建管道事件批处理时，在将一个尺寸过小的批次发送给管道工作任务之前，等待每个事件需要多长时间（毫秒）。                                          5\npipeline.unsafe_shutdown   如果设置为 true，则即使在内存中仍存在 inflight 事件时，也会强制 logstash                                             false\n                           在关闭期间退出。默认情况下，logstash\n                           将拒绝退出，直到所有接收到的事件都被推送到输出。启用此选项可能会导致关机期间数据丢失。\npath.config                主管道的 logstash 配置路径。如果您指定一个目录或通配符，配置文件将按字母顺序从目录中读取。                                           platform-specific. see [dir-layout].\nconfig.string              包含用于主管道的管道配置的字符串。使用与配置文件相同的语法。                                                               none\nconfig.test_and_exit       设置为 true 时，检查配置是否有效，然后退出。请注意，使用此设置不会检查 grok 模式的正确性。 logstash                                 false\n                           可以从目录中读取多个配置文件。如果将此设置与 log.level：debug 结合使用，则 logstash\n                           将记录组合的配置文件，并注掉其源文件的配置块。\nconfig.reload.automatic    设置为 true 时，定期检查配置是否已更改，并在配置更改时重新加载配置。这也可以通过 sighup 信号手动触发。                                   false\nconfig.reload.interval     logstash 检查配置文件更改的时间间隔。                                                                      3s\nconfig.debug               设置为 true                                                                                     false\n                           时，将完全编译的配置显示为调试日志消息。您还必须设置log.level：debug。警告：日志消息将包括任何传递给插件配置作为明文的“密码”选项，并可能导致明文密码出现在您的日志！\nconfig.support_escapes     当设置为 true 时，带引号的字符串将处理转义字符。                                                                  false\nmodules                    配置时，模块必须处于上表所述的嵌套 yaml 结构中。                                                                  none\nhttp.host                  绑定地址                                                                                         "127.0.0.1"\nhttp.port                  绑定端口                                                                                         9600\nlog.level                  日志级别。有效选项：fatal > error > warn > info > debug > trace                                        info\nlog.format                 日志格式。json （json 格式）或 plain （原对象）                                                             plain\npath.logs                  logstash 自身日志的存储路径                                                                           logstash_home/logs\npath.plugins               在哪里可以找到自定义的插件。您可以多次指定此设置以包含多个路径。                                                             \n\n\n# 3. 启动\n\n\n# 3.1. 命令行\n\n通过命令行启动 logstash 的方式如下：\n\nbin/logstash [options]\n\n\n其中 options 是您可以指定用于控制 logstash 执行的命令行标志。\n\n在命令行上设置的任何标志都会覆盖 logstash 设置文件（logstash.yml）中的相应设置，但设置文件本身不会更改。\n\n> 注\n> \n> 虽然可以通过指定命令行参数的方式，来控制 logstash 的运行方式，但显然这么做很麻烦。\n> \n> 建议通过指定配置文件的方式，来控制 logstash 运行，启动命令如下：\n> \n> bin/logstash -f logstash.conf\n> \n> \n> 若想了解更多的命令行参数细节，请参考：https://www.elastic.co/guide/en/logstash/current/running-logstash-command-line.html\n\n\n# 3.2. 配置文件\n\n上节，我们了解到，logstash 可以执行 bin/logstash -f logstash.conf ，按照配置文件中的参数去覆盖默认设置文件（logstash.yml）中的设置。\n\n这节，我们就来学习一下这个配置文件如何配置参数。\n\n# 3.2.1. 配置文件结构\n\n在工作原理一节中，我们已经知道了 logstash 主要有三个工作阶段 input 、filter、output。而 logstash 配置文件文件结构也与之相对应：\n\ninput {}\n\nfilter {}\n\noutput {}\n\n\n> 每个部分都包含一个或多个插件的配置选项。如果指定了多个过滤器，则会按照它们在配置文件中的显示顺序应用它们。\n\n# 3.2.2. 插件配置\n\n插件的配置由插件名称和插件的一个设置块组成。\n\n下面的例子中配置了两个输入文件配置：\n\ninput {\n  file {\n    path => "/var/log/messages"\n    type => "syslog"\n  }\n\n  file {\n    path => "/var/log/apache/access.log"\n    type => "apache"\n  }\n}\n\n\n您可以配置的设置因插件类型而异。你可以参考： input plugins, output plugins, filter plugins, 和 codec plugins 。\n\n# 3.2.3. 值类型\n\n一个插件可以要求设置的值是一个特定的类型，比如布尔值，列表或哈希值。以下值类型受支持。\n\n * array\n\n  users => [ {id => 1, name => bob}, {id => 2, name => jane} ]\n\n\n * lists\n\n  path => [ "/var/log/messages", "/var/log/*.log" ]\n  uris => [ "http://elastic.co", "http://example.net" ]\n\n\n * boolean\n\n  ssl_enable => true\n\n\n * bytes\n\n  my_bytes => "1113"   # 1113 bytes\n  my_bytes => "10mib"  # 10485760 bytes\n  my_bytes => "100kib" # 102400 bytes\n  my_bytes => "180 mb" # 180000000 bytes\n\n\n * codec\n\n  codec => "json"\n\n\n * hash\n\nmatch => {\n  "field1" => "value1"\n  "field2" => "value2"\n  ...\n}\n\n\n * number\n\n  port => 33\n\n\n * password\n\n  my_password => "password"\n\n\n * uri\n\n  my_uri => "http://foo:bar@example.net"\n\n\n * path\n\n  my_path => "/tmp/logstash"\n\n\n * string\n\n * 转义字符\n\n\n# 4. 插件\n\n\n# 4.1. input\n\n> logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。能够以连续的流式传输方式，轻松地从您的日志、指标、web 应用、数据存储以及各种 aws 服务采集数据。\n\n# 4.1.1. 常用 input 插件\n\n * file：从文件系统上的文件读取，就像 unix 命令 tail -0f 一样\n * **syslog：**在众所周知的端口 514 上侦听系统日志消息，并根据 rfc3164 格式进行解析\n * **redis：**从 redis 服务器读取，使用 redis 通道和 redis 列表。 redis 经常用作集中式 logstash 安装中的“代理”，它将来自远程 logstash“托运人”的 logstash 事件排队。\n * **beats：**处理由 filebeat 发送的事件。\n\n更多详情请见：input plugins\n\n\n# 4.2. filter\n\n> 过滤器是 logstash 管道中的中间处理设备。如果符合特定条件，您可以将条件过滤器组合在一起，对事件执行操作。\n\n# 4.2.1. 常用 filter 插件\n\n * **grok：**解析和结构任意文本。 grok 目前是 logstash 中将非结构化日志数据解析为结构化和可查询的最佳方法。\n\n * **mutate：**对事件字段执行一般转换。您可以重命名，删除，替换和修改事件中的字段。\n\n * **drop：**完全放弃一个事件，例如调试事件。\n\n * **clone：**制作一个事件的副本，可能会添加或删除字段。\n\n * **geoip：**添加有关 ip 地址的地理位置的信息（也可以在 kibana 中显示惊人的图表！）\n\n更多详情请见：filter plugins\n\n\n# 4.3. output\n\n> 输出是 logstash 管道的最后阶段。一个事件可以通过多个输出，但是一旦所有输出处理完成，事件就完成了执行。\n\n# 4.3.1. 常用 output 插件\n\n * **elasticsearch：**将事件数据发送给 elasticsearch（推荐模式）。\n * **file：**将事件数据写入文件或磁盘。\n * **graphite：**将事件数据发送给 graphite（一个流行的开源工具，存储和绘制指标。 http://graphite.readthedocs.io/en/latest/）。\n * **statsd：**将事件数据发送到 statsd （这是一种侦听统计数据的服务，如计数器和定时器，通过 udp 发送并将聚合发送到一个或多个可插入的后端服务）。\n\n更多详情请见：output plugins\n\n\n# 4.4. codec\n\n用于格式化对应的内容。\n\n# 4.4.1. 常用 codec 插件\n\n * **json：**以 json 格式对数据进行编码或解码。\n * **multiline：**将多行文本事件（如 java 异常和堆栈跟踪消息）合并为单个事件。\n\n更多插件请见：codec plugins\n\n\n# 5. 实战\n\n前面的内容都是对 logstash 的介绍和原理说明。接下来，我们来实战一些常见的应用场景。\n\n\n# 5.1. 传输控制台数据\n\n> stdin input 插件从标准输入读取事件。这是最简单的 input 插件，一般用于测试场景。\n\n应用\n\n（1）创建 logstash-input-stdin.conf ：\n\ninput { stdin { } }\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：\n\nbin/logstash -f logstash-input-stdin.conf\n\n\n\n# 5.2. 传输 logback 日志\n\n> elk 默认使用的 java 日志工具是 log4j2 ，并不支持 logback 和 log4j。\n> \n> 想使用 logback + logstash ，可以使用 logstash-logback-encoder 。logstash-logback-encoder 提供了 udp / tcp / 异步方式来传输日志数据到 logstash。\n> \n> 如果你使用的是 log4j ，也不是不可以用这种方式，只要引入桥接 jar 包即可。如果你对 log4j 、logback ，或是桥接 jar 包不太了解，可以参考我的这篇博文：细说 java 主流日志工具库 。\n\n# 5.2.1. tcp 应用\n\nlogstash 配置：\n\n（1）创建 logstash-input-tcp.conf ：\n\ninput {\n  # stdin { }\n  tcp {\n    # host:port就是上面appender中的 destination，\n # 这里其实把logstash作为服务，开启9250端口接收logback发出的消息\n    host => "127.0.0.1" port => 9250 mode => "server" tags => ["tags"] codec => json_lines\n  }\n}\noutput {\n  elasticsearch { hosts => ["localhost:9200"] }\n  stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置：\n\n（1）在 java 应用的 pom.xml 中引入 jar 包：\n\n<dependency>\n <groupid>net.logstash.logback</groupid>\n <artifactid>logstash-logback-encoder</artifactid>\n <version>4.11</version>\n</dependency>\n\n\x3c!-- logback 依赖包 --\x3e\n<dependency>\n <groupid>ch.qos.logback</groupid>\n <artifactid>logback-core</artifactid>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupid>ch.qos.logback</groupid>\n <artifactid>logback-classic</artifactid>\n <version>1.2.3</version>\n</dependency>\n<dependency>\n <groupid>ch.qos.logback</groupid>\n <artifactid>logback-access</artifactid>\n <version>1.2.3</version>\n</dependency>\n\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="elk-tcp" class="net.logstash.logback.appender.logstashtcpsocketappender">\n \x3c!--\n destination 是 logstash 服务的 host:port，\n 相当于和 logstash 建立了管道，将日志数据定向传输到 logstash\n --\x3e\n <destination>192.168.28.32:9251</destination>\n <encoder charset="utf-8" class="net.logstash.logback.encoder.logstashencoder"/>\n</appender>\n<logger name="io.github.dunwu.spring" level="trace" additivity="false">\n <appender-ref ref="elk-tcp" />\n</logger>\n\n\n大功告成，此后，io.github.dunwu.spring 包中的 trace 及以上级别的日志信息都会被定向输出到 logstash 服务。\n\n\n\n接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n# 5.2.2. udp 应用\n\nudp 和 tcp 的使用方式大同小异。\n\nlogstash 配置：\n\n（1）创建 logstash-input-udp.conf ：\n\ninput {\nudp {\n  port => 9250\n  codec => json\n}\n}\noutput {\n elasticsearch { hosts => ["localhost:9200"] }\n stdout { codec => rubydebug }\n}\n\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-udp.conf\n\njava 应用配置：\n\n（1）在 java 应用的 pom.xml 中引入 jar 包：\n\n与 tcp 应用 一节中的引入依赖包完全相同。\n\n（2）接着，在 logback.xml 中添加 appender\n\n<appender name="elk-udp" class="net.logstash.logback.appender.logstashsocketappender">\n  <host>192.168.28.32</host>\n  <port>9250</port>\n</appender>\n<logger name="io.github.dunwu.spring" level="trace" additivity="false">\n  <appender-ref ref="elk-udp" />\n</logger>\n\n\n（3）接下来，就是 logback 的具体使用 ，如果对此不了解，不妨参考一下我的这篇博文：细说 java 主流日志工具库 。\n\n实例：我的 logback.xml\n\n\n# 5.3. 传输文件\n\n> 在 java web 领域，需要用到一些重要的工具，例如 tomcat 、nginx 、mysql 等。这些不属于业务应用，但是它们的日志数据对于定位问题、分析统计同样很重要。这时无法使用 logback 方式将它们的日志传输到 logstash。\n> \n> 如何采集这些日志文件呢？别急，你可以使用 logstash 的 file input 插件。\n> \n> 需要注意的是，传输文件这种方式，必须在日志所在的机器上部署 logstash 。\n\n应用\n\nlogstash 配置\n\n（1）创建 logstash-input-file.conf ：\n\ninput {\n file {\n  path => ["/var/log/nginx/access.log"]\n  type => "nginx-access-log"\n  start_position => "beginning"\n }\n}\n\noutput {\n if [type] == "nginx-access-log" {\n  elasticsearch {\n   hosts => ["localhost:9200"]\n   index => "nginx-access-log"\n  }\n }\n}\n\n\n（2）执行 logstash，使用 -f 来指定你的配置文件：bin/logstash -f logstash-input-file.conf\n\n更多配置项可以参考：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html\n\n\n# 6. 小技巧\n\n\n# 6.1. 启动、终止应用\n\n如果你的 logstash 每次都是通过指定配置文件方式启动。不妨建立一个启动脚本。\n\n# cd xxx 进入 logstash 安装目录下的 bin 目录\nlogstash -f logstash.conf\n\n\n如果你的 logstash 运行在 linux 系统下，不妨使用 nohup 来启动一个守护进程。这样做的好处在于，即使关闭终端，应用仍会运行。\n\n创建 startup.sh：\n\nnohup ./logstash -f logstash.conf >> nohup.out 2>&1 &\n\n\n终止应用没有什么好方法，你只能使用 ps -ef | grep logstash ，查出进程，将其 kill 。不过，我们可以写一个脚本来干这件事：\n\n创建 shutdown.sh：\n\n脚本不多解释，请自行领会作用。\n\npid=`ps -ef | grep logstash | awk \'{ print $2}\' | head -n 1`\nkill -9 ${pid}\n\n\n\n# 7. 参考资料\n\n * logstash 官网\n * logstash github\n * logstash 官方文档\n * logstash-logback-encoder\n * elk stack 权威指南\n * elk（elasticsearch、logstash、kibana）安装和配置',charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Elastic 技术栈",frontmatter:{title:"Elastic 技术栈",date:"2022-04-11T16:52:35.000Z",categories:["数据库","搜索引擎数据库","Elastic"],tags:["数据库","搜索引擎数据库","Elastic"],permalink:"/pages/7bf7f7/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/07.%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%95%B0%E6%8D%AE%E5%BA%93/02.Elastic/",relativePath:"12.数据库/07.搜索引擎数据库/02.Elastic/README.md",key:"v-0d48f15b",path:"/pages/7bf7f7/",headers:[{level:2,title:"📖 内容",slug:"📖-内容",normalizedTitle:"📖 内容",charIndex:258},{level:2,title:"📚 资料",slug:"📚-资料",normalizedTitle:"📚 资料",charIndex:401},{level:2,title:"🚪 传送",slug:"🚪-传送",normalizedTitle:"🚪 传送",charIndex:702}],headersStr:"📖 内容 📚 资料 🚪 传送",content:"# Elastic 技术栈\n\n> Elastic 技术栈通常被用来作为日志采集、检索、可视化的解决方案。\n> \n> ELK 是 elastic 公司旗下三款产品 Elasticsearch 、Logstash 、Kibana 的首字母组合。\n> \n> Logstash 传输和处理你的日志、事务或其他数据。\n> \n> Kibana 将 Elasticsearch 的数据分析并渲染为可视化的报表。\n> \n> Elastic 技术栈，在 ELK 的基础上扩展了一些新的产品，如：Beats 、X-Pack 。\n\n\n# 📖 内容\n\n * Elastic 快速入门\n * Elastic 技术栈之 Filebeat\n * Filebeat 运维\n * Elastic 技术栈之 Kibana\n * Kibana 运维\n * Elastic 技术栈之 Logstash\n * Logstash 运维\n\n\n# 📚 资料\n\n * 官方\n   * Logstash 官网\n   * Logstash Github\n   * Logstash 官方文档\n   * Kibana 官网\n   * Kibana Github\n   * Kibana 官方文档\n   * Beats 官网\n   * Beats Github\n   * Beats 官方文档\n * 第三方工具\n   * logstash-logback-encoder\n * 文章\n   * Elasticsearch+Logstash+Kibana 教程\n   * ELK（Elasticsearch、Logstash、Kibana）安装和配置\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"# elastic 技术栈\n\n> elastic 技术栈通常被用来作为日志采集、检索、可视化的解决方案。\n> \n> elk 是 elastic 公司旗下三款产品 elasticsearch 、logstash 、kibana 的首字母组合。\n> \n> logstash 传输和处理你的日志、事务或其他数据。\n> \n> kibana 将 elasticsearch 的数据分析并渲染为可视化的报表。\n> \n> elastic 技术栈，在 elk 的基础上扩展了一些新的产品，如：beats 、x-pack 。\n\n\n# 📖 内容\n\n * elastic 快速入门\n * elastic 技术栈之 filebeat\n * filebeat 运维\n * elastic 技术栈之 kibana\n * kibana 运维\n * elastic 技术栈之 logstash\n * logstash 运维\n\n\n# 📚 资料\n\n * 官方\n   * logstash 官网\n   * logstash github\n   * logstash 官方文档\n   * kibana 官网\n   * kibana github\n   * kibana 官方文档\n   * beats 官网\n   * beats github\n   * beats 官方文档\n * 第三方工具\n   * logstash-logback-encoder\n * 文章\n   * elasticsearch+logstash+kibana 教程\n   * elk（elasticsearch、logstash、kibana）安装和配置\n\n\n# 🚪 传送\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"数据库",frontmatter:{title:"数据库",date:"2022-02-22T21:01:01.000Z",categories:["数据库"],tags:["数据库"],permalink:"/pages/012488/",hidden:!0},regularPath:"/12.%E6%95%B0%E6%8D%AE%E5%BA%93/",relativePath:"12.数据库/README.md",key:"v-72683126",path:"/pages/012488/",headers:[{level:2,title:"数据库综合",slug:"数据库综合",normalizedTitle:"数据库综合",charIndex:118},{level:3,title:"分布式存储原理",slug:"分布式存储原理",normalizedTitle:"分布式存储原理",charIndex:128},{level:4,title:"分布式理论",slug:"分布式理论",normalizedTitle:"分布式理论",charIndex:139},{level:4,title:"分布式关键技术",slug:"分布式关键技术",normalizedTitle:"分布式关键技术",charIndex:211},{level:5,title:"流量调度",slug:"流量调度",normalizedTitle:"流量调度",charIndex:222},{level:5,title:"数据调度",slug:"数据调度",normalizedTitle:"数据调度",charIndex:268},{level:3,title:"其他",slug:"其他",normalizedTitle:"其他",charIndex:352},{level:2,title:"数据库中间件",slug:"数据库中间件",normalizedTitle:"数据库中间件",charIndex:388},{level:2,title:"关系型数据库",slug:"关系型数据库",normalizedTitle:"关系型数据库",charIndex:462},{level:3,title:"公共知识",slug:"公共知识",normalizedTitle:"公共知识",charIndex:498},{level:3,title:"Mysql",slug:"mysql",normalizedTitle:"mysql",charIndex:582},{level:3,title:"其他",slug:"其他-2",normalizedTitle:"其他",charIndex:352},{level:2,title:"文档数据库",slug:"文档数据库",normalizedTitle:"文档数据库",charIndex:915},{level:3,title:"MongoDB",slug:"mongodb",normalizedTitle:"mongodb",charIndex:925},{level:2,title:"KV 数据库",slug:"kv-数据库",normalizedTitle:"kv 数据库",charIndex:1358},{level:3,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:1369},{level:2,title:"列式数据库",slug:"列式数据库",normalizedTitle:"列式数据库",charIndex:1799},{level:3,title:"HBase",slug:"hbase",normalizedTitle:"hbase",charIndex:1809},{level:2,title:"搜索引擎数据库",slug:"搜索引擎数据库",normalizedTitle:"搜索引擎数据库",charIndex:1928},{level:3,title:"Elasticsearch",slug:"elasticsearch",normalizedTitle:"elasticsearch",charIndex:1940},{level:3,title:"Elastic",slug:"elastic",normalizedTitle:"elastic",charIndex:1940},{level:2,title:"资料 📚",slug:"资料-📚",normalizedTitle:"资料 📚",charIndex:2539},{level:3,title:"数据库综合资料",slug:"数据库综合资料",normalizedTitle:"数据库综合资料",charIndex:2549},{level:3,title:"关系型数据库资料",slug:"关系型数据库资料",normalizedTitle:"关系型数据库资料",charIndex:3007},{level:4,title:"Mysql 资料",slug:"mysql-资料",normalizedTitle:"mysql 资料",charIndex:3134},{level:3,title:"Nosql 数据库综合",slug:"nosql-数据库综合",normalizedTitle:"nosql 数据库综合",charIndex:5460},{level:3,title:"列式数据库资料",slug:"列式数据库资料",normalizedTitle:"列式数据库资料",charIndex:6465},{level:4,title:"Cassandra 资料",slug:"cassandra-资料",normalizedTitle:"cassandra 资料",charIndex:6476},{level:4,title:"HBase 资料",slug:"hbase-资料",normalizedTitle:"hbase 资料",charIndex:7004},{level:3,title:"KV 数据库资料",slug:"kv-数据库资料",normalizedTitle:"kv 数据库资料",charIndex:7582},{level:4,title:"Redis 资料",slug:"redis-资料",normalizedTitle:"redis 资料",charIndex:7594},{level:3,title:"文档数据库资料",slug:"文档数据库资料",normalizedTitle:"文档数据库资料",charIndex:8545},{level:4,title:"MongoDB 资料",slug:"mongodb-资料",normalizedTitle:"mongodb 资料",charIndex:8670},{level:3,title:"搜索引擎数据库资料",slug:"搜索引擎数据库资料",normalizedTitle:"搜索引擎数据库资料",charIndex:9075},{level:4,title:"ElasticSearch",slug:"elasticsearch-2",normalizedTitle:"elasticsearch",charIndex:2299},{level:3,title:"图数据库",slug:"图数据库",normalizedTitle:"图数据库",charIndex:9698},{level:3,title:"时序数据库",slug:"时序数据库",normalizedTitle:"时序数据库",charIndex:10265},{level:2,title:"传送 🚪",slug:"传送-🚪",normalizedTitle:"传送 🚪",charIndex:10768}],headersStr:"数据库综合 分布式存储原理 分布式理论 分布式关键技术 流量调度 数据调度 其他 数据库中间件 关系型数据库 公共知识 Mysql 其他 文档数据库 MongoDB KV 数据库 Redis 列式数据库 HBase 搜索引擎数据库 Elasticsearch Elastic 资料 📚 数据库综合资料 关系型数据库资料 Mysql 资料 Nosql 数据库综合 列式数据库资料 Cassandra 资料 HBase 资料 KV 数据库资料 Redis 资料 文档数据库资料 MongoDB 资料 搜索引擎数据库资料 ElasticSearch 图数据库 时序数据库 传送 🚪",content:"DB-TUTORIAL\n\n> 💾 db-tutorial 是一个数据库教程。\n> \n>  * 🔁 项目同步维护：Github | Gitee\n>  * 📖 电子书阅读：Github Pages | Gitee Pages\n\n\n# 数据库综合\n\n\n# 分布式存储原理\n\n# 分布式理论\n\n * 分布式理论\n * 深入剖析共识性算法 Paxos\n * 深入剖析共识性算法 Raft\n * 分布式算法 Gossip\n\n# 分布式关键技术\n\n# 流量调度\n\n * 流量控制\n * 负载均衡\n * 服务路由\n * 分布式会话基本原理\n\n# 数据调度\n\n * 缓存基本原理\n * 读写分离基本原理\n * 分库分表基本原理\n * 分布式 ID 基本原理\n * 分布式事务基本原理\n * 分布式锁基本原理\n\n\n# 其他\n\n * Nosql 技术选型\n * 数据结构与数据库索引\n\n\n# 数据库中间件\n\n * ShardingSphere 简介\n * ShardingSphere Jdbc\n * 版本管理中间件 Flyway\n\n\n# 关系型数据库\n\n> 关系型数据库 整理主流关系型数据库知识点。\n\n\n# 公共知识\n\n * 关系型数据库面试总结 💯\n * SQL Cheat Sheet 是一个 SQL 入门教程。\n * 扩展 SQL 是一个 SQL 入门教程。\n\n\n# Mysql\n\n\n\n * Mysql 应用指南 ⚡\n * Mysql 工作流 - 关键词：连接、缓存、语法分析、优化、执行引擎、redo log、bin log、两阶段提交\n * Mysql 事务 - 关键词：ACID、AUTOCOMMIT、事务隔离级别、死锁、分布式事务\n * Mysql 锁 - 关键词：乐观锁、表级锁、行级锁、意向锁、MVCC、Next-key 锁\n * Mysql 索引 - 关键词：Hash、B 树、聚簇索引、回表\n * Mysql 性能优化\n * Mysql 运维 🔨\n * Mysql 配置 🔨\n * Mysql 问题\n\n\n# 其他\n\n * PostgreSQL 应用指南\n * H2 应用指南\n * SqLite 应用指南\n\n\n# 文档数据库\n\n\n# MongoDB\n\n> MongoDB 是一个基于文档的分布式数据库，由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\n> \n> MongoDB 是一个介于关系型数据库和非关系型数据库之间的产品。它是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。\n> \n> MongoDB 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n * MongoDB 应用指南\n * MongoDB 的 CRUD 操作\n * MongoDB 聚合操作\n * MongoDB 事务\n * MongoDB 建模\n * MongoDB 建模示例\n * MongoDB 索引\n * MongoDB 复制\n * MongoDB 分片\n * MongoDB 运维\n\n\n# KV 数据库\n\n\n# Redis\n\n\n\n * Redis 面试总结 💯\n * Redis 应用指南 ⚡ - 关键词：内存淘汰、事件、事务、管道、发布与订阅\n * Redis 数据类型和应用 - 关键词：STRING、HASH、LIST、SET、ZSET、BitMap、HyperLogLog、Geo\n * Redis 持久化 - 关键词：RDB、AOF、SAVE、BGSAVE、appendfsync\n * Redis 复制 - 关键词：SLAVEOF、SYNC、PSYNC、REPLCONF ACK\n * Redis 哨兵 - 关键词：Sentinel、PING、INFO、Raft\n * Redis 集群 - 关键词：CLUSTER MEET、Hash slot、MOVED、ASK、SLAVEOF no one、redis-trib\n * Redis 实战 - 关键词：缓存、分布式锁、布隆过滤器\n * Redis 运维 🔨 - 关键词：安装、命令、集群、客户端\n\n\n# 列式数据库\n\n\n# HBase\n\n> HBase 📚 因为常用于大数据项目，所以将其文档和源码整理在 bigdata-tutorial 项目中。\n\n * HBase 原理 ⚡\n * HBase 命令\n * HBase 应用\n * HBase 运维\n\n\n# 搜索引擎数据库\n\n\n# Elasticsearch\n\n> Elasticsearch 是一个基于 Lucene 的搜索和数据分析工具，它提供了一个分布式服务。Elasticsearch 是遵从 Apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n * Elasticsearch 面试总结 💯\n * Elasticsearch 快速入门\n * Elasticsearch 简介\n * Elasticsearch 索引\n * Elasticsearch 查询\n * Elasticsearch 高亮\n * Elasticsearch 排序\n * Elasticsearch 聚合\n * Elasticsearch 分析器\n * Elasticsearch 性能优化\n * Elasticsearch Rest API\n * ElasticSearch Java API 之 High Level REST Client\n * Elasticsearch 集群和分片\n * Elasticsearch 运维\n\n\n# Elastic\n\n * Elastic 快速入门\n * Elastic 技术栈之 Filebeat\n * Filebeat 运维\n * Elastic 技术栈之 Kibana\n * Kibana 运维\n * Elastic 技术栈之 Logstash\n * Logstash 运维\n\n\n# 资料 📚\n\n\n# 数据库综合资料\n\n * DB-Engines - 数据库流行度排名\n * 书籍\n   * 《数据密集型应用系统设计》 - 这可能是目前最好的分布式存储书籍，强力推荐【进阶】\n * 教程\n   * CMU 15445 数据库基础课程\n   * CMU 15721 数据库高级课程\n   * 检索技术核心 20 讲 - 极客教程【进阶】\n   * 后端存储实战课 - 极客教程【入门】：讲解存储在电商领域的种种应用和一些基本特性\n * 论文\n   * Efficiency in the Columbia Database Query Optimizer\n   * How Good Are Query Optimizers, Really?\n   * Architecture of a Database System\n   * Data Structures for Databases\n * 文章\n   * Data Structures and Algorithms for Big Databases\n\n\n# 关系型数据库资料\n\n * 综合资料\n   * 《数据库的索引设计与优化》\n   * 《SQL 必知必会》 - SQL 的基本概念和语法【入门】\n * Oracle 资料\n   * 《Oracle Database 9i/10g/11g 编程艺术》\n\n# Mysql 资料\n\n * 官方\n   * Mysql 官网\n   * Mysql 官方文档\n   * 官方 PPT\n     * How to Analyze and Tune MySQL Queries for Better Performance\n     * MySQL Performance Tuning 101\n     * MySQL Performance Schema & Sys Schema\n     * MySQL Performance: Demystified Tuning & Best Practices\n     * MySQL Security Best Practices\n     * MySQL Cluster Deployment Best Practices\n     * MySQL High Availability with InnoDB Cluster\n * 书籍\n   * 《高性能 MySQL》 - 经典，适合 DBA 或作为开发者的参考手册【进阶】\n   * 《MySQL 技术内幕：InnoDB 存储引擎》\n   * 《MySQL 必知必会》 - Mysql 的基本概念和语法【入门】\n * 教程\n   * runoob.com MySQL 教程 - 入门级 SQL 教程\n   * mysql-tutorial\n * 文章\n   * MySQL 索引背后的数据结构及算法原理\n   * Some study on database storage internals\n   * Sharding Pinterest: How we scaled our MySQL fleet\n   * Guide to MySQL High Availability\n   * Choosing MySQL High Availability Solutions\n   * High availability with MariaDB TX: The definitive guide\n   * Mysql 相关经验\n     * Booking.com: Evolution of MySQL System Design ，Booking.com 的 MySQL 数据库使用的演化，其中有很多不错的经验分享，我相信也是很多公司会遇到的的问题。\n     * Tracking the Money - Scaling Financial Reporting at Airbnb ，Airbnb 的数据库扩展的经验分享。\n     * Why Uber Engineering Switched from Postgres to MySQL ，无意比较两个数据库谁好谁不好，推荐这篇 Uber 的长文，主要是想让你从中学习到一些经验和技术细节，这是一篇很不错的文章。\n   * Mysql 集群复制\n     * Monitoring Delayed Replication, With A Focus On MySQL\n     * Mitigating replication lag and reducing read load with freno\n     * Better Parallel Replication for MySQL\n     * Evaluating MySQL Parallel Replication Part 2: Slave Group Commit\n     * Evaluating MySQL Parallel Replication Part 3: Benchmarks in Production\n     * Evaluating MySQL Parallel Replication Part 4: More Benchmarks in Production\n     * Evaluating MySQL Parallel Replication Part 4, Annex: Under the Hood\n   * Mysql 数据分区\n     * StackOverflow: MySQL sharding approaches?\n     * Why you don’t want to shard\n     * [How to Scale Big Data Applications](https://www.percona.com/sites/default/files/presentations/How to Scale Big Data Applications.pdf)\n     * MySQL Sharding with ProxySQL\n   * 各公司的 Mysql 数据分区经验分享\n     * MailChimp: Using Shards to Accommodate Millions of Users\n     * Uber: Code Migration in Production: Rewriting the Sharding Layer of Uber’s Schemaless Datastore\n     * Sharding & IDs at Instagram\n     * Airbnb: How We Partitioned Airbnb’s Main Database in Two Weeks\n * 更多资源\n   * awesome-mysql - MySQL 的资源列表\n\n\n# Nosql 数据库综合\n\n * Martin Fowler 在 YouTube 上分享的 NoSQL 介绍 Introduction To NoSQL， 以及他参与编写的 NoSQL Distilled - NoSQL 精粹，这本书才 100 多页，是本难得的关于 NoSQL 的书，很不错，非常易读。\n * NoSQL Databases: a Survey and Decision Guidance，这篇文章可以带你自上而下地从 CAP 原理到开始了解 NoSQL 的种种技术，是一篇非常不错的文章。\n * Distribution, Data, Deployment: Software Architecture Convergence in Big Data Systems，这是卡内基·梅隆大学的一篇讲分布式大数据系统的论文。其中主要讨论了在大数据时代下的软件工程中的一些关键点，也说到了 NoSQL 数据库。\n * No Relation: The Mixed Blessings of Non-Relational Databases，这篇论文虽然有点年代久远。但这篇论文是 HBase 的基础，你花上一点时间来读读，就可以了解到，对各种非关系型数据存储优缺点的一个很好的比较。\n * NoSQL Data Modeling Techniques ，NoSQL 建模技术。这篇文章我曾经翻译在了 CoolShell 上，标题为 NoSQL 数据建模技术，供你参考。\n   * MongoDB - Data Modeling Introduction ，虽然这是 MongoDB 的数据建模介绍，但是其很多观点可以用于其它的 NoSQL 数据库。\n   * Firebase - Structure Your Database ，Google 的 Firebase 数据库使用 JSON 建模的一些最佳实践。\n * 因为 CAP 原理，所以当你需要选择一个 NoSQL 数据库的时候，你应该看看这篇文档 Visual Guide to NoSQL Systems。\n\n选 SQL 还是 NoSQL，这里有两篇文章，值得你看看。\n\n * SQL vs. NoSQL Databases: What’s the Difference?\n * Salesforce: SQL or NoSQL\n\n\n# 列式数据库资料\n\n# Cassandra 资料\n\n * 沃尔玛实验室有两篇文章值得一读。\n   * Avoid Pitfalls in Scaling Cassandra Cluster at Walmart\n   * Storing Images in Cassandra at Walmart\n * Yelp: How We Scaled Our Ad Analytics with Apache Cassandra ，Yelp 的这篇博客也有一些相关的经验和教训。\n * Discord: How Discord Stores Billions of Messages ，Discord 公司分享的一个如何存储十亿级消息的技术文章。\n * Cassandra at Instagram ，Instagram 的一个 PPT，其中介绍了 Instagram 中是怎么使用 Cassandra 的。\n * Netflix: Benchmarking Cassandra Scalability on AWS - Over a million writes per second ，Netflix 公司在 AWS 上给 Cassandra 做的一个 Benchmark。\n\n# HBase 资料\n\n * Imgur Notification: From MySQL to HBASE\n * Pinterest: Improving HBase Backup Efficiency\n * IBM : Tuning HBase performance\n * HBase File Locality in HDFS\n * Apache Hadoop Goes Realtime at Facebook\n * Storage Infrastructure Behind Facebook Messages: Using HBase at Scale\n * GitHub: Awesome HBase\n\n针对于 HBase 有两本书你可以考虑一下。\n\n * 首先，先推荐两本书，一本是偏实践的《HBase 实战》，另一本是偏大而全的手册型的《HBase 权威指南》。\n * 当然，你也可以看看官方的 The Apache HBase™ Reference Guide\n * 另外两个列数据库：\n   * ClickHouse - Open Source Distributed Column Database at Yandex\n   * Scaling Redshift without Scaling Costs at GIPHY\n\n\n# KV 数据库资料\n\n# Redis 资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n   * Redis 命令参考\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 源码\n   * 《Redis 实战》配套 Python 源码\n * 资源汇总\n   * awesome-redis\n * Redis Client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * CRUG | Redisson PRO vs. Jedis: Which Is Faster? 翻译\n   * redis 分布锁 Redisson 性能测试\n * 文章\n   * Learn Redis the hard way (in production) at Trivago\n   * Twitter: How Twitter Uses Redis To Scale - 105TB RAM, 39MM QPS, 10,000+ Instances\n   * Slack: Scaling Slack’s Job Queue - Robustly Handling Billions of Tasks in Milliseconds Using Kafka and Redis\n   * GitHub: Moving persistent data out of Redis at GitHub\n   * Instagram: Storing Hundreds of Millions of Simple Key-Value Pairs in Redis\n   * Redis in Chat Architecture of Twitch (from 27:22)\n   * Deliveroo: Optimizing Session Key Storage in Redis\n   * Deliveroo: Optimizing Redis Storage\n   * GitHub: Awesome Redis\n\n\n# 文档数据库资料\n\n * Couchbase Ecosystem at LinkedIn\n * SimpleDB at Zendesk\n * Data Points - What the Heck Are Document Databases?\n\n# MongoDB 资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课\n * 数据\n   * mongodb-json-files\n * 文章\n   * Introduction to MongoDB\n   * eBay: Building Mission-Critical Multi-Data Center Applications with MongoDB\n   * The AWS and MongoDB Infrastructure of Parse: Lessons Learned\n   * Migrating Mountains of Mongo Data\n * 更多资源\n   * Github: Awesome MongoDB\n\n\n# 搜索引擎数据库资料\n\n# ElasticSearch\n\n * 官方\n   * Elasticsearch 官网\n   * Elasticsearch Github\n   * Elasticsearch 官方文档\n   * Elasticsearch: The Definitive Guide - ElasticSearch 官方学习资料\n * 书籍\n   * 《Elasticsearch 实战》\n * 教程\n   * ELK Stack 权威指南\n   * Elasticsearch 教程\n * 文章\n   * Elasticsearch+Logstash+Kibana 教程\n   * ELK（Elasticsearch、Logstash、Kibana）安装和配置\n   * 性能调优相关的工程实践\n     * Elasticsearch Performance Tuning Practice at eBay\n     * Elasticsearch at Kickstarter\n     * 9 tips on ElasticSearch configuration for high performance\n     * Elasticsearch In Production - Deployment Best Practices\n * 更多资源\n   * GitHub: Awesome ElasticSearch\n\n\n# 图数据库\n\n * 首先是 IBM Devloperworks 上的两个简介性的 PPT。\n   * Intro to graph databases, Part 1, Graph databases and the CRUD operations\n   * Intro to graph databases, Part 2, Building a recommendation engine with a graph database\n * 然后是一本免费的电子书《Graph Database》。\n * 接下来是一些图数据库的介绍文章。\n   * Handling Billions of Edges in a Graph Database\n   * Neo4j case studies with Walmart, eBay, AirBnB, NASA, etc\n   * FlockDB: Distributed Graph Database for Storing Adjacency Lists at Twitter\n   * JanusGraph: Scalable Graph Database backed by Google, IBM and Hortonworks\n   * Amazon Neptune\n\n\n# 时序数据库\n\n * What is Time-Series Data & Why We Need a Time-Series Database\n * Time Series Data: Why and How to Use a Relational Database instead of NoSQL\n * Beringei: High-performance Time Series Storage Engine @Facebook\n * Introducing Atlas: Netflix’s Primary Telemetry Platform @Netflix\n * Building a Scalable Time Series Database on PostgreSQL\n * Scaling Time Series Data Storage - Part I @Netflix\n * Design of a Cost Efficient Time Series Store for Big Data\n * GitHub: Awesome Time-Series Database\n\n\n# 传送 🚪\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"db-tutorial\n\n> 💾 db-tutorial 是一个数据库教程。\n> \n>  * 🔁 项目同步维护：github | gitee\n>  * 📖 电子书阅读：github pages | gitee pages\n\n\n# 数据库综合\n\n\n# 分布式存储原理\n\n# 分布式理论\n\n * 分布式理论\n * 深入剖析共识性算法 paxos\n * 深入剖析共识性算法 raft\n * 分布式算法 gossip\n\n# 分布式关键技术\n\n# 流量调度\n\n * 流量控制\n * 负载均衡\n * 服务路由\n * 分布式会话基本原理\n\n# 数据调度\n\n * 缓存基本原理\n * 读写分离基本原理\n * 分库分表基本原理\n * 分布式 id 基本原理\n * 分布式事务基本原理\n * 分布式锁基本原理\n\n\n# 其他\n\n * nosql 技术选型\n * 数据结构与数据库索引\n\n\n# 数据库中间件\n\n * shardingsphere 简介\n * shardingsphere jdbc\n * 版本管理中间件 flyway\n\n\n# 关系型数据库\n\n> 关系型数据库 整理主流关系型数据库知识点。\n\n\n# 公共知识\n\n * 关系型数据库面试总结 💯\n * sql cheat sheet 是一个 sql 入门教程。\n * 扩展 sql 是一个 sql 入门教程。\n\n\n# mysql\n\n\n\n * mysql 应用指南 ⚡\n * mysql 工作流 - 关键词：连接、缓存、语法分析、优化、执行引擎、redo log、bin log、两阶段提交\n * mysql 事务 - 关键词：acid、autocommit、事务隔离级别、死锁、分布式事务\n * mysql 锁 - 关键词：乐观锁、表级锁、行级锁、意向锁、mvcc、next-key 锁\n * mysql 索引 - 关键词：hash、b 树、聚簇索引、回表\n * mysql 性能优化\n * mysql 运维 🔨\n * mysql 配置 🔨\n * mysql 问题\n\n\n# 其他\n\n * postgresql 应用指南\n * h2 应用指南\n * sqlite 应用指南\n\n\n# 文档数据库\n\n\n# mongodb\n\n> mongodb 是一个基于文档的分布式数据库，由 c++ 语言编写。旨在为 web 应用提供可扩展的高性能数据存储解决方案。\n> \n> mongodb 是一个介于关系型数据库和非关系型数据库之间的产品。它是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。\n> \n> mongodb 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n * mongodb 应用指南\n * mongodb 的 crud 操作\n * mongodb 聚合操作\n * mongodb 事务\n * mongodb 建模\n * mongodb 建模示例\n * mongodb 索引\n * mongodb 复制\n * mongodb 分片\n * mongodb 运维\n\n\n# kv 数据库\n\n\n# redis\n\n\n\n * redis 面试总结 💯\n * redis 应用指南 ⚡ - 关键词：内存淘汰、事件、事务、管道、发布与订阅\n * redis 数据类型和应用 - 关键词：string、hash、list、set、zset、bitmap、hyperloglog、geo\n * redis 持久化 - 关键词：rdb、aof、save、bgsave、appendfsync\n * redis 复制 - 关键词：slaveof、sync、psync、replconf ack\n * redis 哨兵 - 关键词：sentinel、ping、info、raft\n * redis 集群 - 关键词：cluster meet、hash slot、moved、ask、slaveof no one、redis-trib\n * redis 实战 - 关键词：缓存、分布式锁、布隆过滤器\n * redis 运维 🔨 - 关键词：安装、命令、集群、客户端\n\n\n# 列式数据库\n\n\n# hbase\n\n> hbase 📚 因为常用于大数据项目，所以将其文档和源码整理在 bigdata-tutorial 项目中。\n\n * hbase 原理 ⚡\n * hbase 命令\n * hbase 应用\n * hbase 运维\n\n\n# 搜索引擎数据库\n\n\n# elasticsearch\n\n> elasticsearch 是一个基于 lucene 的搜索和数据分析工具，它提供了一个分布式服务。elasticsearch 是遵从 apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n * elasticsearch 面试总结 💯\n * elasticsearch 快速入门\n * elasticsearch 简介\n * elasticsearch 索引\n * elasticsearch 查询\n * elasticsearch 高亮\n * elasticsearch 排序\n * elasticsearch 聚合\n * elasticsearch 分析器\n * elasticsearch 性能优化\n * elasticsearch rest api\n * elasticsearch java api 之 high level rest client\n * elasticsearch 集群和分片\n * elasticsearch 运维\n\n\n# elastic\n\n * elastic 快速入门\n * elastic 技术栈之 filebeat\n * filebeat 运维\n * elastic 技术栈之 kibana\n * kibana 运维\n * elastic 技术栈之 logstash\n * logstash 运维\n\n\n# 资料 📚\n\n\n# 数据库综合资料\n\n * db-engines - 数据库流行度排名\n * 书籍\n   * 《数据密集型应用系统设计》 - 这可能是目前最好的分布式存储书籍，强力推荐【进阶】\n * 教程\n   * cmu 15445 数据库基础课程\n   * cmu 15721 数据库高级课程\n   * 检索技术核心 20 讲 - 极客教程【进阶】\n   * 后端存储实战课 - 极客教程【入门】：讲解存储在电商领域的种种应用和一些基本特性\n * 论文\n   * efficiency in the columbia database query optimizer\n   * how good are query optimizers, really?\n   * architecture of a database system\n   * data structures for databases\n * 文章\n   * data structures and algorithms for big databases\n\n\n# 关系型数据库资料\n\n * 综合资料\n   * 《数据库的索引设计与优化》\n   * 《sql 必知必会》 - sql 的基本概念和语法【入门】\n * oracle 资料\n   * 《oracle database 9i/10g/11g 编程艺术》\n\n# mysql 资料\n\n * 官方\n   * mysql 官网\n   * mysql 官方文档\n   * 官方 ppt\n     * how to analyze and tune mysql queries for better performance\n     * mysql performance tuning 101\n     * mysql performance schema & sys schema\n     * mysql performance: demystified tuning & best practices\n     * mysql security best practices\n     * mysql cluster deployment best practices\n     * mysql high availability with innodb cluster\n * 书籍\n   * 《高性能 mysql》 - 经典，适合 dba 或作为开发者的参考手册【进阶】\n   * 《mysql 技术内幕：innodb 存储引擎》\n   * 《mysql 必知必会》 - mysql 的基本概念和语法【入门】\n * 教程\n   * runoob.com mysql 教程 - 入门级 sql 教程\n   * mysql-tutorial\n * 文章\n   * mysql 索引背后的数据结构及算法原理\n   * some study on database storage internals\n   * sharding pinterest: how we scaled our mysql fleet\n   * guide to mysql high availability\n   * choosing mysql high availability solutions\n   * high availability with mariadb tx: the definitive guide\n   * mysql 相关经验\n     * booking.com: evolution of mysql system design ，booking.com 的 mysql 数据库使用的演化，其中有很多不错的经验分享，我相信也是很多公司会遇到的的问题。\n     * tracking the money - scaling financial reporting at airbnb ，airbnb 的数据库扩展的经验分享。\n     * why uber engineering switched from postgres to mysql ，无意比较两个数据库谁好谁不好，推荐这篇 uber 的长文，主要是想让你从中学习到一些经验和技术细节，这是一篇很不错的文章。\n   * mysql 集群复制\n     * monitoring delayed replication, with a focus on mysql\n     * mitigating replication lag and reducing read load with freno\n     * better parallel replication for mysql\n     * evaluating mysql parallel replication part 2: slave group commit\n     * evaluating mysql parallel replication part 3: benchmarks in production\n     * evaluating mysql parallel replication part 4: more benchmarks in production\n     * evaluating mysql parallel replication part 4, annex: under the hood\n   * mysql 数据分区\n     * stackoverflow: mysql sharding approaches?\n     * why you don’t want to shard\n     * [how to scale big data applications](https://www.percona.com/sites/default/files/presentations/how to scale big data applications.pdf)\n     * mysql sharding with proxysql\n   * 各公司的 mysql 数据分区经验分享\n     * mailchimp: using shards to accommodate millions of users\n     * uber: code migration in production: rewriting the sharding layer of uber’s schemaless datastore\n     * sharding & ids at instagram\n     * airbnb: how we partitioned airbnb’s main database in two weeks\n * 更多资源\n   * awesome-mysql - mysql 的资源列表\n\n\n# nosql 数据库综合\n\n * martin fowler 在 youtube 上分享的 nosql 介绍 introduction to nosql， 以及他参与编写的 nosql distilled - nosql 精粹，这本书才 100 多页，是本难得的关于 nosql 的书，很不错，非常易读。\n * nosql databases: a survey and decision guidance，这篇文章可以带你自上而下地从 cap 原理到开始了解 nosql 的种种技术，是一篇非常不错的文章。\n * distribution, data, deployment: software architecture convergence in big data systems，这是卡内基·梅隆大学的一篇讲分布式大数据系统的论文。其中主要讨论了在大数据时代下的软件工程中的一些关键点，也说到了 nosql 数据库。\n * no relation: the mixed blessings of non-relational databases，这篇论文虽然有点年代久远。但这篇论文是 hbase 的基础，你花上一点时间来读读，就可以了解到，对各种非关系型数据存储优缺点的一个很好的比较。\n * nosql data modeling techniques ，nosql 建模技术。这篇文章我曾经翻译在了 coolshell 上，标题为 nosql 数据建模技术，供你参考。\n   * mongodb - data modeling introduction ，虽然这是 mongodb 的数据建模介绍，但是其很多观点可以用于其它的 nosql 数据库。\n   * firebase - structure your database ，google 的 firebase 数据库使用 json 建模的一些最佳实践。\n * 因为 cap 原理，所以当你需要选择一个 nosql 数据库的时候，你应该看看这篇文档 visual guide to nosql systems。\n\n选 sql 还是 nosql，这里有两篇文章，值得你看看。\n\n * sql vs. nosql databases: what’s the difference?\n * salesforce: sql or nosql\n\n\n# 列式数据库资料\n\n# cassandra 资料\n\n * 沃尔玛实验室有两篇文章值得一读。\n   * avoid pitfalls in scaling cassandra cluster at walmart\n   * storing images in cassandra at walmart\n * yelp: how we scaled our ad analytics with apache cassandra ，yelp 的这篇博客也有一些相关的经验和教训。\n * discord: how discord stores billions of messages ，discord 公司分享的一个如何存储十亿级消息的技术文章。\n * cassandra at instagram ，instagram 的一个 ppt，其中介绍了 instagram 中是怎么使用 cassandra 的。\n * netflix: benchmarking cassandra scalability on aws - over a million writes per second ，netflix 公司在 aws 上给 cassandra 做的一个 benchmark。\n\n# hbase 资料\n\n * imgur notification: from mysql to hbase\n * pinterest: improving hbase backup efficiency\n * ibm : tuning hbase performance\n * hbase file locality in hdfs\n * apache hadoop goes realtime at facebook\n * storage infrastructure behind facebook messages: using hbase at scale\n * github: awesome hbase\n\n针对于 hbase 有两本书你可以考虑一下。\n\n * 首先，先推荐两本书，一本是偏实践的《hbase 实战》，另一本是偏大而全的手册型的《hbase 权威指南》。\n * 当然，你也可以看看官方的 the apache hbase™ reference guide\n * 另外两个列数据库：\n   * clickhouse - open source distributed column database at yandex\n   * scaling redshift without scaling costs at giphy\n\n\n# kv 数据库资料\n\n# redis 资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n   * redis 命令参考\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 源码\n   * 《redis 实战》配套 python 源码\n * 资源汇总\n   * awesome-redis\n * redis client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * crug | redisson pro vs. jedis: which is faster? 翻译\n   * redis 分布锁 redisson 性能测试\n * 文章\n   * learn redis the hard way (in production) at trivago\n   * twitter: how twitter uses redis to scale - 105tb ram, 39mm qps, 10,000+ instances\n   * slack: scaling slack’s job queue - robustly handling billions of tasks in milliseconds using kafka and redis\n   * github: moving persistent data out of redis at github\n   * instagram: storing hundreds of millions of simple key-value pairs in redis\n   * redis in chat architecture of twitch (from 27:22)\n   * deliveroo: optimizing session key storage in redis\n   * deliveroo: optimizing redis storage\n   * github: awesome redis\n\n\n# 文档数据库资料\n\n * couchbase ecosystem at linkedin\n * simpledb at zendesk\n * data points - what the heck are document databases?\n\n# mongodb 资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课\n * 数据\n   * mongodb-json-files\n * 文章\n   * introduction to mongodb\n   * ebay: building mission-critical multi-data center applications with mongodb\n   * the aws and mongodb infrastructure of parse: lessons learned\n   * migrating mountains of mongo data\n * 更多资源\n   * github: awesome mongodb\n\n\n# 搜索引擎数据库资料\n\n# elasticsearch\n\n * 官方\n   * elasticsearch 官网\n   * elasticsearch github\n   * elasticsearch 官方文档\n   * elasticsearch: the definitive guide - elasticsearch 官方学习资料\n * 书籍\n   * 《elasticsearch 实战》\n * 教程\n   * elk stack 权威指南\n   * elasticsearch 教程\n * 文章\n   * elasticsearch+logstash+kibana 教程\n   * elk（elasticsearch、logstash、kibana）安装和配置\n   * 性能调优相关的工程实践\n     * elasticsearch performance tuning practice at ebay\n     * elasticsearch at kickstarter\n     * 9 tips on elasticsearch configuration for high performance\n     * elasticsearch in production - deployment best practices\n * 更多资源\n   * github: awesome elasticsearch\n\n\n# 图数据库\n\n * 首先是 ibm devloperworks 上的两个简介性的 ppt。\n   * intro to graph databases, part 1, graph databases and the crud operations\n   * intro to graph databases, part 2, building a recommendation engine with a graph database\n * 然后是一本免费的电子书《graph database》。\n * 接下来是一些图数据库的介绍文章。\n   * handling billions of edges in a graph database\n   * neo4j case studies with walmart, ebay, airbnb, nasa, etc\n   * flockdb: distributed graph database for storing adjacency lists at twitter\n   * janusgraph: scalable graph database backed by google, ibm and hortonworks\n   * amazon neptune\n\n\n# 时序数据库\n\n * what is time-series data & why we need a time-series database\n * time series data: why and how to use a relational database instead of nosql\n * beringei: high-performance time series storage engine @facebook\n * introducing atlas: netflix’s primary telemetry platform @netflix\n * building a scalable time series database on postgresql\n * scaling time series data storage - part i @netflix\n * design of a cost efficient time series store for big data\n * github: awesome time-series database\n\n\n# 传送 🚪\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"归档",frontmatter:{archivesPage:!0,title:"归档",permalink:"/archives/",article:!1},regularPath:"/@pages/archivesPage.html",relativePath:"@pages/archivesPage.md",key:"v-c7c85e2e",path:"/archives/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"分类",frontmatter:{categoriesPage:!0,title:"分类",permalink:"/categories/",article:!1},regularPath:"/@pages/categoriesPage.html",relativePath:"@pages/categoriesPage.md",key:"v-03067a49",path:"/categories/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"标签",frontmatter:{tagsPage:!0,title:"标签",permalink:"/tags/",article:!1},regularPath:"/@pages/tagsPage.html",relativePath:"@pages/tagsPage.md",key:"v-be824b2e",path:"/tags/",headersStr:null,content:"",normalizedContent:"",charsets:{},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3},{title:"Home",frontmatter:{home:!0,heroImage:"img/bg.gif",heroText:"DB-TUTORIAL",tagline:"☕ db-tutorial 是一个数据库教程。",bannerBg:"none",postList:"none",footer:"CC-BY-SA-4.0 Licensed | Copyright © 2018-Now Dunwu"},regularPath:"/",relativePath:"README.md",key:"v-91a7d35c",path:"/",headers:[{level:2,title:"分布式",slug:"分布式",normalizedTitle:"分布式",charIndex:105},{level:3,title:"分布式综合",slug:"分布式综合",normalizedTitle:"分布式综合",charIndex:113},{level:3,title:"分布式理论",slug:"分布式理论",normalizedTitle:"分布式理论",charIndex:135},{level:3,title:"分布式关键技术",slug:"分布式关键技术",normalizedTitle:"分布式关键技术",charIndex:265},{level:4,title:"流量调度",slug:"流量调度",normalizedTitle:"流量调度",charIndex:301},{level:4,title:"数据调度",slug:"数据调度",normalizedTitle:"数据调度",charIndex:496},{level:4,title:"资源调度",slug:"资源调度",normalizedTitle:"资源调度",charIndex:733},{level:4,title:"服务治理",slug:"服务治理",normalizedTitle:"服务治理",charIndex:750},{level:2,title:"数据库综合",slug:"数据库综合",normalizedTitle:"数据库综合",charIndex:797},{level:2,title:"数据库中间件",slug:"数据库中间件",normalizedTitle:"数据库中间件",charIndex:836},{level:2,title:"关系型数据库",slug:"关系型数据库",normalizedTitle:"关系型数据库",charIndex:910},{level:3,title:"公共知识",slug:"公共知识",normalizedTitle:"公共知识",charIndex:946},{level:3,title:"Mysql",slug:"mysql",normalizedTitle:"mysql",charIndex:1030},{level:3,title:"其他",slug:"其他",normalizedTitle:"其他",charIndex:1310},{level:2,title:"文档数据库",slug:"文档数据库",normalizedTitle:"文档数据库",charIndex:1363},{level:3,title:"MongoDB",slug:"mongodb",normalizedTitle:"mongodb",charIndex:1373},{level:2,title:"KV 数据库",slug:"kv-数据库",normalizedTitle:"kv 数据库",charIndex:1806},{level:3,title:"Redis",slug:"redis",normalizedTitle:"redis",charIndex:696},{level:2,title:"列式数据库",slug:"列式数据库",normalizedTitle:"列式数据库",charIndex:2247},{level:3,title:"HBase",slug:"hbase",normalizedTitle:"hbase",charIndex:2257},{level:2,title:"搜索引擎数据库",slug:"搜索引擎数据库",normalizedTitle:"搜索引擎数据库",charIndex:2376},{level:3,title:"Elasticsearch",slug:"elasticsearch",normalizedTitle:"elasticsearch",charIndex:2388},{level:3,title:"Elastic",slug:"elastic",normalizedTitle:"elastic",charIndex:2388},{level:2,title:"资料 📚",slug:"资料-📚",normalizedTitle:"资料 📚",charIndex:2987},{level:3,title:"数据库综合资料",slug:"数据库综合资料",normalizedTitle:"数据库综合资料",charIndex:2997},{level:3,title:"关系型数据库资料",slug:"关系型数据库资料",normalizedTitle:"关系型数据库资料",charIndex:3455},{level:4,title:"Mysql 资料",slug:"mysql-资料",normalizedTitle:"mysql 资料",charIndex:3582},{level:3,title:"Nosql 数据库综合",slug:"nosql-数据库综合",normalizedTitle:"nosql 数据库综合",charIndex:5908},{level:3,title:"列式数据库资料",slug:"列式数据库资料",normalizedTitle:"列式数据库资料",charIndex:6913},{level:4,title:"Cassandra 资料",slug:"cassandra-资料",normalizedTitle:"cassandra 资料",charIndex:6924},{level:4,title:"HBase 资料",slug:"hbase-资料",normalizedTitle:"hbase 资料",charIndex:7452},{level:3,title:"KV 数据库资料",slug:"kv-数据库资料",normalizedTitle:"kv 数据库资料",charIndex:8030},{level:4,title:"Redis 资料",slug:"redis-资料",normalizedTitle:"redis 资料",charIndex:8042},{level:3,title:"文档数据库资料",slug:"文档数据库资料",normalizedTitle:"文档数据库资料",charIndex:8993},{level:4,title:"MongoDB 资料",slug:"mongodb-资料",normalizedTitle:"mongodb 资料",charIndex:9118},{level:3,title:"搜索引擎数据库资料",slug:"搜索引擎数据库资料",normalizedTitle:"搜索引擎数据库资料",charIndex:9523},{level:4,title:"ElasticSearch",slug:"elasticsearch-2",normalizedTitle:"elasticsearch",charIndex:2747},{level:3,title:"图数据库",slug:"图数据库",normalizedTitle:"图数据库",charIndex:10146},{level:3,title:"时序数据库",slug:"时序数据库",normalizedTitle:"时序数据库",charIndex:10713},{level:2,title:"传送 🚪",slug:"传送-🚪",normalizedTitle:"传送 🚪",charIndex:11216}],headersStr:"分布式 分布式综合 分布式理论 分布式关键技术 流量调度 数据调度 资源调度 服务治理 数据库综合 数据库中间件 关系型数据库 公共知识 Mysql 其他 文档数据库 MongoDB KV 数据库 Redis 列式数据库 HBase 搜索引擎数据库 Elasticsearch Elastic 资料 📚 数据库综合资料 关系型数据库资料 Mysql 资料 Nosql 数据库综合 列式数据库资料 Cassandra 资料 HBase 资料 KV 数据库资料 Redis 资料 文档数据库资料 MongoDB 资料 搜索引擎数据库资料 ElasticSearch 图数据库 时序数据库 传送 🚪",content:"> 💾 db-tutorial 是一个数据库教程。\n> \n>  * 🔁 项目同步维护：Github | Gitee\n>  * 📖 电子书阅读：Github Pages | Gitee Pages\n\n\n# 分布式\n\n\n# 分布式综合\n\n * 分布式面试总结\n\n\n# 分布式理论\n\n * 分布式理论 - 关键词：拜占庭将军、CAP、BASE、错误的分布式假设\n * 共识性算法 Paxos - 关键词：共识性算法\n * 共识性算法 Raft - 关键词：共识性算法\n * 分布式算法 Gossip - 关键词：数据传播\n\n\n# 分布式关键技术\n\n * 集群\n * 复制\n * 分区\n * 选主\n\n# 流量调度\n\n * 流量控制 - 关键词：限流、熔断、降级、计数器法、时间窗口法、令牌桶法、漏桶法\n * 负载均衡 - 关键词：轮询、随机、最少连接、源地址哈希、一致性哈希、虚拟 hash 槽\n * 服务路由 - 关键词：路由、条件路由、脚本路由、标签路由\n * 服务网关\n * 分布式会话 - 关键词：粘性 Session、Session 复制共享、基于缓存的 session 共享\n\n# 数据调度\n\n * 数据缓存 - 关键词：进程内缓存、分布式缓存、缓存雪崩、缓存穿透、缓存击穿、缓存更新、缓存预热、缓存降级\n * 读写分离\n * 分库分表 - 关键词：分片、路由、迁移、扩容、双写、聚合\n * 分布式 ID - 关键词：UUID、自增序列、雪花算法、Leaf\n * 分布式事务 - 关键词：2PC、3PC、TCC、本地消息表、MQ 消息、SAGA\n * 分布式锁 - 关键词：数据库、Redis、ZooKeeper、互斥、可重入、死锁、容错、自旋尝试\n\n# 资源调度\n\n * 弹性伸缩\n\n# 服务治理\n\n * 服务注册和发现\n * 服务容错\n * 服务编排\n * 服务版本管理\n\n\n# 数据库综合\n\n * Nosql 技术选型\n * 数据结构与数据库索引\n\n\n# 数据库中间件\n\n * ShardingSphere 简介\n * ShardingSphere Jdbc\n * 版本管理中间件 Flyway\n\n\n# 关系型数据库\n\n> 关系型数据库 整理主流关系型数据库知识点。\n\n\n# 公共知识\n\n * 关系型数据库面试总结 💯\n * SQL Cheat Sheet 是一个 SQL 入门教程。\n * 扩展 SQL 是一个 SQL 入门教程。\n\n\n# Mysql\n\n\n\n * Mysql 应用指南 ⚡\n * Mysql 工作流 - 关键词：连接、缓存、语法分析、优化、执行引擎、redo log、bin log、两阶段提交\n * Mysql 事务 - 关键词：ACID、AUTOCOMMIT、事务隔离级别、死锁、分布式事务\n * Mysql 锁 - 关键词：乐观锁、表级锁、行级锁、意向锁、MVCC、Next-key 锁\n * Mysql 索引 - 关键词：Hash、B 树、聚簇索引、回表\n * Mysql 性能优化\n * Mysql 运维 🔨\n * Mysql 配置 🔨\n * Mysql 问题\n\n\n# 其他\n\n * PostgreSQL 应用指南\n * H2 应用指南\n * SqLite 应用指南\n\n\n# 文档数据库\n\n\n# MongoDB\n\n> MongoDB 是一个基于文档的分布式数据库，由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。\n> \n> MongoDB 是一个介于关系型数据库和非关系型数据库之间的产品。它是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。\n> \n> MongoDB 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n * MongoDB 应用指南\n * MongoDB 的 CRUD 操作\n * MongoDB 聚合操作\n * MongoDB 事务\n * MongoDB 建模\n * MongoDB 建模示例\n * MongoDB 索引\n * MongoDB 复制\n * MongoDB 分片\n * MongoDB 运维\n\n\n# KV 数据库\n\n\n# Redis\n\n\n\n * Redis 面试总结 💯\n * Redis 应用指南 ⚡ - 关键词：内存淘汰、事件、事务、管道、发布与订阅\n * Redis 数据类型和应用 - 关键词：STRING、HASH、LIST、SET、ZSET、BitMap、HyperLogLog、Geo\n * Redis 持久化 - 关键词：RDB、AOF、SAVE、BGSAVE、appendfsync\n * Redis 复制 - 关键词：SLAVEOF、SYNC、PSYNC、REPLCONF ACK\n * Redis 哨兵 - 关键词：Sentinel、PING、INFO、Raft\n * Redis 集群 - 关键词：CLUSTER MEET、Hash slot、MOVED、ASK、SLAVEOF no one、redis-trib\n * Redis 实战 - 关键词：缓存、分布式锁、布隆过滤器\n * Redis 运维 🔨 - 关键词：安装、命令、集群、客户端\n\n\n# 列式数据库\n\n\n# HBase\n\n> HBase 📚 因为常用于大数据项目，所以将其文档和源码整理在 bigdata-tutorial 项目中。\n\n * HBase 原理 ⚡\n * HBase 命令\n * HBase 应用\n * HBase 运维\n\n\n# 搜索引擎数据库\n\n\n# Elasticsearch\n\n> Elasticsearch 是一个基于 Lucene 的搜索和数据分析工具，它提供了一个分布式服务。Elasticsearch 是遵从 Apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n * Elasticsearch 面试总结 💯\n * Elasticsearch 快速入门\n * Elasticsearch 简介\n * Elasticsearch 索引\n * Elasticsearch 查询\n * Elasticsearch 高亮\n * Elasticsearch 排序\n * Elasticsearch 聚合\n * Elasticsearch 分析器\n * Elasticsearch 性能优化\n * Elasticsearch Rest API\n * ElasticSearch Java API 之 High Level REST Client\n * Elasticsearch 集群和分片\n * Elasticsearch 运维\n\n\n# Elastic\n\n * Elastic 快速入门\n * Elastic 技术栈之 Filebeat\n * Filebeat 运维\n * Elastic 技术栈之 Kibana\n * Kibana 运维\n * Elastic 技术栈之 Logstash\n * Logstash 运维\n\n\n# 资料 📚\n\n\n# 数据库综合资料\n\n * DB-Engines - 数据库流行度排名\n * 书籍\n   * 《数据密集型应用系统设计》 - 这可能是目前最好的分布式存储书籍，强力推荐【进阶】\n * 教程\n   * CMU 15445 数据库基础课程\n   * CMU 15721 数据库高级课程\n   * 检索技术核心 20 讲 - 极客教程【进阶】\n   * 后端存储实战课 - 极客教程【入门】：讲解存储在电商领域的种种应用和一些基本特性\n * 论文\n   * Efficiency in the Columbia Database Query Optimizer\n   * How Good Are Query Optimizers, Really?\n   * Architecture of a Database System\n   * Data Structures for Databases\n * 文章\n   * Data Structures and Algorithms for Big Databases\n\n\n# 关系型数据库资料\n\n * 综合资料\n   * 《数据库的索引设计与优化》\n   * 《SQL 必知必会》 - SQL 的基本概念和语法【入门】\n * Oracle 资料\n   * 《Oracle Database 9i/10g/11g 编程艺术》\n\n# Mysql 资料\n\n * 官方\n   * Mysql 官网\n   * Mysql 官方文档\n   * 官方 PPT\n     * How to Analyze and Tune MySQL Queries for Better Performance\n     * MySQL Performance Tuning 101\n     * MySQL Performance Schema & Sys Schema\n     * MySQL Performance: Demystified Tuning & Best Practices\n     * MySQL Security Best Practices\n     * MySQL Cluster Deployment Best Practices\n     * MySQL High Availability with InnoDB Cluster\n * 书籍\n   * 《高性能 MySQL》 - 经典，适合 DBA 或作为开发者的参考手册【进阶】\n   * 《MySQL 技术内幕：InnoDB 存储引擎》\n   * 《MySQL 必知必会》 - Mysql 的基本概念和语法【入门】\n * 教程\n   * runoob.com MySQL 教程 - 入门级 SQL 教程\n   * mysql-tutorial\n * 文章\n   * MySQL 索引背后的数据结构及算法原理\n   * Some study on database storage internals\n   * Sharding Pinterest: How we scaled our MySQL fleet\n   * Guide to MySQL High Availability\n   * Choosing MySQL High Availability Solutions\n   * High availability with MariaDB TX: The definitive guide\n   * Mysql 相关经验\n     * Booking.com: Evolution of MySQL System Design ，Booking.com 的 MySQL 数据库使用的演化，其中有很多不错的经验分享，我相信也是很多公司会遇到的的问题。\n     * Tracking the Money - Scaling Financial Reporting at Airbnb ，Airbnb 的数据库扩展的经验分享。\n     * Why Uber Engineering Switched from Postgres to MySQL ，无意比较两个数据库谁好谁不好，推荐这篇 Uber 的长文，主要是想让你从中学习到一些经验和技术细节，这是一篇很不错的文章。\n   * Mysql 集群复制\n     * Monitoring Delayed Replication, With A Focus On MySQL\n     * Mitigating replication lag and reducing read load with freno\n     * Better Parallel Replication for MySQL\n     * Evaluating MySQL Parallel Replication Part 2: Slave Group Commit\n     * Evaluating MySQL Parallel Replication Part 3: Benchmarks in Production\n     * Evaluating MySQL Parallel Replication Part 4: More Benchmarks in Production\n     * Evaluating MySQL Parallel Replication Part 4, Annex: Under the Hood\n   * Mysql 数据分区\n     * StackOverflow: MySQL sharding approaches?\n     * Why you don’t want to shard\n     * [How to Scale Big Data Applications](https://www.percona.com/sites/default/files/presentations/How to Scale Big Data Applications.pdf)\n     * MySQL Sharding with ProxySQL\n   * 各公司的 Mysql 数据分区经验分享\n     * MailChimp: Using Shards to Accommodate Millions of Users\n     * Uber: Code Migration in Production: Rewriting the Sharding Layer of Uber’s Schemaless Datastore\n     * Sharding & IDs at Instagram\n     * Airbnb: How We Partitioned Airbnb’s Main Database in Two Weeks\n * 更多资源\n   * awesome-mysql - MySQL 的资源列表\n\n\n# Nosql 数据库综合\n\n * Martin Fowler 在 YouTube 上分享的 NoSQL 介绍 Introduction To NoSQL， 以及他参与编写的 NoSQL Distilled - NoSQL 精粹，这本书才 100 多页，是本难得的关于 NoSQL 的书，很不错，非常易读。\n * NoSQL Databases: a Survey and Decision Guidance，这篇文章可以带你自上而下地从 CAP 原理到开始了解 NoSQL 的种种技术，是一篇非常不错的文章。\n * Distribution, Data, Deployment: Software Architecture Convergence in Big Data Systems，这是卡内基·梅隆大学的一篇讲分布式大数据系统的论文。其中主要讨论了在大数据时代下的软件工程中的一些关键点，也说到了 NoSQL 数据库。\n * No Relation: The Mixed Blessings of Non-Relational Databases，这篇论文虽然有点年代久远。但这篇论文是 HBase 的基础，你花上一点时间来读读，就可以了解到，对各种非关系型数据存储优缺点的一个很好的比较。\n * NoSQL Data Modeling Techniques ，NoSQL 建模技术。这篇文章我曾经翻译在了 CoolShell 上，标题为 NoSQL 数据建模技术，供你参考。\n   * MongoDB - Data Modeling Introduction ，虽然这是 MongoDB 的数据建模介绍，但是其很多观点可以用于其它的 NoSQL 数据库。\n   * Firebase - Structure Your Database ，Google 的 Firebase 数据库使用 JSON 建模的一些最佳实践。\n * 因为 CAP 原理，所以当你需要选择一个 NoSQL 数据库的时候，你应该看看这篇文档 Visual Guide to NoSQL Systems。\n\n选 SQL 还是 NoSQL，这里有两篇文章，值得你看看。\n\n * SQL vs. NoSQL Databases: What’s the Difference?\n * Salesforce: SQL or NoSQL\n\n\n# 列式数据库资料\n\n# Cassandra 资料\n\n * 沃尔玛实验室有两篇文章值得一读。\n   * Avoid Pitfalls in Scaling Cassandra Cluster at Walmart\n   * Storing Images in Cassandra at Walmart\n * Yelp: How We Scaled Our Ad Analytics with Apache Cassandra ，Yelp 的这篇博客也有一些相关的经验和教训。\n * Discord: How Discord Stores Billions of Messages ，Discord 公司分享的一个如何存储十亿级消息的技术文章。\n * Cassandra at Instagram ，Instagram 的一个 PPT，其中介绍了 Instagram 中是怎么使用 Cassandra 的。\n * Netflix: Benchmarking Cassandra Scalability on AWS - Over a million writes per second ，Netflix 公司在 AWS 上给 Cassandra 做的一个 Benchmark。\n\n# HBase 资料\n\n * Imgur Notification: From MySQL to HBASE\n * Pinterest: Improving HBase Backup Efficiency\n * IBM : Tuning HBase performance\n * HBase File Locality in HDFS\n * Apache Hadoop Goes Realtime at Facebook\n * Storage Infrastructure Behind Facebook Messages: Using HBase at Scale\n * GitHub: Awesome HBase\n\n针对于 HBase 有两本书你可以考虑一下。\n\n * 首先，先推荐两本书，一本是偏实践的《HBase 实战》，另一本是偏大而全的手册型的《HBase 权威指南》。\n * 当然，你也可以看看官方的 The Apache HBase™ Reference Guide\n * 另外两个列数据库：\n   * ClickHouse - Open Source Distributed Column Database at Yandex\n   * Scaling Redshift without Scaling Costs at GIPHY\n\n\n# KV 数据库资料\n\n# Redis 资料\n\n * 官网\n   * Redis 官网\n   * Redis github\n   * Redis 官方文档中文版\n   * Redis 命令参考\n * 书籍\n   * 《Redis 实战》\n   * 《Redis 设计与实现》\n * 源码\n   * 《Redis 实战》配套 Python 源码\n * 资源汇总\n   * awesome-redis\n * Redis Client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * CRUG | Redisson PRO vs. Jedis: Which Is Faster? 翻译\n   * redis 分布锁 Redisson 性能测试\n * 文章\n   * Learn Redis the hard way (in production) at Trivago\n   * Twitter: How Twitter Uses Redis To Scale - 105TB RAM, 39MM QPS, 10,000+ Instances\n   * Slack: Scaling Slack’s Job Queue - Robustly Handling Billions of Tasks in Milliseconds Using Kafka and Redis\n   * GitHub: Moving persistent data out of Redis at GitHub\n   * Instagram: Storing Hundreds of Millions of Simple Key-Value Pairs in Redis\n   * Redis in Chat Architecture of Twitch (from 27:22)\n   * Deliveroo: Optimizing Session Key Storage in Redis\n   * Deliveroo: Optimizing Redis Storage\n   * GitHub: Awesome Redis\n\n\n# 文档数据库资料\n\n * Couchbase Ecosystem at LinkedIn\n * SimpleDB at Zendesk\n * Data Points - What the Heck Are Document Databases?\n\n# MongoDB 资料\n\n * 官方\n   * MongoDB 官网\n   * MongoDB Github\n   * MongoDB 官方免费教程\n * 教程\n   * MongoDB 教程\n   * MongoDB 高手课\n * 数据\n   * mongodb-json-files\n * 文章\n   * Introduction to MongoDB\n   * eBay: Building Mission-Critical Multi-Data Center Applications with MongoDB\n   * The AWS and MongoDB Infrastructure of Parse: Lessons Learned\n   * Migrating Mountains of Mongo Data\n * 更多资源\n   * Github: Awesome MongoDB\n\n\n# 搜索引擎数据库资料\n\n# ElasticSearch\n\n * 官方\n   * Elasticsearch 官网\n   * Elasticsearch Github\n   * Elasticsearch 官方文档\n   * Elasticsearch: The Definitive Guide - ElasticSearch 官方学习资料\n * 书籍\n   * 《Elasticsearch 实战》\n * 教程\n   * ELK Stack 权威指南\n   * Elasticsearch 教程\n * 文章\n   * Elasticsearch+Logstash+Kibana 教程\n   * ELK（Elasticsearch、Logstash、Kibana）安装和配置\n   * 性能调优相关的工程实践\n     * Elasticsearch Performance Tuning Practice at eBay\n     * Elasticsearch at Kickstarter\n     * 9 tips on ElasticSearch configuration for high performance\n     * Elasticsearch In Production - Deployment Best Practices\n * 更多资源\n   * GitHub: Awesome ElasticSearch\n\n\n# 图数据库\n\n * 首先是 IBM Devloperworks 上的两个简介性的 PPT。\n   * Intro to graph databases, Part 1, Graph databases and the CRUD operations\n   * Intro to graph databases, Part 2, Building a recommendation engine with a graph database\n * 然后是一本免费的电子书《Graph Database》。\n * 接下来是一些图数据库的介绍文章。\n   * Handling Billions of Edges in a Graph Database\n   * Neo4j case studies with Walmart, eBay, AirBnB, NASA, etc\n   * FlockDB: Distributed Graph Database for Storing Adjacency Lists at Twitter\n   * JanusGraph: Scalable Graph Database backed by Google, IBM and Hortonworks\n   * Amazon Neptune\n\n\n# 时序数据库\n\n * What is Time-Series Data & Why We Need a Time-Series Database\n * Time Series Data: Why and How to Use a Relational Database instead of NoSQL\n * Beringei: High-performance Time Series Storage Engine @Facebook\n * Introducing Atlas: Netflix’s Primary Telemetry Platform @Netflix\n * Building a Scalable Time Series Database on PostgreSQL\n * Scaling Time Series Data Storage - Part I @Netflix\n * Design of a Cost Efficient Time Series Store for Big Data\n * GitHub: Awesome Time-Series Database\n\n\n# 传送 🚪\n\n◾ 💧 钝悟的 IT 知识图谱 ◾ 🎯 钝悟的博客 ◾",normalizedContent:"> 💾 db-tutorial 是一个数据库教程。\n> \n>  * 🔁 项目同步维护：github | gitee\n>  * 📖 电子书阅读：github pages | gitee pages\n\n\n# 分布式\n\n\n# 分布式综合\n\n * 分布式面试总结\n\n\n# 分布式理论\n\n * 分布式理论 - 关键词：拜占庭将军、cap、base、错误的分布式假设\n * 共识性算法 paxos - 关键词：共识性算法\n * 共识性算法 raft - 关键词：共识性算法\n * 分布式算法 gossip - 关键词：数据传播\n\n\n# 分布式关键技术\n\n * 集群\n * 复制\n * 分区\n * 选主\n\n# 流量调度\n\n * 流量控制 - 关键词：限流、熔断、降级、计数器法、时间窗口法、令牌桶法、漏桶法\n * 负载均衡 - 关键词：轮询、随机、最少连接、源地址哈希、一致性哈希、虚拟 hash 槽\n * 服务路由 - 关键词：路由、条件路由、脚本路由、标签路由\n * 服务网关\n * 分布式会话 - 关键词：粘性 session、session 复制共享、基于缓存的 session 共享\n\n# 数据调度\n\n * 数据缓存 - 关键词：进程内缓存、分布式缓存、缓存雪崩、缓存穿透、缓存击穿、缓存更新、缓存预热、缓存降级\n * 读写分离\n * 分库分表 - 关键词：分片、路由、迁移、扩容、双写、聚合\n * 分布式 id - 关键词：uuid、自增序列、雪花算法、leaf\n * 分布式事务 - 关键词：2pc、3pc、tcc、本地消息表、mq 消息、saga\n * 分布式锁 - 关键词：数据库、redis、zookeeper、互斥、可重入、死锁、容错、自旋尝试\n\n# 资源调度\n\n * 弹性伸缩\n\n# 服务治理\n\n * 服务注册和发现\n * 服务容错\n * 服务编排\n * 服务版本管理\n\n\n# 数据库综合\n\n * nosql 技术选型\n * 数据结构与数据库索引\n\n\n# 数据库中间件\n\n * shardingsphere 简介\n * shardingsphere jdbc\n * 版本管理中间件 flyway\n\n\n# 关系型数据库\n\n> 关系型数据库 整理主流关系型数据库知识点。\n\n\n# 公共知识\n\n * 关系型数据库面试总结 💯\n * sql cheat sheet 是一个 sql 入门教程。\n * 扩展 sql 是一个 sql 入门教程。\n\n\n# mysql\n\n\n\n * mysql 应用指南 ⚡\n * mysql 工作流 - 关键词：连接、缓存、语法分析、优化、执行引擎、redo log、bin log、两阶段提交\n * mysql 事务 - 关键词：acid、autocommit、事务隔离级别、死锁、分布式事务\n * mysql 锁 - 关键词：乐观锁、表级锁、行级锁、意向锁、mvcc、next-key 锁\n * mysql 索引 - 关键词：hash、b 树、聚簇索引、回表\n * mysql 性能优化\n * mysql 运维 🔨\n * mysql 配置 🔨\n * mysql 问题\n\n\n# 其他\n\n * postgresql 应用指南\n * h2 应用指南\n * sqlite 应用指南\n\n\n# 文档数据库\n\n\n# mongodb\n\n> mongodb 是一个基于文档的分布式数据库，由 c++ 语言编写。旨在为 web 应用提供可扩展的高性能数据存储解决方案。\n> \n> mongodb 是一个介于关系型数据库和非关系型数据库之间的产品。它是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。\n> \n> mongodb 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。\n\n * mongodb 应用指南\n * mongodb 的 crud 操作\n * mongodb 聚合操作\n * mongodb 事务\n * mongodb 建模\n * mongodb 建模示例\n * mongodb 索引\n * mongodb 复制\n * mongodb 分片\n * mongodb 运维\n\n\n# kv 数据库\n\n\n# redis\n\n\n\n * redis 面试总结 💯\n * redis 应用指南 ⚡ - 关键词：内存淘汰、事件、事务、管道、发布与订阅\n * redis 数据类型和应用 - 关键词：string、hash、list、set、zset、bitmap、hyperloglog、geo\n * redis 持久化 - 关键词：rdb、aof、save、bgsave、appendfsync\n * redis 复制 - 关键词：slaveof、sync、psync、replconf ack\n * redis 哨兵 - 关键词：sentinel、ping、info、raft\n * redis 集群 - 关键词：cluster meet、hash slot、moved、ask、slaveof no one、redis-trib\n * redis 实战 - 关键词：缓存、分布式锁、布隆过滤器\n * redis 运维 🔨 - 关键词：安装、命令、集群、客户端\n\n\n# 列式数据库\n\n\n# hbase\n\n> hbase 📚 因为常用于大数据项目，所以将其文档和源码整理在 bigdata-tutorial 项目中。\n\n * hbase 原理 ⚡\n * hbase 命令\n * hbase 应用\n * hbase 运维\n\n\n# 搜索引擎数据库\n\n\n# elasticsearch\n\n> elasticsearch 是一个基于 lucene 的搜索和数据分析工具，它提供了一个分布式服务。elasticsearch 是遵从 apache 开源条款的一款开源产品，是当前主流的企业级搜索引擎。\n\n * elasticsearch 面试总结 💯\n * elasticsearch 快速入门\n * elasticsearch 简介\n * elasticsearch 索引\n * elasticsearch 查询\n * elasticsearch 高亮\n * elasticsearch 排序\n * elasticsearch 聚合\n * elasticsearch 分析器\n * elasticsearch 性能优化\n * elasticsearch rest api\n * elasticsearch java api 之 high level rest client\n * elasticsearch 集群和分片\n * elasticsearch 运维\n\n\n# elastic\n\n * elastic 快速入门\n * elastic 技术栈之 filebeat\n * filebeat 运维\n * elastic 技术栈之 kibana\n * kibana 运维\n * elastic 技术栈之 logstash\n * logstash 运维\n\n\n# 资料 📚\n\n\n# 数据库综合资料\n\n * db-engines - 数据库流行度排名\n * 书籍\n   * 《数据密集型应用系统设计》 - 这可能是目前最好的分布式存储书籍，强力推荐【进阶】\n * 教程\n   * cmu 15445 数据库基础课程\n   * cmu 15721 数据库高级课程\n   * 检索技术核心 20 讲 - 极客教程【进阶】\n   * 后端存储实战课 - 极客教程【入门】：讲解存储在电商领域的种种应用和一些基本特性\n * 论文\n   * efficiency in the columbia database query optimizer\n   * how good are query optimizers, really?\n   * architecture of a database system\n   * data structures for databases\n * 文章\n   * data structures and algorithms for big databases\n\n\n# 关系型数据库资料\n\n * 综合资料\n   * 《数据库的索引设计与优化》\n   * 《sql 必知必会》 - sql 的基本概念和语法【入门】\n * oracle 资料\n   * 《oracle database 9i/10g/11g 编程艺术》\n\n# mysql 资料\n\n * 官方\n   * mysql 官网\n   * mysql 官方文档\n   * 官方 ppt\n     * how to analyze and tune mysql queries for better performance\n     * mysql performance tuning 101\n     * mysql performance schema & sys schema\n     * mysql performance: demystified tuning & best practices\n     * mysql security best practices\n     * mysql cluster deployment best practices\n     * mysql high availability with innodb cluster\n * 书籍\n   * 《高性能 mysql》 - 经典，适合 dba 或作为开发者的参考手册【进阶】\n   * 《mysql 技术内幕：innodb 存储引擎》\n   * 《mysql 必知必会》 - mysql 的基本概念和语法【入门】\n * 教程\n   * runoob.com mysql 教程 - 入门级 sql 教程\n   * mysql-tutorial\n * 文章\n   * mysql 索引背后的数据结构及算法原理\n   * some study on database storage internals\n   * sharding pinterest: how we scaled our mysql fleet\n   * guide to mysql high availability\n   * choosing mysql high availability solutions\n   * high availability with mariadb tx: the definitive guide\n   * mysql 相关经验\n     * booking.com: evolution of mysql system design ，booking.com 的 mysql 数据库使用的演化，其中有很多不错的经验分享，我相信也是很多公司会遇到的的问题。\n     * tracking the money - scaling financial reporting at airbnb ，airbnb 的数据库扩展的经验分享。\n     * why uber engineering switched from postgres to mysql ，无意比较两个数据库谁好谁不好，推荐这篇 uber 的长文，主要是想让你从中学习到一些经验和技术细节，这是一篇很不错的文章。\n   * mysql 集群复制\n     * monitoring delayed replication, with a focus on mysql\n     * mitigating replication lag and reducing read load with freno\n     * better parallel replication for mysql\n     * evaluating mysql parallel replication part 2: slave group commit\n     * evaluating mysql parallel replication part 3: benchmarks in production\n     * evaluating mysql parallel replication part 4: more benchmarks in production\n     * evaluating mysql parallel replication part 4, annex: under the hood\n   * mysql 数据分区\n     * stackoverflow: mysql sharding approaches?\n     * why you don’t want to shard\n     * [how to scale big data applications](https://www.percona.com/sites/default/files/presentations/how to scale big data applications.pdf)\n     * mysql sharding with proxysql\n   * 各公司的 mysql 数据分区经验分享\n     * mailchimp: using shards to accommodate millions of users\n     * uber: code migration in production: rewriting the sharding layer of uber’s schemaless datastore\n     * sharding & ids at instagram\n     * airbnb: how we partitioned airbnb’s main database in two weeks\n * 更多资源\n   * awesome-mysql - mysql 的资源列表\n\n\n# nosql 数据库综合\n\n * martin fowler 在 youtube 上分享的 nosql 介绍 introduction to nosql， 以及他参与编写的 nosql distilled - nosql 精粹，这本书才 100 多页，是本难得的关于 nosql 的书，很不错，非常易读。\n * nosql databases: a survey and decision guidance，这篇文章可以带你自上而下地从 cap 原理到开始了解 nosql 的种种技术，是一篇非常不错的文章。\n * distribution, data, deployment: software architecture convergence in big data systems，这是卡内基·梅隆大学的一篇讲分布式大数据系统的论文。其中主要讨论了在大数据时代下的软件工程中的一些关键点，也说到了 nosql 数据库。\n * no relation: the mixed blessings of non-relational databases，这篇论文虽然有点年代久远。但这篇论文是 hbase 的基础，你花上一点时间来读读，就可以了解到，对各种非关系型数据存储优缺点的一个很好的比较。\n * nosql data modeling techniques ，nosql 建模技术。这篇文章我曾经翻译在了 coolshell 上，标题为 nosql 数据建模技术，供你参考。\n   * mongodb - data modeling introduction ，虽然这是 mongodb 的数据建模介绍，但是其很多观点可以用于其它的 nosql 数据库。\n   * firebase - structure your database ，google 的 firebase 数据库使用 json 建模的一些最佳实践。\n * 因为 cap 原理，所以当你需要选择一个 nosql 数据库的时候，你应该看看这篇文档 visual guide to nosql systems。\n\n选 sql 还是 nosql，这里有两篇文章，值得你看看。\n\n * sql vs. nosql databases: what’s the difference?\n * salesforce: sql or nosql\n\n\n# 列式数据库资料\n\n# cassandra 资料\n\n * 沃尔玛实验室有两篇文章值得一读。\n   * avoid pitfalls in scaling cassandra cluster at walmart\n   * storing images in cassandra at walmart\n * yelp: how we scaled our ad analytics with apache cassandra ，yelp 的这篇博客也有一些相关的经验和教训。\n * discord: how discord stores billions of messages ，discord 公司分享的一个如何存储十亿级消息的技术文章。\n * cassandra at instagram ，instagram 的一个 ppt，其中介绍了 instagram 中是怎么使用 cassandra 的。\n * netflix: benchmarking cassandra scalability on aws - over a million writes per second ，netflix 公司在 aws 上给 cassandra 做的一个 benchmark。\n\n# hbase 资料\n\n * imgur notification: from mysql to hbase\n * pinterest: improving hbase backup efficiency\n * ibm : tuning hbase performance\n * hbase file locality in hdfs\n * apache hadoop goes realtime at facebook\n * storage infrastructure behind facebook messages: using hbase at scale\n * github: awesome hbase\n\n针对于 hbase 有两本书你可以考虑一下。\n\n * 首先，先推荐两本书，一本是偏实践的《hbase 实战》，另一本是偏大而全的手册型的《hbase 权威指南》。\n * 当然，你也可以看看官方的 the apache hbase™ reference guide\n * 另外两个列数据库：\n   * clickhouse - open source distributed column database at yandex\n   * scaling redshift without scaling costs at giphy\n\n\n# kv 数据库资料\n\n# redis 资料\n\n * 官网\n   * redis 官网\n   * redis github\n   * redis 官方文档中文版\n   * redis 命令参考\n * 书籍\n   * 《redis 实战》\n   * 《redis 设计与实现》\n * 源码\n   * 《redis 实战》配套 python 源码\n * 资源汇总\n   * awesome-redis\n * redis client\n   * spring-data-redis 官方文档\n   * redisson 官方文档(中文,略有滞后)\n   * redisson 官方文档(英文)\n   * crug | redisson pro vs. jedis: which is faster? 翻译\n   * redis 分布锁 redisson 性能测试\n * 文章\n   * learn redis the hard way (in production) at trivago\n   * twitter: how twitter uses redis to scale - 105tb ram, 39mm qps, 10,000+ instances\n   * slack: scaling slack’s job queue - robustly handling billions of tasks in milliseconds using kafka and redis\n   * github: moving persistent data out of redis at github\n   * instagram: storing hundreds of millions of simple key-value pairs in redis\n   * redis in chat architecture of twitch (from 27:22)\n   * deliveroo: optimizing session key storage in redis\n   * deliveroo: optimizing redis storage\n   * github: awesome redis\n\n\n# 文档数据库资料\n\n * couchbase ecosystem at linkedin\n * simpledb at zendesk\n * data points - what the heck are document databases?\n\n# mongodb 资料\n\n * 官方\n   * mongodb 官网\n   * mongodb github\n   * mongodb 官方免费教程\n * 教程\n   * mongodb 教程\n   * mongodb 高手课\n * 数据\n   * mongodb-json-files\n * 文章\n   * introduction to mongodb\n   * ebay: building mission-critical multi-data center applications with mongodb\n   * the aws and mongodb infrastructure of parse: lessons learned\n   * migrating mountains of mongo data\n * 更多资源\n   * github: awesome mongodb\n\n\n# 搜索引擎数据库资料\n\n# elasticsearch\n\n * 官方\n   * elasticsearch 官网\n   * elasticsearch github\n   * elasticsearch 官方文档\n   * elasticsearch: the definitive guide - elasticsearch 官方学习资料\n * 书籍\n   * 《elasticsearch 实战》\n * 教程\n   * elk stack 权威指南\n   * elasticsearch 教程\n * 文章\n   * elasticsearch+logstash+kibana 教程\n   * elk（elasticsearch、logstash、kibana）安装和配置\n   * 性能调优相关的工程实践\n     * elasticsearch performance tuning practice at ebay\n     * elasticsearch at kickstarter\n     * 9 tips on elasticsearch configuration for high performance\n     * elasticsearch in production - deployment best practices\n * 更多资源\n   * github: awesome elasticsearch\n\n\n# 图数据库\n\n * 首先是 ibm devloperworks 上的两个简介性的 ppt。\n   * intro to graph databases, part 1, graph databases and the crud operations\n   * intro to graph databases, part 2, building a recommendation engine with a graph database\n * 然后是一本免费的电子书《graph database》。\n * 接下来是一些图数据库的介绍文章。\n   * handling billions of edges in a graph database\n   * neo4j case studies with walmart, ebay, airbnb, nasa, etc\n   * flockdb: distributed graph database for storing adjacency lists at twitter\n   * janusgraph: scalable graph database backed by google, ibm and hortonworks\n   * amazon neptune\n\n\n# 时序数据库\n\n * what is time-series data & why we need a time-series database\n * time series data: why and how to use a relational database instead of nosql\n * beringei: high-performance time series storage engine @facebook\n * introducing atlas: netflix’s primary telemetry platform @netflix\n * building a scalable time series database on postgresql\n * scaling time series data storage - part i @netflix\n * design of a cost efficient time series store for big data\n * github: awesome time-series database\n\n\n# 传送 🚪\n\n◾ 💧 钝悟的 it 知识图谱 ◾ 🎯 钝悟的博客 ◾",charsets:{cjk:!0},lastUpdated:"2023/02/10, 11:43:21",lastUpdatedTimestamp:1676000601e3}],themeConfig:{nav:[{text:"数据库综合",link:"/12.数据库/01.数据库综合/"},{text:"数据库中间件",link:"/12.数据库/02.数据库中间件/"},{text:"关系型数据库",link:"/12.数据库/03.关系型数据库/",items:[{text:"综合",link:"/12.数据库/03.关系型数据库/01.综合/"},{text:"Mysql",link:"/12.数据库/03.关系型数据库/02.Mysql/"},{text:"其他",link:"/12.数据库/03.关系型数据库/99.其他/"}]},{text:"文档数据库",items:[{text:"MongoDB",link:"/12.数据库/04.文档数据库/01.MongoDB/"}]},{text:"KV数据库",items:[{text:"Redis",link:"/12.数据库/05.KV数据库/01.Redis/"}]},{text:"搜索引擎数据库",items:[{text:"Elasticsearch",link:"/12.数据库/07.搜索引擎数据库/01.Elasticsearch/"},{text:"Elastic技术栈",link:"/12.数据库/07.搜索引擎数据库/02.Elastic/"}]}],sidebarDepth:2,logo:"https://raw.githubusercontent.com/dunwu/images/dev/common/dunwu-logo.png",repo:"dunwu/db-tutorial",searchMaxSuggestions:10,lastUpdated:"上次更新",docsDir:"docs",editLinks:!0,editLinkText:"📝 帮助改善此页面！",sidebar:{"/12.数据库/":[{title:"数据库综合",collapsable:!1,children:[["01.数据库综合/01.Nosql技术选型.md","Nosql技术选型","/pages/0e1012/"],["01.数据库综合/02.数据结构与数据库索引.md","数据结构与数据库索引","/pages/d7cd88/"]]},{title:"数据库中间件",collapsable:!1,children:[{title:"Shardingsphere",collapsable:!1,children:[["02.数据库中间件/01.Shardingsphere/01.ShardingSphere简介.md","ShardingSphere 简介","/pages/5ed2a2/"],["02.数据库中间件/01.Shardingsphere/02.ShardingSphereJdbc.md","ShardingSphere Jdbc","/pages/8448de/"]]},["02.数据库中间件/02.Flyway.md","版本管理中间件 Flyway","/pages/e2648c/"]]},{title:"关系型数据库",collapsable:!1,children:[{title:"综合",collapsable:!1,children:[["03.关系型数据库/01.综合/01.关系型数据库面试.md","关系型数据库面试","/pages/9bb28f/"],["03.关系型数据库/01.综合/02.SqlCheatSheet.md","sql-cheat-sheet","/pages/b71c9e/"],["03.关系型数据库/01.综合/03.扩展SQL.md","扩展 SQL","/pages/55e9a7/"]]},{title:"Mysql",collapsable:!1,children:[["03.关系型数据库/02.Mysql/01.Mysql应用指南.md","Mysql 应用指南","/pages/5fe0f3/"],["03.关系型数据库/02.Mysql/02.MySQL工作流.md","MySQL 工作流","/pages/8262aa/"],["03.关系型数据库/02.Mysql/03.Mysql事务.md","Mysql 事务","/pages/00b04d/"],["03.关系型数据库/02.Mysql/04.Mysql锁.md","Mysql 锁","/pages/f1f151/"],["03.关系型数据库/02.Mysql/05.Mysql索引.md","Mysql 索引","/pages/fcb19c/"],["03.关系型数据库/02.Mysql/06.Mysql性能优化.md","Mysql 性能优化","/pages/396816/"],["03.关系型数据库/02.Mysql/20.Mysql运维.md","Mysql 运维","/pages/e33b92/"],["03.关系型数据库/02.Mysql/21.Mysql配置.md","Mysql 配置","/pages/5da42d/"],["03.关系型数据库/02.Mysql/99.Mysql常见问题.md","Mysql 常见问题","/pages/7b0caf/"]]},{title:"其他",collapsable:!1,children:[["03.关系型数据库/99.其他/01.PostgreSQL.md","PostgreSQL 应用指南","/pages/52609d/"],["03.关系型数据库/99.其他/02.H2.md","H2 应用指南","/pages/f27c0c/"],["03.关系型数据库/99.其他/03.Sqlite.md","sqlite","/pages/bdcd7e/"]]}]},{title:"文档数据库",collapsable:!1,children:[{title:"MongoDB",collapsable:!1,children:[["04.文档数据库/01.MongoDB/01.MongoDB应用指南.md","MongoDB 应用指南","/pages/3288f3/"],["04.文档数据库/01.MongoDB/02.MongoDB的CRUD操作.md","MongoDB 的 CRUD 操作","/pages/7efbac/"],["04.文档数据库/01.MongoDB/03.MongoDB的聚合操作.md","MongoDB 的聚合操作","/pages/75daa5/"],["04.文档数据库/01.MongoDB/04.MongoDB事务.md","MongoDB 事务","/pages/4574fe/"],["04.文档数据库/01.MongoDB/05.MongoDB建模.md","MongoDB 建模","/pages/562f99/"],["04.文档数据库/01.MongoDB/06.MongoDB建模示例.md","MongoDB 建模示例","/pages/88c7d3/"],["04.文档数据库/01.MongoDB/07.MongoDB索引.md","MongoDB 索引","/pages/10c674/"],["04.文档数据库/01.MongoDB/08.MongoDB复制.md","MongoDB 复制","/pages/505407/"],["04.文档数据库/01.MongoDB/09.MongoDB分片.md","MongoDB 分片","/pages/ad08f5/"],["04.文档数据库/01.MongoDB/20.MongoDB运维.md","MongoDB 运维","/pages/5e3c30/"]]}]},{title:"KV数据库",collapsable:!1,children:[{title:"Redis",collapsable:!1,children:[["05.KV数据库/01.Redis/01.Redis面试总结.md","Redis 面试总结","/pages/451b73/"],["05.KV数据库/01.Redis/02.Redis应用指南.md","Redis 应用指南","/pages/94e9d6/"],["05.KV数据库/01.Redis/03.Redis数据类型和应用.md","Redis 数据类型和应用","/pages/ed757c/"],["05.KV数据库/01.Redis/04.Redis持久化.md","Redis 持久化","/pages/4de901/"],["05.KV数据库/01.Redis/05.Redis复制.md","Redis 复制","/pages/379cd8/"],["05.KV数据库/01.Redis/06.Redis哨兵.md","Redis 哨兵","/pages/615afe/"],["05.KV数据库/01.Redis/07.Redis集群.md","Redis 集群","/pages/77dfbe/"],["05.KV数据库/01.Redis/08.Redis实战.md","Redis 实战","/pages/1fc9c4/"],["05.KV数据库/01.Redis/20.Redis运维.md","Redis 运维","/pages/537098/"]]}]},{title:"列式数据库",collapsable:!1,children:[["06.列式数据库/01.Hbase.md","Hbase","/pages/7ab03c/"],["06.列式数据库/02.Cassandra.md","Cassandra","/pages/ca3ca5/"]]},{title:"搜索引擎数据库",collapsable:!1,children:[{title:"Elasticsearch",collapsable:!1,children:[["07.搜索引擎数据库/01.Elasticsearch/01.Elasticsearch面试总结.md","Elasticsearch 面试总结","/pages/0cb563/"],["07.搜索引擎数据库/01.Elasticsearch/02.Elasticsearch快速入门.md","Elasticsearch 快速入门","/pages/98c3a5/"],["07.搜索引擎数据库/01.Elasticsearch/03.Elasticsearch简介.md","Elasticsearch 简介","/pages/0fb506/"],["07.搜索引擎数据库/01.Elasticsearch/04.Elasticsearch索引.md","Elasticsearch 索引","/pages/293175/"],["07.搜索引擎数据库/01.Elasticsearch/05.Elasticsearch查询.md","Elasticsearch 查询","/pages/83bd15/"],["07.搜索引擎数据库/01.Elasticsearch/06.Elasticsearch高亮.md","Elasticsearch 高亮搜索及显示","/pages/e1b769/"],["07.搜索引擎数据库/01.Elasticsearch/07.Elasticsearch排序.md","Elasticsearch 排序","/pages/24baff/"],["07.搜索引擎数据库/01.Elasticsearch/08.Elasticsearch聚合.md","Elasticsearch 聚合","/pages/f89f66/"],["07.搜索引擎数据库/01.Elasticsearch/09.Elasticsearch分析器.md","Elasticsearch 分析器","/pages/a5a001/"],["07.搜索引擎数据库/01.Elasticsearch/10.Elasticsearch性能优化.md","Elasticsearch 性能优化","/pages/2d95ce/"],["07.搜索引擎数据库/01.Elasticsearch/11.ElasticsearchRestApi.md","Elasticsearch Rest API","/pages/4b1907/"],["07.搜索引擎数据库/01.Elasticsearch/12.ElasticsearchHighLevelRestJavaApi.md","ElasticSearch Java API 之 High Level REST Client","/pages/201e43/"],["07.搜索引擎数据库/01.Elasticsearch/13.Elasticsearch集群和分片.md","Elasticsearch 集群和分片","/pages/9a2546/"],["07.搜索引擎数据库/01.Elasticsearch/20.Elasticsearch运维.md","Elasticsearch 运维","/pages/fdaf15/"]]},{title:"Elastic",collapsable:!1,children:[["07.搜索引擎数据库/02.Elastic/01.Elastic快速入门.md","Elastic 快速入门","/pages/553160/"],["07.搜索引擎数据库/02.Elastic/02.Elastic技术栈之Filebeat.md","Elastic 技术栈之 Filebeat","/pages/b7f079/"],["07.搜索引擎数据库/02.Elastic/03.Filebeat运维.md","Filebeat 运维","/pages/7c067f/"],["07.搜索引擎数据库/02.Elastic/04.Elastic技术栈之Kibana.md","Elastic 技术栈之 Kibana","/pages/002159/"],["07.搜索引擎数据库/02.Elastic/05.Kibana运维.md","Kibana 运维","/pages/fc47af/"],["07.搜索引擎数据库/02.Elastic/06.Elastic技术栈之Logstash.md","Elastic 技术栈之 Logstash","/pages/55ce99/"],["07.搜索引擎数据库/02.Elastic/07.Logstash运维.md","Logstash 运维","/pages/92df30/"]]}]}],catalogue:{}},updateBar:{showToArticle:!0},category:!0,tag:!0,archive:!0,author:{name:"dunwu",href:"https://github.com/dunwu"},social:{icons:[{iconClass:"icon-youjian",title:"发邮件",link:"mailto:forbreak@163.com"},{iconClass:"icon-github",title:"GitHub",link:"https://github.com/dunwu"}]},footer:{createYear:2019,copyrightInfo:"钝悟（dunwu） | CC-BY-SA-4.0"},htmlModules:{pageB:'\n  <div class="wwads-cn wwads-horizontal pageB" data-id="136" style="width:100%;max-height:80px;min-height:auto;"></div>\n  <style>\n    .pageB img{width:80px!important;}\n    .wwads-horizontal .wwads-text, .wwads-content .wwads-text{line-height:1;}\n  </style>\n  ',windowRB:'\n    <div class="wwads-cn wwads-vertical windowRB" data-id="136" style="max-width:160px;\n    min-width: auto;min-height:auto;"></div>\n    <style>\n      .windowRB{ padding: 0;}\n      .windowRB .wwads-img{margin-top: 10px;}\n      .windowRB .wwads-content{margin: 0 10px 10px 10px;}\n      .custom-html-window-rb .close-but{\n        display: none;\n      }\n    </style>\n  '}}};var El=t(96),xl=t(97),Tl=t(11);var kl={computed:{$filterPosts(){return this.$site.pages.filter(n=>{const{frontmatter:{pageComponent:e,article:t,home:a}}=n;return!(e||!1===t||!0===a)})},$sortPosts(){return(n=this.$filterPosts).sort((n,e)=>{const t=n.frontmatter.sticky,a=e.frontmatter.sticky;return t&&a?t==a?Object(Tl.a)(n,e):t-a:t&&!a?-1:!t&&a?1:Object(Tl.a)(n,e)}),n;var n},$sortPostsByDate(){return(n=this.$filterPosts).sort((n,e)=>Object(Tl.a)(n,e)),n;var n},$groupPosts(){return function(n){const e={},t={};for(let a=0,s=n.length;a<s;a++){const{frontmatter:{categories:s,tags:i}}=n[a];"array"===Object(Tl.n)(s)&&s.forEach(t=>{t&&(e[t]||(e[t]=[]),e[t].push(n[a]))}),"array"===Object(Tl.n)(i)&&i.forEach(e=>{e&&(t[e]||(t[e]=[]),t[e].push(n[a]))})}return{categories:e,tags:t}}(this.$sortPosts)},$categoriesAndTags(){return function(n){const e=[],t=[];for(let t in n.categories)e.push({key:t,length:n.categories[t].length});for(let e in n.tags)t.push({key:e,length:n.tags[e].length});return{categories:e,tags:t}}(this.$groupPosts)}}};Gt.component(El.default),Gt.component(xl.default);function Il(n){return n.toString().padStart(2,"0")}t(244);Gt.component("Badge",()=>Promise.all([t.e(0),t.e(3)]).then(t.bind(null,417))),Gt.component("CodeBlock",()=>Promise.resolve().then(t.bind(null,96))),Gt.component("CodeGroup",()=>Promise.resolve().then(t.bind(null,97)));t(245),t(246);var Sl=t(95),ql=t.n(Sl),wl=t(27);let Rl,Al;var zl;"valine"===(zl="gitalk")?t.e(85).then(t.t.bind(null,334,7)).then(n=>Al=n.default):"gitalk"===zl&&Promise.all([t.e(0),t.e(84)]).then(t.t.bind(null,335,7)).then(()=>t.e(83).then(t.t.bind(null,336,7))).then(n=>Rl=n.default);function Dl(n,e){const t={};return Reflect.ownKeys(n).forEach(a=>{if("string"==typeof n[a])try{t[a]=ql.a.render(n[a],e)}catch(e){console.warn(`Comment config option error at key named "${a}"`),console.warn("More info: "+e.message),t[a]=n[a]}else t[a]=n[a]}),t}console.log(`How to use "gitalk" in ${wl.name}@v${wl.version}:`,wl.homepage);const Ll={gitalk:{render(n,e){const t=document.createElement("div");t.id=e;document.querySelector("main.page").appendChild(t);new Rl(Dl({clientID:"7dd8c87a20cff437d2ed",clientSecret:"4e28d81a9a0280796b2b45ce2944424c6f2c1531",repo:"db-tutorial",owner:"dunwu",admin:["dunwu"],pagerDirection:"last",id:"<%- (frontmatter.permalink || frontmatter.to.path).slice(-16) %>",title:"「评论」<%- frontmatter.title %>",labels:["Gitalk","Comment"],body:"页面：<%- window.location.origin + (frontmatter.to.path || window.location.pathname) %>"},{frontmatter:n})).render(e)},clear(n){const e=document.querySelector("#"+n);return e&&e.remove(),!0}},valine:{render(n,e){const t=document.createElement("div");t.id=e;document.querySelector("main.page").appendChild(t),new Al({...Dl({clientID:"7dd8c87a20cff437d2ed",clientSecret:"4e28d81a9a0280796b2b45ce2944424c6f2c1531",repo:"db-tutorial",owner:"dunwu",admin:["dunwu"],pagerDirection:"last",id:"<%- (frontmatter.permalink || frontmatter.to.path).slice(-16) %>",title:"「评论」<%- frontmatter.title %>",labels:["Gitalk","Comment"],body:"页面：<%- window.location.origin + (frontmatter.to.path || window.location.pathname) %>"},{frontmatter:n}),el:"#"+e})},clear(n){const e=document.querySelector("#"+n);return e&&e.remove(),!0}}},Bl="vuepress-plugin-comment";let Ml=null;function Ol(n){return Ll.gitalk.clear(Bl)}function Cl(n){return!1!==n.comment&&!1!==n.comments}function Nl(n){clearTimeout(Ml);if(document.querySelector("main.page"))return Ll.gitalk.render(n,Bl);Ml=setTimeout(()=>Nl(n),200)}var Pl={mounted(){Ml=setTimeout(()=>{const n={to:{},from:{},...this.$frontmatter};Ol()&&Cl(n)&&Nl(n)},1e3),this.$router.afterEach((n,e)=>{if(n&&e&&n.path===e.path)return;const t={to:n,from:e,...this.$frontmatter};Ol()&&Cl(t)&&Nl(t)})}},jl=Object(bl.a)(Pl,(function(){return(0,this._self._c)("div")}),[],!1,null,null,null).exports,Ul=[({Vue:n,options:e,router:t,siteData:a})=>{},({Vue:n,options:e,router:t,siteData:a})=>{a.pages.map(n=>{const{frontmatter:{date:e,author:t}}=n;"string"==typeof e&&"Z"===e.charAt(e.length-1)&&(n.frontmatter.date=function(n){n instanceof Date||(n=new Date(n));return`${n.getUTCFullYear()}-${Il(n.getUTCMonth()+1)}-${Il(n.getUTCDate())} ${Il(n.getUTCHours())}:${Il(n.getUTCMinutes())}:${Il(n.getUTCSeconds())}`}(e)),t?n.author=t:a.themeConfig.author&&(n.author=a.themeConfig.author)}),n.mixin(kl)},{},({Vue:n})=>{n.mixin({computed:{$dataBlock(){return this.$options.__data__block__}}})},{},{},()=>{"undefined"!=typeof window&&function(n,e,t){function a(n){var t=e.createElement("div");t.className="heart",s.push({el:t,x:n.clientX-5,y:n.clientY-5,scale:1,alpha:1,color:"#11a8cd"}),e.body.appendChild(t)}var s=[];n.requestAnimationFrame=n.requestAnimationFrame||n.webkitRequestAnimationFrame||n.mozRequestAnimationFrame||n.oRequestAnimationFrame||n.msRequestAnimationFrame||function(n){setTimeout(n,1e3/60)},function(n){var t=e.createElement("style");t.type="text/css";try{t.appendChild(e.createTextNode(n))}catch(e){t.styleSheet.cssText=n}e.getElementsByTagName("head")[0].appendChild(t)}(".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}"),function(){var e="function"==typeof n.onclick&&n.onclick;n.onclick=function(n){let t=!0;n.path&&n.path.forEach(n=>{1===n.nodeType&&"string"==typeof n.className&&n.className.indexOf("theme-vdoing-content")>-1&&(t=!1)}),t&&(e&&e(),a(n))}}(),function n(){for(var t=0;t<s.length;t++)s[t].alpha<=0?(e.body.removeChild(s[t].el),s.splice(t,1)):(s[t].y--,s[t].scale+=.004,s[t].alpha-=.013,s[t].el.style.cssText="left:"+s[t].x+"px;top:"+s[t].y+"px;opacity:"+s[t].alpha+";transform:scale("+s[t].scale+","+s[t].scale+") rotate(45deg);background:"+s[t].color+";z-index:99999");requestAnimationFrame(n)}()}(window,document)},({Vue:n})=>{n.component("Comment",jl)}],Fl=["Comment"];class $l extends class{constructor(){this.store=new Gt({data:{state:{}}})}$get(n){return this.store.state[n]}$set(n,e){Gt.set(this.store.state,n,e)}$emit(...n){this.store.$emit(...n)}$on(...n){this.store.$on(...n)}}{}Object.assign($l.prototype,{getPageAsyncComponent:io,getLayoutAsyncComponent:ro,getAsyncComponent:oo,getVueComponent:lo});var Ql={install(n){const e=new $l;n.$vuepress=e,n.prototype.$vuepress=e}};function Hl(n,e){const t=e.toLowerCase();return n.options.routes.some(n=>n.path.toLowerCase()===t)}var Gl={props:{pageKey:String,slotKey:{type:String,default:"default"}},render(n){const e=this.pageKey||this.$parent.$page.key;return uo("pageKey",e),Gt.component(e)||Gt.component(e,io(e)),Gt.component(e)?n(e):n("")}},Vl={functional:!0,props:{slotKey:String,required:!0},render:(n,{props:e,slots:t})=>n("div",{class:["content__"+e.slotKey]},t()[e.slotKey])},Kl={computed:{openInNewWindowTitle(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},Wl=(t(252),t(253),Object(bl.a)(Kl,(function(){var n=this._self._c;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports),Jl={functional:!0,render(n,{parent:e,children:t}){if(e._isMounted)return t;e.$once("hook:mounted",()=>{e.$forceUpdate()})}};Gt.config.productionTip=!1,Gt.use($r),Gt.use(Ql),Gt.mixin(function(n,e,t=Gt){!function(n){n.locales&&Object.keys(n.locales).forEach(e=>{n.locales[e].path=e});Object.freeze(n)}(e),t.$vuepress.$set("siteData",e);const a=new(n(t.$vuepress.$get("siteData"))),s=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(a)),i={};return Object.keys(s).reduce((n,e)=>(e.startsWith("$")&&(n[e]=s[e].get),n),i),{computed:i}}(n=>class{setPage(n){this.__page=n}get $site(){return n}get $themeConfig(){return this.$site.themeConfig}get $frontmatter(){return this.$page.frontmatter}get $localeConfig(){const{locales:n={}}=this.$site;let e,t;for(const a in n)"/"===a?t=n[a]:0===this.$page.path.indexOf(a)&&(e=n[a]);return e||t||{}}get $siteTitle(){return this.$localeConfig.title||this.$site.title||""}get $canonicalUrl(){const{canonicalUrl:n}=this.$page.frontmatter;return"string"==typeof n&&n}get $title(){const n=this.$page,{metaTitle:e}=this.$page.frontmatter;if("string"==typeof e)return e;const t=this.$siteTitle,a=n.frontmatter.home?null:n.frontmatter.title||n.title;return t?a?a+" | "+t:t:a||"VuePress"}get $description(){const n=function(n){if(n){const e=n.filter(n=>"description"===n.name)[0];if(e)return e.content}}(this.$page.frontmatter.meta);return n||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}get $lang(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}get $localePath(){return this.$localeConfig.path||"/"}get $themeLocaleConfig(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}get $page(){return this.__page?this.__page:function(n,e){for(let t=0;t<n.length;t++){const a=n[t];if(a.path.toLowerCase()===e.toLowerCase())return a}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}},_l)),Gt.component("Content",Gl),Gt.component("ContentSlotsDistributor",Vl),Gt.component("OutboundLink",Wl),Gt.component("ClientOnly",Jl),Gt.component("Layout",ro("Layout")),Gt.component("NotFound",ro("NotFound")),Gt.prototype.$withBase=function(n){const e=this.$site.base;return"/"===n.charAt(0)?e+n.slice(1):n},window.__VUEPRESS__={version:"1.9.2",hash:"8178829"},async function(n){const e="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:_l.routerBase||_l.base,t=new $r({base:e,mode:"history",fallback:!1,routes:vl,scrollBehavior:(n,e,t)=>t||(n.hash?!Gt.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(n.hash)}:{x:0,y:0})});!function(n){n.beforeEach((e,t,a)=>{if(Hl(n,e.path))a();else if(/(\/|\.html)$/.test(e.path))if(/\/$/.test(e.path)){const t=e.path.replace(/\/$/,"")+".html";Hl(n,t)?a(t):a()}else a();else{const t=e.path+"/",s=e.path+".html";Hl(n,s)?a(s):Hl(n,t)?a(t):a()}})}(t);const a={};try{await Promise.all(Ul.filter(n=>"function"==typeof n).map(e=>e({Vue:Gt,options:a,router:t,siteData:_l,isServer:n})))}catch(n){console.error(n)}return{app:new Gt(Object.assign(a,{router:t,render:n=>n("div",{attrs:{id:"app"}},[n("RouterView",{ref:"layout"}),n("div",{class:"global-ui"},Fl.map(e=>n(e)))])})),router:t}}(!1).then(({app:n,router:e})=>{e.onReady(()=>{n.$mount("#app")})})}]);